![Python](https://img.shields.io/badge/python-3.10%2B-blue.svg)
![License](https://img.shields.io/badge/license-Apache%202.0-green.svg)


# Sumeh â€” Unified Data Quality Framework

Sumeh is a unified dataâ€‘quality validation framework supporting multiple backends (PySpark, Dask, BigQuery) with centralized rule configuration.

## ğŸš€ Installation

```bash
# Using pip
pip install sumeh

# Or with conda-forge
conda install -c conda-forge sumeh
```

**Prerequisites:**  
- PythonÂ 3.10+  
- One or more of: `pyspark`, `dask[dataframe]`, `google-cloud-bigquery`, `cuallee`

## ğŸ” Core API

- **`report(df, rules, name="Quality Check")`**  
  Apply your validation rules over any DataFrame (Pandas, Spark, Dask, or SQL via BigQuery).  
- **`validate(df, rules)`** *(per-engine)*  
  Returns a DataFrame with a `dq_status` column listing violations.  
- **`summarize(qc_df, rules, total_rows)`** *(per-engine)*  
  Consolidates violations into a summary report.

## âš™ï¸ Engines

Each engine implements the `validate()` + `summarize()` pair:

| Engine                | Module                                  | Status          |
|-----------------------|-----------------------------------------|-----------------|
| PySpark               | `sumeh.engine.pyspark_engine`           | âœ… Fully tested |
| Dask                  | `sumeh.engine.dask_engine`              | âœ… Fully tested |
| BigQuery (SQL)        | `sumeh.engine.bigquery_engine` *(stub)* | ğŸ”§ Work in progress |

_Planned:_ native Pandas engine.

## ğŸ— Configuration Sources

Load rules from CSV, S3, MySQL, Postgres, BigQuery table, or AWS Glue:

```python
from sumeh.services.config import (
    get_config_from_csv,
    get_config_from_s3,
    get_config_from_mysql,
    get_config_from_postgresql,
    get_config_from_bigquery,
    get_config_from_glue_data_catalog,
)

rules = get_config_from_csv("rules.csv", delimiter=";")
```

## ğŸƒâ€â™‚ï¸ Typical Workflow

```python
from sumeh import report
from sumeh.engine.dask_engine import validate, summarize
import dask.dataframe as dd

# 1) Load data
ddf = dd.read_csv("data.csv", assume_missing=True)

# 2) Cast types
ddf["join_date"] = dd.to_datetime(ddf["join_date"])

# 3) Run per-engine validation
qc_ddf = validate(ddf, rules)

# 4) Generate summary
total = ddf.shape[0].compute()
report = summarize(qc_ddf, rules, total).compute()
print(report)
```

Or simply:

```python
from sumeh import report

report = report(df, rules, name="My Check")
```

## ğŸ“‹ Rule Definition Example

```json
{
  "field": "customer_id",
  "check_type": "is_complete",
  "threshold": 0.99,
  "value": null,
  "execute": true
}
```

## ğŸ“‚ Project Layout

```
sumeh/
â”œâ”€â”€ __init__.py          # exposes report & version
â”œâ”€â”€ core.py              # report implementation
â”œâ”€â”€ engine/
â”‚   â”œâ”€â”€ pyspark_engine.py
â”‚   â”œâ”€â”€ dask_engine.py
â”‚   â””â”€â”€ bigquery_engine.py  # SQLâ€‘based
â””â”€â”€ services/
    â”œâ”€â”€ config.py        # rule loaders
    â””â”€â”€ utils.py         # internal helpers
tests/
â”œâ”€â”€ data/                # sample inputs
â””â”€â”€ test_*.py            # unit tests
```

## ğŸ“ˆ Roadmap

- [ ] Native Pandas engine  
- [ ] Complete BigQuery engine  
- [ ] Read the Docs documentation  

## ğŸ¤ Contributing

1. Fork & create a feature branch.  
2. Implement new checks or engines, following existing signatures.  
3. Add tests under `tests/`.  
4. Open a PR and ensure CI passes.

## ğŸ“œ License

Licensed under the [ApacheÂ LicenseÂ 2.0](LICENSE).  
```