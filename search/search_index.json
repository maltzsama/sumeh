{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"home","text":""},{"location":"#sumeh-dq","title":"Sumeh DQ","text":"<p>Sumeh is a unified data quality validation framework supporting multiple backends (PySpark, Dask, Polars, DuckDB) with centralized rule configuration.</p>"},{"location":"#installation","title":"\ud83d\ude80 Installation","text":"<pre><code># Using pip\npip install sumeh\n\n# Or with conda-forge\nconda install -c conda-forge sumeh\n</code></pre> <p>Prerequisites: - Python 3.10+ - One or more of: <code>pyspark</code>, <code>dask[dataframe]</code>, <code>polars</code>, <code>duckdb</code>, <code>cuallee</code></p>"},{"location":"#core-api","title":"\ud83d\udd0d Core API","text":"<ul> <li><code>report(df, rules, name=\"Quality Check\")</code>   Apply your validation rules over any DataFrame (Pandas, Spark, Dask, Polars, or DuckDB).  </li> <li><code>validate(df, rules)</code> (per-engine)   Returns a DataFrame with a <code>dq_status</code> column listing violations.  </li> <li><code>summarize(qc_df, rules, total_rows)</code> (per-engine)   Consolidates violations into a summary report.</li> </ul>"},{"location":"#supported-engines","title":"\u2699\ufe0f Supported Engines","text":"<p>Each engine implements the <code>validate()</code> + <code>summarize()</code> pair:</p> Engine Module Status PySpark <code>sumeh.engine.pyspark_engine</code> \u2705 Fully implemented Dask <code>sumeh.engine.dask_engine</code> \u2705 Fully implemented Polars <code>sumeh.engine.polars_engine</code> \u2705 Fully implemented DuckDB <code>sumeh.engine.duckdb_engine</code> \u2705 Fully implemented Pandas <code>sumeh.engine.pandas_engine</code> \u2705 Fully implemented BigQuery (SQL) <code>sumeh.engine.bigquery_engine</code> \ud83d\udd27 Stub implementation"},{"location":"#configuration-sources","title":"\ud83c\udfd7 Configuration Sources","text":"<p>Load rules from CSV, S3, MySQL, Postgres, BigQuery table, or AWS Glue:</p> <pre><code>from sumeh.services.config import (\n    get_config_from_csv,\n    get_config_from_s3,\n    get_config_from_mysql,\n    get_config_from_postgresql,\n    get_config_from_bigquery,\n    get_config_from_glue_data_catalog,\n)\n\nrules = get_config_from_csv(\"rules.csv\", delimiter=\";\")\n</code></pre>"},{"location":"#typical-workflow","title":"\ud83c\udfc3\u200d\u2642\ufe0f Typical Workflow","text":"<pre><code>from sumeh import report\nfrom sumeh.engine.polars_engine import validate, summarize\nimport polars as pl\n\n# 1) Load data\ndf = pl.read_csv(\"data.csv\")\n\n# 2) Run validation\nresult, result_raw = validate(df, rules)\n\n# 3) Generate summary\ntotal = df.height\nreport = summarize(result_raw, rules, total)\nprint(report)\n</code></pre> <p>Or simply:</p> <pre><code>from sumeh import report\n\nreport = report(df, rules, name=\"My Check\")\n</code></pre>"},{"location":"#rule-definition-example","title":"\ud83d\udccb Rule Definition Example","text":"<pre><code>{\n  \"field\": \"customer_id\",\n  \"check_type\": \"is_complete\",\n  \"threshold\": 0.99,\n  \"value\": null,\n  \"execute\": true\n}\n</code></pre>"},{"location":"#supported-validation-rules","title":"Supported Validation Rules","text":""},{"location":"#numeric-checks","title":"Numeric checks","text":"Test Description is_in_millions Retains rows where the column value is less than 1,000,000 (fails the \"in millions\" criteria). is_in_billions Retains rows where the column value is less than 1,000,000,000 (fails the \"in billions\" criteria)."},{"location":"#completeness-uniqueness","title":"Completeness &amp; Uniqueness","text":"Test Description is_complete Filters rows where the column value is null. are_complete Filters rows where any of the specified columns are null. is_unique Identifies rows with duplicate values in the specified column. are_unique Identifies rows with duplicate combinations of the specified columns. is_primary_key Alias for <code>is_unique</code> (checks uniqueness of a single column). is_composite_key Alias for <code>are_unique</code> (checks combined uniqueness of multiple columns)."},{"location":"#comparison-range","title":"Comparison &amp; Range","text":"Test Description is_equal Filters rows where the column is not equal to the provided value (null-safe). is_equal_than Alias for <code>is_equal</code>. is_between Filters rows where the column value is outside the numeric range <code>[min, max]</code>. is_greater_than Filters rows where the column value is \u2264 the threshold (fails \"greater than\"). is_greater_or_equal_than Filters rows where the column value is &lt; the threshold (fails \"greater or equal\"). is_less_than Filters rows where the column value is \u2265 the threshold (fails \"less than\"). is_less_or_equal_than Filters rows where the column value is &gt; the threshold (fails \"less or equal\"). is_positive Filters rows where the column value is &lt; 0 (fails \"positive\"). is_negative Filters rows where the column value is \u2265 0 (fails \"negative\")."},{"location":"#membership-pattern","title":"Membership &amp; Pattern","text":"Test Description is_contained_in Filters rows where the column value is not in the provided list. not_contained_in Filters rows where the column value is in the provided list. has_pattern Filters rows where the column value does not match the specified regex. is_legit Filters rows where the column value is null or contains whitespace (i.e., not <code>\\S+</code>)."},{"location":"#aggregate-checks","title":"Aggregate checks","text":"Test Description has_min Returns all rows if the column's minimum value causes failure (value &lt; threshold); otherwise returns empty. has_max Returns all rows if the column's maximum value causes failure (value &gt; threshold); otherwise returns empty. has_sum Returns all rows if the column's sum causes failure (sum &gt; threshold); otherwise returns empty. has_mean Returns all rows if the column's mean causes failure (mean &gt; threshold); otherwise returns empty. has_std Returns all rows if the column's standard deviation causes failure (std &gt; threshold); otherwise returns empty. has_cardinality Returns all rows if the number of distinct values causes failure (count &gt; threshold); otherwise returns empty. has_infogain Same logic as <code>has_cardinality</code> (proxy for information gain). has_entropy Same logic as <code>has_cardinality</code> (proxy for entropy)."},{"location":"#sql-schema","title":"SQL &amp; Schema","text":"Test Description satisfies Filters rows where the SQL expression (based on <code>rule[\"value\"]</code>) is not satisfied. validate_schema Compares the DataFrame's actual schema against the expected one and returns a match flag + error list. validate Executes a list of named rules and returns two DataFrames: one with aggregated status and one with raw violations."},{"location":"#date-related-checks","title":"Date-related checks","text":"Test Description is_t_minus_1 Retains rows where the date in the column is not equal to yesterday (T\u20131). is_t_minus_2 Retains rows where the date in the column is not equal to two days ago (T\u20132). is_t_minus_3 Retains rows where the date in the column is not equal to three days ago (T\u20133). is_today Retains rows where the date in the column is not equal to today. is_yesterday Retains rows where the date in the column is not equal to yesterday. is_on_weekday Retains rows where the date in the column NOT FALLS on a weekend (fails \"weekday\"). is_on_weekend Retains rows where the date in the column NOT FALLS on a weekday (fails \"weekend\"). is_on_monday Retains rows where the date in the column is not Monday. is_on_tuesday Retains rows where the date in the column is not Tuesday. is_on_wednesday Retains rows where the date in the column is not Wednesday. is_on_thursday Retains rows where the date in the column is not Thursday. is_on_friday Retains rows where the date in the column is not Friday. is_on_saturday Retains rows where the date in the column is not Saturday. is_on_sunday Retains rows where the date in the column is not Sunday. validate_date_format Filters rows where the date doesn't match the expected format or is null. is_future_date Filters rows where the date in the column is not after today. is_past_date Filters rows where the date in the column is not before today. is_date_after Filters rows where the date in the column is not before the date provided in the rule. is_date_before Filters rows where the date in the column is not after the date provided in the rule. is_date_between Filters rows where the date in the column is not outside the range <code>[start, end]</code>. all_date_checks Alias for <code>is_past_date</code> (same logic: date before today)."},{"location":"#project-layout","title":"\ud83d\udcc2 Project Layout","text":"<pre><code>sumeh/\n\u251c\u2500\u2500 poetry.lock\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 README.md\n\u2514\u2500\u2500 sumeh\n    \u251c\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 cli.py\n    \u251c\u2500\u2500 core.py\n    \u251c\u2500\u2500 engine\n    \u2502   \u251c\u2500\u2500 __init__.py\n    \u2502   \u251c\u2500\u2500 bigquery_engine.py\n    \u2502   \u251c\u2500\u2500 dask_engine.py\n    \u2502   \u251c\u2500\u2500 duckdb_engine.py\n    \u2502   \u251c\u2500\u2500 pandas_engine.py\n    \u2502   \u251c\u2500\u2500 polars_engine.py\n    \u2502   \u2514\u2500\u2500 pyspark_engine.py\n    \u2514\u2500\u2500 services\n        \u251c\u2500\u2500 __init__.py\n        \u251c\u2500\u2500 config.py\n        \u251c\u2500\u2500 index.html\n        \u2514\u2500\u2500 utils.py\n</code></pre>"},{"location":"#roadmap","title":"\ud83d\udcc8 Roadmap","text":"<ul> <li>[ ] Complete BigQuery engine implementation</li> <li>\u2705 Complete Pandas engine implementation</li> <li>\u2705 Enhanced documentation</li> <li>\u2705 More validation rule types</li> <li>[ ] Performance optimizations</li> </ul>"},{"location":"#contributing","title":"\ud83e\udd1d Contributing","text":"<ol> <li>Fork &amp; create a feature branch  </li> <li>Implement new checks or engines, following existing signatures  </li> <li>Add tests under <code>tests/</code> </li> <li>Open a PR and ensure CI passes</li> </ol>"},{"location":"#license","title":"\ud83d\udcdc License","text":"<p>Licensed under the Apache License 2.0.</p>"},{"location":"quickstart/","title":"Quickstart \ud83d\ude80","text":"<p>A concise guide to get started with Sumeh\u2019s unified data quality framework.</p>"},{"location":"quickstart/#1-installation","title":"1. Installation \ud83d\udcbb","text":"<p>Install Sumeh via pip (recommended) or conda:</p> <pre><code>pip install sumeh\n# or\nconda install -c conda-forge sumeh\n</code></pre>"},{"location":"quickstart/#2-loading-rules-and-schema-configuration","title":"2. Loading Rules and Schema Configuration \u2699\ufe0f","text":"<p>Use <code>get_rules_config</code> and <code>get_schema_config</code> to fetch your validation rules and expected schema from various sources.</p> <pre><code>from sumeh import get_rules_config, get_schema_config\n\n# Load rules from CSV\nrules = get_rules_config(\"path/to/rules.csv\", delimiter=';')\n\n# Load expected schema from Glue Data Catalog\nschema = get_schema_config(\n    \"glue\",\n    catalog_name=\"my_catalog\",\n    database_name=\"my_db\",\n    table_name=\"my_table\"\n)\n</code></pre> <p>Supported rule/schema sources include:</p> <ul> <li><code>bigquery://project.dataset.table</code> \ud83c\udf10</li> <li><code>s3://bucket/path</code> \u2601\ufe0f</li> <li>Local CSV (<code>*.csv</code>) \ud83d\udcc4</li> <li>Relational (\"mysql\", \"postgresql\") via kwargs \ud83d\uddc4\ufe0f</li> <li>AWS Glue (<code>\"glue\"</code>) \ud83d\udd25</li> <li>DuckDB (<code>duckdb://db_path.table</code>) \ud83e\udd86</li> <li>Databricks (<code>databricks://catalog.schema.table</code>) \ud83d\udc8e</li> </ul>"},{"location":"quickstart/#3-schema-validation","title":"3. Schema Validation \ud83d\udcd0","text":"<p>Before validating data, ensure your DataFrame or connection matches the expected schema:</p> <pre><code>from sumeh import validate_schema\n\n# For a Spark DataFrame:\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\ndf = spark.read.parquet(\"data.parquet\")\n\nis_valid, errors = validate_schema(\n    df,\n    expected=schema,\n    engine=\"pyspark_engine\"\n)\nif not is_valid:\n    print(\"Schema mismatches:\", errors)\n</code></pre>"},{"location":"quickstart/#4-data-validation","title":"4. Data Validation \ud83d\udd0d","text":"<p>Apply your loaded rules to any supported DataFrame using <code>validate</code>:</p> <pre><code>from sumeh import validate\n\n# Example with Pandas:\nimport pandas as pd\ndf = pd.read_csv(\"data.csv\")\n\n# Validate (detects engine automatically)\nresult = validate(df, rules)\n# `result` structure depends on engine (e.g., CheckResult for cuallee engines)\n</code></pre>"},{"location":"quickstart/#5-summarization","title":"5. Summarization \ud83d\udcca","text":"<p>Generate a tabular summary of violations and pass rates with <code>summarize</code>:</p> <pre><code>from sumeh import summarize\n\n# For DataFrames requiring manual total_rows (e.g., Pandas):\ntotal = len(df)\nsummary_df = summarize(\n    df=result,       # could be validation output or raw DataFrame\n    rules=rules,\n    total_rows=total\n)\nprint(summary_df)\n</code></pre>"},{"location":"quickstart/#6-one-step-reporting","title":"6. One-Step Reporting \ud83d\udcdd","text":"<p>Use <code>report</code> for an end-to-end quality check and summary in one call:</p> <pre><code>from sumeh import report\n\nreport_df = report(\n    df,              # your DataFrame or connection\n    rules,\n    name=\"My Quality Check\"\n)\nprint(report_df)\n</code></pre> <p>For deeper customization and engine-specific options, explore the full API and examples in the Sumeh repository.</p>"},{"location":"api/cli/","title":"Module <code>sumeh.cli</code>","text":""},{"location":"api/cli/#sumeh.cli.serve_index","title":"<code>serve_index()</code>","text":"<p>Serves the index.html file for initial configuration and opens it in a browser.</p> <p>This function determines the path to the 'index.html' file, changes the working directory to the appropriate location, and starts a simple HTTP server to serve the file. It also automatically opens the served page in the default web browser.</p> <p>The server runs on localhost at port 8000. The process continues until interrupted by the user (via a KeyboardInterrupt), at which point the server shuts down.</p> <p>Raises:</p> Type Description <code>KeyboardInterrupt</code> <p>If the server is manually interrupted by the user.</p> Source code in <code>sumeh/cli.py</code> <pre><code>def serve_index():\n    \"\"\"\n    Serves the index.html file for initial configuration and opens it in a browser.\n\n    This function determines the path to the 'index.html' file, changes the\n    working directory to the appropriate location, and starts a simple HTTP server\n    to serve the file. It also automatically opens the served page in the default\n    web browser.\n\n    The server runs on localhost at port 8000. The process continues until\n    interrupted by the user (via a KeyboardInterrupt), at which point the server\n    shuts down.\n\n    Raises:\n        KeyboardInterrupt: If the server is manually interrupted by the user.\n    \"\"\"  # Determine the directory of the index.html file\n    base_dir = os.path.dirname(os.path.abspath(__file__))\n    html_path = os.path.join(base_dir, \"services\")\n\n    # Change the current working directory to the location of index.html\n    os.chdir(html_path)\n\n    # Define the port and URL\n    port = 8000\n    url = f\"http://localhost:{port}\"\n\n    # Serve the HTML file\n    with TCPServer((\"localhost\", port), SimpleHTTPRequestHandler) as httpd:\n        print(f\"Serving index.html at {url}\")\n\n        # Open the URL in the default web browser\n        webbrowser.open(url)\n\n        try:\n            httpd.serve_forever()\n        except KeyboardInterrupt:\n            print(\"\\nShutting down server.\")\n            httpd.server_close()\n</code></pre>"},{"location":"api/core/","title":"Module <code>sumeh.core</code>","text":"<p>This module provides a set of functions and utilities for data validation, schema retrieval, and summarization. It supports multiple data sources and engines, including BigQuery, S3, CSV files, MySQL, PostgreSQL, AWS Glue, DuckDB, and Databricks.</p> <p>Functions:</p> Name Description <code>get_rules_config</code> <p>str, **kwargs) -&gt; List[Dict[str, Any]]: Retrieves configuration rules based on the specified source.</p> <code>get_schema_config</code> <p>str, **kwargs) -&gt; List[Dict[str, Any]]: Retrieves the schema configuration based on the provided data source.</p> <code>__detect_engine</code> <code>validate_schema</code> <p>Any, expected: List[Dict[str, Any]], engine: str, **engine_kwargs) -&gt; Tuple[bool, List[Tuple[str, str]]]:</p> <code>validate</code> <code>summarize</code> <p>list[dict], **context):</p> <code>report</code> <p>list[dict], name: str = \"Quality Check\"):</p> Constants <p>_CONFIG_DISPATCH: A dictionary mapping data source types (e.g., \"mysql\", \"postgresql\")   to their respective configuration retrieval functions.</p> Imports <p>cuallee: Provides the <code>Check</code> and <code>CheckLevel</code> classes for data validation. warnings: Used to issue warnings for unknown rule names. importlib: Dynamically imports modules based on engine detection. typing: Provides type hints for function arguments and return values. re: Used for regular expression matching in source string parsing. sumeh.services.config: Contains functions for retrieving configurations and schemas   from various data sources. sumeh.services.utils: Provides utility functions for value conversion and URI parsing.</p> <p>The module uses Python's structural pattern matching (<code>match-case</code>) to handle different data source types and validation rules. The <code>report</code> function supports a wide range of validation checks, including completeness, uniqueness, value comparisons, patterns, and date-related checks. The <code>validate</code> and <code>summarize</code> functions dynamically detect the appropriate engine based on the input DataFrame type and delegate the processing to the corresponding engine module.</p>"},{"location":"api/core/#sumeh.core._CONFIG_DISPATCH","title":"<code>_CONFIG_DISPATCH = {'mysql': get_config_from_mysql, 'postgresql': get_config_from_postgresql}</code>  <code>module-attribute</code>","text":""},{"location":"api/core/#sumeh.core.__convert_value","title":"<code>__convert_value(value)</code>","text":"<p>Converts the provided value to the appropriate type (date, float, or int).</p> <p>Depending on the format of the input value, it will be converted to a datetime object, a floating-point number (float), or an integer (int).</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>The value to be converted, represented as a string.</p> required <p>Returns:</p> Type Description <p>Union[datetime, float, int]: The converted value, which can be a datetime object, float, or int.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the value does not match an expected format.</p> Source code in <code>sumeh/services/utils.py</code> <pre><code>def __convert_value(value):\n    \"\"\"\n    Converts the provided value to the appropriate type (date, float, or int).\n\n    Depending on the format of the input value, it will be converted to a datetime object,\n    a floating-point number (float), or an integer (int).\n\n    Args:\n        value (str): The value to be converted, represented as a string.\n\n    Returns:\n        Union[datetime, float, int]: The converted value, which can be a datetime object, float, or int.\n\n    Raises:\n        ValueError: If the value does not match an expected format.\n    \"\"\"\n    from datetime import datetime\n\n    value = value.strip()\n    try:\n        if \"-\" in value:\n            return datetime.strptime(value, \"%Y-%m-%d\")\n        else:\n            return datetime.strptime(value, \"%d/%m/%Y\")\n    except ValueError:\n        if \".\" in value:\n            return float(value)\n        return int(value)\n</code></pre>"},{"location":"api/core/#sumeh.core.__detect_engine","title":"<code>__detect_engine(df)</code>","text":"<p>Detects the engine type of the given DataFrame based on its module.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <p>The DataFrame object whose engine type is to be detected.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>A string representing the detected engine type. Possible values are: - \"pyspark_engine\" for PySpark DataFrames - \"dask_engine\" for Dask DataFrames - \"polars_engine\" for Polars DataFrames - \"pandas_engine\" for Pandas DataFrames - \"duckdb_engine\" for DuckDB or BigQuery DataFrames</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the DataFrame type is unsupported.</p> Source code in <code>sumeh/core.py</code> <pre><code>def __detect_engine(df):\n    \"\"\"\n    Detects the engine type of the given DataFrame based on its module.\n\n    Args:\n        df: The DataFrame object whose engine type is to be detected.\n\n    Returns:\n        str: A string representing the detected engine type. Possible values are:\n            - \"pyspark_engine\" for PySpark DataFrames\n            - \"dask_engine\" for Dask DataFrames\n            - \"polars_engine\" for Polars DataFrames\n            - \"pandas_engine\" for Pandas DataFrames\n            - \"duckdb_engine\" for DuckDB or BigQuery DataFrames\n\n    Raises:\n        TypeError: If the DataFrame type is unsupported.\n    \"\"\"\n    mod = df.__class__.__module__\n    match mod:\n        case m if m.startswith(\"pyspark\"):\n            return \"pyspark_engine\"\n        case m if m.startswith(\"dask\"):\n            return \"dask_engine\"\n        case m if m.startswith(\"polars\"):\n            return \"polars_engine\"\n        case m if m.startswith(\"pandas\"):\n            return \"pandas_engine\"\n        case m if m.startswith(\"duckdb\"):\n            return \"duckdb_engine\"\n        case m if m.startswith(\"bigquery\"):\n            return \"duckdb_engine\"\n        case _:\n            raise TypeError(f\"Unsupported DataFrame type: {type(df)}\")\n</code></pre>"},{"location":"api/core/#sumeh.core.__parse_databricks_uri","title":"<code>__parse_databricks_uri(uri)</code>","text":"<p>Parses a Databricks URI into its catalog, schema, and table components.</p> <p>The URI is expected to follow the format <code>protocol://catalog.schema.table</code> or <code>protocol://schema.table</code>. If the catalog is not provided, it will be set to <code>None</code>. If the schema is not provided, the current database from the active Spark session will be used.</p> <p>Parameters:</p> Name Type Description Default <code>uri</code> <code>str</code> <p>The Databricks URI to parse.</p> required <p>Returns:</p> Type Description <code>Dict[str, Optional[str]]</code> <p>Dict[str, Optional[str]]: A dictionary containing the parsed components: - \"catalog\" (Optional[str]): The catalog name, or <code>None</code> if not provided. - \"schema\" (Optional[str]): The schema name, or the current database if not provided. - \"table\" (Optional[str]): The table name.</p> Source code in <code>sumeh/services/utils.py</code> <pre><code>def __parse_databricks_uri(uri: str) -&gt; Dict[str, Optional[str]]:\n    \"\"\"\n    Parses a Databricks URI into its catalog, schema, and table components.\n\n    The URI is expected to follow the format `protocol://catalog.schema.table` or\n    `protocol://schema.table`. If the catalog is not provided, it will be set to `None`.\n    If the schema is not provided, the current database from the active Spark session\n    will be used.\n\n    Args:\n        uri (str): The Databricks URI to parse.\n\n    Returns:\n        Dict[str, Optional[str]]: A dictionary containing the parsed components:\n            - \"catalog\" (Optional[str]): The catalog name, or `None` if not provided.\n            - \"schema\" (Optional[str]): The schema name, or the current database if not provided.\n            - \"table\" (Optional[str]): The table name.\n    \"\"\"\n    _, path = uri.split(\"://\", 1)\n    parts = path.split(\".\")\n    if len(parts) == 3:\n        catalog, schema, table = parts\n    elif len(parts) == 2:\n        catalog, schema, table = None, parts[0], parts[1]\n    else:\n        from pyspark.sql import SparkSession\n\n        spark = SparkSession.builder.getOrCreate()\n        catalog = None\n        schema = spark.catalog.currentDatabase()\n        table = parts[0]\n    return {\"catalog\": catalog, \"schema\": schema, \"table\": table}\n</code></pre>"},{"location":"api/core/#sumeh.core.get_config_from_bigquery","title":"<code>get_config_from_bigquery(project_id, dataset_id, table_id, credentials_path=None, query=None)</code>","text":"<p>Retrieves configuration data from a Google BigQuery table.</p> <p>Parameters:</p> Name Type Description Default <code>project_id</code> <code>str</code> <p>Google Cloud project ID.</p> required <code>dataset_id</code> <code>str</code> <p>BigQuery dataset ID.</p> required <code>table_id</code> <code>str</code> <p>BigQuery table ID.</p> required <code>credentials_path</code> <code>Optional[str]</code> <p>Path to service account credentials file (if not provided, defaults to default credentials).</p> <code>None</code> <code>query</code> <code>Optional[str]</code> <p>Custom SQL query to fetch data (if not provided, defaults to SELECT *).</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Dict[str, str]]</code> <p>List[Dict[str, str]]: A list of dictionaries representing the parsed configuration data.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If there is an error while querying BigQuery.</p> Source code in <code>sumeh/services/config.py</code> <pre><code>def get_config_from_bigquery(\n    project_id: str,\n    dataset_id: str,\n    table_id: str,\n    credentials_path: Optional[str] = None,\n    query: Optional[str] = None,\n) -&gt; List[Dict[str, str]]:\n    \"\"\"\n    Retrieves configuration data from a Google BigQuery table.\n\n    Args:\n        project_id (str): Google Cloud project ID.\n        dataset_id (str): BigQuery dataset ID.\n        table_id (str): BigQuery table ID.\n        credentials_path (Optional[str]): Path to service account credentials file (if not provided, defaults to default credentials).\n        query (Optional[str]): Custom SQL query to fetch data (if not provided, defaults to SELECT *).\n\n    Returns:\n        List[Dict[str, str]]: A list of dictionaries representing the parsed configuration data.\n\n    Raises:\n        RuntimeError: If there is an error while querying BigQuery.\n    \"\"\"\n    from google.cloud import bigquery\n    from google.auth.exceptions import DefaultCredentialsError\n\n    if query is None:\n        query = f\"SELECT * FROM `{project_id}.{dataset_id}.{table_id}`\"\n\n    try:\n        client = bigquery.Client(\n            project=project_id,\n            credentials=(\n                None\n                if credentials_path is None\n                else bigquery.Credentials.from_service_account_file(credentials_path)\n            ),\n        )\n\n        # Execute the query and convert the result to a pandas DataFrame\n        data = client.query(query).to_dataframe()\n\n        # Convert the DataFrame to a list of dictionaries\n        data_dict = data.to_dict(orient=\"records\")\n\n        # Parse the data and return the result\n        return __parse_data(data_dict)\n\n    except DefaultCredentialsError as e:\n        raise RuntimeError(f\"Credentials error: {e}\") from e\n\n    except Exception as e:\n        raise RuntimeError(f\"Error occurred while querying BigQuery: {e}\") from e\n</code></pre>"},{"location":"api/core/#sumeh.core.get_config_from_csv","title":"<code>get_config_from_csv(file_path, delimiter=',')</code>","text":"<p>Retrieves configuration data from a CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The local file path to the CSV file.</p> required <code>delimiter</code> <code>Optional[str]</code> <p>The delimiter used in the CSV file (default is \",\").</p> <code>','</code> <p>Returns:</p> Type Description <code>List[Dict[str, str]]</code> <p>List[Dict[str, str]]: A list of dictionaries representing the parsed configuration data.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If there is an error reading or processing the file.</p> Source code in <code>sumeh/services/config.py</code> <pre><code>def get_config_from_csv(\n    file_path: str, delimiter: Optional[str] = \",\"\n) -&gt; List[Dict[str, str]]:\n    \"\"\"\n    Retrieves configuration data from a CSV file.\n\n    Args:\n        file_path (str): The local file path to the CSV file.\n        delimiter (Optional[str]): The delimiter used in the CSV file (default is \",\").\n\n    Returns:\n        List[Dict[str, str]]: A list of dictionaries representing the parsed configuration data.\n\n    Raises:\n        RuntimeError: If there is an error reading or processing the file.\n    \"\"\"\n    try:\n        file_content = __read_local_file(file_path)\n        result = __read_csv_file(file_content, delimiter)\n\n        return __parse_data(result)\n\n    except FileNotFoundError as e:\n        raise RuntimeError(f\"File '{file_path}' not found. Error: {e}\") from e\n\n    except ValueError as e:\n        raise ValueError(\n            f\"Error while parsing CSV file '{file_path}'. Error: {e}\"\n        ) from e\n\n    except Exception as e:\n        # Catch any unexpected exceptions\n        raise RuntimeError(\n            f\"Unexpected error while processing CSV file '{file_path}'. Error: {e}\"\n        ) from e\n</code></pre>"},{"location":"api/core/#sumeh.core.get_config_from_databricks","title":"<code>get_config_from_databricks(catalog, schema, table, **kwargs)</code>","text":"<p>Retrieves configuration data from a Databricks table and returns it as a list of dictionaries.</p> <p>Parameters:</p> Name Type Description Default <code>catalog</code> <code>Optional[str]</code> <p>The catalog name in Databricks. If provided, it will be included in the table's full path.</p> required <code>schema</code> <code>Optional[str]</code> <p>The schema name in Databricks. If provided, it will be included in the table's full path.</p> required <code>table</code> <code>str</code> <p>The name of the table to retrieve data from.</p> required <code>**kwargs</code> <p>Additional keyword arguments (currently unused).</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List[Dict[str, Any]]: A list of dictionaries, where each dictionary represents a row of data from the table.</p> Source code in <code>sumeh/services/config.py</code> <pre><code>def get_config_from_databricks(\n    catalog: Optional[str], schema: Optional[str], table: str, **kwargs\n) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Retrieves configuration data from a Databricks table and returns it as a list of dictionaries.\n\n    Args:\n        catalog (Optional[str]): The catalog name in Databricks. If provided, it will be included in the table's full path.\n        schema (Optional[str]): The schema name in Databricks. If provided, it will be included in the table's full path.\n        table (str): The name of the table to retrieve data from.\n        **kwargs: Additional keyword arguments (currently unused).\n\n    Returns:\n        List[Dict[str, Any]]: A list of dictionaries, where each dictionary represents a row of data from the table.\n    \"\"\"\n    from pyspark.sql import SparkSession\n\n    spark = SparkSession.builder.getOrCreate()\n    if catalog and schema:\n        full = f\"{catalog}.{schema}.{table}\"\n    elif schema:\n        full = f\"{schema}.{table}\"\n    else:\n        full = table\n    if \"query\" in kwargs.keys():\n        df = spark.sql(f\"select * from {full} where {kwargs['query']}\")\n    else:\n        df = spark.table(full)\n    return [row.asDict() for row in df.collect()]\n</code></pre>"},{"location":"api/core/#sumeh.core.get_config_from_duckdb","title":"<code>get_config_from_duckdb(db_path, table=None, query=None, conn=None)</code>","text":"<p>Retrieve configuration data from a DuckDB database.</p> <p>This function fetches data from a DuckDB database either by executing a custom SQL query or by selecting all rows from a specified table. The data is then parsed into a list of dictionaries.</p> <p>Parameters:</p> Name Type Description Default <code>db_path</code> <code>str</code> <p>The path to the DuckDB database file.</p> required <code>table</code> <code>str</code> <p>The name of the table to fetch data from. Defaults to None.</p> <code>None</code> <code>query</code> <code>str</code> <p>A custom SQL query to execute. Defaults to None.</p> <code>None</code> <code>conn</code> <p>A valid DuckDB connection object.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List[Dict[str, Any]]: A list of dictionaries representing the fetched data.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If neither <code>table</code> nor <code>query</code> is provided, or if a valid <code>conn</code> is not supplied.</p> Example <p>import duckdb conn = duckdb.connect('my_db.duckdb') config = get_config_from_duckdb('my_db.duckdb', table='rules', conn=conn)</p> Source code in <code>sumeh/services/config.py</code> <pre><code>def get_config_from_duckdb(\n    db_path: str, table: str = None, query: str = None, conn=None\n) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Retrieve configuration data from a DuckDB database.\n\n    This function fetches data from a DuckDB database either by executing a custom SQL query\n    or by selecting all rows from a specified table. The data is then parsed into a list of\n    dictionaries.\n\n    Args:\n        db_path (str): The path to the DuckDB database file.\n        table (str, optional): The name of the table to fetch data from. Defaults to None.\n        query (str, optional): A custom SQL query to execute. Defaults to None.\n        conn: A valid DuckDB connection object.\n\n    Returns:\n        List[Dict[str, Any]]: A list of dictionaries representing the fetched data.\n\n    Raises:\n        ValueError: If neither `table` nor `query` is provided, or if a valid `conn` is not supplied.\n\n    Example:\n        &gt;&gt;&gt; import duckdb\n        &gt;&gt;&gt; conn = duckdb.connect('my_db.duckdb')\n        &gt;&gt;&gt; config = get_config_from_duckdb('my_db.duckdb', table='rules', conn=conn)\n    \"\"\"\n\n    if query:\n        df = conn.execute(query).fetchdf()\n    elif table:\n        df = conn.execute(f\"SELECT * FROM {table}\").fetchdf()\n    else:\n        raise ValueError(\n            \"DuckDB configuration requires:\\n\"\n            \"1. Either a `table` name or custom `query`\\n\"\n            \"2. A valid database `conn` connection object\\n\"\n            \"Example: get_config('duckdb', table='rules', conn=duckdb.connect('my_db.duckdb'))\"\n        )\n\n    return __parse_data(df.to_dict(orient=\"records\"))\n</code></pre>"},{"location":"api/core/#sumeh.core.get_config_from_glue_data_catalog","title":"<code>get_config_from_glue_data_catalog(glue_context, database_name, table_name, query=None)</code>","text":"<p>Retrieves configuration data from AWS Glue Data Catalog.</p> <p>Parameters:</p> Name Type Description Default <code>glue_context</code> <p>An instance of <code>GlueContext</code>.</p> required <code>database_name</code> <code>str</code> <p>Glue database name.</p> required <code>table_name</code> <code>str</code> <p>Glue table name.</p> required <code>query</code> <code>Optional[str]</code> <p>Custom SQL query to fetch data (if provided).</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Dict[str, str]]</code> <p>List[Dict[str, str]]: A list of dictionaries representing the parsed configuration data.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If there is an error querying Glue Data Catalog.</p> Source code in <code>sumeh/services/config.py</code> <pre><code>def get_config_from_glue_data_catalog(\n    glue_context, database_name: str, table_name: str, query: Optional[str] = None\n) -&gt; List[Dict[str, str]]:\n    \"\"\"\n    Retrieves configuration data from AWS Glue Data Catalog.\n\n    Args:\n        glue_context: An instance of `GlueContext`.\n        database_name (str): Glue database name.\n        table_name (str): Glue table name.\n        query (Optional[str]): Custom SQL query to fetch data (if provided).\n\n    Returns:\n        List[Dict[str, str]]: A list of dictionaries representing the parsed configuration data.\n\n    Raises:\n        RuntimeError: If there is an error querying Glue Data Catalog.\n    \"\"\"\n    from awsglue.context import GlueContext\n\n    if not isinstance(glue_context, GlueContext):\n        raise ValueError(\"The provided context is not a valid GlueContext.\")\n\n    spark = glue_context.spark_session\n\n    try:\n        dynamic_frame = glue_context.create_dynamic_frame.from_catalog(\n            database=database_name, table_name=table_name\n        )\n\n        data_frame = dynamic_frame.toDF()\n\n        if query:\n            data_frame.createOrReplaceTempView(\"table_name\")\n            data_frame = spark.sql(query)\n\n        data_dict = [row.asDict() for row in data_frame.collect()]\n\n        return __parse_data(data_dict)\n\n    except Exception as e:\n        raise RuntimeError(\n            f\"Error occurred while querying Glue Data Catalog: {e}\"\n        ) from e\n</code></pre>"},{"location":"api/core/#sumeh.core.get_config_from_mysql","title":"<code>get_config_from_mysql(connection=None, host=None, user=None, password=None, database=None, port=3306, schema=None, table=None, query=None)</code>","text":"<p>Retrieves configuration data from a MySQL database.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Optional</code> <p>An existing MySQL connection object.</p> <code>None</code> <code>host</code> <code>Optional[str]</code> <p>Host of the MySQL server.</p> <code>None</code> <code>user</code> <code>Optional[str]</code> <p>Username to connect to MySQL.</p> <code>None</code> <code>password</code> <code>Optional[str]</code> <p>Password for the MySQL user.</p> <code>None</code> <code>database</code> <code>Optional[str]</code> <p>Database name to query.</p> <code>None</code> <code>port</code> <code>Optional[int]</code> <p>The port for the MySQL connection (default is 3306).</p> <code>3306</code> <code>schema</code> <code>Optional[str]</code> <p>Schema name if query is not provided.</p> <code>None</code> <code>table</code> <code>Optional[str]</code> <p>Table name if query is not provided.</p> <code>None</code> <code>query</code> <code>Optional[str]</code> <p>Custom SQL query to fetch data (if not provided, <code>schema</code> and <code>table</code> must be given).</p> <code>None</code> <p>Returns:</p> Type Description <p>List[Dict[str, Any]]: A list of dictionaries representing the parsed configuration data.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If neither <code>query</code> nor both <code>schema</code> and <code>table</code> are provided.</p> <code>ConnectionError</code> <p>If there is an error connecting to MySQL.</p> <code>RuntimeError</code> <p>If there is an error executing the query or processing the data.</p> Source code in <code>sumeh/services/config.py</code> <pre><code>def get_config_from_mysql(\n    connection: Optional = None,\n    host: Optional[str] = None,\n    user: Optional[str] = None,\n    password: Optional[str] = None,\n    database: Optional[str] = None,\n    port: Optional[int] = 3306,\n    schema: Optional[str] = None,\n    table: Optional[str] = None,\n    query: Optional[str] = None,\n):\n    \"\"\"\n    Retrieves configuration data from a MySQL database.\n\n    Args:\n        connection (Optional): An existing MySQL connection object.\n        host (Optional[str]): Host of the MySQL server.\n        user (Optional[str]): Username to connect to MySQL.\n        password (Optional[str]): Password for the MySQL user.\n        database (Optional[str]): Database name to query.\n        port (Optional[int]): The port for the MySQL connection (default is 3306).\n        schema (Optional[str]): Schema name if query is not provided.\n        table (Optional[str]): Table name if query is not provided.\n        query (Optional[str]): Custom SQL query to fetch data (if not provided, `schema` and `table` must be given).\n\n    Returns:\n        List[Dict[str, Any]]: A list of dictionaries representing the parsed configuration data.\n\n    Raises:\n        ValueError: If neither `query` nor both `schema` and `table` are provided.\n        ConnectionError: If there is an error connecting to MySQL.\n        RuntimeError: If there is an error executing the query or processing the data.\n    \"\"\"\n    import mysql.connector\n    import pandas as pd\n\n    if query is None and (schema is None or table is None):\n        raise ValueError(\n            \"You must provide either a 'query' or both 'schema' and 'table'.\"\n        )\n\n    if query is None:\n        query = f\"SELECT * FROM {schema}.{table}\"\n\n    try:\n        connection = connection or __create_connection(\n            mysql.connector.connect, host, user, password, database, port\n        )\n        data = pd.read_sql(query, connection)\n        data_dict = data.to_dict(orient=\"records\")\n        return __parse_data(data_dict)\n\n    except mysql.connector.Error as e:\n        raise ConnectionError(f\"Error connecting to MySQL database: {e}\")\n\n    except Exception as e:\n        raise RuntimeError(f\"Error executing the query or processing data: {e}\")\n\n    finally:\n        if connection and host is not None:\n            connection.close()\n</code></pre>"},{"location":"api/core/#sumeh.core.get_config_from_postgresql","title":"<code>get_config_from_postgresql(connection=None, host=None, user=None, password=None, database=None, port=5432, schema=None, table=None, query=None)</code>","text":"<p>Retrieves configuration data from a PostgreSQL database.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Optional</code> <p>An existing PostgreSQL connection object.</p> <code>None</code> <code>host</code> <code>Optional[str]</code> <p>Host of the PostgreSQL server.</p> <code>None</code> <code>user</code> <code>Optional[str]</code> <p>Username to connect to PostgreSQL.</p> <code>None</code> <code>password</code> <code>Optional[str]</code> <p>Password for the PostgreSQL user.</p> <code>None</code> <code>database</code> <code>Optional[str]</code> <p>Database name to query.</p> <code>None</code> <code>port</code> <code>Optional[int]</code> <p>The port for the PostgreSQL connection (default is 5432).</p> <code>5432</code> <code>schema</code> <code>Optional[str]</code> <p>Schema name if query is not provided.</p> <code>None</code> <code>table</code> <code>Optional[str]</code> <p>Table name if query is not provided.</p> <code>None</code> <code>query</code> <code>Optional[str]</code> <p>Custom SQL query to fetch data (if not provided, <code>schema</code> and <code>table</code> must be given).</p> <code>None</code> <p>Returns:</p> Type Description <code>list[dict]</code> <p>List[Dict[str, Any]]: A list of dictionaries representing the parsed configuration data.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If neither <code>query</code> nor both <code>schema</code> and <code>table</code> are provided.</p> <code>ConnectionError</code> <p>If there is an error connecting to PostgreSQL.</p> <code>RuntimeError</code> <p>If there is an error executing the query or processing the data.</p> Source code in <code>sumeh/services/config.py</code> <pre><code>def get_config_from_postgresql(\n    connection: Optional = None,\n    host: Optional[str] = None,\n    user: Optional[str] = None,\n    password: Optional[str] = None,\n    database: Optional[str] = None,\n    port: Optional[int] = 5432,\n    schema: Optional[str] = None,\n    table: Optional[str] = None,\n    query: Optional[str] = None,\n) -&gt; list[dict]:\n    \"\"\"\n    Retrieves configuration data from a PostgreSQL database.\n\n    Args:\n        connection (Optional): An existing PostgreSQL connection object.\n        host (Optional[str]): Host of the PostgreSQL server.\n        user (Optional[str]): Username to connect to PostgreSQL.\n        password (Optional[str]): Password for the PostgreSQL user.\n        database (Optional[str]): Database name to query.\n        port (Optional[int]): The port for the PostgreSQL connection (default is 5432).\n        schema (Optional[str]): Schema name if query is not provided.\n        table (Optional[str]): Table name if query is not provided.\n        query (Optional[str]): Custom SQL query to fetch data (if not provided, `schema` and `table` must be given).\n\n    Returns:\n        List[Dict[str, Any]]: A list of dictionaries representing the parsed configuration data.\n\n    Raises:\n        ValueError: If neither `query` nor both `schema` and `table` are provided.\n        ConnectionError: If there is an error connecting to PostgreSQL.\n        RuntimeError: If there is an error executing the query or processing the data.\n    \"\"\"\n    import psycopg2\n    import pandas as pd\n\n    if query is None and (schema is None or table is None):\n        raise ValueError(\n            \"You must provide either a 'query' or both 'schema' and 'table'.\"\n        )\n\n    if query is None:\n        query = f\"SELECT * FROM {schema}.{table}\"\n\n    try:\n        connection = connection or __create_connection(\n            psycopg2.connect, host, user, password, database, port\n        )\n\n        data = pd.read_sql(query, connection)\n\n        data_dict = data.to_dict(orient=\"records\")\n        return __parse_data(data_dict)\n\n    except psycopg2.Error as e:\n        raise ConnectionError(f\"Error connecting to PostgreSQL database: {e}\")\n\n    except Exception as e:\n        raise RuntimeError(f\"Error executing the query or processing data: {e}\")\n\n    finally:\n        if connection and host is not None:\n            connection.close()\n</code></pre>"},{"location":"api/core/#sumeh.core.get_config_from_s3","title":"<code>get_config_from_s3(s3_path, delimiter=',')</code>","text":"<p>Retrieves configuration data from a CSV file stored in an S3 bucket.</p> <p>Parameters:</p> Name Type Description Default <code>s3_path</code> <code>str</code> <p>The S3 path to the CSV file.</p> required <code>delimiter</code> <code>Optional[str]</code> <p>The delimiter used in the CSV file (default is \",\").</p> <code>','</code> <p>Returns:</p> Type Description <p>List[Dict[str, Any]]: A list of dictionaries representing the parsed configuration data.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If there is an error reading or processing the S3 file.</p> Source code in <code>sumeh/services/config.py</code> <pre><code>def get_config_from_s3(s3_path: str, delimiter: Optional[str] = \",\"):\n    \"\"\"\n    Retrieves configuration data from a CSV file stored in an S3 bucket.\n\n    Args:\n        s3_path (str): The S3 path to the CSV file.\n        delimiter (Optional[str]): The delimiter used in the CSV file (default is \",\").\n\n    Returns:\n        List[Dict[str, Any]]: A list of dictionaries representing the parsed configuration data.\n\n    Raises:\n        RuntimeError: If there is an error reading or processing the S3 file.\n    \"\"\"\n    try:\n        file_content = __read_s3_file(s3_path)\n        data = __read_csv_file(file_content, delimiter)\n        return __parse_data(data)\n\n    except Exception as e:\n        raise RuntimeError(f\"Error reading or processing the S3 file: {e}\")\n</code></pre>"},{"location":"api/core/#sumeh.core.get_rules_config","title":"<code>get_rules_config(source, **kwargs)</code>","text":"<p>Retrieve configuration rules based on the specified source.</p> <p>Dispatches to the appropriate loader according to the format of <code>source</code>, returning a list of parsed rule dictionaries.</p> Supported sources <ul> <li><code>bigquery://&lt;project&gt;.&lt;dataset&gt;.&lt;table&gt;</code></li> <li><code>s3://&lt;bucket&gt;/&lt;path&gt;</code></li> <li><code>&lt;file&gt;.csv</code></li> <li><code>\"mysql\"</code> or <code>\"postgresql\"</code> (requires host/user/etc. in kwargs)</li> <li><code>\"glue\"</code> (AWS Glue Data Catalog)</li> <li><code>duckdb://&lt;db_path&gt;.&lt;table&gt;</code></li> <li><code>databricks://&lt;catalog&gt;.&lt;schema&gt;.&lt;table&gt;</code></li> </ul> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>Identifier of the rules configuration location. Determines which handler is invoked.</p> required <code>**kwargs</code> <p>Loader-specific parameters (e.g. <code>host</code>, <code>user</code>, <code>password</code>, <code>connection</code>, <code>query</code>).</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List[Dict[str, Any]]: A list of dictionaries, each representing a validation rule with keys like <code>\"field\"</code>, <code>\"check_type\"</code>, <code>\"value\"</code>, <code>\"threshold\"</code>, and <code>\"execute\"</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>source</code> does not match any supported format.</p> Source code in <code>sumeh/core.py</code> <pre><code>def get_rules_config(source: str, **kwargs) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Retrieve configuration rules based on the specified source.\n\n    Dispatches to the appropriate loader according to the format of `source`,\n    returning a list of parsed rule dictionaries.\n\n    Supported sources:\n      - `bigquery://&lt;project&gt;.&lt;dataset&gt;.&lt;table&gt;`\n      - `s3://&lt;bucket&gt;/&lt;path&gt;`\n      - `&lt;file&gt;.csv`\n      - `\"mysql\"` or `\"postgresql\"` (requires host/user/etc. in kwargs)\n      - `\"glue\"` (AWS Glue Data Catalog)\n      - `duckdb://&lt;db_path&gt;.&lt;table&gt;`\n      - `databricks://&lt;catalog&gt;.&lt;schema&gt;.&lt;table&gt;`\n\n    Args:\n        source (str):\n            Identifier of the rules configuration location. Determines which\n            handler is invoked.\n        **kwargs:\n            Loader-specific parameters (e.g. `host`, `user`, `password`,\n            `connection`, `query`).\n\n    Returns:\n        List[Dict[str, Any]]:\n            A list of dictionaries, each representing a validation rule with keys\n            like `\"field\"`, `\"check_type\"`, `\"value\"`, `\"threshold\"`, and `\"execute\"`.\n\n    Raises:\n        ValueError:\n            If `source` does not match any supported format.\n    \"\"\"\n    match source:\n        case s if s.startswith(\"bigquery://\"):\n            _, path = s.split(\"://\", 1)\n            project, dataset, table = path.split(\".\")\n            return get_config_from_bigquery(\n                project_id=project,\n                dataset_id=dataset,\n                table_id=table,\n                **kwargs,\n            )\n\n        case s if s.startswith(\"s3://\"):\n            return get_config_from_s3(s, **kwargs)\n\n        case s if re.search(r\"\\.csv$\", s, re.IGNORECASE):\n            return get_config_from_csv(s, **kwargs)\n\n        case \"mysql\" | \"postgresql\" as driver:\n            loader = _CONFIG_DISPATCH[driver]\n            return loader(**kwargs)\n\n        case \"glue\":\n            return get_config_from_glue_data_catalog(**kwargs)\n\n        case s if s.startswith(\"duckdb://\"):\n            _, path = s.split(\"://\", 1)\n            db_path, table = path.rsplit(\".\", 1)\n            conn = kwargs.pop(\"conn\", None)\n            return get_config_from_duckdb(\n                conn=conn,\n                table=table,\n            )\n\n        case s if s.startswith(\"databricks://\"):\n            parts = __parse_databricks_uri(s)\n            return get_config_from_databricks(\n                catalog=parts[\"catalog\"],\n                schema=parts[\"schema\"],\n                table=parts[\"table\"],\n                **kwargs,\n            )\n\n        case _:\n            raise ValueError(f\"Unknow source: {source}\")\n</code></pre>"},{"location":"api/core/#sumeh.core.get_schema_config","title":"<code>get_schema_config(source, **kwargs)</code>","text":"<p>Retrieve the schema configuration based on the provided data source.</p> <p>This function determines the appropriate method to extract schema information based on the format or type of the <code>source</code> string. It supports various data sources such as BigQuery, S3, CSV files, MySQL, PostgreSQL, AWS Glue, DuckDB, and Databricks.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>A string representing the data source. The format of the string determines the method used to retrieve the schema. Supported formats include: <code>bigquery://&lt;project&gt;.&lt;dataset&gt;.&lt;table&gt;</code>, <code>s3://&lt;bucket&gt;/&lt;path&gt;</code>, <code>&lt;file&gt;.csv</code>, <code>mysql</code>, <code>postgresql</code>, <code>glue</code>, <code>duckdb://&lt;db_path&gt;.&lt;table&gt;</code>, <code>databricks://&lt;catalog&gt;.&lt;schema&gt;.&lt;table&gt;</code></p> required <code>**kwargs</code> <p>Additional keyword arguments required by specific schema retrieval methods. For example: For DuckDB: <code>conn</code> (a database connection object). For other sources: Additional parameters specific to the source.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List[Dict[str, Any]]: A list of dictionaries representing the schema</p> <code>List[Dict[str, Any]]</code> <p>configuration. Each dictionary contains details about a column in the</p> <code>List[Dict[str, Any]]</code> <p>schema.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the <code>source</code> string does not match any supported format.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; get_schema_config(\"bigquery://my_project.my_dataset.my_table\")\n&gt;&gt;&gt; get_schema_config(\"s3://my_bucket/my_file.csv\")\n&gt;&gt;&gt; get_schema_config(\"mysql\", host=\"localhost\", user=\"root\", password=\"password\")\n</code></pre> Source code in <code>sumeh/core.py</code> <pre><code>def get_schema_config(source: str, **kwargs) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Retrieve the schema configuration based on the provided data source.\n\n    This function determines the appropriate method to extract schema information\n    based on the format or type of the `source` string. It supports various data\n    sources such as BigQuery, S3, CSV files, MySQL, PostgreSQL, AWS Glue, DuckDB,\n    and Databricks.\n\n    Args:\n        source (str):\n            A string representing the data source. The format of the\n            string determines the method used to retrieve the schema. Supported\n            formats include: `bigquery://&lt;project&gt;.&lt;dataset&gt;.&lt;table&gt;`, `s3://&lt;bucket&gt;/&lt;path&gt;`,\n            `&lt;file&gt;.csv`, `mysql`, `postgresql`, `glue`, `duckdb://&lt;db_path&gt;.&lt;table&gt;`,\n            `databricks://&lt;catalog&gt;.&lt;schema&gt;.&lt;table&gt;`\n        **kwargs: Additional keyword arguments required by specific schema\n            retrieval methods. For example:\n            For DuckDB: `conn` (a database connection object).\n            For other sources: Additional parameters specific to the source.\n\n    Returns:\n        List[Dict[str, Any]]: A list of dictionaries representing the schema\n        configuration. Each dictionary contains details about a column in the\n        schema.\n\n    Raises:\n        ValueError: If the `source` string does not match any supported format.\n\n    Examples:\n        &gt;&gt;&gt; get_schema_config(\"bigquery://my_project.my_dataset.my_table\")\n        &gt;&gt;&gt; get_schema_config(\"s3://my_bucket/my_file.csv\")\n        &gt;&gt;&gt; get_schema_config(\"mysql\", host=\"localhost\", user=\"root\", password=\"password\")\n    \"\"\"\n    match source:\n        case s if s.startswith(\"bigquery://\"):\n            _, path = s.split(\"://\", 1)\n            project, dataset, table = path.split(\".\")\n            return get_schema_from_bigquery(\n                project_id=project,\n                dataset_id=dataset,\n                table_id=table,\n                **kwargs,\n            )\n\n        case s if s.startswith(\"s3://\"):\n            return get_schema_from_s3(s, **kwargs)\n\n        case s if re.search(r\"\\.csv$\", s, re.IGNORECASE):\n            return get_schema_from_csv(s, **kwargs)\n\n        case \"mysql\":\n            return get_schema_from_mysql(**kwargs)\n\n        case \"postgresql\":\n            return get_schema_from_postgresql(**kwargs)\n\n        case \"glue\":\n            return get_schema_from_glue(**kwargs)\n\n        case s if s.startswith(\"duckdb://\"):\n            conn = kwargs.pop(\"conn\")\n            _, path = s.split(\"://\", 1)\n            db_path, table = path.rsplit(\".\", 1)\n            return get_schema_from_duckdb(conn=conn, table=table)\n\n        case s if s.startswith(\"databricks://\"):\n            parts = __parse_databricks_uri(s)\n            return get_schema_from_databricks(\n                catalog=parts[\"catalog\"],\n                schema=parts[\"schema\"],\n                table=parts[\"table\"],\n                **kwargs,\n            )\n\n        case _:\n            raise ValueError(f\"Unknow source: {source}\")\n</code></pre>"},{"location":"api/core/#sumeh.core.report","title":"<code>report(df, rules, name='Quality Check')</code>","text":"<p>Performs a quality check on the given DataFrame based on the provided rules.</p> <p>The function iterates over a list of rules and applies different checks to the specified fields of the DataFrame. The checks include validation of completeness, uniqueness, specific values, patterns, and other conditions. Each rule corresponds to a particular type of validation, such as 'is_complete', 'is_greater_than', 'has_mean', etc. After applying the checks, the function returns the result of the validation.</p> <p>Parameters: - df (DataFrame): The DataFrame to be validated. - rules (list of dict): A list of rules defining the checks to be performed.     Each rule is a dictionary with the following keys:     - \"check_type\": The type of check to apply.     - \"field\": The column of the DataFrame to check.     - \"value\" (optional): The value used for comparison in some checks (e.g., for 'is_greater_than').     - \"threshold\" (optional): A percentage threshold to be applied in some checks. - name (str): The name of the quality check (default is \"Quality Check\").</p> <p>Returns: - quality_check (CheckResult): The result of the quality validation.</p> <p>Warnings: - If an unknown rule name is encountered, a warning is generated.</p> Source code in <code>sumeh/core.py</code> <pre><code>def report(df, rules: list[dict], name: str = \"Quality Check\"):\n    \"\"\"\n    Performs a quality check on the given DataFrame based on the provided rules.\n\n    The function iterates over a list of rules and applies different checks to the\n    specified fields of the DataFrame. The checks include validation of completeness,\n    uniqueness, specific values, patterns, and other conditions. Each rule corresponds\n    to a particular type of validation, such as 'is_complete', 'is_greater_than',\n    'has_mean', etc. After applying the checks, the function returns the result of\n    the validation.\n\n    Parameters:\n    - df (DataFrame): The DataFrame to be validated.\n    - rules (list of dict): A list of rules defining the checks to be performed.\n        Each rule is a dictionary with the following keys:\n        - \"check_type\": The type of check to apply.\n        - \"field\": The column of the DataFrame to check.\n        - \"value\" (optional): The value used for comparison in some checks (e.g., for 'is_greater_than').\n        - \"threshold\" (optional): A percentage threshold to be applied in some checks.\n    - name (str): The name of the quality check (default is \"Quality Check\").\n\n    Returns:\n    - quality_check (CheckResult): The result of the quality validation.\n\n    Warnings:\n    - If an unknown rule name is encountered, a warning is generated.\n    \"\"\"\n\n    check = Check(CheckLevel.WARNING, name)\n    for rule in rules:\n        rule_name = rule[\"check_type\"]\n        field = rule[\"field\"]\n        threshold = rule.get(\"threshold\", 1.0)\n        threshold = 1.0 if threshold is None else threshold\n\n        match rule_name:\n\n            case \"is_complete\":\n                check = check.is_complete(field, pct=threshold)\n\n            case \"is_unique\":\n                check = check.is_unique(field, pct=threshold)\n\n            case \"is_primary_key\":\n                check = check.is_primary_key(field, pct=threshold)\n\n            case \"are_complete\":\n                check = check.are_complete(field, pct=threshold)\n\n            case \"are_unique\":\n                check = check.are_complete(field, pct=threshold)\n\n            case \"is_composite_key\":\n                check = check.are_complete(field, pct=threshold)\n\n            case \"is_greater_than\":\n                value = __convert_value(rule[\"value\"])\n                check = check.is_greater_than(field, value, pct=threshold)\n\n            case \"is_positive\":\n                check = check.is_positive(field, pct=threshold)\n\n            case \"is_negative\":\n                check = check.is_negative(field, pct=threshold)\n\n            case \"is_greater_or_equal_than\":\n                value = __convert_value(rule[\"value\"])\n                check = check.is_greater_or_equal_than(field, value, pct=threshold)\n\n            case \"is_less_than\":\n                value = __convert_value(rule[\"value\"])\n                check = check.is_less_than(field, value, pct=threshold)\n\n            case \"is_less_or_equal_than\":\n                value = __convert_value(rule[\"value\"])\n                check = check.is_less_or_equal_than(field, value, pct=threshold)\n\n            case \"is_equal_than\":\n                value = __convert_value(rule[\"value\"])\n                check = check.is_equal_than(field, value, pct=threshold)\n\n            case \"is_contained_in\" | \"is_in\":\n                values = rule[\"value\"]\n                values = values.replace(\"[\", \"\").replace(\"]\", \"\").split(\",\")\n                values = tuple([value.strip() for value in values])\n                check = check.is_contained_in(field, values, pct=threshold)\n\n            case \"not_contained_in\" | \"not_in\":\n                values = rule[\"value\"]\n                values = values.replace(\"[\", \"\").replace(\"]\", \"\").split(\",\")\n                values = tuple([value.strip() for value in values])\n                check = check.is_contained_in(field, values, pct=threshold)\n\n            case \"is_between\":\n                values = rule[\"value\"]\n                values = values.replace(\"[\", \"\").replace(\"]\", \"\").split(\",\")\n                values = tuple(__convert_value(value) for value in values)\n                check = check.is_between(field, values, pct=threshold)\n\n            case \"has_pattern\":\n                pattern = rule[\"value\"]\n                check = check.has_pattern(field, pattern, pct=threshold)\n\n            case \"is_legit\":\n                check = check.is_legit(field, pct=threshold)\n\n            case \"has_min\":\n                value = __convert_value(rule[\"value\"])\n                check = check.has_min(field, value)\n\n            case \"has_max\":\n                value = __convert_value(rule[\"value\"])\n                check = check.has_max(field, value)\n\n            case \"has_std\":\n                value = __convert_value(rule[\"value\"])\n                check = check.has_std(field, value)\n\n            case \"has_mean\":\n                value = __convert_value(rule[\"value\"])\n                check = check.has_mean(field, value)\n\n            case \"has_sum\":\n                value = __convert_value(rule[\"value\"])\n                check = check.has_sum(field, value)\n\n            case \"has_cardinality\":\n                value = __convert_value(rule[\"value\"])\n                check = check.has_cardinality(field, value)\n\n            case \"has_infogain\":\n                check = check.has_infogain(field, pct=threshold)\n\n            case \"has_entropy\":\n                value = __convert_value(rule[\"value\"])\n                check = check.has_entropy(field, value)\n\n            case \"is_in_millions\":\n                check = check.is_in_millions(field, pct=threshold)\n\n            case \"is_in_billions\":\n                check = check.is_in_millions(field, pct=threshold)\n\n            case \"is_t_minus_1\":\n                check = check.is_t_minus_1(field, pct=threshold)\n\n            case \"is_t_minus_2\":\n                check = check.is_t_minus_2(field, pct=threshold)\n\n            case \"is_t_minus_3\":\n                check = check.is_t_minus_3(field, pct=threshold)\n\n            case \"is_today\":\n                check = check.is_today(field, pct=threshold)\n\n            case \"is_yesterday\":\n                check = check.is_yesterday(field, pct=threshold)\n\n            case \"is_on_weekday\":\n                check = check.is_on_weekday(field, pct=threshold)\n\n            case \"is_on_weekend\":\n                check = check.is_on_weekend(field, pct=threshold)\n\n            case \"is_on_monday\":\n                check = check.is_on_monday(field, pct=threshold)\n\n            case \"is_on_tuesday\":\n                check = check.is_on_tuesday(field, pct=threshold)\n\n            case \"is_on_wednesday\":\n                check = check.is_on_wednesday(field, pct=threshold)\n\n            case \"is_on_thursday\":\n                check = check.is_on_thursday(field, pct=threshold)\n\n            case \"is_on_friday\":\n                check = check.is_on_friday(field, pct=threshold)\n\n            case \"is_on_saturday\":\n                check = check.is_on_saturday(field, pct=threshold)\n\n            case \"is_on_sunday\":\n                check = check.is_on_sunday(field, pct=threshold)\n\n            case \"satisfies\":\n                predicate = rule[\"value\"]\n                check = check.satisfies(field, predicate, pct=threshold)\n\n            case _:\n                warnings.warn(f\"Unknown rule name: {rule_name}, {field}\")\n\n    quality_check = check.validate(df)\n    return quality_check\n</code></pre>"},{"location":"api/core/#sumeh.core.summarize","title":"<code>summarize(df, rules, **context)</code>","text":"<p>Summarizes a DataFrame based on the provided rules and context.</p> <p>This function dynamically detects the appropriate engine to use for summarization based on the type of the input DataFrame. It delegates the summarization process to the corresponding engine module.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <p>The input DataFrame to be summarized. The type of the DataFrame determines the engine used for summarization.</p> required <code>rules</code> <code>list[dict]</code> <p>A list of dictionaries defining the summarization rules. Each dictionary specifies the operations or transformations to be applied.</p> required <code>**context</code> <p>Additional context parameters required by specific engines. Common parameters include: - conn: A database connection object (used by certain engines like DuckDB). - total_rows: The total number of rows in the DataFrame (optional).</p> <code>{}</code> <p>Returns:</p> Type Description <p>The summarized DataFrame as processed by the appropriate engine.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the type of the input DataFrame is unsupported.</p> Notes <ul> <li>The function uses the <code>__detect_engine</code> method to determine the engine name   based on the input DataFrame.</li> <li>Supported engines are dynamically imported from the <code>sumeh.engine</code> package.</li> <li>The \"duckdb_engine\" case requires a database connection (<code>conn</code>) to be passed   in the context.</li> </ul> Example <p>summarized_df = summarize(df, rules=[{\"operation\": \"sum\", \"column\": \"sales\"}], conn=my_conn)</p> Source code in <code>sumeh/core.py</code> <pre><code>def summarize(df, rules: list[dict], **context):\n    \"\"\"\n    Summarizes a DataFrame based on the provided rules and context.\n\n    This function dynamically detects the appropriate engine to use for summarization\n    based on the type of the input DataFrame. It delegates the summarization process\n    to the corresponding engine module.\n\n    Args:\n        df: The input DataFrame to be summarized. The type of the DataFrame determines\n            the engine used for summarization.\n        rules (list[dict]): A list of dictionaries defining the summarization rules.\n            Each dictionary specifies the operations or transformations to be applied.\n        **context: Additional context parameters required by specific engines. Common\n            parameters include:\n            - conn: A database connection object (used by certain engines like DuckDB).\n            - total_rows: The total number of rows in the DataFrame (optional).\n\n    Returns:\n        The summarized DataFrame as processed by the appropriate engine.\n\n    Raises:\n        TypeError: If the type of the input DataFrame is unsupported.\n\n    Notes:\n        - The function uses the `__detect_engine` method to determine the engine name\n          based on the input DataFrame.\n        - Supported engines are dynamically imported from the `sumeh.engine` package.\n        - The \"duckdb_engine\" case requires a database connection (`conn`) to be passed\n          in the context.\n\n    Example:\n        &gt;&gt;&gt; summarized_df = summarize(df, rules=[{\"operation\": \"sum\", \"column\": \"sales\"}], conn=my_conn)\n    \"\"\"\n    engine_name = __detect_engine(df)\n    engine = import_module(f\"sumeh.engine.{engine_name}\")\n    match engine_name:\n        case \"duckdb_engine\":\n            return engine.summarize(\n                df_rel=df,\n                rules=rules,\n                conn=context.get(\"conn\"),\n                total_rows=context.get(\"total_rows\"),\n            )\n        case _:\n            return engine.summarize(df, rules, total_rows=context.get(\"total_rows\"))\n\n    raise TypeError(f\"Unsupported DataFrame type: {type(df)}\")\n</code></pre>"},{"location":"api/core/#sumeh.core.validate","title":"<code>validate(df, rules, **context)</code>","text":"<p>Validates a DataFrame against a set of rules using the appropriate engine.</p> <p>This function dynamically detects the engine to use based on the input DataFrame and delegates the validation process to the corresponding engine's implementation.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to be validated.</p> required <code>rules</code> <code>list or dict</code> <p>The validation rules to be applied to the DataFrame.</p> required <code>**context</code> <p>Additional context parameters that may be required by the engine. - conn (optional): A database connection object, required for certain engines   like \"duckdb_engine\".</p> <code>{}</code> <p>Returns:</p> Type Description <p>bool or dict: The result of the validation process. The return type and structure</p> <p>depend on the specific engine's implementation.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If the required engine module cannot be imported.</p> <code>AttributeError</code> <p>If the detected engine does not have a <code>validate</code> method.</p> Notes <ul> <li>The engine is dynamically determined based on the DataFrame type or other   characteristics.</li> <li>For \"duckdb_engine\", a database connection object should be provided in the   context under the key \"conn\".</li> </ul> Source code in <code>sumeh/core.py</code> <pre><code>def validate(df, rules, **context):\n    \"\"\"\n    Validates a DataFrame against a set of rules using the appropriate engine.\n\n    This function dynamically detects the engine to use based on the input\n    DataFrame and delegates the validation process to the corresponding engine's\n    implementation.\n\n    Args:\n        df (DataFrame): The input DataFrame to be validated.\n        rules (list or dict): The validation rules to be applied to the DataFrame.\n        **context: Additional context parameters that may be required by the engine.\n            - conn (optional): A database connection object, required for certain engines\n              like \"duckdb_engine\".\n\n    Returns:\n        bool or dict: The result of the validation process. The return type and structure\n        depend on the specific engine's implementation.\n\n    Raises:\n        ImportError: If the required engine module cannot be imported.\n        AttributeError: If the detected engine does not have a `validate` method.\n\n    Notes:\n        - The engine is dynamically determined based on the DataFrame type or other\n          characteristics.\n        - For \"duckdb_engine\", a database connection object should be provided in the\n          context under the key \"conn\".\n    \"\"\"\n    engine_name = __detect_engine(df)\n    engine = import_module(f\"sumeh.engine.{engine_name}\")\n\n    match engine_name:\n        case \"duckdb_engine\":\n            return engine.validate(df, rules, context.get(\"conn\"))\n        case _:\n            return engine.validate(df, rules)\n</code></pre>"},{"location":"api/core/#sumeh.core.validate_schema","title":"<code>validate_schema(df_or_conn, expected, engine, **engine_kwargs)</code>","text":"<p>Validates the schema of a given data source or connection against an expected schema.</p> <p>Parameters:</p> Name Type Description Default <code>df_or_conn</code> <code>Any</code> <p>The data source or connection to validate. This can be a DataFrame,               database connection, or other supported data structure.</p> required <code>expected</code> <code>List[Dict[str, Any]]</code> <p>A list of dictionaries defining the expected schema.                              Each dictionary should describe a column or field,                              including its name, type, and other attributes.</p> required <code>engine</code> <code>str</code> <p>The name of the engine to use for validation. This determines the           specific validation logic to apply based on the data source type.</p> required <code>**engine_kwargs</code> <p>Additional keyword arguments to pass to the engine's validation logic.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tuple[bool, List[Tuple[str, str]]]</code> <p>Tuple[bool, List[Tuple[str, str]]]: A tuple where the first element is a boolean indicating                                 whether the schema is valid, and the second element is                                 a list of tuples containing error messages for any                                 validation failures. Each tuple consists of the field                                 name and the corresponding error message.</p> Source code in <code>sumeh/core.py</code> <pre><code>def validate_schema(\n    df_or_conn: Any, expected: List[Dict[str, Any]], engine: str, **engine_kwargs\n) -&gt; Tuple[bool, List[Tuple[str, str]]]:\n    \"\"\"\n    Validates the schema of a given data source or connection against an expected schema.\n\n    Args:\n        df_or_conn (Any): The data source or connection to validate. This can be a DataFrame,\n                          database connection, or other supported data structure.\n        expected (List[Dict[str, Any]]): A list of dictionaries defining the expected schema.\n                                         Each dictionary should describe a column or field,\n                                         including its name, type, and other attributes.\n        engine (str): The name of the engine to use for validation. This determines the\n                      specific validation logic to apply based on the data source type.\n        **engine_kwargs: Additional keyword arguments to pass to the engine's validation logic.\n\n    Returns:\n        Tuple[bool, List[Tuple[str, str]]]: A tuple where the first element is a boolean indicating\n                                            whether the schema is valid, and the second element is\n                                            a list of tuples containing error messages for any\n                                            validation failures. Each tuple consists of the field\n                                            name and the corresponding error message.\n    \"\"\"\n    engine_name = __detect_engine(df_or_conn)\n    engine = import_module(f\"sumeh.engine.{engine_name}\")\n    return engine.validate_schema(df_or_conn, expected=expected, **engine_kwargs)\n</code></pre>"},{"location":"api/engine/engine-dask/","title":"Module <code>sumeh.engine.dask_engine</code>","text":"<p>This module provides a set of data quality validation functions for Dask DataFrames. It includes various checks such as completeness, uniqueness, value range, patterns, and schema validation. The module also provides utilities for summarizing validation results and schema comparison.</p> <p>Functions:</p> Name Description <code>is_positive</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>is_negative</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>is_in_millions</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>is_in_billions</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>is_t_minus_1</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>is_t_minus_2</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>is_t_minus_3</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>is_today</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>is_yesterday</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>is_on_weekday</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>is_on_weekend</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>is_on_monday</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>is_on_tuesday</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>is_on_wednesday</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>is_on_thursday</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>is_on_friday</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>is_on_saturday</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>is_on_sunday</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>is_complete</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>is_unique</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>are_complete</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>are_unique</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>is_greater_than</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>is_greater_or_equal_than</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>is_less_than</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>is_less_or_equal_than</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>is_equal</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>is_equal_than</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>is_contained_in</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>not_contained_in</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>is_between</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>has_pattern</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>is_legit</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>is_primary_key</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>is_composite_key</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>has_max</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>has_min</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>has_std</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>has_mean</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>has_sum</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>has_cardinality</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>has_infogain</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>has_entropy</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>satisfies</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>validate</code> <p>dd.DataFrame, rules: list[dict]) -&gt; tuple[dd.DataFrame, dd.DataFrame]:</p> <code>summarize</code> <p>dd.DataFrame, rules: list[dict], total_rows: int) -&gt; pd.DataFrame:</p> <code>validate_schema</code> <p>dd.DataFrame, expected: List[Dict[str, Any]]) -&gt; Tuple[bool, List[Tuple[str, str]]]:</p>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.__compare_schemas","title":"<code>__compare_schemas(actual, expected)</code>","text":"<p>Compare two lists of schema definitions and identify discrepancies.</p> <p>Parameters:</p> Name Type Description Default <code>actual</code> <code>List[SchemaDef]</code> <p>The list of actual schema definitions.</p> required <code>expected</code> <code>List[SchemaDef]</code> <p>The list of expected schema definitions.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>Tuple[bool, List[Tuple[str, str]]]: A tuple where the first element is a boolean indicating</p> <code>List[Tuple[str, str]]</code> <p>whether the schemas match (True if they match, False otherwise), and the second element</p> <code>Tuple[bool, List[Tuple[str, str]]]</code> <p>is a list of tuples describing the discrepancies. Each tuple contains: - The field name (str). - A description of the discrepancy (str), such as \"missing\", \"type mismatch\",   \"nullable but expected non-nullable\", or \"extra column\".</p> Notes <ul> <li>A field is considered \"missing\" if it exists in the expected schema but not in the actual schema.</li> <li>A \"type mismatch\" occurs if the data type of a field in the actual schema does not match   the expected data type.</li> <li>A field is considered \"nullable but expected non-nullable\" if it is nullable in the actual   schema but not nullable in the expected schema.</li> <li>An \"extra column\" is a field that exists in the actual schema but not in the expected schema.</li> </ul> Source code in <code>sumeh/services/utils.py</code> <pre><code>def __compare_schemas(\n    actual: List[SchemaDef],\n    expected: List[SchemaDef],\n) -&gt; Tuple[bool, List[Tuple[str, str]]]:\n    \"\"\"\n    Compare two lists of schema definitions and identify discrepancies.\n\n    Args:\n        actual (List[SchemaDef]): The list of actual schema definitions.\n        expected (List[SchemaDef]): The list of expected schema definitions.\n\n    Returns:\n        Tuple[bool, List[Tuple[str, str]]]: A tuple where the first element is a boolean indicating\n        whether the schemas match (True if they match, False otherwise), and the second element\n        is a list of tuples describing the discrepancies. Each tuple contains:\n            - The field name (str).\n            - A description of the discrepancy (str), such as \"missing\", \"type mismatch\",\n              \"nullable but expected non-nullable\", or \"extra column\".\n\n    Notes:\n        - A field is considered \"missing\" if it exists in the expected schema but not in the actual schema.\n        - A \"type mismatch\" occurs if the data type of a field in the actual schema does not match\n          the expected data type.\n        - A field is considered \"nullable but expected non-nullable\" if it is nullable in the actual\n          schema but not nullable in the expected schema.\n        - An \"extra column\" is a field that exists in the actual schema but not in the expected schema.\n    \"\"\"\n\n    exp_map = {c[\"field\"]: c for c in expected}\n    act_map = {c[\"field\"]: c for c in actual}\n\n    erros: List[Tuple[str, str]] = []\n\n    for fld, exp in exp_map.items():\n        if fld not in act_map:\n            erros.append((fld, \"missing\"))\n            continue\n        act = act_map[fld]\n        if act[\"data_type\"] != exp[\"data_type\"]:\n            erros.append(\n                (\n                    fld,\n                    f\"type mismatch (got {act['data_type']}, expected {exp['data_type']})\",\n                )\n            )\n\n        if act[\"nullable\"] and not exp[\"nullable\"]:\n            erros.append((fld, \"nullable but expected non-nullable\"))\n\n        if exp.get(\"max_length\") is not None:\n            pass\n\n    # 2. campos extras (se quiser)\n    extras = set(act_map) - set(exp_map)\n    for fld in extras:\n        erros.append((fld, \"extra column\"))\n\n    return len(erros) == 0, erros\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.__convert_value","title":"<code>__convert_value(value)</code>","text":"<p>Converts the provided value to the appropriate type (date, float, or int).</p> <p>Depending on the format of the input value, it will be converted to a datetime object, a floating-point number (float), or an integer (int).</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>The value to be converted, represented as a string.</p> required <p>Returns:</p> Type Description <p>Union[datetime, float, int]: The converted value, which can be a datetime object, float, or int.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the value does not match an expected format.</p> Source code in <code>sumeh/services/utils.py</code> <pre><code>def __convert_value(value):\n    \"\"\"\n    Converts the provided value to the appropriate type (date, float, or int).\n\n    Depending on the format of the input value, it will be converted to a datetime object,\n    a floating-point number (float), or an integer (int).\n\n    Args:\n        value (str): The value to be converted, represented as a string.\n\n    Returns:\n        Union[datetime, float, int]: The converted value, which can be a datetime object, float, or int.\n\n    Raises:\n        ValueError: If the value does not match an expected format.\n    \"\"\"\n    from datetime import datetime\n\n    value = value.strip()\n    try:\n        if \"-\" in value:\n            return datetime.strptime(value, \"%Y-%m-%d\")\n        else:\n            return datetime.strptime(value, \"%d/%m/%Y\")\n    except ValueError:\n        if \".\" in value:\n            return float(value)\n        return int(value)\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.__dask_schema_to_list","title":"<code>__dask_schema_to_list(df)</code>","text":"<p>Convert the schema of a Dask DataFrame into a list of dictionaries.</p> <p>Each dictionary in the resulting list represents a column in the DataFrame and contains metadata about the column, including its name, data type, nullability, and maximum length.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The Dask DataFrame whose schema is to be converted.</p> required <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List[Dict[str, Any]]: A list of dictionaries, where each dictionary</p> <code>List[Dict[str, Any]]</code> <p>contains the following keys: - \"field\" (str): The name of the column. - \"data_type\" (str): The data type of the column, converted to a lowercase string. - \"nullable\" (bool): Always set to True, indicating the column is nullable. - \"max_length\" (None): Always set to None, as maximum length is not determined.</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def __dask_schema_to_list(df: dd.DataFrame) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Convert the schema of a Dask DataFrame into a list of dictionaries.\n\n    Each dictionary in the resulting list represents a column in the DataFrame\n    and contains metadata about the column, including its name, data type,\n    nullability, and maximum length.\n\n    Args:\n        df (dd.DataFrame): The Dask DataFrame whose schema is to be converted.\n\n    Returns:\n        List[Dict[str, Any]]: A list of dictionaries, where each dictionary\n        contains the following keys:\n            - \"field\" (str): The name of the column.\n            - \"data_type\" (str): The data type of the column, converted to a lowercase string.\n            - \"nullable\" (bool): Always set to True, indicating the column is nullable.\n            - \"max_length\" (None): Always set to None, as maximum length is not determined.\n    \"\"\"\n    return [\n        {\n            \"field\": col,\n            \"data_type\": str(dtype).lower(),\n            \"nullable\": True,\n            \"max_length\": None,\n        }\n        for col, dtype in df.dtypes.items()\n    ]\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.__extract_params","title":"<code>__extract_params(rule)</code>","text":"Source code in <code>sumeh/services/utils.py</code> <pre><code>def __extract_params(rule: dict) -&gt; tuple:\n    rule_name = rule[\"check_type\"]\n    field = rule[\"field\"]\n    raw_value = rule.get(\"value\")\n    if isinstance(raw_value, str) and raw_value not in (None, \"\", \"NULL\"):\n        try:\n            value = __convert_value(raw_value)\n        except ValueError:\n            value = raw_value\n    else:\n        value = raw_value\n    value = value if value not in (None, \"\", \"NULL\") else \"\"\n    return field, rule_name, value\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine._rules_to_df","title":"<code>_rules_to_df(rules)</code>","text":"<p>Converts a list of rule dictionaries into a pandas DataFrame.</p> <p>Each rule dictionary is expected to have the following keys: - \"field\": The column(s) the rule applies to. Can be a string or a list of strings. - \"check_type\": The type of rule or check being applied. - \"threshold\" (optional): A numeric value representing the pass threshold. Defaults to 1.0 if not provided. - \"value\" (optional): Additional value associated with the rule. - \"execute\" (optional): A boolean indicating whether the rule should be executed. Defaults to True if not provided.</p> <p>Rules with \"execute\" set to False are skipped. The resulting DataFrame contains unique rows based on the combination of \"column\" and \"rule\".</p> <p>Parameters:</p> Name Type Description Default <code>rules</code> <code>list[dict]</code> <p>A list of dictionaries representing the rules.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame with the following columns: - \"column\": The column(s) the rule applies to, joined by a comma if multiple. - \"rule\": The type of rule or check being applied. - \"pass_threshold\": The numeric pass threshold for the rule. - \"value\": Additional value associated with the rule, if any.</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def _rules_to_df(rules: list[dict]) -&gt; pd.DataFrame:\n    \"\"\"\n    Converts a list of rule dictionaries into a pandas DataFrame.\n\n    Each rule dictionary is expected to have the following keys:\n    - \"field\": The column(s) the rule applies to. Can be a string or a list of strings.\n    - \"check_type\": The type of rule or check being applied.\n    - \"threshold\" (optional): A numeric value representing the pass threshold. Defaults to 1.0 if not provided.\n    - \"value\" (optional): Additional value associated with the rule.\n    - \"execute\" (optional): A boolean indicating whether the rule should be executed. Defaults to True if not provided.\n\n    Rules with \"execute\" set to False are skipped. The resulting DataFrame contains unique rows based on the combination\n    of \"column\" and \"rule\".\n\n    Args:\n        rules (list[dict]): A list of dictionaries representing the rules.\n\n    Returns:\n        pd.DataFrame: A DataFrame with the following columns:\n            - \"column\": The column(s) the rule applies to, joined by a comma if multiple.\n            - \"rule\": The type of rule or check being applied.\n            - \"pass_threshold\": The numeric pass threshold for the rule.\n            - \"value\": Additional value associated with the rule, if any.\n    \"\"\"\n    rows = []\n    for r in rules:\n        if not r.get(\"execute\", True):\n            continue\n        coln = \",\".join(r[\"field\"]) if isinstance(r[\"field\"], list) else r[\"field\"]\n        rows.append(\n            {\n                \"column\": coln.strip(),\n                \"rule\": r[\"check_type\"],\n                \"pass_threshold\": float(r.get(\"threshold\") or 1.0),\n                \"value\": r.get(\"value\") or None,\n            }\n        )\n    return pd.DataFrame(rows).drop_duplicates([\"column\", \"rule\"])\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.all_date_checks","title":"<code>all_date_checks(df, rule)</code>","text":"<p>Applies date validation checks on a Dask DataFrame based on the provided rule.</p> <p>This function serves as an alias for the <code>is_past_date</code> function, which performs checks to determine if dates in the DataFrame meet the specified criteria.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The Dask DataFrame containing the data to be validated.</p> required <code>rule</code> <code>dict</code> <p>A dictionary specifying the validation rules to be applied.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A Dask DataFrame with the results of the date validation checks.</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def all_date_checks(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Applies date validation checks on a Dask DataFrame based on the provided rule.\n\n    This function serves as an alias for the `is_past_date` function, which performs\n    checks to determine if dates in the DataFrame meet the specified criteria.\n\n    Args:\n        df (dd.DataFrame): The Dask DataFrame containing the data to be validated.\n        rule (dict): A dictionary specifying the validation rules to be applied.\n\n    Returns:\n        dd.DataFrame: A Dask DataFrame with the results of the date validation checks.\n    \"\"\"\n    return is_past_date(df, rule)\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.are_complete","title":"<code>are_complete(df, rule)</code>","text":"<p>Checks if the specified fields in a Dask DataFrame are complete (non-null) based on the provided rule and returns a DataFrame of violations.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to check for completeness.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include the fields to check, the type of check, and the expected value.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A DataFrame containing rows that violate the completeness</p> <code>DataFrame</code> <p>rule, with an additional column <code>dq_status</code> indicating the rule details</p> <code>DataFrame</code> <p>in the format \"{fields}:{check}:{value}\".</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def are_complete(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Checks if the specified fields in a Dask DataFrame are complete (non-null)\n    based on the provided rule and returns a DataFrame of violations.\n\n    Args:\n        df (dd.DataFrame): The input Dask DataFrame to check for completeness.\n        rule (dict): A dictionary containing the rule parameters. It should\n            include the fields to check, the type of check, and the expected value.\n\n    Returns:\n        dd.DataFrame: A DataFrame containing rows that violate the completeness\n        rule, with an additional column `dq_status` indicating the rule details\n        in the format \"{fields}:{check}:{value}\".\n    \"\"\"\n    fields, check, value = __extract_params(rule)\n    mask = ~reduce(operator.and_, [df[f].notnull() for f in fields])\n    viol = df[mask]\n    return viol.assign(dq_status=f\"{str(fields)}:{check}:{value}\")\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.are_unique","title":"<code>are_unique(df, rule)</code>","text":"<p>Checks if the specified fields in a Dask DataFrame contain unique combinations of values.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to include: - 'fields': A list of column names to check for uniqueness. - 'check': A string describing the type of check being performed. - 'value': A value associated with the rule (used for status reporting).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A DataFrame containing rows that violate the uniqueness rule,</p> <code>DataFrame</code> <p>with an additional column <code>dq_status</code> indicating the rule that was violated.</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def are_unique(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Checks if the specified fields in a Dask DataFrame contain unique combinations of values.\n\n    Parameters:\n        df (dd.DataFrame): The input Dask DataFrame to be checked.\n        rule (dict): A dictionary containing the rule parameters. It is expected to include:\n            - 'fields': A list of column names to check for uniqueness.\n            - 'check': A string describing the type of check being performed.\n            - 'value': A value associated with the rule (used for status reporting).\n\n    Returns:\n        dd.DataFrame: A DataFrame containing rows that violate the uniqueness rule,\n        with an additional column `dq_status` indicating the rule that was violated.\n    \"\"\"\n    fields, check, value = __extract_params(rule)\n    combo = (\n        df[fields]\n        .astype(str)\n        .apply(lambda row: \"|\".join(row.values), axis=1, meta=(\"combo\", \"object\"))\n    )\n    counts = combo.value_counts().compute()\n    dupes = counts[counts &gt; 1].index.tolist()\n    viol = df[combo.isin(dupes)]\n    return viol.assign(dq_status=f\"{str(fields)}:{check}:{value}\")\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.has_cardinality","title":"<code>has_cardinality(df, rule)</code>","text":"<p>Checks if the cardinality (number of unique values) of a specified field in a Dask DataFrame exceeds a given threshold and returns a modified DataFrame based on the result.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to evaluate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The column name to check cardinality for. - 'check' (str): A descriptive label for the check (used in the output). - 'value' (int): The maximum allowed cardinality.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: If the cardinality of the specified field exceeds the given value,</p> <code>DataFrame</code> <p>returns the original DataFrame with an additional column <code>dq_status</code> indicating</p> <code>DataFrame</code> <p>the rule violation. Otherwise, returns an empty DataFrame with the same structure</p> <code>DataFrame</code> <p>as the input DataFrame.</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def has_cardinality(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Checks if the cardinality (number of unique values) of a specified field in a Dask DataFrame\n    exceeds a given threshold and returns a modified DataFrame based on the result.\n\n    Args:\n        df (dd.DataFrame): The input Dask DataFrame to evaluate.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field' (str): The column name to check cardinality for.\n            - 'check' (str): A descriptive label for the check (used in the output).\n            - 'value' (int): The maximum allowed cardinality.\n\n    Returns:\n        dd.DataFrame: If the cardinality of the specified field exceeds the given value,\n        returns the original DataFrame with an additional column `dq_status` indicating\n        the rule violation. Otherwise, returns an empty DataFrame with the same structure\n        as the input DataFrame.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    card = df[field].nunique().compute() or 0\n    if card &gt; value:\n        return df.assign(dq_status=f\"{field}:{check}:{value}\")\n    return df.head(0).pipe(dd.from_pandas, npartitions=1)\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.has_entropy","title":"<code>has_entropy(df, rule)</code>","text":"<p>Evaluates the entropy of a specified field in a Dask DataFrame and applies a rule to determine if the entropy exceeds a given threshold. If the threshold is exceeded, a new column <code>dq_status</code> is added to the DataFrame with information about the rule violation. Otherwise, an empty DataFrame is returned.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to evaluate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - <code>field</code> (str): The column name to evaluate. - <code>check</code> (str): The type of check being performed (used for status message). - <code>value</code> (float): The threshold value for the entropy.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A DataFrame with the <code>dq_status</code> column added if the entropy exceeds the threshold,</p> <code>DataFrame</code> <p>or an empty DataFrame if the threshold is not exceeded.</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def has_entropy(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Evaluates the entropy of a specified field in a Dask DataFrame and applies a rule to determine\n    if the entropy exceeds a given threshold. If the threshold is exceeded, a new column `dq_status`\n    is added to the DataFrame with information about the rule violation. Otherwise, an empty DataFrame\n    is returned.\n\n    Args:\n        df (dd.DataFrame): The input Dask DataFrame to evaluate.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - `field` (str): The column name to evaluate.\n            - `check` (str): The type of check being performed (used for status message).\n            - `value` (float): The threshold value for the entropy.\n\n    Returns:\n        dd.DataFrame: A DataFrame with the `dq_status` column added if the entropy exceeds the threshold,\n        or an empty DataFrame if the threshold is not exceeded.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    ent = df[field].nunique().compute() or 0.0\n    if ent &gt; value:\n        return df.assign(dq_status=f\"{field}:{check}:{value}\")\n    return df.head(0).pipe(dd.from_pandas, npartitions=1)\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.has_infogain","title":"<code>has_infogain(df, rule)</code>","text":"<p>Evaluates whether a given field in a Dask DataFrame satisfies an information gain condition based on the specified rule. If the condition is met, the DataFrame is updated with a <code>dq_status</code> column indicating the rule applied. Otherwise, an empty DataFrame is returned.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to evaluate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The column name to evaluate. - 'check' (str): The type of check being performed (used for status annotation). - 'value' (float): The threshold value for the information gain.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: The original DataFrame with an added <code>dq_status</code> column if the condition</p> <code>DataFrame</code> <p>is met, or an empty DataFrame if the condition is not satisfied.</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def has_infogain(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Evaluates whether a given field in a Dask DataFrame satisfies an information gain condition\n    based on the specified rule. If the condition is met, the DataFrame is updated with a\n    `dq_status` column indicating the rule applied. Otherwise, an empty DataFrame is returned.\n\n    Args:\n        df (dd.DataFrame): The input Dask DataFrame to evaluate.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field' (str): The column name to evaluate.\n            - 'check' (str): The type of check being performed (used for status annotation).\n            - 'value' (float): The threshold value for the information gain.\n\n    Returns:\n        dd.DataFrame: The original DataFrame with an added `dq_status` column if the condition\n        is met, or an empty DataFrame if the condition is not satisfied.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    ig = df[field].nunique().compute() or 0.0\n    if ig &gt; value:\n        return df.assign(dq_status=f\"{field}:{check}:{value}\")\n    return df.head(0).pipe(dd.from_pandas, npartitions=1)\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.has_max","title":"<code>has_max(df, rule)</code>","text":"<p>Identifies rows in a Dask DataFrame where the value of a specified field exceeds a given maximum value.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The name of the column to check. - 'check': A string describing the check (e.g., 'max'). - 'value': The maximum allowable value for the specified field.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame containing only the rows that violate the rule.           An additional column <code>dq_status</code> is added to indicate the rule violation           in the format \"{field}:{check}:{value}\".</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def has_max(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Identifies rows in a Dask DataFrame where the value of a specified field exceeds a given maximum value.\n\n    Parameters:\n        df (dd.DataFrame): The input Dask DataFrame to be checked.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field': The name of the column to check.\n            - 'check': A string describing the check (e.g., 'max').\n            - 'value': The maximum allowable value for the specified field.\n\n    Returns:\n        dd.DataFrame: A new Dask DataFrame containing only the rows that violate the rule.\n                      An additional column `dq_status` is added to indicate the rule violation\n                      in the format \"{field}:{check}:{value}\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    viol = df[df[field] &gt; value]\n    return viol.assign(dq_status=f\"{field}:{check}:{value}\")\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.has_mean","title":"<code>has_mean(df, rule)</code>","text":"<p>Checks if the mean of a specified field in a Dask DataFrame satisfies a given condition.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to evaluate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule to apply. It should include: - 'field' (str): The column name to calculate the mean for. - 'check' (str): The type of check to perform (e.g., 'greater_than'). - 'value' (float): The threshold value to compare the mean against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame with an additional column <code>dq_status</code> if the mean</p> <code>DataFrame</code> <p>satisfies the condition. If the condition is not met, an empty Dask DataFrame is returned.</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def has_mean(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Checks if the mean of a specified field in a Dask DataFrame satisfies a given condition.\n\n    Parameters:\n        df (dd.DataFrame): The input Dask DataFrame to evaluate.\n        rule (dict): A dictionary containing the rule to apply. It should include:\n            - 'field' (str): The column name to calculate the mean for.\n            - 'check' (str): The type of check to perform (e.g., 'greater_than').\n            - 'value' (float): The threshold value to compare the mean against.\n\n    Returns:\n        dd.DataFrame: A new Dask DataFrame with an additional column `dq_status` if the mean\n        satisfies the condition. If the condition is not met, an empty Dask DataFrame is returned.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    mean_val = df[field].mean().compute() or 0.0\n    if mean_val &gt; value:\n        return df.assign(dq_status=f\"{field}:{check}:{value}\")\n    return df.head(0).pipe(dd.from_pandas, npartitions=1)\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.has_min","title":"<code>has_min(df, rule)</code>","text":"<p>Checks if the values in a specified field of a Dask DataFrame are greater than or equal to a given minimum value. Returns a DataFrame containing rows that violate this rule, with an additional column indicating the data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to validate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The column name to check. - 'check': The type of check being performed (e.g., 'min'). - 'value': The minimum value to compare against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A DataFrame containing rows that do not meet the minimum value</p> <code>DataFrame</code> <p>requirement, with an additional column <code>dq_status</code> indicating the rule</p> <code>DataFrame</code> <p>violation in the format \"field:check:value\".</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def has_min(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Checks if the values in a specified field of a Dask DataFrame are greater than\n    or equal to a given minimum value. Returns a DataFrame containing rows that\n    violate this rule, with an additional column indicating the data quality status.\n\n    Args:\n        df (dd.DataFrame): The input Dask DataFrame to validate.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field': The column name to check.\n            - 'check': The type of check being performed (e.g., 'min').\n            - 'value': The minimum value to compare against.\n\n    Returns:\n        dd.DataFrame: A DataFrame containing rows that do not meet the minimum value\n        requirement, with an additional column `dq_status` indicating the rule\n        violation in the format \"field:check:value\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    viol = df[df[field] &lt; value]\n    return viol.assign(dq_status=f\"{field}:{check}:{value}\")\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.has_pattern","title":"<code>has_pattern(df, rule)</code>","text":"<p>Identifies rows in a Dask DataFrame that do not match a specified pattern.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The column name in the DataFrame to apply the pattern check. - 'check': A descriptive label for the type of check being performed. - 'value': The regex pattern to match against the specified column.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A DataFrame containing rows that do not match the specified pattern.           An additional column <code>dq_status</code> is added, indicating the rule details           in the format \"{field}:{check}:{value}\".</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def has_pattern(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Identifies rows in a Dask DataFrame that do not match a specified pattern.\n\n    Args:\n        df (dd.DataFrame): The input Dask DataFrame to be checked.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field': The column name in the DataFrame to apply the pattern check.\n            - 'check': A descriptive label for the type of check being performed.\n            - 'value': The regex pattern to match against the specified column.\n\n    Returns:\n        dd.DataFrame: A DataFrame containing rows that do not match the specified pattern.\n                      An additional column `dq_status` is added, indicating the rule details\n                      in the format \"{field}:{check}:{value}\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    viol = df[~df[field].str.match(value, na=False)]\n    return viol.assign(dq_status=f\"{field}:{check}:{value}\")\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.has_std","title":"<code>has_std(df, rule)</code>","text":"<p>Checks if the standard deviation of a specified field in a Dask DataFrame exceeds a given value.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to evaluate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The name of the column to calculate the standard deviation for. - 'check' (str): A descriptive label for the check being performed. - 'value' (float): The threshold value for the standard deviation.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: - If the standard deviation of the specified field exceeds the given value,   returns the original DataFrame with an additional column <code>dq_status</code> indicating the rule details. - If the standard deviation does not exceed the value, returns an empty DataFrame with the same structure.</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def has_std(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Checks if the standard deviation of a specified field in a Dask DataFrame exceeds a given value.\n\n    Parameters:\n        df (dd.DataFrame): The input Dask DataFrame to evaluate.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field' (str): The name of the column to calculate the standard deviation for.\n            - 'check' (str): A descriptive label for the check being performed.\n            - 'value' (float): The threshold value for the standard deviation.\n\n    Returns:\n        dd.DataFrame:\n            - If the standard deviation of the specified field exceeds the given value,\n              returns the original DataFrame with an additional column `dq_status` indicating the rule details.\n            - If the standard deviation does not exceed the value, returns an empty DataFrame with the same structure.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    std_val = df[field].std().compute() or 0.0\n    if std_val &gt; value:\n        return df.assign(dq_status=f\"{field}:{check}:{value}\")\n    return df.head(0).pipe(dd.from_pandas, npartitions=1)\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.has_sum","title":"<code>has_sum(df, rule)</code>","text":"<p>Checks if the sum of a specified field in a Dask DataFrame exceeds a given value and returns a modified DataFrame with a status column if the condition is met.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to evaluate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The column name to sum. - 'check' (str): A descriptive label for the check (used in the status message). - 'value' (float): The threshold value to compare the sum against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame. If the sum exceeds the threshold, the DataFrame</p> <code>DataFrame</code> <p>will include a <code>dq_status</code> column with a status message. Otherwise, an empty</p> <code>DataFrame</code> <p>DataFrame with the same structure as the input is returned.</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def has_sum(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Checks if the sum of a specified field in a Dask DataFrame exceeds a given value\n    and returns a modified DataFrame with a status column if the condition is met.\n\n    Args:\n        df (dd.DataFrame): The input Dask DataFrame to evaluate.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field' (str): The column name to sum.\n            - 'check' (str): A descriptive label for the check (used in the status message).\n            - 'value' (float): The threshold value to compare the sum against.\n\n    Returns:\n        dd.DataFrame: A new Dask DataFrame. If the sum exceeds the threshold, the DataFrame\n        will include a `dq_status` column with a status message. Otherwise, an empty\n        DataFrame with the same structure as the input is returned.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    sum_val = df[field].sum().compute() or 0.0\n    if sum_val &gt; value:\n        return df.assign(dq_status=f\"{field}:{check}:{value}\")\n    return df.head(0).pipe(dd.from_pandas, npartitions=1)\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.is_between","title":"<code>is_between(df, rule)</code>","text":"<p>Filters a Dask DataFrame to identify rows where a specified field's value does not fall within a given range.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The column name in the DataFrame to check. - 'check': The type of check being performed (e.g., \"between\"). - 'value': A string representing the range in the format \"[lo,hi]\".</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new DataFrame containing only the rows that violate</p> <code>DataFrame</code> <p>the specified range condition. An additional column <code>dq_status</code> is</p> <code>DataFrame</code> <p>added to indicate the field, check, and value that caused the violation.</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def is_between(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Filters a Dask DataFrame to identify rows where a specified field's value\n    does not fall within a given range.\n\n    Args:\n        df (dd.DataFrame): The input Dask DataFrame to be checked.\n        rule (dict): A dictionary containing the rule parameters. It should\n            include:\n            - 'field': The column name in the DataFrame to check.\n            - 'check': The type of check being performed (e.g., \"between\").\n            - 'value': A string representing the range in the format \"[lo,hi]\".\n\n    Returns:\n        dd.DataFrame: A new DataFrame containing only the rows that violate\n        the specified range condition. An additional column `dq_status` is\n        added to indicate the field, check, and value that caused the violation.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    lo, hi = value.strip(\"[]\").split(\",\")\n    viol = df[~df[field].between(__convert_value(lo), __convert_value(hi))]\n    return viol.assign(dq_status=f\"{field}:{check}:{value}\")\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.is_complete","title":"<code>is_complete(df, rule)</code>","text":"<p>Checks for completeness of a specified field in a Dask DataFrame based on a given rule.</p> <p>This function identifies rows where the specified field is null and marks them as violations. It then assigns a data quality status to these rows in the resulting DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The Dask DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The name of the field to check for completeness. - 'check': The type of check being performed (e.g., 'is_complete'). - 'value': Additional value associated with the rule (not used in this function).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A DataFrame containing rows where the specified field is null,</p> <code>DataFrame</code> <p>with an additional column <code>dq_status</code> indicating the data quality status in the format</p> <code>DataFrame</code> <p>\"{field}:{check}:{value}\".</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def is_complete(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Checks for completeness of a specified field in a Dask DataFrame based on a given rule.\n\n    This function identifies rows where the specified field is null and marks them as violations.\n    It then assigns a data quality status to these rows in the resulting DataFrame.\n\n    Args:\n        df (dd.DataFrame): The Dask DataFrame to be checked.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field': The name of the field to check for completeness.\n            - 'check': The type of check being performed (e.g., 'is_complete').\n            - 'value': Additional value associated with the rule (not used in this function).\n\n    Returns:\n        dd.DataFrame: A DataFrame containing rows where the specified field is null,\n        with an additional column `dq_status` indicating the data quality status in the format\n        \"{field}:{check}:{value}\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    viol = df[df[field].isnull()]\n    return viol.assign(dq_status=f\"{field}:{check}:{value}\")\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.is_composite_key","title":"<code>is_composite_key(df, rule)</code>","text":"<p>Determines if the given DataFrame satisfies the composite key condition based on the provided rule.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A Dask DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary defining the composite key rule.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A Dask DataFrame indicating whether the composite key condition is met.</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def is_composite_key(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Determines if the given DataFrame satisfies the composite key condition based on the provided rule.\n\n    Args:\n        df (dd.DataFrame): A Dask DataFrame to be checked.\n        rule (dict): A dictionary defining the composite key rule.\n\n    Returns:\n        dd.DataFrame: A Dask DataFrame indicating whether the composite key condition is met.\n    \"\"\"\n    return are_unique(df, rule)\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.is_contained_in","title":"<code>is_contained_in(df, rule)</code>","text":"<p>Filters a Dask DataFrame to identify rows where the values in a specified field are not contained within a given list of allowed values.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The column name in the DataFrame to check. - 'check': The type of check being performed (e.g., \"is_contained_in\"). - 'value': A string representation of a list of allowed values (e.g., \"[value1, value2]\").</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame containing only the rows that violate the rule.</p> <code>DataFrame</code> <p>An additional column <code>dq_status</code> is added to indicate the rule violation in the format:</p> <code>DataFrame</code> <p>\"{field}:{check}:{value}\".</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def is_contained_in(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Filters a Dask DataFrame to identify rows where the values in a specified field\n    are not contained within a given list of allowed values.\n\n    Args:\n        df (dd.DataFrame): The input Dask DataFrame to be checked.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field': The column name in the DataFrame to check.\n            - 'check': The type of check being performed (e.g., \"is_contained_in\").\n            - 'value': A string representation of a list of allowed values (e.g., \"[value1, value2]\").\n\n    Returns:\n        dd.DataFrame: A new Dask DataFrame containing only the rows that violate the rule.\n        An additional column `dq_status` is added to indicate the rule violation in the format:\n        \"{field}:{check}:{value}\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    lst = [v.strip() for v in value.strip(\"[]\").split(\",\")]\n    viol = df[~df[field].isin(lst)]\n    return viol.assign(dq_status=f\"{field}:{check}:{value}\")\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.is_date_after","title":"<code>is_date_after(df, rule)</code>","text":"<p>Filters a Dask DataFrame to identify rows where a specified date field is earlier than a given reference date.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - field (str): The name of the column to check. - check (str): A descriptive label for the check (used in the   output status). - date_str (str): The reference date as a string in a format   compatible with <code>pd.Timestamp</code>.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame containing only the rows where the</p> <code>DataFrame</code> <p>specified date field is earlier than the reference date. An additional</p> <code>DataFrame</code> <p>column <code>dq_status</code> is added, which contains a string describing the</p> <code>DataFrame</code> <p>rule violation in the format <code>field:check:date_str</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the <code>rule</code> dictionary does not contain the required keys.</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def is_date_after(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Filters a Dask DataFrame to identify rows where a specified date field is\n    earlier than a given reference date.\n\n    Args:\n        df (dd.DataFrame): The input Dask DataFrame to be checked.\n        rule (dict): A dictionary containing the rule parameters. It should\n            include:\n            - field (str): The name of the column to check.\n            - check (str): A descriptive label for the check (used in the\n              output status).\n            - date_str (str): The reference date as a string in a format\n              compatible with `pd.Timestamp`.\n\n    Returns:\n        dd.DataFrame: A new Dask DataFrame containing only the rows where the\n        specified date field is earlier than the reference date. An additional\n        column `dq_status` is added, which contains a string describing the\n        rule violation in the format `field:check:date_str`.\n\n    Raises:\n        ValueError: If the `rule` dictionary does not contain the required keys.\n    \"\"\"\n    field, check, date_str = __extract_params(rule)\n    ref = pd.Timestamp(date_str)\n    col_dt = dd.to_datetime(df[field], errors=\"coerce\")\n    viol = df[col_dt &lt; ref]\n    return viol.assign(dq_status=f\"{field}:{check}:{date_str}\")\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.is_date_before","title":"<code>is_date_before(df, rule)</code>","text":"<p>Checks if the values in a specified date column of a Dask DataFrame are before a given reference date.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame containing the data to be validated.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The name of the column to check. - 'check': A descriptive string for the check (e.g., \"is_date_before\"). - 'date_str': The reference date as a string in a format parsable by pandas.Timestamp.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame containing only the rows where the date in the specified column</p> <code>DataFrame</code> <p>is after the reference date. An additional column 'dq_status' is added to indicate the validation</p> <code>DataFrame</code> <p>status in the format \"{field}:{check}:{date_str}\".</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def is_date_before(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Checks if the values in a specified date column of a Dask DataFrame are before a given reference date.\n\n    Parameters:\n        df (dd.DataFrame): The input Dask DataFrame containing the data to be validated.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field': The name of the column to check.\n            - 'check': A descriptive string for the check (e.g., \"is_date_before\").\n            - 'date_str': The reference date as a string in a format parsable by pandas.Timestamp.\n\n    Returns:\n        dd.DataFrame: A new Dask DataFrame containing only the rows where the date in the specified column\n        is after the reference date. An additional column 'dq_status' is added to indicate the validation\n        status in the format \"{field}:{check}:{date_str}\".\n    \"\"\"\n    field, check, date_str = __extract_params(rule)\n    ref = pd.Timestamp(date_str)\n    col_dt = dd.to_datetime(df[field], errors=\"coerce\")\n    viol = df[col_dt &gt; ref]\n    return viol.assign(dq_status=f\"{field}:{check}:{date_str}\")\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.is_date_between","title":"<code>is_date_between(df, rule)</code>","text":"<p>Filters a Dask DataFrame to identify rows where a date field does not fall within a specified range.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame containing the data to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The name of the column in the DataFrame to check. - 'check': A string representing the type of check (used for status annotation). - 'val': A string representing the date range in the format \"[start_date, end_date]\".</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A DataFrame containing rows where the date field does not fall within the specified range.           An additional column 'dq_status' is added to indicate the rule violation in the format           \"{field}:{check}:{val}\".</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def is_date_between(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Filters a Dask DataFrame to identify rows where a date field does not fall within a specified range.\n\n    Args:\n        df (dd.DataFrame): The input Dask DataFrame containing the data to be checked.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field': The name of the column in the DataFrame to check.\n            - 'check': A string representing the type of check (used for status annotation).\n            - 'val': A string representing the date range in the format \"[start_date, end_date]\".\n\n    Returns:\n        dd.DataFrame: A DataFrame containing rows where the date field does not fall within the specified range.\n                      An additional column 'dq_status' is added to indicate the rule violation in the format\n                      \"{field}:{check}:{val}\".\n    \"\"\"\n    field, check, val = __extract_params(rule)\n    start, end = [pd.Timestamp(v.strip()) for v in val.strip(\"[]\").split(\",\")]\n    col_dt = dd.to_datetime(df[field], errors=\"coerce\")\n    mask = (col_dt &gt;= start) &amp; (col_dt &lt;= end)\n    viol = df[~mask]\n    return viol.assign(dq_status=f\"{field}:{check}:{val}\")\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.is_equal","title":"<code>is_equal(df, rule)</code>","text":"<p>Filters a Dask DataFrame to identify rows where a specified field does not equal a given value.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The column name in the DataFrame to be checked. - 'check': The type of check to perform (expected to be 'equal' for this function). - 'value': The value to compare against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new DataFrame containing rows that violate the equality rule.           An additional column <code>dq_status</code> is added, indicating the rule details           in the format \"{field}:{check}:{value}\".</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def is_equal(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Filters a Dask DataFrame to identify rows where a specified field does not equal a given value.\n\n    Args:\n        df (dd.DataFrame): The input Dask DataFrame to be checked.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field': The column name in the DataFrame to be checked.\n            - 'check': The type of check to perform (expected to be 'equal' for this function).\n            - 'value': The value to compare against.\n\n    Returns:\n        dd.DataFrame: A new DataFrame containing rows that violate the equality rule.\n                      An additional column `dq_status` is added, indicating the rule details\n                      in the format \"{field}:{check}:{value}\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    viol = df[~df[field].eq(value)]\n    return viol.assign(dq_status=f\"{field}:{check}:{value}\")\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.is_equal_than","title":"<code>is_equal_than(df, rule)</code>","text":"<p>Filters a Dask DataFrame to identify rows where the specified field does not equal the given value.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The column name in the DataFrame to check. - 'check': The type of check being performed (expected to be 'equal' for this function). - 'value': The value to compare against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new DataFrame containing rows that violate the equality rule.           An additional column <code>dq_status</code> is added, indicating the rule details           in the format \"{field}:{check}:{value}\".</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def is_equal_than(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Filters a Dask DataFrame to identify rows where the specified field does not equal the given value.\n\n    Args:\n        df (dd.DataFrame): The input Dask DataFrame to be checked.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field': The column name in the DataFrame to check.\n            - 'check': The type of check being performed (expected to be 'equal' for this function).\n            - 'value': The value to compare against.\n\n    Returns:\n        dd.DataFrame: A new DataFrame containing rows that violate the equality rule.\n                      An additional column `dq_status` is added, indicating the rule details\n                      in the format \"{field}:{check}:{value}\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    viol = df[~df[field].eq(value)]\n    return viol.assign(dq_status=f\"{field}:{check}:{value}\")\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.is_future_date","title":"<code>is_future_date(df, rule)</code>","text":"<p>Checks for rows in a Dask DataFrame where the specified date field contains a future date.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to validate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to include: - field: The name of the column to check. - check: A descriptive label for the check (used in the output). - _: Additional parameters (ignored in this function).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame containing only the rows where the date in the specified</p> <code>DataFrame</code> <p>field is in the future. An additional column <code>dq_status</code> is added to indicate the status</p> <code>DataFrame</code> <p>of the validation in the format: \"::\". Notes <ul> <li>The function coerces the specified column to datetime format, and invalid parsing results   in NaT (Not a Time).</li> <li>Rows with NaT in the specified column are excluded from the output.</li> <li>The current date is determined using the system's local date.</li> </ul> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def is_future_date(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Checks for rows in a Dask DataFrame where the specified date field contains a future date.\n\n    Args:\n        df (dd.DataFrame): The input Dask DataFrame to validate.\n        rule (dict): A dictionary containing the rule parameters. It is expected to include:\n            - field: The name of the column to check.\n            - check: A descriptive label for the check (used in the output).\n            - _: Additional parameters (ignored in this function).\n\n    Returns:\n        dd.DataFrame: A new Dask DataFrame containing only the rows where the date in the specified\n        field is in the future. An additional column `dq_status` is added to indicate the status\n        of the validation in the format: \"&lt;field&gt;:&lt;check&gt;:&lt;current_date&gt;\".\n\n    Notes:\n        - The function coerces the specified column to datetime format, and invalid parsing results\n          in NaT (Not a Time).\n        - Rows with NaT in the specified column are excluded from the output.\n        - The current date is determined using the system's local date.\n    \"\"\"\n    field, check, _ = __extract_params(rule)\n    today = pd.Timestamp(date.today())\n    col_dt = dd.to_datetime(df[field], errors=\"coerce\")\n    viol = df[col_dt &gt; today]\n    return viol.assign(dq_status=f\"{field}:{check}:{today.date().isoformat()}\")\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.is_greater_or_equal_than","title":"<code>is_greater_or_equal_than(df, rule)</code>","text":"<p>Filters a Dask DataFrame to identify rows where a specified field's value is less than a given threshold, and annotates the resulting rows with a data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should          include the following keys:          - 'field': The column name in the DataFrame to check.          - 'check': The type of check being performed (e.g., 'greater_or_equal').          - 'value': The threshold value to compare against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame containing only the rows that           violate the rule, with an additional column <code>dq_status</code>           indicating the field, check type, and threshold value.</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def is_greater_or_equal_than(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Filters a Dask DataFrame to identify rows where a specified field's value\n    is less than a given threshold, and annotates the resulting rows with a\n    data quality status.\n\n    Args:\n        df (dd.DataFrame): The input Dask DataFrame to be checked.\n        rule (dict): A dictionary containing the rule parameters. It should\n                     include the following keys:\n                     - 'field': The column name in the DataFrame to check.\n                     - 'check': The type of check being performed (e.g., 'greater_or_equal').\n                     - 'value': The threshold value to compare against.\n\n    Returns:\n        dd.DataFrame: A new Dask DataFrame containing only the rows that\n                      violate the rule, with an additional column `dq_status`\n                      indicating the field, check type, and threshold value.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    viol = df[df[field] &lt; value]\n    return viol.assign(dq_status=f\"{field}:{check}:{value}\")\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.is_greater_than","title":"<code>is_greater_than(df, rule)</code>","text":"<p>Filters a Dask DataFrame to identify rows where the value in a specified field is greater than a given threshold and annotates the result with a data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The column name to check. - 'check' (str): The type of check being performed (e.g., 'greater_than'). - 'value' (numeric): The threshold value to compare against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A filtered DataFrame containing rows that violate the rule,</p> <code>DataFrame</code> <p>with an additional column <code>dq_status</code> indicating the rule details in the format</p> <code>DataFrame</code> <p>\"{field}:{check}:{value}\".</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def is_greater_than(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Filters a Dask DataFrame to identify rows where the value in a specified field\n    is greater than a given threshold and annotates the result with a data quality status.\n\n    Args:\n        df (dd.DataFrame): The input Dask DataFrame to be checked.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field' (str): The column name to check.\n            - 'check' (str): The type of check being performed (e.g., 'greater_than').\n            - 'value' (numeric): The threshold value to compare against.\n\n    Returns:\n        dd.DataFrame: A filtered DataFrame containing rows that violate the rule,\n        with an additional column `dq_status` indicating the rule details in the format\n        \"{field}:{check}:{value}\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    viol = df[df[field] &gt; value]\n    return viol.assign(dq_status=f\"{field}:{check}:{value}\")\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.is_in","title":"<code>is_in(df, rule)</code>","text":"<p>Checks if the specified rule is contained within the given Dask DataFrame.</p> <p>This function acts as a wrapper for the <code>is_contained_in</code> function, passing the provided DataFrame and rule to it.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The Dask DataFrame to evaluate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary representing the rule to check against the DataFrame.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A Dask DataFrame resulting from the evaluation of the rule.</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def is_in(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Checks if the specified rule is contained within the given Dask DataFrame.\n\n    This function acts as a wrapper for the `is_contained_in` function,\n    passing the provided DataFrame and rule to it.\n\n    Args:\n        df (dd.DataFrame): The Dask DataFrame to evaluate.\n        rule (dict): A dictionary representing the rule to check against the DataFrame.\n\n    Returns:\n        dd.DataFrame: A Dask DataFrame resulting from the evaluation of the rule.\n    \"\"\"\n    return is_contained_in(df, rule)\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.is_in_billions","title":"<code>is_in_billions(df, rule)</code>","text":"<p>Identifies rows in a Dask DataFrame where the value in a specified field is greater than or equal to one billion and marks them with a data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected          to include the field name, check type, and value.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A Dask DataFrame containing only the rows where the specified           field's value is greater than or equal to one billion. An           additional column <code>dq_status</code> is added, indicating the field,           check type, and value that triggered the rule.</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def is_in_billions(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Identifies rows in a Dask DataFrame where the value in a specified field\n    is greater than or equal to one billion and marks them with a data quality status.\n\n    Args:\n        df (dd.DataFrame): The input Dask DataFrame to be checked.\n        rule (dict): A dictionary containing the rule parameters. It is expected\n                     to include the field name, check type, and value.\n\n    Returns:\n        dd.DataFrame: A Dask DataFrame containing only the rows where the specified\n                      field's value is greater than or equal to one billion. An\n                      additional column `dq_status` is added, indicating the field,\n                      check type, and value that triggered the rule.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    viol = df[df[field] &lt; 1_000_000_000]\n    return viol.assign(dq_status=f\"{field}:{check}:{value}\")\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.is_in_millions","title":"<code>is_in_millions(df, rule)</code>","text":"<p>Checks if the values in a specified field of a Dask DataFrame are in the millions (greater than or equal to 1,000,000) and returns a DataFrame of violations.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to check.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to          include the field name, check type, and value.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A DataFrame containing rows where the specified field's value           is greater than or equal to 1,000,000. An additional column           <code>dq_status</code> is added to indicate the field, check, and value           that triggered the violation.</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def is_in_millions(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Checks if the values in a specified field of a Dask DataFrame are in the millions\n    (greater than or equal to 1,000,000) and returns a DataFrame of violations.\n\n    Args:\n        df (dd.DataFrame): The input Dask DataFrame to check.\n        rule (dict): A dictionary containing the rule parameters. It is expected to\n                     include the field name, check type, and value.\n\n    Returns:\n        dd.DataFrame: A DataFrame containing rows where the specified field's value\n                      is greater than or equal to 1,000,000. An additional column\n                      `dq_status` is added to indicate the field, check, and value\n                      that triggered the violation.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    viol = df[df[field] &lt; 1_000_000]\n    return viol.assign(dq_status=f\"{field}:{check}:{value}\")\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.is_legit","title":"<code>is_legit(df, rule)</code>","text":"<p>Validates a Dask DataFrame against a specified rule and returns rows that violate the rule.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The Dask DataFrame to validate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the validation rule. It should include: - 'field': The column name in the DataFrame to validate. - 'check': The type of validation check (e.g., regex, condition). - 'value': The value or pattern to validate against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new DataFrame containing rows that violate the rule, with an additional</p> <code>DataFrame</code> <p>column <code>dq_status</code> indicating the field, check, and value of the failed validation.</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def is_legit(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Validates a Dask DataFrame against a specified rule and returns rows that violate the rule.\n\n    Args:\n        df (dd.DataFrame): The Dask DataFrame to validate.\n        rule (dict): A dictionary containing the validation rule. It should include:\n            - 'field': The column name in the DataFrame to validate.\n            - 'check': The type of validation check (e.g., regex, condition).\n            - 'value': The value or pattern to validate against.\n\n    Returns:\n        dd.DataFrame: A new DataFrame containing rows that violate the rule, with an additional\n        column `dq_status` indicating the field, check, and value of the failed validation.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    s = df[field].astype(\"string\")\n    mask = s.notnull() &amp; s.str.contains(r\"^\\S+$\", na=False)\n    viol = df[~mask]\n    return viol.assign(dq_status=f\"{field}:{check}:{value}\")\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.is_less_or_equal_than","title":"<code>is_less_or_equal_than(df, rule)</code>","text":"<p>Filters a Dask DataFrame to identify rows where the value in a specified field is greater than a given threshold, violating a \"less or equal than\" rule.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The column name in the DataFrame to be checked. - 'check': The type of check being performed (e.g., \"less_or_equal_than\"). - 'value': The threshold value to compare against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new DataFrame containing only the rows that violate the rule.</p> <code>DataFrame</code> <p>An additional column <code>dq_status</code> is added to indicate the rule violation</p> <code>DataFrame</code> <p>in the format \"{field}:{check}:{value}\".</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def is_less_or_equal_than(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Filters a Dask DataFrame to identify rows where the value in a specified field\n    is greater than a given threshold, violating a \"less or equal than\" rule.\n\n    Args:\n        df (dd.DataFrame): The input Dask DataFrame to be checked.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field': The column name in the DataFrame to be checked.\n            - 'check': The type of check being performed (e.g., \"less_or_equal_than\").\n            - 'value': The threshold value to compare against.\n\n    Returns:\n        dd.DataFrame: A new DataFrame containing only the rows that violate the rule.\n        An additional column `dq_status` is added to indicate the rule violation\n        in the format \"{field}:{check}:{value}\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    viol = df[df[field] &gt; value]\n    return viol.assign(dq_status=f\"{field}:{check}:{value}\")\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.is_less_than","title":"<code>is_less_than(df, rule)</code>","text":"<p>Filters a Dask DataFrame to identify rows where the value in a specified field is greater than or equal to a given threshold, and marks them with a data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The column name to check. - 'check' (str): The type of check being performed (e.g., \"less_than\"). - 'value' (numeric): The threshold value for the check.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame containing only the rows that violate the rule,</p> <code>DataFrame</code> <p>with an additional column <code>dq_status</code> indicating the rule that was violated in the</p> <code>DataFrame</code> <p>format \"{field}:{check}:{value}\".</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def is_less_than(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Filters a Dask DataFrame to identify rows where the value in a specified field\n    is greater than or equal to a given threshold, and marks them with a data quality status.\n\n    Args:\n        df (dd.DataFrame): The input Dask DataFrame to be checked.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field' (str): The column name to check.\n            - 'check' (str): The type of check being performed (e.g., \"less_than\").\n            - 'value' (numeric): The threshold value for the check.\n\n    Returns:\n        dd.DataFrame: A new Dask DataFrame containing only the rows that violate the rule,\n        with an additional column `dq_status` indicating the rule that was violated in the\n        format \"{field}:{check}:{value}\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    viol = df[df[field] &gt;= value]\n    return viol.assign(dq_status=f\"{field}:{check}:{value}\")\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.is_negative","title":"<code>is_negative(df, rule)</code>","text":"<p>Identifies rows in a Dask DataFrame where the specified field does not satisfy a \"negative\" check.</p> <p>This function filters the DataFrame to find rows where the value in the specified field is greater than or equal to 0 (i.e., not negative). It then assigns a new column <code>dq_status</code> to indicate the rule that was violated.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - <code>field</code> (str): The name of the column to check. - <code>check</code> (str): The type of check being performed (e.g., \"negative\"). - <code>value</code> (any): The expected value or condition for the check.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame containing only the rows that violate the rule,</p> <code>DataFrame</code> <p>with an additional column <code>dq_status</code> describing the violation in the format</p> <code>DataFrame</code> <p>\"{field}:{check}:{value}\".</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def is_negative(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Identifies rows in a Dask DataFrame where the specified field does not satisfy a \"negative\" check.\n\n    This function filters the DataFrame to find rows where the value in the specified field\n    is greater than or equal to 0 (i.e., not negative). It then assigns a new column `dq_status`\n    to indicate the rule that was violated.\n\n    Args:\n        df (dd.DataFrame): The input Dask DataFrame to be checked.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - `field` (str): The name of the column to check.\n            - `check` (str): The type of check being performed (e.g., \"negative\").\n            - `value` (any): The expected value or condition for the check.\n\n    Returns:\n        dd.DataFrame: A new Dask DataFrame containing only the rows that violate the rule,\n        with an additional column `dq_status` describing the violation in the format\n        \"{field}:{check}:{value}\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    viol = df[df[field] &gt;= 0]\n    return viol.assign(dq_status=f\"{field}:{check}:{value}\")\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.is_on_friday","title":"<code>is_on_friday(df, rule)</code>","text":"<p>Filters a Dask DataFrame to identify rows where a specified date field falls on a Friday.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame containing the data to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to have the following keys: - field (str): The name of the column in the DataFrame to check. - check (str): A descriptive string for the check being performed. - value (str): A value associated with the rule, used for status annotation.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame containing only the rows where the specified</p> <code>DataFrame</code> <p>date field falls on a Friday. An additional column <code>dq_status</code> is added to the</p> <code>DataFrame</code> <p>DataFrame, containing a string in the format \"{field}:{check}:{value}\" to indicate</p> <code>DataFrame</code> <p>the rule applied.</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def is_on_friday(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Filters a Dask DataFrame to identify rows where a specified date field falls on a Friday.\n\n    Args:\n        df (dd.DataFrame): The input Dask DataFrame containing the data to be checked.\n        rule (dict): A dictionary containing the rule parameters. It is expected to have\n            the following keys:\n            - field (str): The name of the column in the DataFrame to check.\n            - check (str): A descriptive string for the check being performed.\n            - value (str): A value associated with the rule, used for status annotation.\n\n    Returns:\n        dd.DataFrame: A new Dask DataFrame containing only the rows where the specified\n        date field falls on a Friday. An additional column `dq_status` is added to the\n        DataFrame, containing a string in the format \"{field}:{check}:{value}\" to indicate\n        the rule applied.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    col_dt = dd.to_datetime(df[field], errors=\"coerce\")\n    viol = df[col_dt.dt.weekday != 4]\n    return viol.assign(dq_status=f\"{field}:{check}:{value}\")\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.is_on_monday","title":"<code>is_on_monday(df, rule)</code>","text":"<p>Filters a Dask DataFrame to identify rows where the date in a specified column falls on a Monday.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame containing the data to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to include: - 'field': The name of the column in the DataFrame to check. - 'check': A string representing the type of check (used for status assignment). - 'value': A value associated with the rule (used for status assignment).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame containing only the rows where the date in the specified</p> <code>DataFrame</code> <p>column falls on a Monday. An additional column <code>dq_status</code> is added to indicate the rule</p> <code>DataFrame</code> <p>applied in the format \"{field}:{check}:{value}\".</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def is_on_monday(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Filters a Dask DataFrame to identify rows where the date in a specified column falls on a Monday.\n\n    Args:\n        df (dd.DataFrame): The input Dask DataFrame containing the data to be checked.\n        rule (dict): A dictionary containing the rule parameters. It is expected to include:\n            - 'field': The name of the column in the DataFrame to check.\n            - 'check': A string representing the type of check (used for status assignment).\n            - 'value': A value associated with the rule (used for status assignment).\n\n    Returns:\n        dd.DataFrame: A new Dask DataFrame containing only the rows where the date in the specified\n        column falls on a Monday. An additional column `dq_status` is added to indicate the rule\n        applied in the format \"{field}:{check}:{value}\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    col_dt = dd.to_datetime(df[field], errors=\"coerce\")\n    viol = df[col_dt.dt.weekday != 0]\n    return viol.assign(dq_status=f\"{field}:{check}:{value}\")\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.is_on_saturday","title":"<code>is_on_saturday(df, rule)</code>","text":"<p>Filters a Dask DataFrame to identify rows where the date in a specified column falls on a Saturday.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame containing the data to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to include: - 'field': The name of the column in the DataFrame to check. - 'check': A string representing the type of check (used for status assignment). - 'value': A value associated with the rule (used for status assignment).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame containing only the rows where the date in the specified</p> <code>DataFrame</code> <p>column falls on a Saturday. An additional column <code>dq_status</code> is added to indicate the rule</p> <code>DataFrame</code> <p>applied in the format \"{field}:{check}:{value}\".</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def is_on_saturday(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Filters a Dask DataFrame to identify rows where the date in a specified column falls on a Saturday.\n\n    Args:\n        df (dd.DataFrame): The input Dask DataFrame containing the data to be checked.\n        rule (dict): A dictionary containing the rule parameters. It is expected to include:\n            - 'field': The name of the column in the DataFrame to check.\n            - 'check': A string representing the type of check (used for status assignment).\n            - 'value': A value associated with the rule (used for status assignment).\n\n    Returns:\n        dd.DataFrame: A new Dask DataFrame containing only the rows where the date in the specified\n        column falls on a Saturday. An additional column `dq_status` is added to indicate the rule\n        applied in the format \"{field}:{check}:{value}\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    col_dt = dd.to_datetime(df[field], errors=\"coerce\")\n    viol = df[col_dt.dt.weekday != 5]\n    return viol.assign(dq_status=f\"{field}:{check}:{value}\")\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.is_on_sunday","title":"<code>is_on_sunday(df, rule)</code>","text":"<p>Filters a Dask DataFrame to identify rows where a specified date field falls on a Sunday.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame containing the data to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to include: - field (str): The name of the column in the DataFrame to check. - check (str): A descriptive string for the check being performed. - value (str): A value associated with the rule, used for status annotation.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame containing only the rows where the specified</p> <code>DataFrame</code> <p>date field falls on a Sunday. An additional column <code>dq_status</code> is added to indicate</p> <code>DataFrame</code> <p>the rule applied in the format \"{field}:{check}:{value}\".</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def is_on_sunday(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Filters a Dask DataFrame to identify rows where a specified date field falls on a Sunday.\n\n    Args:\n        df (dd.DataFrame): The input Dask DataFrame containing the data to be checked.\n        rule (dict): A dictionary containing the rule parameters. It is expected to include:\n            - field (str): The name of the column in the DataFrame to check.\n            - check (str): A descriptive string for the check being performed.\n            - value (str): A value associated with the rule, used for status annotation.\n\n    Returns:\n        dd.DataFrame: A new Dask DataFrame containing only the rows where the specified\n        date field falls on a Sunday. An additional column `dq_status` is added to indicate\n        the rule applied in the format \"{field}:{check}:{value}\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    col_dt = dd.to_datetime(df[field], errors=\"coerce\")\n    viol = df[col_dt.dt.weekday != 6]\n    return viol.assign(dq_status=f\"{field}:{check}:{value}\")\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.is_on_thursday","title":"<code>is_on_thursday(df, rule)</code>","text":"<p>Filters a Dask DataFrame to identify rows where the specified date field falls on a Thursday.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame containing the data to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to include: - field (str): The name of the column in the DataFrame to check. - check (str): A descriptive string for the type of check being performed. - value (str): A value associated with the rule (not used in the logic but included in the output).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame containing only the rows where the specified date field</p> <code>DataFrame</code> <p>falls on a Thursday. An additional column <code>dq_status</code> is added to indicate the rule applied</p> <code>DataFrame</code> <p>in the format \"{field}:{check}:{value}\".</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def is_on_thursday(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Filters a Dask DataFrame to identify rows where the specified date field falls on a Thursday.\n\n    Args:\n        df (dd.DataFrame): The input Dask DataFrame containing the data to be checked.\n        rule (dict): A dictionary containing the rule parameters. It is expected to include:\n            - field (str): The name of the column in the DataFrame to check.\n            - check (str): A descriptive string for the type of check being performed.\n            - value (str): A value associated with the rule (not used in the logic but included in the output).\n\n    Returns:\n        dd.DataFrame: A new Dask DataFrame containing only the rows where the specified date field\n        falls on a Thursday. An additional column `dq_status` is added to indicate the rule applied\n        in the format \"{field}:{check}:{value}\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    col_dt = dd.to_datetime(df[field], errors=\"coerce\")\n    viol = df[col_dt.dt.weekday != 3]\n    return viol.assign(dq_status=f\"{field}:{check}:{value}\")\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.is_on_tuesday","title":"<code>is_on_tuesday(df, rule)</code>","text":"<p>Filters a Dask DataFrame to identify rows where the specified date field falls on a Tuesday.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame containing the data to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to include: - 'field': The name of the column in the DataFrame to check. - 'check': A string representing the type of check (used for status annotation). - 'value': A value associated with the rule (used for status annotation).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame containing only the rows where the specified date field</p> <code>DataFrame</code> <p>falls on a Tuesday. An additional column <code>dq_status</code> is added to indicate the rule applied</p> <code>DataFrame</code> <p>in the format \"{field}:{check}:{value}\".</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def is_on_tuesday(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Filters a Dask DataFrame to identify rows where the specified date field falls on a Tuesday.\n\n    Args:\n        df (dd.DataFrame): The input Dask DataFrame containing the data to be checked.\n        rule (dict): A dictionary containing the rule parameters. It is expected to include:\n            - 'field': The name of the column in the DataFrame to check.\n            - 'check': A string representing the type of check (used for status annotation).\n            - 'value': A value associated with the rule (used for status annotation).\n\n    Returns:\n        dd.DataFrame: A new Dask DataFrame containing only the rows where the specified date field\n        falls on a Tuesday. An additional column `dq_status` is added to indicate the rule applied\n        in the format \"{field}:{check}:{value}\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    col_dt = dd.to_datetime(df[field], errors=\"coerce\")\n    viol = df[col_dt.dt.weekday != 1]\n    return viol.assign(dq_status=f\"{field}:{check}:{value}\")\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.is_on_wednesday","title":"<code>is_on_wednesday(df, rule)</code>","text":"<p>Filters a Dask DataFrame to identify rows where the date in a specified column falls on a Wednesday.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame containing the data to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to include: - <code>field</code> (str): The name of the column in the DataFrame to check. - <code>check</code> (str): A descriptive string for the check being performed. - <code>value</code> (str): A value associated with the rule (not directly used in the function).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame containing only the rows where the date in the specified column</p> <code>DataFrame</code> <p>falls on a Wednesday. An additional column <code>dq_status</code> is added to indicate the rule applied in the</p> <code>DataFrame</code> <p>format <code>{field}:{check}:{value}</code>.</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def is_on_wednesday(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Filters a Dask DataFrame to identify rows where the date in a specified column falls on a Wednesday.\n\n    Args:\n        df (dd.DataFrame): The input Dask DataFrame containing the data to be checked.\n        rule (dict): A dictionary containing the rule parameters. It is expected to include:\n            - `field` (str): The name of the column in the DataFrame to check.\n            - `check` (str): A descriptive string for the check being performed.\n            - `value` (str): A value associated with the rule (not directly used in the function).\n\n    Returns:\n        dd.DataFrame: A new Dask DataFrame containing only the rows where the date in the specified column\n        falls on a Wednesday. An additional column `dq_status` is added to indicate the rule applied in the\n        format `{field}:{check}:{value}`.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    col_dt = dd.to_datetime(df[field], errors=\"coerce\")\n    viol = df[col_dt.dt.weekday != 2]\n    return viol.assign(dq_status=f\"{field}:{check}:{value}\")\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.is_on_weekday","title":"<code>is_on_weekday(df, rule)</code>","text":"<p>Filters a Dask DataFrame to include only rows where the date in the specified field falls on a weekday (Monday to Friday) and assigns a data quality status column.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame containing the data to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to have the following keys: - field (str): The name of the column in the DataFrame containing date values. - check (str): A descriptive string for the check being performed. - value (str): A value associated with the rule, used for constructing the <code>dq_status</code> column.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame containing only the rows where the date in the specified field</p> <code>DataFrame</code> <p>falls on a weekday. An additional column <code>dq_status</code> is added to indicate the rule applied.</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def is_on_weekday(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Filters a Dask DataFrame to include only rows where the date in the specified field falls on a weekday\n    (Monday to Friday) and assigns a data quality status column.\n\n    Args:\n        df (dd.DataFrame): The input Dask DataFrame containing the data to be filtered.\n        rule (dict): A dictionary containing the rule parameters. It is expected to have the following keys:\n            - field (str): The name of the column in the DataFrame containing date values.\n            - check (str): A descriptive string for the check being performed.\n            - value (str): A value associated with the rule, used for constructing the `dq_status` column.\n\n    Returns:\n        dd.DataFrame: A new Dask DataFrame containing only the rows where the date in the specified field\n        falls on a weekday. An additional column `dq_status` is added to indicate the rule applied.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    col_dt = dd.to_datetime(df[field], errors=\"coerce\")\n    dow = col_dt.dt.weekday\n    viol = df[(dow &gt;= 5) &amp; (dow &lt;= 6)]\n    return viol.assign(dq_status=f\"{field}:{check}:{value}\")\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.is_on_weekend","title":"<code>is_on_weekend(df, rule)</code>","text":"<p>Identifies rows in a Dask DataFrame where the date in a specified column falls on a weekend.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame containing the data to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to have          the following keys:          - 'field': The name of the column in the DataFrame to check.          - 'check': A string representing the type of check (used for status annotation).          - 'value': A value associated with the rule (used for status annotation).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame containing only the rows where the date in the specified           column falls on a weekend (Saturday or Sunday). An additional column <code>dq_status</code>           is added to indicate the rule applied in the format \"{field}:{check}:{value}\".</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def is_on_weekend(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Identifies rows in a Dask DataFrame where the date in a specified column falls on a weekend.\n\n    Args:\n        df (dd.DataFrame): The input Dask DataFrame containing the data to be checked.\n        rule (dict): A dictionary containing the rule parameters. It is expected to have\n                     the following keys:\n                     - 'field': The name of the column in the DataFrame to check.\n                     - 'check': A string representing the type of check (used for status annotation).\n                     - 'value': A value associated with the rule (used for status annotation).\n\n    Returns:\n        dd.DataFrame: A new Dask DataFrame containing only the rows where the date in the specified\n                      column falls on a weekend (Saturday or Sunday). An additional column `dq_status`\n                      is added to indicate the rule applied in the format \"{field}:{check}:{value}\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    col_dt = dd.to_datetime(df[field], errors=\"coerce\")\n    dow = col_dt.dt.weekday\n    viol = df[(dow &gt;= 0) &amp; (dow &lt;= 4)]\n    return viol.assign(dq_status=f\"{field}:{check}:{value}\")\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.is_past_date","title":"<code>is_past_date(df, rule)</code>","text":"<p>Checks if the values in a specified date column of a Dask DataFrame are in the past.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame containing the data to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to include          the field name to check, the check type, and additional parameters.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A Dask DataFrame containing rows where the date in the specified column           is in the past. An additional column <code>dq_status</code> is added to indicate           the field, check type, and the date of the check in the format           \"field:check:YYYY-MM-DD\".</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def is_past_date(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Checks if the values in a specified date column of a Dask DataFrame are in the past.\n\n    Args:\n        df (dd.DataFrame): The input Dask DataFrame containing the data to be checked.\n        rule (dict): A dictionary containing the rule parameters. It is expected to include\n                     the field name to check, the check type, and additional parameters.\n\n    Returns:\n        dd.DataFrame: A Dask DataFrame containing rows where the date in the specified column\n                      is in the past. An additional column `dq_status` is added to indicate\n                      the field, check type, and the date of the check in the format\n                      \"field:check:YYYY-MM-DD\".\n    \"\"\"\n    field, check, _ = __extract_params(rule)\n    today = pd.Timestamp(date.today())\n    col_dt = dd.to_datetime(df[field], errors=\"coerce\")\n    viol = df[col_dt &lt; today]\n    return viol.assign(dq_status=f\"{field}:{check}:{today.date().isoformat()}\")\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.is_positive","title":"<code>is_positive(df, rule)</code>","text":"<p>Checks if the values in a specified field of a Dask DataFrame are positive.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to validate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The name of the field to check. - 'check': The type of check being performed (e.g., \"is_positive\"). - 'value': The expected value or condition (e.g., \"0\").</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A DataFrame containing rows where the specified field has</p> <code>DataFrame</code> <p>negative values, with an additional column <code>dq_status</code> indicating the</p> <code>DataFrame</code> <p>field, check, and value that failed.</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def is_positive(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Checks if the values in a specified field of a Dask DataFrame are positive.\n\n    Args:\n        df (dd.DataFrame): The input Dask DataFrame to validate.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field': The name of the field to check.\n            - 'check': The type of check being performed (e.g., \"is_positive\").\n            - 'value': The expected value or condition (e.g., \"0\").\n\n    Returns:\n        dd.DataFrame: A DataFrame containing rows where the specified field has\n        negative values, with an additional column `dq_status` indicating the\n        field, check, and value that failed.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    viol = df[df[field] &lt; 0]\n    return viol.assign(dq_status=f\"{field}:{check}:{value}\")\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.is_primary_key","title":"<code>is_primary_key(df, rule)</code>","text":"<p>Determines if the specified rule identifies a primary key in the given Dask DataFrame.</p> <p>This function checks whether the combination of columns specified in the rule results in unique values across the DataFrame, effectively identifying a primary key.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The Dask DataFrame to evaluate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary defining the rule to check for primary key uniqueness.          Typically, this includes the column(s) to be evaluated.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A Dask DataFrame indicating whether the rule satisfies the primary key condition.</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def is_primary_key(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Determines if the specified rule identifies a primary key in the given Dask DataFrame.\n\n    This function checks whether the combination of columns specified in the rule\n    results in unique values across the DataFrame, effectively identifying a primary key.\n\n    Args:\n        df (dd.DataFrame): The Dask DataFrame to evaluate.\n        rule (dict): A dictionary defining the rule to check for primary key uniqueness.\n                     Typically, this includes the column(s) to be evaluated.\n\n    Returns:\n        dd.DataFrame: A Dask DataFrame indicating whether the rule satisfies the primary key condition.\n    \"\"\"\n    return is_unique(df, rule)\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.is_t_minus_1","title":"<code>is_t_minus_1(df, rule)</code>","text":"<p>Filters a Dask DataFrame to identify rows where a specified datetime column matches the date of \"T-1\" (yesterday) and assigns a data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame containing the data to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to include the following keys: - 'field': The name of the column to check. - 'check': A string describing the check being performed. - 'value': Additional value or metadata related to the check.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame containing only the rows where the</p> <code>DataFrame</code> <p>specified column matches \"T-1\". An additional column <code>dq_status</code> is added</p> <code>DataFrame</code> <p>to indicate the data quality status in the format \"{field}:{check}:{value}\".</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def is_t_minus_1(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Filters a Dask DataFrame to identify rows where a specified datetime column\n    matches the date of \"T-1\" (yesterday) and assigns a data quality status.\n\n    Args:\n        df (dd.DataFrame): The input Dask DataFrame containing the data to be checked.\n        rule (dict): A dictionary containing the rule parameters. It is expected\n            to include the following keys:\n            - 'field': The name of the column to check.\n            - 'check': A string describing the check being performed.\n            - 'value': Additional value or metadata related to the check.\n\n    Returns:\n        dd.DataFrame: A new Dask DataFrame containing only the rows where the\n        specified column matches \"T-1\". An additional column `dq_status` is added\n        to indicate the data quality status in the format \"{field}:{check}:{value}\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    target = pd.Timestamp(date.today() - pd.Timedelta(days=1))\n    col_dt = dd.to_datetime(df[field], errors=\"coerce\")\n    viol = df[col_dt != target]\n    return viol.assign(dq_status=f\"{field}:{check}:{value}\")\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.is_t_minus_2","title":"<code>is_t_minus_2(df, rule)</code>","text":"<p>Filters a Dask DataFrame to identify rows where a specified datetime column matches the date two days prior to the current date.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame containing the data to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to include the following keys: - 'field': The name of the column in the DataFrame to check. - 'check': A string representing the type of check (used for metadata). - 'value': A value associated with the rule (used for metadata).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame containing only the rows where the specified</p> <code>DataFrame</code> <p>column matches the target date (two days prior to the current date). An additional</p> <code>DataFrame</code> <p>column <code>dq_status</code> is added to indicate the rule applied in the format</p> <code>DataFrame</code> <p>\"{field}:{check}:{value}\".</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def is_t_minus_2(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Filters a Dask DataFrame to identify rows where a specified datetime column\n    matches the date two days prior to the current date.\n\n    Args:\n        df (dd.DataFrame): The input Dask DataFrame containing the data to be filtered.\n        rule (dict): A dictionary containing the rule parameters. It is expected to\n            include the following keys:\n            - 'field': The name of the column in the DataFrame to check.\n            - 'check': A string representing the type of check (used for metadata).\n            - 'value': A value associated with the rule (used for metadata).\n\n    Returns:\n        dd.DataFrame: A new Dask DataFrame containing only the rows where the specified\n        column matches the target date (two days prior to the current date). An additional\n        column `dq_status` is added to indicate the rule applied in the format\n        \"{field}:{check}:{value}\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    target = pd.Timestamp(date.today() - pd.Timedelta(days=2))\n    col_dt = dd.to_datetime(df[field], errors=\"coerce\")\n    viol = df[col_dt != target]\n    return viol.assign(dq_status=f\"{field}:{check}:{value}\")\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.is_t_minus_3","title":"<code>is_t_minus_3(df, rule)</code>","text":"<p>Filters a Dask DataFrame to identify rows where the specified date field matches exactly three days prior to the current date.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame containing the data to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing rule parameters. It is expected to include          the field name to check, the type of check, and the value (unused in this function).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A filtered Dask DataFrame containing only the rows where the specified           date field matches three days prior to the current date. An additional           column <code>dq_status</code> is added to indicate the rule applied in the format           \"{field}:{check}:{value}\".</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def is_t_minus_3(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Filters a Dask DataFrame to identify rows where the specified date field matches\n    exactly three days prior to the current date.\n\n    Args:\n        df (dd.DataFrame): The input Dask DataFrame containing the data to be checked.\n        rule (dict): A dictionary containing rule parameters. It is expected to include\n                     the field name to check, the type of check, and the value (unused in this function).\n\n    Returns:\n        dd.DataFrame: A filtered Dask DataFrame containing only the rows where the specified\n                      date field matches three days prior to the current date. An additional\n                      column `dq_status` is added to indicate the rule applied in the format\n                      \"{field}:{check}:{value}\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    target = pd.Timestamp(date.today() - pd.Timedelta(days=3))\n    col_dt = dd.to_datetime(df[field], errors=\"coerce\")\n    viol = df[col_dt != target]\n    return viol.assign(dq_status=f\"{field}:{check}:{value}\")\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.is_today","title":"<code>is_today(df, rule)</code>","text":"<p>Filters a Dask DataFrame to identify rows where the specified field matches today's date.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to have the following keys: - field (str): The name of the column in the DataFrame to check. - check (str): A descriptive label for the type of check being performed. - value (str): A descriptive label for the expected value.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame containing only the rows where the specified</p> <code>DataFrame</code> <p>field matches today's date. An additional column <code>dq_status</code> is added to indicate</p> <code>DataFrame</code> <p>the rule applied in the format \"{field}:{check}:{value}\".</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def is_today(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Filters a Dask DataFrame to identify rows where the specified field matches today's date.\n\n    Args:\n        df (dd.DataFrame): The input Dask DataFrame to be filtered.\n        rule (dict): A dictionary containing the rule parameters. It is expected to have\n            the following keys:\n            - field (str): The name of the column in the DataFrame to check.\n            - check (str): A descriptive label for the type of check being performed.\n            - value (str): A descriptive label for the expected value.\n\n    Returns:\n        dd.DataFrame: A new Dask DataFrame containing only the rows where the specified\n        field matches today's date. An additional column `dq_status` is added to indicate\n        the rule applied in the format \"{field}:{check}:{value}\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    target = pd.Timestamp(date.today())\n    col_dt = dd.to_datetime(df[field], errors=\"coerce\")\n    viol = df[col_dt != target]\n    return viol.assign(dq_status=f\"{field}:{check}:{value}\")\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.is_unique","title":"<code>is_unique(df, rule)</code>","text":"<p>Checks for uniqueness of a specified field in a Dask DataFrame based on a given rule.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The Dask DataFrame to check for uniqueness.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to include: - 'field': The column name to check for uniqueness. - 'check': The type of check being performed (e.g., \"unique\"). - 'value': Additional value or metadata related to the check.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A DataFrame containing rows that violate the uniqueness rule,</p> <code>DataFrame</code> <p>with an additional column <code>dq_status</code> indicating the rule that was violated</p> <code>DataFrame</code> <p>in the format \"{field}:{check}:{value}\".</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def is_unique(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Checks for uniqueness of a specified field in a Dask DataFrame based on a given rule.\n\n    Parameters:\n        df (dd.DataFrame): The Dask DataFrame to check for uniqueness.\n        rule (dict): A dictionary containing the rule parameters. It is expected to include:\n            - 'field': The column name to check for uniqueness.\n            - 'check': The type of check being performed (e.g., \"unique\").\n            - 'value': Additional value or metadata related to the check.\n\n    Returns:\n        dd.DataFrame: A DataFrame containing rows that violate the uniqueness rule,\n        with an additional column `dq_status` indicating the rule that was violated\n        in the format \"{field}:{check}:{value}\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    counts = df[field].value_counts().compute()\n    dup_vals = counts[counts &gt; 1].index.tolist()\n    viol = df[df[field].isin(dup_vals)]\n    return viol.assign(dq_status=f\"{field}:{check}:{value}\")\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.is_yesterday","title":"<code>is_yesterday(df, rule)</code>","text":"<p>Determines if the rows in a Dask DataFrame correspond to \"yesterday\" based on a given rule.</p> <p>This function acts as a wrapper for the <code>is_t_minus_1</code> function, applying the same logic to check if the data corresponds to the previous day.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to evaluate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule or criteria          to determine \"yesterday\".</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A Dask DataFrame with the evaluation results.</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def is_yesterday(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Determines if the rows in a Dask DataFrame correspond to \"yesterday\"\n    based on a given rule.\n\n    This function acts as a wrapper for the `is_t_minus_1` function,\n    applying the same logic to check if the data corresponds to the\n    previous day.\n\n    Args:\n        df (dd.DataFrame): The input Dask DataFrame to evaluate.\n        rule (dict): A dictionary containing the rule or criteria\n                     to determine \"yesterday\".\n\n    Returns:\n        dd.DataFrame: A Dask DataFrame with the evaluation results.\n    \"\"\"\n    return is_t_minus_1(df, rule)\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.not_contained_in","title":"<code>not_contained_in(df, rule)</code>","text":"<p>Filters a Dask DataFrame to identify rows where the specified field's value is contained in a given list, and assigns a data quality status to the resulting rows.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The column name in the DataFrame to check. - 'check': The type of check being performed (e.g., \"not_contained_in\"). - 'value': A string representation of a list of values to check against,   formatted as \"[value1, value2, ...]\".</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new DataFrame containing only the rows where the specified</p> <code>DataFrame</code> <p>field's value is in the provided list, with an additional column <code>dq_status</code></p> <code>DataFrame</code> <p>indicating the rule applied in the format \"field:check:value\".</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def not_contained_in(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Filters a Dask DataFrame to identify rows where the specified field's value is\n    contained in a given list, and assigns a data quality status to the resulting rows.\n\n    Args:\n        df (dd.DataFrame): The input Dask DataFrame to be filtered.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field': The column name in the DataFrame to check.\n            - 'check': The type of check being performed (e.g., \"not_contained_in\").\n            - 'value': A string representation of a list of values to check against,\n              formatted as \"[value1, value2, ...]\".\n\n    Returns:\n        dd.DataFrame: A new DataFrame containing only the rows where the specified\n        field's value is in the provided list, with an additional column `dq_status`\n        indicating the rule applied in the format \"field:check:value\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    lst = [v.strip() for v in value.strip(\"[]\").split(\",\")]\n    viol = df[df[field].isin(lst)]\n    return viol.assign(dq_status=f\"{field}:{check}:{value}\")\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.not_in","title":"<code>not_in(df, rule)</code>","text":"<p>Filters a Dask DataFrame by excluding rows where the specified rule is satisfied.</p> <p>This function delegates the filtering logic to the <code>not_contained_in</code> function.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary defining the filtering rule. The structure and          interpretation of this rule depend on the implementation of          <code>not_contained_in</code>.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame with rows excluded based on the rule.</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def not_in(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Filters a Dask DataFrame by excluding rows where the specified rule is satisfied.\n\n    This function delegates the filtering logic to the `not_contained_in` function.\n\n    Args:\n        df (dd.DataFrame): The input Dask DataFrame to be filtered.\n        rule (dict): A dictionary defining the filtering rule. The structure and\n                     interpretation of this rule depend on the implementation of\n                     `not_contained_in`.\n\n    Returns:\n        dd.DataFrame: A new Dask DataFrame with rows excluded based on the rule.\n    \"\"\"\n    return not_contained_in(df, rule)\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.satisfies","title":"<code>satisfies(df, rule)</code>","text":"<p>Filters a Dask DataFrame based on a rule and returns rows that do not satisfy the rule.</p> <p>The function evaluates a rule on the given Dask DataFrame and identifies rows that violate the rule. The rule is specified as a dictionary containing a field, a check, and a value. The rule's logical expression is converted to Python syntax for evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary specifying the rule to evaluate. It should contain: - 'field': The column name in the DataFrame to evaluate. - 'check': The type of check or condition to apply. - 'value': The value or expression to evaluate against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame containing rows that do not satisfy the rule.</p> <code>DataFrame</code> <p>An additional column <code>dq_status</code> is added, which contains a string in the format</p> <code>DataFrame</code> <p>\"{field}:{check}:{value}\" to indicate the rule that was violated.</p> Example <p>import dask.dataframe as dd data = {'col1': [1, 2, 3], 'col2': [4, 5, 6]} df = dd.from_pandas(pd.DataFrame(data), npartitions=1) rule = {'field': 'col1', 'check': '&gt;', 'value': '2'} result = satisfies(df, rule) result.compute()</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def satisfies(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Filters a Dask DataFrame based on a rule and returns rows that do not satisfy the rule.\n\n    The function evaluates a rule on the given Dask DataFrame and identifies rows that\n    violate the rule. The rule is specified as a dictionary containing a field, a check,\n    and a value. The rule's logical expression is converted to Python syntax for evaluation.\n\n    Args:\n        df (dd.DataFrame): The input Dask DataFrame to be filtered.\n        rule (dict): A dictionary specifying the rule to evaluate. It should contain:\n            - 'field': The column name in the DataFrame to evaluate.\n            - 'check': The type of check or condition to apply.\n            - 'value': The value or expression to evaluate against.\n\n    Returns:\n        dd.DataFrame: A new Dask DataFrame containing rows that do not satisfy the rule.\n        An additional column `dq_status` is added, which contains a string in the format\n        \"{field}:{check}:{value}\" to indicate the rule that was violated.\n\n    Example:\n        &gt;&gt;&gt; import dask.dataframe as dd\n        &gt;&gt;&gt; data = {'col1': [1, 2, 3], 'col2': [4, 5, 6]}\n        &gt;&gt;&gt; df = dd.from_pandas(pd.DataFrame(data), npartitions=1)\n        &gt;&gt;&gt; rule = {'field': 'col1', 'check': '&gt;', 'value': '2'}\n        &gt;&gt;&gt; result = satisfies(df, rule)\n        &gt;&gt;&gt; result.compute()\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    py_expr = value\n    py_expr = re.sub(r\"(?&lt;![=!&lt;&gt;])=(?!=)\", \"==\", py_expr)\n    py_expr = re.sub(r\"\\bAND\\b\", \"&amp;\", py_expr, flags=re.IGNORECASE)\n    py_expr = re.sub(r\"\\bOR\\b\", \"|\", py_expr, flags=re.IGNORECASE)\n\n    def _filter_viol(pdf: pd.DataFrame) -&gt; pd.DataFrame:\n        mask = pdf.eval(py_expr)\n        return pdf.loc[~mask]\n\n    meta = df._meta\n    viol = df.map_partitions(_filter_viol, meta=meta)\n    return viol.assign(dq_status=f\"{field}:{check}:{value}\")\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.summarize","title":"<code>summarize(qc_ddf, rules, total_rows)</code>","text":"<p>Summarizes quality check results by evaluating rules against a Dask DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>qc_ddf</code> <code>DataFrame</code> <p>A Dask DataFrame containing quality check results. The DataFrame must include a \"dq_status\" column with rule violations in the format \"column:rule:value\".</p> required <code>rules</code> <code>list[dict]</code> <p>A list of dictionaries representing the rules to be evaluated. Each dictionary should include keys such as \"column\", \"rule\", \"value\", and \"pass_threshold\".</p> required <code>total_rows</code> <code>int</code> <p>The total number of rows in the original dataset.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A summarized Pandas DataFrame containing the following columns: - id: Unique identifier for each rule evaluation. - timestamp: Timestamp of the summary generation. - check: The type of check performed (e.g., \"Quality Check\"). - level: The severity level of the check (e.g., \"WARNING\"). - column: The column name associated with the rule. - rule: The rule being evaluated. - value: The value associated with the rule. - rows: The total number of rows in the dataset. - violations: The number of rows that violated the rule. - pass_rate: The proportion of rows that passed the rule. - pass_threshold: The threshold for passing the rule. - status: The status of the rule evaluation (\"PASS\" or \"FAIL\").</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def summarize(qc_ddf: dd.DataFrame, rules: list[dict], total_rows: int) -&gt; pd.DataFrame:\n    \"\"\"\n    Summarizes quality check results by evaluating rules against a Dask DataFrame.\n\n    Args:\n        qc_ddf (dd.DataFrame): A Dask DataFrame containing quality check results.\n            The DataFrame must include a \"dq_status\" column with rule violations\n            in the format \"column:rule:value\".\n        rules (list[dict]): A list of dictionaries representing the rules to be\n            evaluated. Each dictionary should include keys such as \"column\",\n            \"rule\", \"value\", and \"pass_threshold\".\n        total_rows (int): The total number of rows in the original dataset.\n\n    Returns:\n        pd.DataFrame: A summarized Pandas DataFrame containing the following columns:\n            - id: Unique identifier for each rule evaluation.\n            - timestamp: Timestamp of the summary generation.\n            - check: The type of check performed (e.g., \"Quality Check\").\n            - level: The severity level of the check (e.g., \"WARNING\").\n            - column: The column name associated with the rule.\n            - rule: The rule being evaluated.\n            - value: The value associated with the rule.\n            - rows: The total number of rows in the dataset.\n            - violations: The number of rows that violated the rule.\n            - pass_rate: The proportion of rows that passed the rule.\n            - pass_threshold: The threshold for passing the rule.\n            - status: The status of the rule evaluation (\"PASS\" or \"FAIL\").\n    \"\"\"\n    df = qc_ddf.compute()\n\n    df = df[df[\"dq_status\"].astype(bool)]\n    split = df[\"dq_status\"].str.split(\":\", expand=True)\n    split.columns = [\"column\", \"rule\", \"value\"]\n    viol_count = (\n        split.groupby([\"column\", \"rule\", \"value\"], dropna=False)\n        .size()\n        .reset_index(name=\"violations\")\n    )\n\n    rules_df = _rules_to_df(rules)\n\n    rules_df[\"value\"] = rules_df[\"value\"].fillna(\"\")\n    viol_count[\"value\"] = viol_count[\"value\"].fillna(\"\")\n\n    summary = (\n        rules_df.merge(viol_count, on=[\"column\", \"rule\", \"value\"], how=\"left\")\n        .assign(\n            violations=lambda df: df[\"violations\"].fillna(0).astype(int),\n            rows=total_rows,\n            pass_rate=lambda df: (total_rows - df[\"violations\"]) / total_rows,\n            status=lambda df: np.where(\n                df[\"pass_rate\"] &gt;= df[\"pass_threshold\"], \"PASS\", \"FAIL\"\n            ),\n            timestamp=datetime.now().replace(second=0, microsecond=0),\n            check=\"Quality Check\",\n            level=\"WARNING\",\n        )\n        .reset_index(drop=True)\n    )\n\n    summary.insert(0, \"id\", [str(uuid.uuid4()) for _ in range(len(summary))])\n\n    summary = summary[\n        [\n            \"id\",\n            \"timestamp\",\n            \"check\",\n            \"level\",\n            \"column\",\n            \"rule\",\n            \"value\",\n            \"rows\",\n            \"violations\",\n            \"pass_rate\",\n            \"pass_threshold\",\n            \"status\",\n        ]\n    ]\n\n    return dd.from_pandas(summary, npartitions=1)\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.validate","title":"<code>validate(df, rules)</code>","text":"<p>Validate a Dask DataFrame against a set of rules and return the aggregated results and raw violations.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to validate.</p> required <code>rules</code> <code>list[dict]</code> <p>A list of validation rules. Each rule is a dictionary containing the following keys: - \"check_type\" (str): The name of the validation function to execute. - \"value\" (optional): The value to be used in the validation function. - \"execute\" (optional, bool): Whether to execute the rule. Defaults to True.</p> required <p>Returns:</p> Type Description <code>tuple[DataFrame, DataFrame]</code> <p>tuple[dd.DataFrame, dd.DataFrame]: - The first DataFrame contains the aggregated validation results,   with a concatenated \"dq_status\" column indicating the validation status. - The second DataFrame contains the raw violations for each rule.</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def validate(df: dd.DataFrame, rules: list[dict]) -&gt; tuple[dd.DataFrame, dd.DataFrame]:\n    \"\"\"\n    Validate a Dask DataFrame against a set of rules and return the aggregated results\n    and raw violations.\n\n    Args:\n        df (dd.DataFrame): The input Dask DataFrame to validate.\n        rules (list[dict]): A list of validation rules. Each rule is a dictionary\n            containing the following keys:\n            - \"check_type\" (str): The name of the validation function to execute.\n            - \"value\" (optional): The value to be used in the validation function.\n            - \"execute\" (optional, bool): Whether to execute the rule. Defaults to True.\n\n    Returns:\n        tuple[dd.DataFrame, dd.DataFrame]:\n            - The first DataFrame contains the aggregated validation results,\n              with a concatenated \"dq_status\" column indicating the validation status.\n            - The second DataFrame contains the raw violations for each rule.\n    \"\"\"\n    empty = dd.from_pandas(\n        pd.DataFrame(columns=df.columns.tolist() + [\"dq_status\"]), npartitions=1\n    )\n    raw_df = empty\n\n    for rule in rules:\n        if not rule.get(\"execute\", True):\n            continue\n        rule_name = rule[\"check_type\"]\n        func = globals().get(rule_name)\n        if func is None:\n            warnings.warn(f\"Unknown rule: {rule_name}\")\n            continue\n\n        raw_val = rule.get(\"value\")\n        try:\n            value = (\n                __convert_value(raw_val)\n                if isinstance(raw_val, str) and raw_val not in (\"\", \"NULL\")\n                else raw_val\n            )\n        except ValueError:\n            value = raw_val\n\n        viol = func(df, rule)\n        raw_df = dd.concat([raw_df, viol], interleave_partitions=True)\n\n    group_cols = [c for c in df.columns if c != \"dq_status\"]\n\n    def _concat_status(series: pd.Series) -&gt; str:\n        return \";\".join([s for s in series.astype(str) if s])\n\n    agg_df = (\n        raw_df.groupby(group_cols)\n        .dq_status.apply(_concat_status, meta=(\"dq_status\", \"object\"))\n        .reset_index()\n    )\n\n    return agg_df, raw_df\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.validate_date_format","title":"<code>validate_date_format(df, rule)</code>","text":"<p>Validates the date format of a specified column in a Dask DataFrame.</p> <p>This function checks whether the values in a specified column of the DataFrame conform to a given date format. Rows with invalid date formats are returned with an additional column indicating the validation status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The Dask DataFrame to validate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the validation rule. It should          include the following keys:          - 'field': The name of the column to validate.          - 'check': A string describing the validation check.          - 'fmt': The expected date format (e.g., '%Y-%m-%d').</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A DataFrame containing rows where the date format           validation failed. An additional column <code>dq_status</code>           is added, which contains a string describing the           validation status in the format \"{field}:{check}:{fmt}\".</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def validate_date_format(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Validates the date format of a specified column in a Dask DataFrame.\n\n    This function checks whether the values in a specified column of the\n    DataFrame conform to a given date format. Rows with invalid date formats\n    are returned with an additional column indicating the validation status.\n\n    Args:\n        df (dd.DataFrame): The Dask DataFrame to validate.\n        rule (dict): A dictionary containing the validation rule. It should\n                     include the following keys:\n                     - 'field': The name of the column to validate.\n                     - 'check': A string describing the validation check.\n                     - 'fmt': The expected date format (e.g., '%Y-%m-%d').\n\n    Returns:\n        dd.DataFrame: A DataFrame containing rows where the date format\n                      validation failed. An additional column `dq_status`\n                      is added, which contains a string describing the\n                      validation status in the format \"{field}:{check}:{fmt}\".\n    \"\"\"\n    field, check, fmt = __extract_params(rule)\n    col_dt = dd.to_datetime(df[field], format=fmt, errors=\"coerce\")\n    viol = df[col_dt.isna()]\n    return viol.assign(dq_status=f\"{field}:{check}:{fmt}\")\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.validate_schema","title":"<code>validate_schema(df, expected)</code>","text":"<p>Validates the schema of a Dask DataFrame against an expected schema.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The Dask DataFrame whose schema is to be validated.</p> required <code>expected</code> <code>List[Dict[str, Any]]</code> <p>A list of dictionaries representing the expected schema. Each dictionary should define the expected column name and its properties.</p> required <p>Returns:</p> Type Description <code>Tuple[bool, List[Tuple[str, str]]]</code> <p>Tuple[bool, List[Tuple[str, str]]]: A tuple where the first element is a boolean indicating whether the schema matches the expected schema, and the second element is a list of tuples containing mismatched column names and their respective issues.</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def validate_schema(\n    df: dd.DataFrame, expected: List[Dict[str, Any]]\n) -&gt; Tuple[bool, List[Tuple[str, str]]]:\n    \"\"\"\n    Validates the schema of a Dask DataFrame against an expected schema.\n\n    Args:\n        df (dd.DataFrame): The Dask DataFrame whose schema is to be validated.\n        expected (List[Dict[str, Any]]): A list of dictionaries representing the expected schema.\n            Each dictionary should define the expected column name and its properties.\n\n    Returns:\n        Tuple[bool, List[Tuple[str, str]]]: A tuple where the first element is a boolean indicating\n            whether the schema matches the expected schema, and the second element is a list of\n            tuples containing mismatched column names and their respective issues.\n    \"\"\"\n    actual = __dask_schema_to_list(df)\n    return __compare_schemas(actual, expected)\n</code></pre>"},{"location":"api/engine/engine-duckdb/","title":"Module <code>sumeh.engine.duckdb_engine</code>","text":"<p>This module provides utilities for generating and validating SQL expressions and data quality rules using DuckDB. It includes functions for building SQL expressions, validating dataframes against rules, summarizing rule violations, and schema validation.</p> <p>Classes:</p> Name Description <code>__RuleCtx</code> <p>A dataclass representing the context required to generate SQL       expressions for data quality rules.</p> <p>Functions:</p> Name Description <code>__escape_single_quotes</code> <p>Escapes single quotes in a string for SQL compatibility.</p> <code>__format_sequence</code> <p>Formats a sequence (list, tuple, or string) into a SQL-compatible representation for IN/NOT IN clauses.</p> <code>_is_complete</code> <p>Generates a SQL expression to check if a column is not NULL.</p> <code>_are_complete</code> <p>Generates a SQL expression to check if all columns in a list are not NULL.</p> <code>_is_unique</code> <p>Generates a SQL expression to check if a column has unique values.</p> <code>_are_unique</code> <p>Generates a SQL expression to check if a combination of columns has unique values.</p> <code>_is_greater_than</code> <p>Generates a SQL expression to check if a column's value is greater than a given value.</p> <code>_is_less_than</code> <p>Generates a SQL expression to check if a column's value is less than a given value.</p> <code>_is_greater_or_equal_than</code> <p>Generates a SQL expression to check if a column's value is greater than or equal to a given value.</p> <code>_is_less_or_equal_than</code> <p>Generates a SQL expression to check if a column's value is less than or equal to a given value.</p> <code>_is_equal_than</code> <p>Generates a SQL expression to check if a column's value is equal to a given value.</p> <code>_is_between</code> <p>Generates a SQL expression to check if a column's value is between two values.</p> <code>_has_pattern</code> <p>Generates a SQL expression to check if a column's value matches a regular expression pattern.</p> <code>_is_contained_in</code> <p>Generates a SQL expression to check if a column's value is in a given sequence.</p> <code>_not_contained_in</code> <p>Generates a SQL expression to check if a column's value is not in a given sequence.</p> <code>_satisfies</code> <p>Generates a SQL expression based on a custom condition provided as a string.</p> <code>_build_union_sql</code> <p>Builds a SQL query that combines multiple rule-based conditions into a UNION ALL query.</p> <code>validate</code> <p>Validates a DuckDB dataframe against a set of rules and returns the results.</p> <code>summarize</code> <p>Summarizes rule violations and calculates pass rates for each rule.</p> <code>validate_schema</code> <p>Validates the schema of a DuckDB table against an expected schema.</p>"},{"location":"api/engine/engine-duckdb/#sumeh.engine.duckdb_engine.__RULE_DISPATCH","title":"<code>__RULE_DISPATCH = {'is_complete': _is_complete, 'are_complete': _are_complete, 'is_unique': _is_unique, 'are_unique': _are_unique, 'is_greater_than': _is_greater_than, 'is_less_than': _is_less_than, 'is_greater_or_equal_than': _is_greater_or_equal_than, 'is_less_or_equal_than': _is_less_or_equal_than, 'is_equal_than': _is_equal_than, 'is_in_millions': _is_in_millions, 'is_in_billions': _is_in_billions, 'is_between': _is_between, 'has_pattern': _has_pattern, 'is_contained_in': _is_contained_in, 'not_contained_in': _not_contained_in, 'satisfies': _satisfies, 'validate_date_format': _validate_date_format, 'is_future_date': _is_future_date, 'is_past_date': _is_past_date, 'is_date_after': _is_date_after, 'is_date_before': _is_date_before, 'is_date_between': _is_date_between, 'all_date_checks': _all_date_checks}</code>  <code>module-attribute</code>","text":""},{"location":"api/engine/engine-duckdb/#sumeh.engine.duckdb_engine.__RuleCtx","title":"<code>__RuleCtx</code>  <code>dataclass</code>","text":"<p>__RuleCtx is a context class used to define rules for processing data.</p> <p>Attributes:</p> Name Type Description <code>column</code> <code>Any</code> <p>Represents the column(s) to which the rule applies.           It can be a string (for a single column) or a list of strings (for multiple columns).</p> <code>value</code> <code>Any</code> <p>The value associated with the rule. The type of this value depends on the specific rule implementation.</p> <code>name</code> <code>str</code> <p>The name of the rule, typically used to indicate the type of check being performed.</p> Source code in <code>sumeh/engine/duckdb_engine.py</code> <pre><code>@dataclass(slots=True)\nclass __RuleCtx:\n    \"\"\"\n    __RuleCtx is a context class used to define rules for processing data.\n\n    Attributes:\n        column (Any): Represents the column(s) to which the rule applies.\n                      It can be a string (for a single column) or a list of strings (for multiple columns).\n        value (Any): The value associated with the rule. The type of this value depends on the specific rule implementation.\n        name (str): The name of the rule, typically used to indicate the type of check being performed.\n    \"\"\"\n\n    column: Any  # str ou list[str]\n    value: Any\n    name: str  # check_type\n</code></pre>"},{"location":"api/engine/engine-duckdb/#sumeh.engine.duckdb_engine.__compare_schemas","title":"<code>__compare_schemas(actual, expected)</code>","text":"<p>Compare two lists of schema definitions and identify discrepancies.</p> <p>Parameters:</p> Name Type Description Default <code>actual</code> <code>List[SchemaDef]</code> <p>The list of actual schema definitions.</p> required <code>expected</code> <code>List[SchemaDef]</code> <p>The list of expected schema definitions.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>Tuple[bool, List[Tuple[str, str]]]: A tuple where the first element is a boolean indicating</p> <code>List[Tuple[str, str]]</code> <p>whether the schemas match (True if they match, False otherwise), and the second element</p> <code>Tuple[bool, List[Tuple[str, str]]]</code> <p>is a list of tuples describing the discrepancies. Each tuple contains: - The field name (str). - A description of the discrepancy (str), such as \"missing\", \"type mismatch\",   \"nullable but expected non-nullable\", or \"extra column\".</p> Notes <ul> <li>A field is considered \"missing\" if it exists in the expected schema but not in the actual schema.</li> <li>A \"type mismatch\" occurs if the data type of a field in the actual schema does not match   the expected data type.</li> <li>A field is considered \"nullable but expected non-nullable\" if it is nullable in the actual   schema but not nullable in the expected schema.</li> <li>An \"extra column\" is a field that exists in the actual schema but not in the expected schema.</li> </ul> Source code in <code>sumeh/services/utils.py</code> <pre><code>def __compare_schemas(\n    actual: List[SchemaDef],\n    expected: List[SchemaDef],\n) -&gt; Tuple[bool, List[Tuple[str, str]]]:\n    \"\"\"\n    Compare two lists of schema definitions and identify discrepancies.\n\n    Args:\n        actual (List[SchemaDef]): The list of actual schema definitions.\n        expected (List[SchemaDef]): The list of expected schema definitions.\n\n    Returns:\n        Tuple[bool, List[Tuple[str, str]]]: A tuple where the first element is a boolean indicating\n        whether the schemas match (True if they match, False otherwise), and the second element\n        is a list of tuples describing the discrepancies. Each tuple contains:\n            - The field name (str).\n            - A description of the discrepancy (str), such as \"missing\", \"type mismatch\",\n              \"nullable but expected non-nullable\", or \"extra column\".\n\n    Notes:\n        - A field is considered \"missing\" if it exists in the expected schema but not in the actual schema.\n        - A \"type mismatch\" occurs if the data type of a field in the actual schema does not match\n          the expected data type.\n        - A field is considered \"nullable but expected non-nullable\" if it is nullable in the actual\n          schema but not nullable in the expected schema.\n        - An \"extra column\" is a field that exists in the actual schema but not in the expected schema.\n    \"\"\"\n\n    exp_map = {c[\"field\"]: c for c in expected}\n    act_map = {c[\"field\"]: c for c in actual}\n\n    erros: List[Tuple[str, str]] = []\n\n    for fld, exp in exp_map.items():\n        if fld not in act_map:\n            erros.append((fld, \"missing\"))\n            continue\n        act = act_map[fld]\n        if act[\"data_type\"] != exp[\"data_type\"]:\n            erros.append(\n                (\n                    fld,\n                    f\"type mismatch (got {act['data_type']}, expected {exp['data_type']})\",\n                )\n            )\n\n        if act[\"nullable\"] and not exp[\"nullable\"]:\n            erros.append((fld, \"nullable but expected non-nullable\"))\n\n        if exp.get(\"max_length\") is not None:\n            pass\n\n    # 2. campos extras (se quiser)\n    extras = set(act_map) - set(exp_map)\n    for fld in extras:\n        erros.append((fld, \"extra column\"))\n\n    return len(erros) == 0, erros\n</code></pre>"},{"location":"api/engine/engine-duckdb/#sumeh.engine.duckdb_engine.__duckdb_schema_to_list","title":"<code>__duckdb_schema_to_list(conn, table)</code>","text":"<p>Retrieve the schema of a DuckDB table as a list of dictionaries. This function queries the schema of the specified table in a DuckDB database and returns a list of dictionaries where each dictionary represents a column in the table, including its name, data type, nullability, and maximum length. Args:     conn (dk.DuckDBPyConnection): The DuckDB connection object.     table (str): The name of the table whose schema is to be retrieved. Returns:     List[Dict[str, Any]]: A list of dictionaries, each containing the following keys:         - \"field\" (str): The name of the column.         - \"data_type\" (str): The data type of the column in lowercase.         - \"nullable\" (bool): Whether the column allows NULL values.         - \"max_length\" (None): Always None, as DuckDB does not provide maximum length information.</p> Source code in <code>sumeh/engine/duckdb_engine.py</code> <pre><code>def __duckdb_schema_to_list(\n    conn: dk.DuckDBPyConnection, table: str\n) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Retrieve the schema of a DuckDB table as a list of dictionaries.\n    This function queries the schema of the specified table in a DuckDB database\n    and returns a list of dictionaries where each dictionary represents a column\n    in the table, including its name, data type, nullability, and maximum length.\n    Args:\n        conn (dk.DuckDBPyConnection): The DuckDB connection object.\n        table (str): The name of the table whose schema is to be retrieved.\n    Returns:\n        List[Dict[str, Any]]: A list of dictionaries, each containing the following keys:\n            - \"field\" (str): The name of the column.\n            - \"data_type\" (str): The data type of the column in lowercase.\n            - \"nullable\" (bool): Whether the column allows NULL values.\n            - \"max_length\" (None): Always None, as DuckDB does not provide maximum length information.\n    \"\"\"\n\n    df_info = conn.execute(f\"PRAGMA table_info('{table}')\").fetchdf()\n    return [\n        {\n            \"field\": row[\"name\"],\n            \"data_type\": row[\"type\"].lower(),\n            \"nullable\": not bool(row[\"notnull\"]),\n            \"max_length\": None,\n        }\n        for _, row in df_info.iterrows()\n    ]\n</code></pre>"},{"location":"api/engine/engine-duckdb/#sumeh.engine.duckdb_engine.__escape_single_quotes","title":"<code>__escape_single_quotes(txt)</code>","text":"<p>Escapes single quotes in a given string by replacing each single quote with two single quotes. This is commonly used to sanitize strings for use in SQL queries.</p> <p>Parameters:</p> Name Type Description Default <code>txt</code> <code>str</code> <p>The input string to process.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The processed string with single quotes escaped.</p> Source code in <code>sumeh/engine/duckdb_engine.py</code> <pre><code>def __escape_single_quotes(txt: str) -&gt; str:\n    \"\"\"\n    Escapes single quotes in a given string by replacing each single quote\n    with two single quotes. This is commonly used to sanitize strings for\n    use in SQL queries.\n\n    Args:\n        txt (str): The input string to process.\n\n    Returns:\n        str: The processed string with single quotes escaped.\n    \"\"\"\n    return txt.replace(\"'\", \"''\")\n</code></pre>"},{"location":"api/engine/engine-duckdb/#sumeh.engine.duckdb_engine.__format_sequence","title":"<code>__format_sequence(value)</code>","text":"<p>Formats a sequence-like input into a string representation suitable for SQL queries.</p> Converts inputs into a tuple-like string format <ul> <li>'BR,US' -&gt; \"('BR','US')\"</li> <li>['BR', 'US'] -&gt; \"('BR','US')\"</li> <li>('BR', 'US') -&gt; \"('BR','US')\"</li> </ul> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>The input value to be formatted. Can be a string, list, or tuple.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A string representation of the input in the format \"('item1','item2',...)\".</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input value is None or cannot be interpreted as a sequence.</p> Notes <ul> <li>If the input is a string, it attempts to parse it as a Python literal.</li> <li>If parsing fails, it splits the string by commas and processes the resulting parts.</li> <li>Empty or invalid elements are ignored in the output.</li> </ul> Source code in <code>sumeh/engine/duckdb_engine.py</code> <pre><code>def __format_sequence(value: Any) -&gt; str:\n    \"\"\"\n    Formats a sequence-like input into a string representation suitable for SQL queries.\n\n    Converts inputs into a tuple-like string format:\n        - 'BR,US' -&gt; \"('BR','US')\"\n        - ['BR', 'US'] -&gt; \"('BR','US')\"\n        - ('BR', 'US') -&gt; \"('BR','US')\"\n\n    Args:\n        value (Any): The input value to be formatted. Can be a string, list, or tuple.\n\n    Returns:\n        str: A string representation of the input in the format \"('item1','item2',...)\".\n\n    Raises:\n        ValueError: If the input value is None or cannot be interpreted as a sequence.\n\n    Notes:\n        - If the input is a string, it attempts to parse it as a Python literal.\n        - If parsing fails, it splits the string by commas and processes the resulting parts.\n        - Empty or invalid elements are ignored in the output.\n    \"\"\"\n\n    if value is None:\n        raise ValueError(\"value cannot be None for IN/NOT IN\")\n\n    if isinstance(value, (list, tuple)):\n        seq = value\n    else:\n        try:  # tenta interpretar como literal Python\n            seq = ast.literal_eval(value)\n            if not isinstance(seq, (list, tuple)):\n                raise ValueError\n        except Exception:\n            seq = [v.strip(\" []()'\\\"\") for v in str(value).split(\",\")]\n\n    return \"(\" + \",\".join(repr(str(x).strip()) for x in seq if x != \"\") + \")\"\n</code></pre>"},{"location":"api/engine/engine-duckdb/#sumeh.engine.duckdb_engine.__rules_to_duckdb_df","title":"<code>__rules_to_duckdb_df(rules)</code>","text":"<p>Converts a list of rule dictionaries into a DuckDB-compatible SQL query string. Each rule in the input list is processed to generate a SQL <code>SELECT</code> statement with the following fields: - <code>col</code>: The column name(s) associated with the rule. - <code>rule</code>: The name of the rule (check type). - <code>pass_threshold</code>: A numeric threshold value for the rule, defaulting to 1.0 if not provided. - <code>value</code>: The value associated with the rule, which can be a string, list, tuple, or <code>NULL</code>. Rules with the <code>execute</code> field set to <code>False</code> are skipped. If the input list is empty or all rules are skipped, the function returns a SQL query that selects <code>NULL</code> values with a <code>LIMIT 0</code>. Args:     rules (List[Dict]): A list of dictionaries, where each dictionary represents a rule         with the following keys:         - <code>field</code> (str or list): The column(s) associated with the rule.         - <code>check_type</code> (str): The name of the rule.         - <code>value</code> (optional): The value associated with the rule.         - <code>threshold</code> (optional, float): The numeric threshold for the rule.         - <code>execute</code> (optional, bool): Whether the rule should be executed (default is <code>True</code>). Returns:     str: A DuckDB-compatible SQL query string representing the rules.</p> Source code in <code>sumeh/engine/duckdb_engine.py</code> <pre><code>def __rules_to_duckdb_df(rules: List[Dict]) -&gt; str:\n    \"\"\"\n    Converts a list of rule dictionaries into a DuckDB-compatible SQL query string.\n    Each rule in the input list is processed to generate a SQL `SELECT` statement\n    with the following fields:\n    - `col`: The column name(s) associated with the rule.\n    - `rule`: The name of the rule (check type).\n    - `pass_threshold`: A numeric threshold value for the rule, defaulting to 1.0 if not provided.\n    - `value`: The value associated with the rule, which can be a string, list, tuple, or `NULL`.\n    Rules with the `execute` field set to `False` are skipped.\n    If the input list is empty or all rules are skipped, the function returns a SQL query\n    that selects `NULL` values with a `LIMIT 0`.\n    Args:\n        rules (List[Dict]): A list of dictionaries, where each dictionary represents a rule\n            with the following keys:\n            - `field` (str or list): The column(s) associated with the rule.\n            - `check_type` (str): The name of the rule.\n            - `value` (optional): The value associated with the rule.\n            - `threshold` (optional, float): The numeric threshold for the rule.\n            - `execute` (optional, bool): Whether the rule should be executed (default is `True`).\n    Returns:\n        str: A DuckDB-compatible SQL query string representing the rules.\n    \"\"\"\n\n    parts: List[str] = []\n\n    for r in rules:\n        if not r.get(\"execute\", True):\n            continue\n\n        ctx = __RuleCtx(column=r[\"field\"], value=r.get(\"value\"), name=r[\"check_type\"])\n\n        # Formata\u00e7\u00e3o da coluna (string ou lista)\n        col = \", \".join(ctx.column) if isinstance(ctx.column, list) else ctx.column\n        col_sql = f\"'{__escape_single_quotes(col.strip())}'\"\n\n        # Formata\u00e7\u00e3o do nome da regra\n        rule_sql = f\"'{__escape_single_quotes(ctx.name)}'\"\n\n        # Threshold com fallback seguro\n        try:\n            thr = float(r.get(\"threshold\", 1.0))\n        except (TypeError, ValueError):\n            thr = 1.0\n\n        # Formata\u00e7\u00e3o do valor\n        if ctx.value is None:\n            val_sql = \"NULL\"\n        elif isinstance(ctx.value, str):\n            val_sql = f\"'{__escape_single_quotes(ctx.value)}'\"\n        elif isinstance(ctx.value, (list, tuple)):\n            try:\n                val_sql = __format_sequence(ctx.value)\n            except ValueError:\n                val_sql = \"NULL\"\n        else:\n            val_sql = str(ctx.value)\n\n        parts.append(\n            f\"SELECT {col_sql} AS col, \"\n            f\"{rule_sql} AS rule, \"\n            f\"{thr} AS pass_threshold, \"\n            f\"{val_sql} AS value\"\n        )\n\n    if not parts:\n        return \"SELECT NULL AS col, NULL AS rule, NULL AS pass_threshold, NULL AS value LIMIT 0\"\n\n    union_sql = \"\\nUNION ALL\\n\".join(parts)\n    return (\n        \"SELECT DISTINCT col, rule, pass_threshold, value\\n\"\n        \"FROM (\\n\"\n        f\"{union_sql}\\n\"\n        \") AS t\"\n    )\n</code></pre>"},{"location":"api/engine/engine-duckdb/#sumeh.engine.duckdb_engine._all_date_checks","title":"<code>_all_date_checks(r)</code>","text":"<p>Perform all date-related checks on the given rule context.</p> <p>Parameters:</p> Name Type Description Default <code>r</code> <code>__RuleCtx</code> <p>The rule context containing the data to be checked.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The result of the date check, typically indicating whether the date is in the past.</p> Source code in <code>sumeh/engine/duckdb_engine.py</code> <pre><code>def _all_date_checks(r: __RuleCtx) -&gt; str:\n    \"\"\"\n    Perform all date-related checks on the given rule context.\n\n    Args:\n        r (__RuleCtx): The rule context containing the data to be checked.\n\n    Returns:\n        str: The result of the date check, typically indicating whether the date is in the past.\n    \"\"\"\n    return is_past_date(r)\n</code></pre>"},{"location":"api/engine/engine-duckdb/#sumeh.engine.duckdb_engine._are_complete","title":"<code>_are_complete(r)</code>","text":"<p>Constructs a SQL condition string that checks if all specified columns in a rule context are not NULL.</p> <p>Parameters:</p> Name Type Description Default <code>r</code> <code>__RuleCtx</code> <p>The rule context containing the list of column names to check.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A SQL condition string in the format \"(column1 IS NOT NULL AND column2 IS NOT NULL ...)\".</p> Source code in <code>sumeh/engine/duckdb_engine.py</code> <pre><code>def _are_complete(r: __RuleCtx) -&gt; str:\n    \"\"\"\n    Constructs a SQL condition string that checks if all specified columns in a rule context are not NULL.\n\n    Args:\n        r (__RuleCtx): The rule context containing the list of column names to check.\n\n    Returns:\n        str: A SQL condition string in the format \"(column1 IS NOT NULL AND column2 IS NOT NULL ...)\".\n    \"\"\"\n    parts = \" AND \".join(f\"{c} IS NOT NULL\" for c in r.column)\n    return f\"({parts})\"\n</code></pre>"},{"location":"api/engine/engine-duckdb/#sumeh.engine.duckdb_engine._are_unique","title":"<code>_are_unique(r)</code>","text":"<p>Generates a SQL query string to check if the combination of specified columns in a table is unique for each row.</p> <p>Parameters:</p> Name Type Description Default <code>r</code> <code>__RuleCtx</code> <p>A context object containing the column names to be checked            for uniqueness.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A SQL query string that evaluates whether the combination of the  specified columns is unique for each row in the table.</p> Source code in <code>sumeh/engine/duckdb_engine.py</code> <pre><code>def _are_unique(r: __RuleCtx) -&gt; str:\n    \"\"\"\n    Generates a SQL query string to check if the combination of specified columns\n    in a table is unique for each row.\n\n    Args:\n        r (__RuleCtx): A context object containing the column names to be checked\n                       for uniqueness.\n\n    Returns:\n        str: A SQL query string that evaluates whether the combination of the\n             specified columns is unique for each row in the table.\n    \"\"\"\n    combo_outer = \" || '|' || \".join(f\"tbl.{c}\" for c in r.column)\n    combo_inner = \" || '|' || \".join(f\"d2.{c}\" for c in r.column)\n\n    return (\n        f\"(SELECT COUNT(*)                  \\n\"\n        f\"   FROM tbl AS d2                 \\n\"\n        f\"   WHERE ({combo_inner}) = ({combo_outer})\\n\"\n        f\") = 1\"\n    )\n</code></pre>"},{"location":"api/engine/engine-duckdb/#sumeh.engine.duckdb_engine._build_union_sql","title":"<code>_build_union_sql(rules)</code>","text":"<p>Constructs a SQL query that combines multiple rule-based checks into a single query using UNION ALL. Each rule specifies a condition to be checked on a table, and the resulting query flags rows that do not satisfy the condition.</p> <p>Parameters:</p> Name Type Description Default <code>rules</code> <code>List[Dict]</code> <p>A list of dictionaries where each dictionary represents a rule. Each rule should contain the following keys: - \"check_type\" (str): The type of check to perform. - \"field\" (str): The column name to apply the check on. - \"value\" (optional): The value to use in the check. - \"execute\" (optional, bool): Whether to execute the rule. Defaults to True.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A SQL query string that combines all active rules using UNION ALL. If no</p> <code>str</code> <p>rules are active, returns a query that produces an empty result set.</p> Notes <ul> <li>If a rule's \"check_type\" is not recognized, a warning is issued, and the rule   is skipped.</li> <li>The resulting query includes a \"dq_status\" column that indicates the rule   that flagged the row, formatted as \"column:check_type:value\".</li> <li>If no rules are active, the query returns an empty result set with the same   structure as the input table.</li> </ul> Source code in <code>sumeh/engine/duckdb_engine.py</code> <pre><code>def _build_union_sql(rules: List[Dict]) -&gt; str:\n    \"\"\"\n    Constructs a SQL query that combines multiple rule-based checks into a single query\n    using UNION ALL. Each rule specifies a condition to be checked on a table, and the\n    resulting query flags rows that do not satisfy the condition.\n\n    Args:\n        rules (List[Dict]): A list of dictionaries where each dictionary represents a rule.\n            Each rule should contain the following keys:\n            - \"check_type\" (str): The type of check to perform.\n            - \"field\" (str): The column name to apply the check on.\n            - \"value\" (optional): The value to use in the check.\n            - \"execute\" (optional, bool): Whether to execute the rule. Defaults to True.\n\n    Returns:\n        str: A SQL query string that combines all active rules using UNION ALL. If no\n        rules are active, returns a query that produces an empty result set.\n\n    Notes:\n        - If a rule's \"check_type\" is not recognized, a warning is issued, and the rule\n          is skipped.\n        - The resulting query includes a \"dq_status\" column that indicates the rule\n          that flagged the row, formatted as \"column:check_type:value\".\n        - If no rules are active, the query returns an empty result set with the same\n          structure as the input table.\n    \"\"\"\n    pieces: list[str] = []\n\n    for r in rules:\n        if not r.get(\"execute\", True):\n            continue\n\n        check = r[\"check_type\"]\n        builder = __RULE_DISPATCH.get(check)\n        if builder is None:\n            warnings.warn(f\"Regra desconhecida: {check}\")\n            continue\n\n        ctx = __RuleCtx(\n            column=r[\"field\"],\n            value=r.get(\"value\"),\n            name=check,\n        )\n\n        expr_ok = builder(ctx)  # condi\u00e7\u00e3o \u201cpassa\u201d\n        dq_tag = __escape_single_quotes(f\"{ctx.column}:{check}:{ctx.value}\")\n        pieces.append(\n            f\"SELECT *, '{dq_tag}' AS dq_status FROM tbl WHERE NOT ({expr_ok})\"\n        )\n\n    # se n\u00e3o h\u00e1 regras ativas: devolve DF vazio\n    if not pieces:\n        return \"SELECT *, '' AS dq_status FROM tbl WHERE 1=0\"\n\n    return \"\\nUNION ALL\\n\".join(pieces)\n</code></pre>"},{"location":"api/engine/engine-duckdb/#sumeh.engine.duckdb_engine._has_pattern","title":"<code>_has_pattern(r)</code>","text":"<p>Constructs a SQL expression to check if a column's value matches a given regular expression pattern.</p> <p>Parameters:</p> Name Type Description Default <code>r</code> <code>__RuleCtx</code> <p>An object containing the column name and the value to be used as the pattern.            The <code>value</code> attribute is expected to be a string or convertible to a string,            and the <code>column</code> attribute is the name of the column to be checked.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A SQL expression string that uses the REGEXP_MATCHES function to evaluate  whether the column matches the escaped pattern.</p> Source code in <code>sumeh/engine/duckdb_engine.py</code> <pre><code>def _has_pattern(r: __RuleCtx) -&gt; str:\n    \"\"\"\n    Constructs a SQL expression to check if a column's value matches a given regular expression pattern.\n\n    Args:\n        r (__RuleCtx): An object containing the column name and the value to be used as the pattern.\n                       The `value` attribute is expected to be a string or convertible to a string,\n                       and the `column` attribute is the name of the column to be checked.\n\n    Returns:\n        str: A SQL expression string that uses the REGEXP_MATCHES function to evaluate\n             whether the column matches the escaped pattern.\n    \"\"\"\n    pat = __escape_single_quotes(str(r.value))\n    return f\"REGEXP_MATCHES({r.column}, '{pat}')\"\n</code></pre>"},{"location":"api/engine/engine-duckdb/#sumeh.engine.duckdb_engine._is_between","title":"<code>_is_between(r)</code>","text":"<p>Constructs a SQL BETWEEN clause for a given rule context.</p> <p>Parameters:</p> Name Type Description Default <code>r</code> <code>__RuleCtx</code> <p>The rule context containing the column name and value(s).            The <code>value</code> attribute can be a list, tuple, or a string            representation of a range (e.g., \"lo, hi\").</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A SQL BETWEEN clause in the format \"column BETWEEN lo AND hi\".</p> Notes <ul> <li>If <code>r.value</code> is a list or tuple, it is expected to contain exactly two elements   representing the lower (lo) and upper (hi) bounds.</li> <li>If <code>r.value</code> is a string, it will be split by commas and stripped of any   surrounding brackets, parentheses, or quotes to extract the bounds.</li> </ul> Source code in <code>sumeh/engine/duckdb_engine.py</code> <pre><code>def _is_between(r: __RuleCtx) -&gt; str:\n    \"\"\"\n    Constructs a SQL BETWEEN clause for a given rule context.\n\n    Args:\n        r (__RuleCtx): The rule context containing the column name and value(s).\n                       The `value` attribute can be a list, tuple, or a string\n                       representation of a range (e.g., \"lo, hi\").\n\n    Returns:\n        str: A SQL BETWEEN clause in the format \"column BETWEEN lo AND hi\".\n\n    Notes:\n        - If `r.value` is a list or tuple, it is expected to contain exactly two elements\n          representing the lower (lo) and upper (hi) bounds.\n        - If `r.value` is a string, it will be split by commas and stripped of any\n          surrounding brackets, parentheses, or quotes to extract the bounds.\n    \"\"\"\n    val = r.value\n    if isinstance(val, (list, tuple)):\n        lo, hi = val\n    else:\n        lo, hi, *_ = [v.strip(\" []()'\\\"\") for v in str(val).split(\",\")]\n    return f\"{r.column} BETWEEN {lo} AND {hi}\"\n</code></pre>"},{"location":"api/engine/engine-duckdb/#sumeh.engine.duckdb_engine._is_complete","title":"<code>_is_complete(r)</code>","text":"<p>Constructs a SQL condition to check if a column is not NULL.</p> <p>Parameters:</p> Name Type Description Default <code>r</code> <code>__RuleCtx</code> <p>An object containing context information, including the column name.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A SQL condition string in the format \" IS NOT NULL\". Source code in <code>sumeh/engine/duckdb_engine.py</code> <pre><code>def _is_complete(r: __RuleCtx) -&gt; str:\n    \"\"\"\n    Constructs a SQL condition to check if a column is not NULL.\n\n    Args:\n        r (__RuleCtx): An object containing context information, including the column name.\n\n    Returns:\n        str: A SQL condition string in the format \"&lt;column&gt; IS NOT NULL\".\n    \"\"\"\n    return f\"{r.column} IS NOT NULL\"\n</code></pre>"},{"location":"api/engine/engine-duckdb/#sumeh.engine.duckdb_engine._is_contained_in","title":"<code>_is_contained_in(r)</code>","text":"<p>Generates a SQL fragment that checks if a column's value is contained within a sequence of values.</p> <p>Parameters:</p> Name Type Description Default <code>r</code> <code>__RuleCtx</code> <p>A context object containing the column name and the sequence of values.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A SQL fragment in the format \" IN (, , ...)\". Source code in <code>sumeh/engine/duckdb_engine.py</code> <pre><code>def _is_contained_in(r: __RuleCtx) -&gt; str:\n    \"\"\"\n    Generates a SQL fragment that checks if a column's value is contained within a sequence of values.\n\n    Args:\n        r (__RuleCtx): A context object containing the column name and the sequence of values.\n\n    Returns:\n        str: A SQL fragment in the format \"&lt;column&gt; IN (&lt;value1&gt;, &lt;value2&gt;, ...)\".\n    \"\"\"\n    return f\"{r.column} IN {__format_sequence(r.value)}\"\n</code></pre>"},{"location":"api/engine/engine-duckdb/#sumeh.engine.duckdb_engine._is_date_after","title":"<code>_is_date_after(r)</code>","text":"<p>Generates a SQL condition to check if a column's date value is earlier than a specified date.</p> <p>Parameters:</p> Name Type Description Default <code>r</code> <code>__RuleCtx</code> <p>A context object containing the column name and the date value to compare.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A SQL condition string in the format \" &lt; DATE ''\". Source code in <code>sumeh/engine/duckdb_engine.py</code> <pre><code>def _is_date_after(r: __RuleCtx) -&gt; str:\n    \"\"\"\n    Generates a SQL condition to check if a column's date value is earlier than a specified date.\n\n    Args:\n        r (__RuleCtx): A context object containing the column name and the date value to compare.\n\n    Returns:\n        str: A SQL condition string in the format \"&lt;column&gt; &lt; DATE '&lt;value&gt;'\".\n    \"\"\"\n    return f\"{r.column} &lt; DATE '{r.value}'\"\n</code></pre>"},{"location":"api/engine/engine-duckdb/#sumeh.engine.duckdb_engine._is_date_before","title":"<code>_is_date_before(r)</code>","text":"<p>Generates a SQL condition to check if a column's date is after a specified date.</p> <p>Parameters:</p> Name Type Description Default <code>r</code> <code>__RuleCtx</code> <p>A context object containing the column name and the date value.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A SQL condition string in the format \" &gt; DATE ''\". Source code in <code>sumeh/engine/duckdb_engine.py</code> <pre><code>def _is_date_before(r: __RuleCtx) -&gt; str:\n    \"\"\"\n    Generates a SQL condition to check if a column's date is after a specified date.\n\n    Args:\n        r (__RuleCtx): A context object containing the column name and the date value.\n\n    Returns:\n        str: A SQL condition string in the format \"&lt;column&gt; &gt; DATE '&lt;value&gt;'\".\n    \"\"\"\n    return f\"{r.column} &gt; DATE '{r.value}'\"\n</code></pre>"},{"location":"api/engine/engine-duckdb/#sumeh.engine.duckdb_engine._is_date_between","title":"<code>_is_date_between(r)</code>","text":"<p>Constructs a SQL condition to check if a column's date value is not within a specified range.</p> <p>Parameters:</p> Name Type Description Default <code>r</code> <code>__RuleCtx</code> <p>A rule context object containing: - r.value (str): A string representation of the date range in the format \"[start_date, end_date]\". - r.column (str): The name of the column to be checked.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A SQL condition string in the format:  \"NOT (column BETWEEN DATE 'start_date' AND DATE 'end_date')\".</p> Source code in <code>sumeh/engine/duckdb_engine.py</code> <pre><code>def _is_date_between(r: __RuleCtx) -&gt; str:\n    \"\"\"\n    Constructs a SQL condition to check if a column's date value is not within a specified range.\n\n    Args:\n        r (__RuleCtx): A rule context object containing:\n            - r.value (str): A string representation of the date range in the format \"[start_date, end_date]\".\n            - r.column (str): The name of the column to be checked.\n\n    Returns:\n        str: A SQL condition string in the format:\n             \"NOT (column BETWEEN DATE 'start_date' AND DATE 'end_date')\".\n    \"\"\"\n    start, end = [d.strip() for d in r.value.strip(\"[]\").split(\",\")]\n    return f\"NOT ({r.column} BETWEEN DATE '{start}' AND DATE '{end}')\"\n</code></pre>"},{"location":"api/engine/engine-duckdb/#sumeh.engine.duckdb_engine._is_equal_than","title":"<code>_is_equal_than(r)</code>","text":"<p>Generates a SQL equality condition string for a given rule context.</p> <p>Parameters:</p> Name Type Description Default <code>r</code> <code>__RuleCtx</code> <p>The rule context containing the column and value to compare.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A string representing the SQL equality condition in the format \"column = value\".</p> Source code in <code>sumeh/engine/duckdb_engine.py</code> <pre><code>def _is_equal_than(r: __RuleCtx) -&gt; str:\n    \"\"\"\n    Generates a SQL equality condition string for a given rule context.\n\n    Args:\n        r (__RuleCtx): The rule context containing the column and value to compare.\n\n    Returns:\n        str: A string representing the SQL equality condition in the format \"column = value\".\n    \"\"\"\n    return f\"{r.column} = {r.value}\"\n</code></pre>"},{"location":"api/engine/engine-duckdb/#sumeh.engine.duckdb_engine._is_future_date","title":"<code>_is_future_date(r)</code>","text":"<p>Constructs a SQL condition to check if the values in a specified column represent future dates relative to the current date.</p> <p>Parameters:</p> Name Type Description Default <code>r</code> <code>__RuleCtx</code> <p>A context object containing metadata, including the column name.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A SQL condition string in the format \" &gt; CURRENT_DATE\". Source code in <code>sumeh/engine/duckdb_engine.py</code> <pre><code>def _is_future_date(r: __RuleCtx) -&gt; str:\n    \"\"\"\n    Constructs a SQL condition to check if the values in a specified column\n    represent future dates relative to the current date.\n\n    Args:\n        r (__RuleCtx): A context object containing metadata, including the column name.\n\n    Returns:\n        str: A SQL condition string in the format \"&lt;column&gt; &gt; CURRENT_DATE\".\n    \"\"\"\n    return f\"{r.column} &gt; CURRENT_DATE\"\n</code></pre>"},{"location":"api/engine/engine-duckdb/#sumeh.engine.duckdb_engine._is_greater_or_equal_than","title":"<code>_is_greater_or_equal_than(r)</code>","text":"<p>Generates a SQL expression to check if a column's value is greater than or equal to a specified value.</p> <p>Parameters:</p> Name Type Description Default <code>r</code> <code>__RuleCtx</code> <p>A context object containing the column name and the value to compare.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A SQL expression in the format \" &gt;= \". Source code in <code>sumeh/engine/duckdb_engine.py</code> <pre><code>def _is_greater_or_equal_than(r: __RuleCtx) -&gt; str:\n    \"\"\"\n    Generates a SQL expression to check if a column's value is greater than or equal to a specified value.\n\n    Args:\n        r (__RuleCtx): A context object containing the column name and the value to compare.\n\n    Returns:\n        str: A SQL expression in the format \"&lt;column&gt; &gt;= &lt;value&gt;\".\n    \"\"\"\n    return f\"{r.column} &gt;= {r.value}\"\n</code></pre>"},{"location":"api/engine/engine-duckdb/#sumeh.engine.duckdb_engine._is_greater_than","title":"<code>_is_greater_than(r)</code>","text":"<p>Generates a SQL condition string to check if a column's value is greater than a specified value.</p> <p>Parameters:</p> Name Type Description Default <code>r</code> <code>__RuleCtx</code> <p>A context object containing the column name and the value to compare.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A SQL condition string in the format \" &gt; \". Source code in <code>sumeh/engine/duckdb_engine.py</code> <pre><code>def _is_greater_than(r: __RuleCtx) -&gt; str:\n    \"\"\"\n    Generates a SQL condition string to check if a column's value is greater than a specified value.\n\n    Args:\n        r (__RuleCtx): A context object containing the column name and the value to compare.\n\n    Returns:\n        str: A SQL condition string in the format \"&lt;column&gt; &gt; &lt;value&gt;\".\n    \"\"\"\n    return f\"{r.column} &gt; {r.value}\"\n</code></pre>"},{"location":"api/engine/engine-duckdb/#sumeh.engine.duckdb_engine._is_in","title":"<code>_is_in(r)</code>","text":"<p>Determines if a rule context is contained within a specific condition.</p> <p>Parameters:</p> Name Type Description Default <code>r</code> <code>__RuleCtx</code> <p>The rule context to evaluate.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A string indicating whether the rule context is contained.</p> Source code in <code>sumeh/engine/duckdb_engine.py</code> <pre><code>def _is_in(r: __RuleCtx) -&gt; str:\n    \"\"\"\n    Determines if a rule context is contained within a specific condition.\n\n    Args:\n        r (__RuleCtx): The rule context to evaluate.\n\n    Returns:\n        str: A string indicating whether the rule context is contained.\n    \"\"\"\n    return is_contained_in(r)\n</code></pre>"},{"location":"api/engine/engine-duckdb/#sumeh.engine.duckdb_engine._is_in_billions","title":"<code>_is_in_billions(r)</code>","text":"<p>Generates a condition string to check if a column's value is in the billions.</p> <p>Parameters:</p> Name Type Description Default <code>r</code> <code>__RuleCtx</code> <p>A context object containing the column to be evaluated.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A string representing the condition where the column's value is</p> <code>str</code> <p>greater than or equal to 1 billion.</p> Source code in <code>sumeh/engine/duckdb_engine.py</code> <pre><code>def _is_in_billions(r: __RuleCtx) -&gt; str:\n    \"\"\"\n    Generates a condition string to check if a column's value is in the billions.\n\n    Args:\n        r (__RuleCtx): A context object containing the column to be evaluated.\n\n    Returns:\n        str: A string representing the condition where the column's value is\n        greater than or equal to 1 billion.\n    \"\"\"\n    return f\"{r.column} &gt;= 1000000000\"\n</code></pre>"},{"location":"api/engine/engine-duckdb/#sumeh.engine.duckdb_engine._is_in_millions","title":"<code>_is_in_millions(r)</code>","text":"<p>Generates a condition string to check if the values in a specified column are greater than or equal to one million.</p> <p>Parameters:</p> Name Type Description Default <code>r</code> <code>__RuleCtx</code> <p>A context object containing the column to be evaluated.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A string representing the condition to check if the column values</p> <code>str</code> <p>are in the millions.</p> Source code in <code>sumeh/engine/duckdb_engine.py</code> <pre><code>def _is_in_millions(r: __RuleCtx) -&gt; str:\n    \"\"\"\n    Generates a condition string to check if the values in a specified column\n    are greater than or equal to one million.\n\n    Args:\n        r (__RuleCtx): A context object containing the column to be evaluated.\n\n    Returns:\n        str: A string representing the condition to check if the column values\n        are in the millions.\n    \"\"\"\n    return f\"{r.column} &gt;= 1000000\"\n</code></pre>"},{"location":"api/engine/engine-duckdb/#sumeh.engine.duckdb_engine._is_less_or_equal_than","title":"<code>_is_less_or_equal_than(r)</code>","text":"<p>Generates a SQL condition string that checks if a column's value is less than or equal to a specified value.</p> <p>Parameters:</p> Name Type Description Default <code>r</code> <code>__RuleCtx</code> <p>A context object containing the column name and the value to compare.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A SQL condition string in the format \" &lt;= \". Source code in <code>sumeh/engine/duckdb_engine.py</code> <pre><code>def _is_less_or_equal_than(r: __RuleCtx) -&gt; str:\n    \"\"\"\n    Generates a SQL condition string that checks if a column's value is less than or equal to a specified value.\n\n    Args:\n        r (__RuleCtx): A context object containing the column name and the value to compare.\n\n    Returns:\n        str: A SQL condition string in the format \"&lt;column&gt; &lt;= &lt;value&gt;\".\n    \"\"\"\n    return f\"{r.column} &lt;= {r.value}\"\n</code></pre>"},{"location":"api/engine/engine-duckdb/#sumeh.engine.duckdb_engine._is_less_than","title":"<code>_is_less_than(r)</code>","text":"<p>Generates a SQL condition string that checks if a column's value is less than a specified value.</p> <p>Parameters:</p> Name Type Description Default <code>r</code> <code>__RuleCtx</code> <p>A context object containing the column name and the value to compare against.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A SQL condition string in the format \" &lt; \". Source code in <code>sumeh/engine/duckdb_engine.py</code> <pre><code>def _is_less_than(r: __RuleCtx) -&gt; str:\n    \"\"\"\n    Generates a SQL condition string that checks if a column's value is less than a specified value.\n\n    Args:\n        r (__RuleCtx): A context object containing the column name and the value to compare against.\n\n    Returns:\n        str: A SQL condition string in the format \"&lt;column&gt; &lt; &lt;value&gt;\".\n    \"\"\"\n    return f\"{r.column} &lt; {r.value}\"\n</code></pre>"},{"location":"api/engine/engine-duckdb/#sumeh.engine.duckdb_engine._is_on_friday","title":"<code>_is_on_friday(r)</code>","text":"<p>Generates a SQL expression to check if the date in the specified column falls on a Friday.</p> <p>Parameters:</p> Name Type Description Default <code>r</code> <code>__RuleCtx</code> <p>A context object containing the column to be evaluated.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A SQL expression that evaluates to True if the date in the column</p> <code>str</code> <p>is a Friday, otherwise False.</p> Source code in <code>sumeh/engine/duckdb_engine.py</code> <pre><code>def _is_on_friday(r: __RuleCtx) -&gt; str:\n    \"\"\"\n    Generates a SQL expression to check if the date in the specified column\n    falls on a Friday.\n\n    Args:\n        r (__RuleCtx): A context object containing the column to be evaluated.\n\n    Returns:\n        str: A SQL expression that evaluates to True if the date in the column\n        is a Friday, otherwise False.\n    \"\"\"\n    return f\"EXTRACT(DOW FROM {r.column}) = 5\"\n</code></pre>"},{"location":"api/engine/engine-duckdb/#sumeh.engine.duckdb_engine._is_on_monday","title":"<code>_is_on_monday(r)</code>","text":"<p>Generates a SQL expression to check if the date in the specified column falls on a Monday.</p> <p>Parameters:</p> Name Type Description Default <code>r</code> <code>__RuleCtx</code> <p>A context object containing the column to be evaluated.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A SQL expression that evaluates to True if the date in the column</p> <code>str</code> <p>is a Monday, otherwise False.</p> Source code in <code>sumeh/engine/duckdb_engine.py</code> <pre><code>def _is_on_monday(r: __RuleCtx) -&gt; str:\n    \"\"\"\n    Generates a SQL expression to check if the date in the specified column\n    falls on a Monday.\n\n    Args:\n        r (__RuleCtx): A context object containing the column to be evaluated.\n\n    Returns:\n        str: A SQL expression that evaluates to True if the date in the column\n        is a Monday, otherwise False.\n    \"\"\"\n    return f\"EXTRACT(DOW FROM {r.column}) = 1\"\n</code></pre>"},{"location":"api/engine/engine-duckdb/#sumeh.engine.duckdb_engine._is_on_saturday","title":"<code>_is_on_saturday(r)</code>","text":"<p>Generates a SQL expression to check if the date in the specified column falls on a Saturday.</p> <p>Parameters:</p> Name Type Description Default <code>r</code> <code>__RuleCtx</code> <p>A context object containing the column to evaluate.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A SQL expression that evaluates to True if the date in the column</p> <code>str</code> <p>is a Saturday, otherwise False.</p> Source code in <code>sumeh/engine/duckdb_engine.py</code> <pre><code>def _is_on_saturday(r: __RuleCtx) -&gt; str:\n    \"\"\"\n    Generates a SQL expression to check if the date in the specified column\n    falls on a Saturday.\n\n    Args:\n        r (__RuleCtx): A context object containing the column to evaluate.\n\n    Returns:\n        str: A SQL expression that evaluates to True if the date in the column\n        is a Saturday, otherwise False.\n    \"\"\"\n    return f\"EXTRACT(DOW FROM {r.column}) = 6\"\n</code></pre>"},{"location":"api/engine/engine-duckdb/#sumeh.engine.duckdb_engine._is_on_sunday","title":"<code>_is_on_sunday(r)</code>","text":"<p>Generates a SQL expression to check if the date in the specified column falls on a Sunday.</p> <p>Parameters:</p> Name Type Description Default <code>r</code> <code>__RuleCtx</code> <p>A context object containing the column to evaluate.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A SQL expression that evaluates to True if the date in the column</p> <code>str</code> <p>is a Sunday, otherwise False.</p> Source code in <code>sumeh/engine/duckdb_engine.py</code> <pre><code>def _is_on_sunday(r: __RuleCtx) -&gt; str:\n    \"\"\"\n    Generates a SQL expression to check if the date in the specified column\n    falls on a Sunday.\n\n    Args:\n        r (__RuleCtx): A context object containing the column to evaluate.\n\n    Returns:\n        str: A SQL expression that evaluates to True if the date in the column\n        is a Sunday, otherwise False.\n    \"\"\"\n    return f\"EXTRACT(DOW FROM {r.column}) = 0\"\n</code></pre>"},{"location":"api/engine/engine-duckdb/#sumeh.engine.duckdb_engine._is_on_thursday","title":"<code>_is_on_thursday(r)</code>","text":"<p>Generates a SQL expression to check if the day of the week for a given column is Thursday.</p> <p>Parameters:</p> Name Type Description Default <code>r</code> <code>__RuleCtx</code> <p>A context object containing the column to evaluate.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A SQL expression that evaluates to True if the day of the week is Thursday, otherwise False.</p> Source code in <code>sumeh/engine/duckdb_engine.py</code> <pre><code>def _is_on_thursday(r: __RuleCtx) -&gt; str:\n    \"\"\"\n    Generates a SQL expression to check if the day of the week for a given column is Thursday.\n\n    Args:\n        r (__RuleCtx): A context object containing the column to evaluate.\n\n    Returns:\n        str: A SQL expression that evaluates to True if the day of the week is Thursday, otherwise False.\n    \"\"\"\n    return f\"EXTRACT(DOW FROM {r.column}) = 4\"\n</code></pre>"},{"location":"api/engine/engine-duckdb/#sumeh.engine.duckdb_engine._is_on_tuesday","title":"<code>_is_on_tuesday(r)</code>","text":"<p>Generates a SQL expression to check if the day of the week for a given column is Tuesday.</p> <p>Parameters:</p> Name Type Description Default <code>r</code> <code>__RuleCtx</code> <p>A context object containing the column to be evaluated.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A SQL expression that evaluates to True if the day of the week is Tuesday, otherwise False.</p> Source code in <code>sumeh/engine/duckdb_engine.py</code> <pre><code>def _is_on_tuesday(r: __RuleCtx) -&gt; str:\n    \"\"\"\n    Generates a SQL expression to check if the day of the week for a given column is Tuesday.\n\n    Args:\n        r (__RuleCtx): A context object containing the column to be evaluated.\n\n    Returns:\n        str: A SQL expression that evaluates to True if the day of the week is Tuesday, otherwise False.\n    \"\"\"\n    return f\"EXTRACT(DOW FROM {r.column}) = 2\"\n</code></pre>"},{"location":"api/engine/engine-duckdb/#sumeh.engine.duckdb_engine._is_on_wednesday","title":"<code>_is_on_wednesday(r)</code>","text":"<p>Generates a SQL expression to check if the date in the specified column falls on a Wednesday.</p> <p>Parameters:</p> Name Type Description Default <code>r</code> <code>__RuleCtx</code> <p>A context object containing the column to be evaluated.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A SQL expression that evaluates to True if the date in the column</p> <code>str</code> <p>is a Wednesday, otherwise False.</p> Source code in <code>sumeh/engine/duckdb_engine.py</code> <pre><code>def _is_on_wednesday(r: __RuleCtx) -&gt; str:\n    \"\"\"\n    Generates a SQL expression to check if the date in the specified column\n    falls on a Wednesday.\n\n    Args:\n        r (__RuleCtx): A context object containing the column to be evaluated.\n\n    Returns:\n        str: A SQL expression that evaluates to True if the date in the column\n        is a Wednesday, otherwise False.\n    \"\"\"\n    return f\"EXTRACT(DOW FROM {r.column}) = 3\"\n</code></pre>"},{"location":"api/engine/engine-duckdb/#sumeh.engine.duckdb_engine._is_on_weekday","title":"<code>_is_on_weekday(r)</code>","text":"<p>Generates a SQL expression to check if the values in a specified column correspond to weekdays (Monday to Friday).</p> <p>Parameters:</p> Name Type Description Default <code>r</code> <code>__RuleCtx</code> <p>A context object containing the column to be evaluated.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A SQL expression that evaluates whether the day of the week  extracted from the column is between 1 (Monday) and 5 (Friday).</p> Source code in <code>sumeh/engine/duckdb_engine.py</code> <pre><code>def _is_on_weekday(r: __RuleCtx) -&gt; str:\n    \"\"\"\n    Generates a SQL expression to check if the values in a specified column\n    correspond to weekdays (Monday to Friday).\n\n    Args:\n        r (__RuleCtx): A context object containing the column to be evaluated.\n\n    Returns:\n        str: A SQL expression that evaluates whether the day of the week\n             extracted from the column is between 1 (Monday) and 5 (Friday).\n    \"\"\"\n    return f\"EXTRACT(DOW FROM {r.column}) BETWEEN 1 AND 5\"\n</code></pre>"},{"location":"api/engine/engine-duckdb/#sumeh.engine.duckdb_engine._is_on_weekend","title":"<code>_is_on_weekend(r)</code>","text":"<p>Generates a SQL expression to determine if a date column falls on a weekend.</p> <p>Parameters:</p> Name Type Description Default <code>r</code> <code>__RuleCtx</code> <p>A context object containing the column to evaluate.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A SQL expression that evaluates to True if the date in the specified column  is on a Saturday (6) or Sunday (0), otherwise False.</p> Source code in <code>sumeh/engine/duckdb_engine.py</code> <pre><code>def _is_on_weekend(r: __RuleCtx) -&gt; str:\n    \"\"\"\n    Generates a SQL expression to determine if a date column falls on a weekend.\n\n    Args:\n        r (__RuleCtx): A context object containing the column to evaluate.\n\n    Returns:\n        str: A SQL expression that evaluates to True if the date in the specified column\n             is on a Saturday (6) or Sunday (0), otherwise False.\n    \"\"\"\n    return f\"(EXTRACT(DOW FROM {r.column}) = 0 OR EXTRACT(DOW FROM {r.column}) = 6)\"\n</code></pre>"},{"location":"api/engine/engine-duckdb/#sumeh.engine.duckdb_engine._is_past_date","title":"<code>_is_past_date(r)</code>","text":"<p>Generates a SQL condition to check if the values in a specified column are earlier than the current date.</p> <p>Parameters:</p> Name Type Description Default <code>r</code> <code>__RuleCtx</code> <p>A context object containing the column to be evaluated.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A SQL condition string in the format \" &lt; CURRENT_DATE\". Source code in <code>sumeh/engine/duckdb_engine.py</code> <pre><code>def _is_past_date(r: __RuleCtx) -&gt; str:\n    \"\"\"\n    Generates a SQL condition to check if the values in a specified column are earlier than the current date.\n\n    Args:\n        r (__RuleCtx): A context object containing the column to be evaluated.\n\n    Returns:\n        str: A SQL condition string in the format \"&lt;column&gt; &lt; CURRENT_DATE\".\n    \"\"\"\n    return f\"{r.column} &lt; CURRENT_DATE\"\n</code></pre>"},{"location":"api/engine/engine-duckdb/#sumeh.engine.duckdb_engine._is_t_minus_1","title":"<code>_is_t_minus_1(r)</code>","text":"<p>Determines if the given rule context corresponds to \"T minus 1\" (yesterday). Args:     r (__RuleCtx): The rule context to evaluate. Returns:     str: A string indicating whether the rule context matches \"T minus 1\".</p> Source code in <code>sumeh/engine/duckdb_engine.py</code> <pre><code>def _is_t_minus_1(r: __RuleCtx) -&gt; str:\n    \"\"\"\n    Determines if the given rule context corresponds to \"T minus 1\" (yesterday).\n    Args:\n        r (__RuleCtx): The rule context to evaluate.\n    Returns:\n        str: A string indicating whether the rule context matches \"T minus 1\".\n    \"\"\"\n    return _is_yesterday(r)\n</code></pre>"},{"location":"api/engine/engine-duckdb/#sumeh.engine.duckdb_engine._is_t_minus_2","title":"<code>_is_t_minus_2(r)</code>","text":"<p>Generates a SQL condition string that checks if the given column equals the date two days prior to the current date.</p> <p>Parameters:</p> Name Type Description Default <code>r</code> <code>__RuleCtx</code> <p>A context object containing the column name to be used in the condition.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A SQL condition string in the format \" = current_date - 2\". Source code in <code>sumeh/engine/duckdb_engine.py</code> <pre><code>def _is_t_minus_2(r: __RuleCtx) -&gt; str:\n    \"\"\"\n    Generates a SQL condition string that checks if the given column equals the date two days prior to the current date.\n\n    Args:\n        r (__RuleCtx): A context object containing the column name to be used in the condition.\n\n    Returns:\n        str: A SQL condition string in the format \"&lt;column&gt; = current_date - 2\".\n    \"\"\"\n    return f\"{r.column} = current_date - 2\"\n</code></pre>"},{"location":"api/engine/engine-duckdb/#sumeh.engine.duckdb_engine._is_t_minus_3","title":"<code>_is_t_minus_3(r)</code>","text":"<p>Generates a SQL condition string that checks if a given column's value is equal to the current date minus 3 days.</p> <p>Parameters:</p> Name Type Description Default <code>r</code> <code>__RuleCtx</code> <p>A context object containing the column name to be used            in the SQL condition.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A SQL condition string in the format  \" = current_date - 3\". Source code in <code>sumeh/engine/duckdb_engine.py</code> <pre><code>def _is_t_minus_3(r: __RuleCtx) -&gt; str:\n    \"\"\"\n    Generates a SQL condition string that checks if a given column's value\n    is equal to the current date minus 3 days.\n\n    Args:\n        r (__RuleCtx): A context object containing the column name to be used\n                       in the SQL condition.\n\n    Returns:\n        str: A SQL condition string in the format\n             \"&lt;column_name&gt; = current_date - 3\".\n    \"\"\"\n    return f\"{r.column} = current_date - 3\"\n</code></pre>"},{"location":"api/engine/engine-duckdb/#sumeh.engine.duckdb_engine._is_today","title":"<code>_is_today(r)</code>","text":"<p>Generates a SQL condition to check if the given column corresponds to the current date.</p> <p>Parameters:</p> Name Type Description Default <code>r</code> <code>__RuleCtx</code> <p>A context object containing the column to be checked.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A SQL condition string in the format \" = current_date\". Source code in <code>sumeh/engine/duckdb_engine.py</code> <pre><code>def _is_today(r: __RuleCtx) -&gt; str:\n    \"\"\"\n    Generates a SQL condition to check if the given column corresponds to the current date.\n\n    Args:\n        r (__RuleCtx): A context object containing the column to be checked.\n\n    Returns:\n        str: A SQL condition string in the format \"&lt;column&gt; = current_date\".\n    \"\"\"\n    return f\"{r.column} = current_date\"\n</code></pre>"},{"location":"api/engine/engine-duckdb/#sumeh.engine.duckdb_engine._is_unique","title":"<code>_is_unique(r)</code>","text":"<p>Generates a SQL expression to check if a column value is unique within a table.</p> <p>Parameters:</p> Name Type Description Default <code>r</code> <code>__RuleCtx</code> <p>A context object containing metadata, including the column name to check.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A SQL string that evaluates to True if the column value is unique, otherwise False.</p> Source code in <code>sumeh/engine/duckdb_engine.py</code> <pre><code>def _is_unique(r: __RuleCtx) -&gt; str:\n    \"\"\"\n    Generates a SQL expression to check if a column value is unique within a table.\n\n    Args:\n        r (__RuleCtx): A context object containing metadata, including the column name to check.\n\n    Returns:\n        str: A SQL string that evaluates to True if the column value is unique, otherwise False.\n    \"\"\"\n    return (\n        f\"(SELECT COUNT(*)                            \\n\"\n        f\"   FROM tbl AS d2                           \\n\"\n        f\"   WHERE d2.{r.column} = tbl.{r.column}     \\n\"\n        f\") = 1\"\n    )\n</code></pre>"},{"location":"api/engine/engine-duckdb/#sumeh.engine.duckdb_engine._is_yesterday","title":"<code>_is_yesterday(r)</code>","text":"<p>Generates a SQL condition to check if the values in a specified column correspond to yesterday's date.</p> <p>Parameters:</p> Name Type Description Default <code>r</code> <code>__RuleCtx</code> <p>A context object containing the column to be evaluated.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A SQL condition string that checks if the column's value is equal</p> <code>str</code> <p>to the current date minus one day.</p> Source code in <code>sumeh/engine/duckdb_engine.py</code> <pre><code>def _is_yesterday(r: __RuleCtx) -&gt; str:\n    \"\"\"\n    Generates a SQL condition to check if the values in a specified column\n    correspond to yesterday's date.\n\n    Args:\n        r (__RuleCtx): A context object containing the column to be evaluated.\n\n    Returns:\n        str: A SQL condition string that checks if the column's value is equal\n        to the current date minus one day.\n    \"\"\"\n    return f\"{r.column} = current_date - 1\"\n</code></pre>"},{"location":"api/engine/engine-duckdb/#sumeh.engine.duckdb_engine._not_contained_in","title":"<code>_not_contained_in(r)</code>","text":"<p>Generates a SQL expression that checks if a column's value is not contained within a specified sequence of values.</p> <p>Parameters:</p> Name Type Description Default <code>r</code> <code>__RuleCtx</code> <p>A context object containing the column name and the sequence            of values to check against.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A SQL string in the format \" NOT IN (, , ...)\". Source code in <code>sumeh/engine/duckdb_engine.py</code> <pre><code>def _not_contained_in(r: __RuleCtx) -&gt; str:\n    \"\"\"\n    Generates a SQL expression that checks if a column's value is not contained\n    within a specified sequence of values.\n\n    Args:\n        r (__RuleCtx): A context object containing the column name and the sequence\n                       of values to check against.\n\n    Returns:\n        str: A SQL string in the format \"&lt;column&gt; NOT IN (&lt;value1&gt;, &lt;value2&gt;, ...)\".\n    \"\"\"\n    return f\"{r.column} NOT IN {__format_sequence(r.value)}\"\n</code></pre>"},{"location":"api/engine/engine-duckdb/#sumeh.engine.duckdb_engine._not_in","title":"<code>_not_in(r)</code>","text":"<p>Generates a string representation for a \"not in\" rule context.</p> <p>Parameters:</p> Name Type Description Default <code>r</code> <code>__RuleCtx</code> <p>The rule context to be evaluated.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A string representation indicating the \"not in\" condition.</p> Source code in <code>sumeh/engine/duckdb_engine.py</code> <pre><code>def _not_in(r: __RuleCtx) -&gt; str:\n    \"\"\"\n    Generates a string representation for a \"not in\" rule context.\n\n    Args:\n        r (__RuleCtx): The rule context to be evaluated.\n\n    Returns:\n        str: A string representation indicating the \"not in\" condition.\n    \"\"\"\n    return not_contained_in(r)\n</code></pre>"},{"location":"api/engine/engine-duckdb/#sumeh.engine.duckdb_engine._satisfies","title":"<code>_satisfies(r)</code>","text":"<p>Constructs a string representation of the given rule context.</p> <p>Parameters:</p> Name Type Description Default <code>r</code> <code>__RuleCtx</code> <p>The rule context containing a value to be formatted.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A string in the format \"(value)\" where 'value' is the value of the rule context.</p> Source code in <code>sumeh/engine/duckdb_engine.py</code> <pre><code>def _satisfies(r: __RuleCtx) -&gt; str:\n    \"\"\"\n    Constructs a string representation of the given rule context.\n\n    Args:\n        r (__RuleCtx): The rule context containing a value to be formatted.\n\n    Returns:\n        str: A string in the format \"(value)\" where 'value' is the value of the rule context.\n    \"\"\"\n    return f\"({r.value})\"\n</code></pre>"},{"location":"api/engine/engine-duckdb/#sumeh.engine.duckdb_engine._validate_date_format","title":"<code>_validate_date_format(r)</code>","text":"<p>Validates a date format string by translating tokens into a regular expression and generating a validation condition.</p> <p>Parameters:</p> Name Type Description Default <code>r</code> <code>__RuleCtx</code> <p>A context object containing the <code>value</code> attribute, which is</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A string representing a validation condition that checks if the column</p> <code>str</code> <p>is either NULL or does not match the expected date format.</p> Source code in <code>sumeh/engine/duckdb_engine.py</code> <pre><code>def _validate_date_format(r: __RuleCtx) -&gt; str:\n    \"\"\"\n    Validates a date format string by translating tokens into a regular expression\n    and generating a validation condition.\n\n    Args:\n        r (__RuleCtx): A context object containing the `value` attribute, which is\n        the date format string to validate, and the `column` attribute, which is\n        the name of the column to validate.\n\n    Returns:\n        str: A string representing a validation condition that checks if the column\n        is either NULL or does not match the expected date format.\n    \"\"\"\n    fmt = r.value\n    # translate tokens to regex\n    token_map = {\n        \"DD\": r\"(0[1-9]|[12][0-9]|3[01])\",\n        \"MM\": r\"(0[1-9]|1[0-2])\",\n        \"YYYY\": r\"(19|20)\\d\\d\",\n        \"YY\": r\"\\d\\d\",\n    }\n    regex = fmt\n    for tok, pat in token_map.items():\n        regex = regex.replace(tok, pat)\n    # escape any remaining dots/spaces\n    regex = regex.replace(\".\", r\"\\.\").replace(\" \", r\"\\s\")\n    return f\"{r.column} IS NULL OR NOT REGEXP_MATCHES({r.column}, '^{regex}$')\"\n</code></pre>"},{"location":"api/engine/engine-duckdb/#sumeh.engine.duckdb_engine.summarize","title":"<code>summarize(df_rel, rules, conn, total_rows=None)</code>","text":"<p>Summarizes data quality checks for a given DuckDB relation based on specified rules.</p> <p>Parameters:</p> Name Type Description Default <code>df_rel</code> <code>DuckDBPyRelation</code> <p>The DuckDB relation containing the data to be analyzed.</p> required <code>rules</code> <code>List[Dict]</code> <p>A list of dictionaries defining the data quality rules to be applied.</p> required <code>conn</code> <code>DuckDBPyConnection</code> <p>The DuckDB connection used to execute SQL queries.</p> required <code>total_rows</code> <code>Optional[int]</code> <p>The total number of rows in the dataset. If not provided,                         it must be calculated externally.</p> <code>None</code> <p>Returns:</p> Type Description <code>DuckDBPyRelation</code> <p>dk.DuckDBPyRelation: A DuckDB relation containing the summary of data quality checks,                     including pass rates, violation counts, and statuses for each rule.</p> Notes <ul> <li>The function creates a temporary view named \"violations_raw\" from the input relation.</li> <li>It uses SQL to compute violations, pass rates, and statuses based on the provided rules.</li> <li>The output includes metadata such as timestamps, rule thresholds, and overall status     (PASS/FAIL) for each rule.</li> </ul> Source code in <code>sumeh/engine/duckdb_engine.py</code> <pre><code>def summarize(\n    df_rel: dk.DuckDBPyRelation,\n    rules: List[Dict],\n    conn: dk.DuckDBPyConnection,\n    total_rows: Optional[int] = None,\n) -&gt; dk.DuckDBPyRelation:\n    \"\"\"\n    Summarizes data quality checks for a given DuckDB relation based on specified rules.\n\n    Args:\n        df_rel (dk.DuckDBPyRelation): The DuckDB relation containing the data to be analyzed.\n        rules (List[Dict]): A list of dictionaries defining the data quality rules to be applied.\n        conn (dk.DuckDBPyConnection): The DuckDB connection used to execute SQL queries.\n        total_rows (Optional[int]): The total number of rows in the dataset. If not provided,\n                                    it must be calculated externally.\n\n    Returns:\n        dk.DuckDBPyRelation: A DuckDB relation containing the summary of data quality checks,\n                                including pass rates, violation counts, and statuses for each rule.\n\n    Notes:\n        - The function creates a temporary view named \"violations_raw\" from the input relation.\n        - It uses SQL to compute violations, pass rates, and statuses based on the provided rules.\n        - The output includes metadata such as timestamps, rule thresholds, and overall status\n            (PASS/FAIL) for each rule.\n    \"\"\"\n    rules_sql = __rules_to_duckdb_df(rules)\n\n    df_rel.create_view(\"violations_raw\")\n\n    sql = f\"\"\"\n        WITH\n        rules      AS ({rules_sql}),\n        violations AS (\n            SELECT\n                split_part(dq_status, ':', 1) AS col,\n                split_part(dq_status, ':', 2) AS rule,\n                split_part(dq_status, ':', 2) AS value,\n                COUNT(*)               AS violations\n            FROM violations_raw\n            WHERE dq_status IS NOT NULL\n                AND dq_status &lt;&gt; ''\n            GROUP BY col, rule, value, value\n        ),\n        total_rows AS (\n            SELECT {total_rows} AS cnt\n        )\n        SELECT\n        uuid()                                          AS id,\n        date_trunc('minute', NOW())                     AS timestamp,\n        'Quality Check'                                 AS check,\n        'WARNING'                                       AS level,\n        r.col         AS col,\n        r.rule,\n        r.value,\n        tr.cnt                                         AS rows,\n        COALESCE(v.violations, 0)                      AS violations,\n        (tr.cnt - COALESCE(v.violations, 0))::DOUBLE / tr.cnt          AS pass_rate,\n        r.pass_threshold,\n        CASE\n            WHEN (tr.cnt - COALESCE(v.violations,0))::DOUBLE / tr.cnt \n             &gt;= r.pass_threshold THEN 'PASS'\n            ELSE 'FAIL'\n        END                                            AS status\n        FROM rules r\n        LEFT JOIN violations v ON r.col = v.col AND r.rule = v.rule,\n            total_rows tr\n    \"\"\"\n    return conn.sql(sql)\n</code></pre>"},{"location":"api/engine/engine-duckdb/#sumeh.engine.duckdb_engine.validate","title":"<code>validate(df_rel, rules, conn)</code>","text":"<p>Validates a DuckDB relation against a set of rules and returns the processed relation.</p> <p>Parameters:</p> Name Type Description Default <code>df_rel</code> <code>DuckDBPyRelation</code> <p>The input DuckDB relation to be validated.</p> required <code>rules</code> <code>List[Dict]</code> <p>A list of dictionaries representing validation rules.</p> required <code>conn</code> <code>DuckDBPyConnection</code> <p>The DuckDB connection object used for executing SQL queries.</p> required <p>Returns:</p> Type Description <code>DuckDBPyRelation</code> <p>dk.DuckDBPyRelation: A tuple containing: - The final DuckDB relation with aggregated validation statuses. - The raw DuckDB relation resulting from applying the validation rules.</p> Notes <ul> <li>The function creates a temporary view of the input relation named \"tbl\".</li> <li>Validation rules are combined into a union SQL query using <code>_build_union_sql</code>.</li> <li>The final relation includes all original columns and an aggregated <code>dq_status</code> column.</li> </ul> Source code in <code>sumeh/engine/duckdb_engine.py</code> <pre><code>def validate(\n    df_rel: dk.DuckDBPyRelation, rules: List[Dict], conn: dk.DuckDBPyConnection\n) -&gt; dk.DuckDBPyRelation:\n    \"\"\"\n    Validates a DuckDB relation against a set of rules and returns the processed relation.\n\n    Args:\n        df_rel (dk.DuckDBPyRelation): The input DuckDB relation to be validated.\n        rules (List[Dict]): A list of dictionaries representing validation rules.\n        conn (dk.DuckDBPyConnection): The DuckDB connection object used for executing SQL queries.\n\n    Returns:\n        dk.DuckDBPyRelation: A tuple containing:\n            - The final DuckDB relation with aggregated validation statuses.\n            - The raw DuckDB relation resulting from applying the validation rules.\n\n    Notes:\n        - The function creates a temporary view of the input relation named \"tbl\".\n        - Validation rules are combined into a union SQL query using `_build_union_sql`.\n        - The final relation includes all original columns and an aggregated `dq_status` column.\n    \"\"\"\n    df_rel.create_view(\"tbl\")\n\n    union_sql = _build_union_sql(rules)\n\n    cols_df = conn.sql(\"PRAGMA table_info('tbl')\").df()\n    colnames = cols_df[\"name\"].tolist()\n    cols_sql = \", \".join(colnames)\n\n    raw_sql = f\"\"\"\n        {union_sql}\n    \"\"\"\n\n    raw = conn.sql(raw_sql)\n\n    final_sql = f\"\"\"\n    SELECT\n        {cols_sql},\n        STRING_AGG(dq_status, ';') AS dq_status\n    FROM raw\n    GROUP BY {cols_sql}\n    \"\"\"\n    final = conn.sql(final_sql)\n\n    return final, raw\n</code></pre>"},{"location":"api/engine/engine-duckdb/#sumeh.engine.duckdb_engine.validate_schema","title":"<code>validate_schema(conn, expected, table)</code>","text":"<p>Validates the schema of a DuckDB table against an expected schema.</p> <p>Parameters:</p> Name Type Description Default <code>conn</code> <code>DuckDBPyConnection</code> <p>The DuckDB connection object.</p> required <code>expected</code> <code>List[Dict[str, Any]]</code> <p>A list of dictionaries representing the expected schema. Each dictionary should define the expected attributes of the schema, such as column names and types.</p> required <code>table</code> <code>str</code> <p>The name of the table whose schema is to be validated.</p> required <p>Returns:</p> Type Description <code>Tuple[bool, List[Tuple[str, str]]]</code> <p>Tuple[bool, List[Tuple[str, str]]]: A tuple where the first element is a boolean indicating whether the actual schema matches the expected schema, and the second element is a list of tuples describing the mismatches (if any). Each tuple contains the column name and a description of the mismatch.</p> Source code in <code>sumeh/engine/duckdb_engine.py</code> <pre><code>def validate_schema(\n    conn: dk.DuckDBPyConnection, expected: List[Dict[str, Any]], table: str\n) -&gt; Tuple[bool, List[Tuple[str, str]]]:\n    \"\"\"\n    Validates the schema of a DuckDB table against an expected schema.\n\n    Args:\n        conn (dk.DuckDBPyConnection): The DuckDB connection object.\n        expected (List[Dict[str, Any]]): A list of dictionaries representing the expected schema.\n            Each dictionary should define the expected attributes of the schema, such as column names and types.\n        table (str): The name of the table whose schema is to be validated.\n\n    Returns:\n        Tuple[bool, List[Tuple[str, str]]]: A tuple where the first element is a boolean indicating\n            whether the actual schema matches the expected schema, and the second element is a list\n            of tuples describing the mismatches (if any). Each tuple contains the column name and\n            a description of the mismatch.\n    \"\"\"\n    actual = __duckdb_schema_to_list(conn, table)\n    return __compare_schemas(actual, expected)\n</code></pre>"},{"location":"api/engine/engine-pandas/","title":"Module <code>sumeh.engine.pandas_engine</code>","text":"<p>This module provides a set of data quality validation functions using the Pandas library. It includes various checks for data validation, such as completeness, uniqueness, range checks, pattern matching, date validations, SQL-style custom expressions, and schema validation.</p> <p>Functions:</p> Name Description <code>is_positive</code> <p>Filters rows where the specified field is less than zero.</p> <code>is_negative</code> <p>Filters rows where the specified field is greater than or equal to zero.</p> <code>is_in_millions</code> <p>Retains rows where the field value is at least 1,000,000 and flags them with dq_status.</p> <code>is_in_billions</code> <p>Retains rows where the field value is at least 1,000,000,000 and flags them with dq_status.</p> <code>is_t_minus_1</code> <p>Retains rows where the date field equals yesterday (T-1) and flags them with dq_status.</p> <code>is_t_minus_2</code> <p>Retains rows where the date field equals two days ago (T-2) and flags them with dq_status.</p> <code>is_t_minus_3</code> <p>Retains rows where the date field equals three days ago (T-3) and flags them with dq_status.</p> <code>is_today</code> <p>Retains rows where the date field equals today and flags them with dq_status.</p> <code>is_yesterday</code> <p>Retains rows where the date field equals yesterday and flags them with dq_status.</p> <code>is_on_weekday</code> <p>Retains rows where the date field falls on a weekday (Mon-Fri) and flags them with dq_status.</p> <code>is_on_weekend</code> <p>Retains rows where the date field is on a weekend (Sat-Sun) and flags them with dq_status.</p> <code>is_on_monday</code> <p>Retains rows where the date field is on Monday and flags them with dq_status.</p> <code>is_on_tuesday</code> <p>Retains rows where the date field is on Tuesday and flags them with dq_status.</p> <code>is_on_wednesday</code> <p>Retains rows where the date field is on Wednesday and flags them with dq_status.</p> <code>is_on_thursday</code> <p>Retains rows where the date field is on Thursday and flags them with dq_status.</p> <code>is_on_friday</code> <p>Retains rows where the date field is on Friday and flags them with dq_status.</p> <code>is_on_saturday</code> <p>Retains rows where the date field is on Saturday and flags them with dq_status.</p> <code>is_on_sunday</code> <p>Retains rows where the date field is on Sunday and flags them with dq_status.</p> <code>is_complete</code> <p>Filters rows where the specified field is null.</p> <code>is_unique</code> <p>Filters rows with duplicate values in the specified field.</p> <code>are_complete</code> <p>Filters rows where any of the specified fields are null.</p> <code>are_unique</code> <p>Filters rows with duplicate combinations of the specified fields.</p> <code>is_greater_than</code> <p>Filters rows where the specified field is less than or equal to the given value.</p> <code>is_greater_or_equal_than</code> <p>Filters rows where the specified field is less than the given value.</p> <code>is_less_than</code> <p>Filters rows where the specified field is greater than or equal to the given value.</p> <code>is_less_or_equal_than</code> <p>Filters rows where the specified field is greater than the given value.</p> <code>is_equal</code> <p>Filters rows where the specified field is not equal to the given value.</p> <code>is_equal_than</code> <p>Alias for <code>is_equal</code>.</p> <code>is_contained_in</code> <p>Filters rows where the specified field is not in the given list of values.</p> <code>not_contained_in</code> <p>Filters rows where the specified field is in the given list of values.</p> <code>is_between</code> <p>Filters rows where the specified field is not within the given range.</p> <code>has_pattern</code> <p>Filters rows where the specified field does not match the given regex pattern.</p> <code>is_legit</code> <p>Filters rows where the specified field is null or contains whitespace.</p> <code>has_max</code> <p>Filters rows where the specified field exceeds the given maximum value.</p> <code>has_min</code> <p>Filters rows where the specified field is below the given minimum value.</p> <code>has_std</code> <p>Checks if the standard deviation of the specified field exceeds the given value.</p> <code>has_mean</code> <p>Checks if the mean of the specified field exceeds the given value.</p> <code>has_sum</code> <p>Checks if the sum of the specified field exceeds the given value.</p> <code>has_cardinality</code> <p>Checks if the cardinality (number of unique values) of the specified field exceeds the given value.</p> <code>has_infogain</code> <p>Placeholder for information gain validation (currently uses cardinality).</p> <code>has_entropy</code> <p>Placeholder for entropy validation (currently uses cardinality).</p> <code>satisfies</code> <p>Filters rows that do not satisfy the given custom expression.</p> <code>validate_date_format</code> <p>Filters rows where the specified field does not match the expected date format or is null.</p> <code>is_future_date</code> <p>Filters rows where the specified date field is after today\u2019s date.</p> <code>is_past_date</code> <p>Filters rows where the specified date field is before today\u2019s date.</p> <code>is_date_between</code> <p>Filters rows where the specified date field is not within the given [start,end] range.</p> <code>is_date_after</code> <p>Filters rows where the specified date field is before the given date.</p> <code>is_date_before</code> <p>Filters rows where the specified date field is after the given date.</p> <code>all_date_checks</code> <p>Alias for <code>is_past_date</code> (checks date against today).</p> <code>validate</code> <p>Validates a DataFrame against a list of rules and returns the original DataFrame with data quality status and a DataFrame of violations.</p> <code>__build_rules_df</code> <p>Converts a list of rules into a Pandas DataFrame for summarization.</p> <code>summarize</code> <p>Summarizes the results of data quality checks, including pass rates and statuses.</p> <code>validate_schema</code> <p>Validates the schema of a DataFrame against an expected schema and returns a boolean result and a list of errors.</p>"},{"location":"api/engine/engine-pandas/#sumeh.engine.pandas_engine.__build_rules_df","title":"<code>__build_rules_df(rules)</code>","text":"<p>Builds a pandas DataFrame from a list of rule dictionaries.</p> <p>Parameters:</p> Name Type Description Default <code>rules</code> <code>List[dict]</code> <p>A list of dictionaries where each dictionary represents a rule. Each rule dictionary may contain the following keys:     - \"field\" (str or list): The column(s) the rule applies to.     - \"check_type\" (str): The type of check or rule to apply.     - \"value\" (optional): The value associated with the rule.     - \"threshold\" (optional): A numeric threshold for the rule. Defaults to 1.0 if not provided or invalid.     - \"execute\" (optional): A boolean indicating whether the rule should be executed. Defaults to True.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing the processed rules with the following columns: - \"column\": The column(s) the rule applies to, as a comma-separated string if multiple. - \"rule\": The type of check or rule. - \"value\": The value associated with the rule, or an empty string if not provided. - \"pass_threshold\": The numeric threshold for the rule.</p> Notes <ul> <li>Rules with \"execute\" set to False are skipped.</li> <li>Duplicate rows based on \"column\", \"rule\", and \"value\" are removed from the resulting DataFrame.</li> </ul> Source code in <code>sumeh/engine/pandas_engine.py</code> <pre><code>def __build_rules_df(rules: List[dict]) -&gt; pd.DataFrame:\n    \"\"\"\n    Builds a pandas DataFrame from a list of rule dictionaries.\n\n    Args:\n        rules (List[dict]): A list of dictionaries where each dictionary represents a rule.\n            Each rule dictionary may contain the following keys:\n                - \"field\" (str or list): The column(s) the rule applies to.\n                - \"check_type\" (str): The type of check or rule to apply.\n                - \"value\" (optional): The value associated with the rule.\n                - \"threshold\" (optional): A numeric threshold for the rule. Defaults to 1.0 if not provided or invalid.\n                - \"execute\" (optional): A boolean indicating whether the rule should be executed. Defaults to True.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing the processed rules with the following columns:\n            - \"column\": The column(s) the rule applies to, as a comma-separated string if multiple.\n            - \"rule\": The type of check or rule.\n            - \"value\": The value associated with the rule, or an empty string if not provided.\n            - \"pass_threshold\": The numeric threshold for the rule.\n\n    Notes:\n        - Rules with \"execute\" set to False are skipped.\n        - Duplicate rows based on \"column\", \"rule\", and \"value\" are removed from the resulting DataFrame.\n    \"\"\"\n    rows = []\n    for r in rules:\n        if not r.get(\"execute\", True):\n            continue\n\n        col = \",\".join(r[\"field\"]) if isinstance(r[\"field\"], list) else r[\"field\"]\n\n        thr_raw = r.get(\"threshold\")\n        try:\n            thr = float(thr_raw) if thr_raw is not None else 1.0\n        except (TypeError, ValueError):\n            thr = 1.0\n\n        val = r.get(\"value\")\n        rows.append(\n            {\n                \"column\": col,\n                \"rule\": r[\"check_type\"],\n                \"value\": val if val is not None else \"\",\n                \"pass_threshold\": thr,\n            }\n        )\n\n    df_rules = pd.DataFrame(rows)\n    if not df_rules.empty:\n        df_rules = df_rules.drop_duplicates(subset=[\"column\", \"rule\", \"value\"])\n    return df_rules\n</code></pre>"},{"location":"api/engine/engine-pandas/#sumeh.engine.pandas_engine.__compare_schemas","title":"<code>__compare_schemas(actual, expected)</code>","text":"<p>Compare two lists of schema definitions and identify discrepancies.</p> <p>Parameters:</p> Name Type Description Default <code>actual</code> <code>List[SchemaDef]</code> <p>The list of actual schema definitions.</p> required <code>expected</code> <code>List[SchemaDef]</code> <p>The list of expected schema definitions.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>Tuple[bool, List[Tuple[str, str]]]: A tuple where the first element is a boolean indicating</p> <code>List[Tuple[str, str]]</code> <p>whether the schemas match (True if they match, False otherwise), and the second element</p> <code>Tuple[bool, List[Tuple[str, str]]]</code> <p>is a list of tuples describing the discrepancies. Each tuple contains: - The field name (str). - A description of the discrepancy (str), such as \"missing\", \"type mismatch\",   \"nullable but expected non-nullable\", or \"extra column\".</p> Notes <ul> <li>A field is considered \"missing\" if it exists in the expected schema but not in the actual schema.</li> <li>A \"type mismatch\" occurs if the data type of a field in the actual schema does not match   the expected data type.</li> <li>A field is considered \"nullable but expected non-nullable\" if it is nullable in the actual   schema but not nullable in the expected schema.</li> <li>An \"extra column\" is a field that exists in the actual schema but not in the expected schema.</li> </ul> Source code in <code>sumeh/services/utils.py</code> <pre><code>def __compare_schemas(\n    actual: List[SchemaDef],\n    expected: List[SchemaDef],\n) -&gt; Tuple[bool, List[Tuple[str, str]]]:\n    \"\"\"\n    Compare two lists of schema definitions and identify discrepancies.\n\n    Args:\n        actual (List[SchemaDef]): The list of actual schema definitions.\n        expected (List[SchemaDef]): The list of expected schema definitions.\n\n    Returns:\n        Tuple[bool, List[Tuple[str, str]]]: A tuple where the first element is a boolean indicating\n        whether the schemas match (True if they match, False otherwise), and the second element\n        is a list of tuples describing the discrepancies. Each tuple contains:\n            - The field name (str).\n            - A description of the discrepancy (str), such as \"missing\", \"type mismatch\",\n              \"nullable but expected non-nullable\", or \"extra column\".\n\n    Notes:\n        - A field is considered \"missing\" if it exists in the expected schema but not in the actual schema.\n        - A \"type mismatch\" occurs if the data type of a field in the actual schema does not match\n          the expected data type.\n        - A field is considered \"nullable but expected non-nullable\" if it is nullable in the actual\n          schema but not nullable in the expected schema.\n        - An \"extra column\" is a field that exists in the actual schema but not in the expected schema.\n    \"\"\"\n\n    exp_map = {c[\"field\"]: c for c in expected}\n    act_map = {c[\"field\"]: c for c in actual}\n\n    erros: List[Tuple[str, str]] = []\n\n    for fld, exp in exp_map.items():\n        if fld not in act_map:\n            erros.append((fld, \"missing\"))\n            continue\n        act = act_map[fld]\n        if act[\"data_type\"] != exp[\"data_type\"]:\n            erros.append(\n                (\n                    fld,\n                    f\"type mismatch (got {act['data_type']}, expected {exp['data_type']})\",\n                )\n            )\n\n        if act[\"nullable\"] and not exp[\"nullable\"]:\n            erros.append((fld, \"nullable but expected non-nullable\"))\n\n        if exp.get(\"max_length\") is not None:\n            pass\n\n    # 2. campos extras (se quiser)\n    extras = set(act_map) - set(exp_map)\n    for fld in extras:\n        erros.append((fld, \"extra column\"))\n\n    return len(erros) == 0, erros\n</code></pre>"},{"location":"api/engine/engine-pandas/#sumeh.engine.pandas_engine.__convert_value","title":"<code>__convert_value(value)</code>","text":"<p>Converts the provided value to the appropriate type (date, float, or int).</p> <p>Depending on the format of the input value, it will be converted to a datetime object, a floating-point number (float), or an integer (int).</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>The value to be converted, represented as a string.</p> required <p>Returns:</p> Type Description <p>Union[datetime, float, int]: The converted value, which can be a datetime object, float, or int.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the value does not match an expected format.</p> Source code in <code>sumeh/services/utils.py</code> <pre><code>def __convert_value(value):\n    \"\"\"\n    Converts the provided value to the appropriate type (date, float, or int).\n\n    Depending on the format of the input value, it will be converted to a datetime object,\n    a floating-point number (float), or an integer (int).\n\n    Args:\n        value (str): The value to be converted, represented as a string.\n\n    Returns:\n        Union[datetime, float, int]: The converted value, which can be a datetime object, float, or int.\n\n    Raises:\n        ValueError: If the value does not match an expected format.\n    \"\"\"\n    from datetime import datetime\n\n    value = value.strip()\n    try:\n        if \"-\" in value:\n            return datetime.strptime(value, \"%Y-%m-%d\")\n        else:\n            return datetime.strptime(value, \"%d/%m/%Y\")\n    except ValueError:\n        if \".\" in value:\n            return float(value)\n        return int(value)\n</code></pre>"},{"location":"api/engine/engine-pandas/#sumeh.engine.pandas_engine.__extract_params","title":"<code>__extract_params(rule)</code>","text":"Source code in <code>sumeh/services/utils.py</code> <pre><code>def __extract_params(rule: dict) -&gt; tuple:\n    rule_name = rule[\"check_type\"]\n    field = rule[\"field\"]\n    raw_value = rule.get(\"value\")\n    if isinstance(raw_value, str) and raw_value not in (None, \"\", \"NULL\"):\n        try:\n            value = __convert_value(raw_value)\n        except ValueError:\n            value = raw_value\n    else:\n        value = raw_value\n    value = value if value not in (None, \"\", \"NULL\") else \"\"\n    return field, rule_name, value\n</code></pre>"},{"location":"api/engine/engine-pandas/#sumeh.engine.pandas_engine.__pandas_schema_to_list","title":"<code>__pandas_schema_to_list(df, expected)</code>","text":"Source code in <code>sumeh/engine/pandas_engine.py</code> <pre><code>def __pandas_schema_to_list(df, expected) -&gt; Tuple[bool, List[Tuple[str, str]]]:\n    actual = [\n        {\n            \"field\": c,\n            \"data_type\": str(dtype).lower(),\n            \"nullable\": True,\n            \"max_length\": None,\n        }\n        for c, dtype in df.dtypes.items()\n    ]\n    return __compare_schemas(actual, expected)\n</code></pre>"},{"location":"api/engine/engine-pandas/#sumeh.engine.pandas_engine.__transform_date_format_in_pattern","title":"<code>__transform_date_format_in_pattern(date_format)</code>","text":"Source code in <code>sumeh/services/utils.py</code> <pre><code>def __transform_date_format_in_pattern(date_format):\n    date_patterns = {\n        \"DD\": \"(0[1-9]|[12][0-9]|3[01])\",\n        \"MM\": \"(0[1-9]|1[012])\",\n        \"YYYY\": \"(19|20)\\\\d\\\\d\",\n        \"YY\": \"\\\\d\\\\d\",\n        \" \": \"\\\\s\",\n        \".\": \"\\\\.\",\n    }\n\n    date_pattern = date_format\n    for single_format, pattern in date_patterns.items():\n        date_pattern = date_pattern.replace(single_format, pattern)\n\n    return date_pattern\n</code></pre>"},{"location":"api/engine/engine-pandas/#sumeh.engine.pandas_engine._day_of_week","title":"<code>_day_of_week(df, rule, dow)</code>","text":"<p>Filters a DataFrame to include only rows where the day of the week of a specified datetime field matches the given day.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing a datetime field.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing rule parameters. The function expects this to be parsed by <code>__extract_params</code>.</p> required <code>dow</code> <code>int</code> <p>The day of the week to filter by (0=Monday, 6=Sunday).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A filtered DataFrame containing only rows where the day of the week matches <code>dow</code>.           An additional column, \"dq_status\", is added to indicate the rule applied.</p> Source code in <code>sumeh/engine/pandas_engine.py</code> <pre><code>def _day_of_week(df: pd.DataFrame, rule: dict, dow: int) -&gt; pd.DataFrame:\n    \"\"\"\n    Filters a DataFrame to include only rows where the day of the week of a specified datetime field matches the given day.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame containing a datetime field.\n        rule (dict): A dictionary containing rule parameters. The function expects this to be parsed by `__extract_params`.\n        dow (int): The day of the week to filter by (0=Monday, 6=Sunday).\n\n    Returns:\n        pd.DataFrame: A filtered DataFrame containing only rows where the day of the week matches `dow`.\n                      An additional column, \"dq_status\", is added to indicate the rule applied.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    mask = df[field].dt.dayofweek != dow\n    out = df[mask].copy()\n    out[\"dq_status\"] = f\"{field}:{check}:{value}\"\n    return out\n</code></pre>"},{"location":"api/engine/engine-pandas/#sumeh.engine.pandas_engine.all_date_checks","title":"<code>all_date_checks(df, rule)</code>","text":"<p>Applies all date-related validation checks on the given DataFrame based on the specified rule.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing the data to be validated.</p> required <code>rule</code> <code>dict</code> <p>A dictionary specifying the validation rules to be applied.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame with the results of the date validation checks.</p> Source code in <code>sumeh/engine/pandas_engine.py</code> <pre><code>def all_date_checks(df: pd.DataFrame, rule: dict) -&gt; pd.DataFrame:\n    \"\"\"\n    Applies all date-related validation checks on the given DataFrame based on the specified rule.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame containing the data to be validated.\n        rule (dict): A dictionary specifying the validation rules to be applied.\n\n    Returns:\n        pd.DataFrame: A DataFrame with the results of the date validation checks.\n    \"\"\"\n    return is_past_date(df, rule)\n</code></pre>"},{"location":"api/engine/engine-pandas/#sumeh.engine.pandas_engine.are_complete","title":"<code>are_complete(df, rule)</code>","text":"<p>Checks for completeness of specified fields in a DataFrame based on a given rule.</p> <p>This function identifies rows in the DataFrame where any of the specified fields contain missing values (NaN). It returns a DataFrame containing only the rows that violate the completeness rule, along with an additional column <code>dq_status</code> that describes the rule violation.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to check for completeness.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to include the following keys: - fields: A list of column names to check for completeness. - check: A string describing the type of check (e.g., \"completeness\"). - value: A value associated with the rule (e.g., a threshold or description).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing rows that violate the completeness rule.</p> <code>DataFrame</code> <p>The returned DataFrame includes all original columns and an additional column</p> <code>DataFrame</code> <p><code>dq_status</code> that describes the rule violation in the format \"fields:check:value\".</p> Source code in <code>sumeh/engine/pandas_engine.py</code> <pre><code>def are_complete(df: pd.DataFrame, rule: dict) -&gt; pd.DataFrame:\n    \"\"\"\n    Checks for completeness of specified fields in a DataFrame based on a given rule.\n\n    This function identifies rows in the DataFrame where any of the specified fields\n    contain missing values (NaN). It returns a DataFrame containing only the rows\n    that violate the completeness rule, along with an additional column `dq_status`\n    that describes the rule violation.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame to check for completeness.\n        rule (dict): A dictionary containing the rule parameters. It is expected to\n            include the following keys:\n            - fields: A list of column names to check for completeness.\n            - check: A string describing the type of check (e.g., \"completeness\").\n            - value: A value associated with the rule (e.g., a threshold or description).\n\n    Returns:\n        pd.DataFrame: A DataFrame containing rows that violate the completeness rule.\n        The returned DataFrame includes all original columns and an additional column\n        `dq_status` that describes the rule violation in the format \"fields:check:value\".\n    \"\"\"\n    fields, check, value = __extract_params(rule)\n    mask = df[fields].isna().any(axis=1)\n    viol = df[mask].copy()\n    viol[\"dq_status\"] = f\"{fields}:{check}:{value}\"\n    return viol\n</code></pre>"},{"location":"api/engine/engine-pandas/#sumeh.engine.pandas_engine.are_unique","title":"<code>are_unique(df, rule)</code>","text":"<p>Checks for duplicate rows in the specified fields of a DataFrame based on a given rule.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to check for uniqueness.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - fields: A list of column names to check for uniqueness. - check: A string representing the type of check (e.g., \"unique\"). - value: A value associated with the rule (e.g., a description or identifier).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing the rows that violate the uniqueness rule.           An additional column 'dq_status' is added to indicate the rule           that was violated in the format \"{fields}:{check}:{value}\".</p> Source code in <code>sumeh/engine/pandas_engine.py</code> <pre><code>def are_unique(df: pd.DataFrame, rule: dict) -&gt; pd.DataFrame:\n    \"\"\"\n    Checks for duplicate rows in the specified fields of a DataFrame based on a given rule.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame to check for uniqueness.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - fields: A list of column names to check for uniqueness.\n            - check: A string representing the type of check (e.g., \"unique\").\n            - value: A value associated with the rule (e.g., a description or identifier).\n\n    Returns:\n        pd.DataFrame: A DataFrame containing the rows that violate the uniqueness rule.\n                      An additional column 'dq_status' is added to indicate the rule\n                      that was violated in the format \"{fields}:{check}:{value}\".\n    \"\"\"\n    fields, check, value = __extract_params(rule)\n    combo = df[fields].astype(str).agg(\"|\".join, axis=1)\n    dup = combo.duplicated(keep=False)\n    viol = df[dup].copy()\n    viol[\"dq_status\"] = f\"{fields}:{check}:{value}\"\n    return viol\n</code></pre>"},{"location":"api/engine/engine-pandas/#sumeh.engine.pandas_engine.has_cardinality","title":"<code>has_cardinality(df, rule)</code>","text":"<p>Checks if the cardinality (number of unique values) of a specified field in the DataFrame exceeds a given value and returns a modified DataFrame if the condition is met.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to check.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The column name in the DataFrame to check. - 'check': The type of check being performed (e.g., 'cardinality'). - 'value': The threshold value for the cardinality.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: - If the cardinality of the specified field exceeds the given value,   a copy of the DataFrame is returned with an additional column 'dq_status'   indicating the field, check, and value. - If the cardinality does not exceed the value, an empty DataFrame is returned.</p> Source code in <code>sumeh/engine/pandas_engine.py</code> <pre><code>def has_cardinality(df: pd.DataFrame, rule: dict) -&gt; pd.DataFrame:\n    \"\"\"\n    Checks if the cardinality (number of unique values) of a specified field in the DataFrame\n    exceeds a given value and returns a modified DataFrame if the condition is met.\n\n    Parameters:\n        df (pd.DataFrame): The input DataFrame to check.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field': The column name in the DataFrame to check.\n            - 'check': The type of check being performed (e.g., 'cardinality').\n            - 'value': The threshold value for the cardinality.\n\n    Returns:\n        pd.DataFrame:\n            - If the cardinality of the specified field exceeds the given value,\n              a copy of the DataFrame is returned with an additional column 'dq_status'\n              indicating the field, check, and value.\n            - If the cardinality does not exceed the value, an empty DataFrame is returned.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    card = df[field].nunique(dropna=True) or 0\n    if card &gt; value:\n        out = df.copy()\n        out[\"dq_status\"] = f\"{field}:{check}:{value}\"\n        return out\n    return df.iloc[0:0].copy()\n</code></pre>"},{"location":"api/engine/engine-pandas/#sumeh.engine.pandas_engine.has_entropy","title":"<code>has_entropy(df, rule)</code>","text":"<p>Checks if the given DataFrame satisfies a specific rule related to entropy.</p> <p>This function is a wrapper around the <code>has_cardinality</code> function, delegating the rule-checking logic to it.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to be evaluated.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule to be applied.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The resulting DataFrame after applying the rule.</p> Source code in <code>sumeh/engine/pandas_engine.py</code> <pre><code>def has_entropy(df: pd.DataFrame, rule: dict) -&gt; pd.DataFrame:\n    \"\"\"\n    Checks if the given DataFrame satisfies a specific rule related to entropy.\n\n    This function is a wrapper around the `has_cardinality` function, delegating\n    the rule-checking logic to it.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame to be evaluated.\n        rule (dict): A dictionary containing the rule to be applied.\n\n    Returns:\n        pd.DataFrame: The resulting DataFrame after applying the rule.\n    \"\"\"\n    return has_cardinality(df, rule)\n</code></pre>"},{"location":"api/engine/engine-pandas/#sumeh.engine.pandas_engine.has_infogain","title":"<code>has_infogain(df, rule)</code>","text":"<p>Checks if the given DataFrame satisfies the information gain criteria defined by the provided rule. This function internally delegates the operation to the <code>has_cardinality</code> function.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to be evaluated.</p> required <code>rule</code> <code>dict</code> <p>A dictionary defining the rule for information gain.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The resulting DataFrame after applying the rule.</p> Source code in <code>sumeh/engine/pandas_engine.py</code> <pre><code>def has_infogain(df: pd.DataFrame, rule: dict) -&gt; pd.DataFrame:\n    \"\"\"\n    Checks if the given DataFrame satisfies the information gain criteria\n    defined by the provided rule. This function internally delegates the\n    operation to the `has_cardinality` function.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame to be evaluated.\n        rule (dict): A dictionary defining the rule for information gain.\n\n    Returns:\n        pd.DataFrame: The resulting DataFrame after applying the rule.\n    \"\"\"\n    return has_cardinality(df, rule)\n</code></pre>"},{"location":"api/engine/engine-pandas/#sumeh.engine.pandas_engine.has_max","title":"<code>has_max(df, rule)</code>","text":"<p>Identifies rows in a DataFrame where the value in a specified field exceeds a given maximum value.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The column name to check. - 'check' (str): The type of check being performed (e.g., 'max'). - 'value' (numeric): The maximum allowable value for the specified field.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing rows that violate the rule, with an additional column</p> <code>DataFrame</code> <p>'dq_status' indicating the rule violation in the format \"field:check:value\".</p> Source code in <code>sumeh/engine/pandas_engine.py</code> <pre><code>def has_max(df: pd.DataFrame, rule: dict) -&gt; pd.DataFrame:\n    \"\"\"\n    Identifies rows in a DataFrame where the value in a specified field exceeds a given maximum value.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame to be checked.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field' (str): The column name to check.\n            - 'check' (str): The type of check being performed (e.g., 'max').\n            - 'value' (numeric): The maximum allowable value for the specified field.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing rows that violate the rule, with an additional column\n        'dq_status' indicating the rule violation in the format \"field:check:value\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    viol = df[df[field] &gt; value].copy()\n    viol[\"dq_status\"] = f\"{field}:{check}:{value}\"\n    return viol\n</code></pre>"},{"location":"api/engine/engine-pandas/#sumeh.engine.pandas_engine.has_mean","title":"<code>has_mean(df, rule)</code>","text":"<p>Checks if the mean of a specified column in a DataFrame satisfies a given condition.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to evaluate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The column name to calculate the mean for. - 'check' (str): The condition to check (e.g., 'greater_than'). - 'value' (float): The threshold value to compare the mean against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A copy of the input DataFrame with an additional column 'dq_status'</p> <code>DataFrame</code> <p>if the condition is met. The 'dq_status' column contains a string in the format</p> <code>DataFrame</code> <p>\"{field}:{check}:{value}\". If the condition is not met, an empty DataFrame is returned.</p> Source code in <code>sumeh/engine/pandas_engine.py</code> <pre><code>def has_mean(df: pd.DataFrame, rule: dict) -&gt; pd.DataFrame:\n    \"\"\"\n    Checks if the mean of a specified column in a DataFrame satisfies a given condition.\n\n    Parameters:\n        df (pd.DataFrame): The input DataFrame to evaluate.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field' (str): The column name to calculate the mean for.\n            - 'check' (str): The condition to check (e.g., 'greater_than').\n            - 'value' (float): The threshold value to compare the mean against.\n\n    Returns:\n        pd.DataFrame: A copy of the input DataFrame with an additional column 'dq_status'\n        if the condition is met. The 'dq_status' column contains a string in the format\n        \"{field}:{check}:{value}\". If the condition is not met, an empty DataFrame is returned.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    mean_val = df[field].mean(skipna=True) or 0.0\n    if mean_val &gt; value:\n        out = df.copy()\n        out[\"dq_status\"] = f\"{field}:{check}:{value}\"\n        return out\n    return df.iloc[0:0].copy()\n</code></pre>"},{"location":"api/engine/engine-pandas/#sumeh.engine.pandas_engine.has_min","title":"<code>has_min(df, rule)</code>","text":"<p>Filters a DataFrame to identify rows where a specified field's value is less than a given threshold.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The column name in the DataFrame to be checked. - 'check': The type of check being performed (e.g., 'min'). - 'value': The threshold value for the check.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A new DataFrame containing rows that violate the rule, with an additional</p> <code>DataFrame</code> <p>column 'dq_status' indicating the field, check type, and threshold value.</p> Source code in <code>sumeh/engine/pandas_engine.py</code> <pre><code>def has_min(df: pd.DataFrame, rule: dict) -&gt; pd.DataFrame:\n    \"\"\"\n    Filters a DataFrame to identify rows where a specified field's value is less than a given threshold.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame to be checked.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field': The column name in the DataFrame to be checked.\n            - 'check': The type of check being performed (e.g., 'min').\n            - 'value': The threshold value for the check.\n\n    Returns:\n        pd.DataFrame: A new DataFrame containing rows that violate the rule, with an additional\n        column 'dq_status' indicating the field, check type, and threshold value.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    viol = df[df[field] &lt; value].copy()\n    viol[\"dq_status\"] = f\"{field}:{check}:{value}\"\n    return viol\n</code></pre>"},{"location":"api/engine/engine-pandas/#sumeh.engine.pandas_engine.has_pattern","title":"<code>has_pattern(df, rule)</code>","text":"<p>Checks if the values in a specified column of a DataFrame match a given pattern.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to check.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The column name in the DataFrame to check. - 'check': A descriptive label for the check being performed. - 'pattern': The regex pattern to match against the column values.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing rows that do not match the pattern.           An additional column 'dq_status' is added to indicate the           field, check, and pattern that caused the violation.</p> Source code in <code>sumeh/engine/pandas_engine.py</code> <pre><code>def has_pattern(df: pd.DataFrame, rule: dict) -&gt; pd.DataFrame:\n    \"\"\"\n    Checks if the values in a specified column of a DataFrame match a given pattern.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame to check.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field': The column name in the DataFrame to check.\n            - 'check': A descriptive label for the check being performed.\n            - 'pattern': The regex pattern to match against the column values.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing rows that do not match the pattern.\n                      An additional column 'dq_status' is added to indicate the\n                      field, check, and pattern that caused the violation.\n    \"\"\"\n    field, check, pattern = __extract_params(rule)\n    viol = df[~df[field].astype(str).str.contains(pattern, na=False)].copy()\n    viol[\"dq_status\"] = f\"{field}:{check}:{pattern}\"\n    return viol\n</code></pre>"},{"location":"api/engine/engine-pandas/#sumeh.engine.pandas_engine.has_std","title":"<code>has_std(df, rule)</code>","text":"<p>Checks if the standard deviation of a specified field in the DataFrame exceeds a given value.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to evaluate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The column name in the DataFrame to calculate the standard deviation for. - 'check': A string representing the type of check (not used in the logic but included in the output). - 'value': A numeric threshold to compare the standard deviation against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: - If the standard deviation of the specified field exceeds the given value,   returns a copy of the DataFrame with an additional column 'dq_status' indicating the rule details. - If the standard deviation does not exceed the value, returns an empty DataFrame with the same structure as the input.</p> Source code in <code>sumeh/engine/pandas_engine.py</code> <pre><code>def has_std(df: pd.DataFrame, rule: dict) -&gt; pd.DataFrame:\n    \"\"\"\n    Checks if the standard deviation of a specified field in the DataFrame exceeds a given value.\n\n    Parameters:\n        df (pd.DataFrame): The input DataFrame to evaluate.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field': The column name in the DataFrame to calculate the standard deviation for.\n            - 'check': A string representing the type of check (not used in the logic but included in the output).\n            - 'value': A numeric threshold to compare the standard deviation against.\n\n    Returns:\n        pd.DataFrame:\n            - If the standard deviation of the specified field exceeds the given value,\n              returns a copy of the DataFrame with an additional column 'dq_status' indicating the rule details.\n            - If the standard deviation does not exceed the value, returns an empty DataFrame with the same structure as the input.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    std_val = df[field].std(skipna=True) or 0.0\n    if std_val &gt; value:\n        out = df.copy()\n        out[\"dq_status\"] = f\"{field}:{check}:{value}\"\n        return out\n    return df.iloc[0:0].copy()\n</code></pre>"},{"location":"api/engine/engine-pandas/#sumeh.engine.pandas_engine.has_sum","title":"<code>has_sum(df, rule)</code>","text":"<p>Checks if the sum of values in a specified column of a DataFrame exceeds a given threshold.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing the data to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The column name to calculate the sum for. - 'check' (str): A descriptive label for the check (used in the output). - 'value' (float): The threshold value to compare the sum against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: - If the sum of the specified column exceeds the threshold, returns a copy of the input DataFrame   with an additional column 'dq_status' indicating the rule that was applied. - If the sum does not exceed the threshold, returns an empty DataFrame with the same structure as the input.</p> Source code in <code>sumeh/engine/pandas_engine.py</code> <pre><code>def has_sum(df: pd.DataFrame, rule: dict) -&gt; pd.DataFrame:\n    \"\"\"\n    Checks if the sum of values in a specified column of a DataFrame exceeds a given threshold.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame containing the data to be checked.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field' (str): The column name to calculate the sum for.\n            - 'check' (str): A descriptive label for the check (used in the output).\n            - 'value' (float): The threshold value to compare the sum against.\n\n    Returns:\n        pd.DataFrame:\n            - If the sum of the specified column exceeds the threshold, returns a copy of the input DataFrame\n              with an additional column 'dq_status' indicating the rule that was applied.\n            - If the sum does not exceed the threshold, returns an empty DataFrame with the same structure as the input.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    sum_val = df[field].sum(skipna=True) or 0.0\n    if sum_val &gt; value:\n        out = df.copy()\n        out[\"dq_status\"] = f\"{field}:{check}:{value}\"\n        return out\n    return df.iloc[0:0].copy()\n</code></pre>"},{"location":"api/engine/engine-pandas/#sumeh.engine.pandas_engine.is_between","title":"<code>is_between(df, rule)</code>","text":"<p>Filters a DataFrame to identify rows where a specified field's values are not within a given range.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The column name in the DataFrame to check. - 'check': A descriptive label for the check being performed. - 'value': A string representation of the range in the format '[lo, hi]'.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing rows that violate the range condition.           An additional column 'dq_status' is added to indicate the rule violation in the format 'field:check:value'.</p> Source code in <code>sumeh/engine/pandas_engine.py</code> <pre><code>def is_between(df: pd.DataFrame, rule: dict) -&gt; pd.DataFrame:\n    \"\"\"\n    Filters a DataFrame to identify rows where a specified field's values are not within a given range.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame to be checked.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field': The column name in the DataFrame to check.\n            - 'check': A descriptive label for the check being performed.\n            - 'value': A string representation of the range in the format '[lo, hi]'.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing rows that violate the range condition.\n                      An additional column 'dq_status' is added to indicate the rule violation in the format 'field:check:value'.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    lo, hi = [__convert_value(x) for x in str(value).strip(\"[]\").split(\",\")]\n    viol = df[~df[field].between(lo, hi)].copy()\n    viol[\"dq_status\"] = f\"{field}:{check}:{value}\"\n    return viol\n</code></pre>"},{"location":"api/engine/engine-pandas/#sumeh.engine.pandas_engine.is_complete","title":"<code>is_complete(df, rule)</code>","text":"<p>Checks for missing values in a specified field of a DataFrame based on a given rule.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to check for completeness.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The name of the field/column to check for missing values. - 'check': The type of check being performed (not used in this function). - 'value': Additional value associated with the rule (not used in this function).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing rows where the specified field has missing values.           An additional column 'dq_status' is added to indicate the rule that was violated.</p> Source code in <code>sumeh/engine/pandas_engine.py</code> <pre><code>def is_complete(df: pd.DataFrame, rule: dict) -&gt; pd.DataFrame:\n    \"\"\"\n    Checks for missing values in a specified field of a DataFrame based on a given rule.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame to check for completeness.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field': The name of the field/column to check for missing values.\n            - 'check': The type of check being performed (not used in this function).\n            - 'value': Additional value associated with the rule (not used in this function).\n\n    Returns:\n        pd.DataFrame: A DataFrame containing rows where the specified field has missing values.\n                      An additional column 'dq_status' is added to indicate the rule that was violated.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    viol = df[df[field].isna()].copy()\n    viol[\"dq_status\"] = f\"{field}:{check}:{value}\"\n    return viol\n</code></pre>"},{"location":"api/engine/engine-pandas/#sumeh.engine.pandas_engine.is_contained_in","title":"<code>is_contained_in(df, rule)</code>","text":"<p>Filters a DataFrame to identify rows where the values in a specified field are not contained within a given set of values.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected          to include the following keys:          - 'field': The column name in the DataFrame to check.          - 'check': A descriptive string for the check being performed.          - 'value': A list or string representation of the allowed values.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing rows from the input DataFrame that           do not meet the rule criteria. An additional column           'dq_status' is added to indicate the rule violation in           the format \"field:check:value\".</p> Source code in <code>sumeh/engine/pandas_engine.py</code> <pre><code>def is_contained_in(df: pd.DataFrame, rule: dict) -&gt; pd.DataFrame:\n    \"\"\"\n    Filters a DataFrame to identify rows where the values in a specified field\n    are not contained within a given set of values.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame to be checked.\n        rule (dict): A dictionary containing the rule parameters. It is expected\n                     to include the following keys:\n                     - 'field': The column name in the DataFrame to check.\n                     - 'check': A descriptive string for the check being performed.\n                     - 'value': A list or string representation of the allowed values.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing rows from the input DataFrame that\n                      do not meet the rule criteria. An additional column\n                      'dq_status' is added to indicate the rule violation in\n                      the format \"field:check:value\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    vals = re.findall(r\"'([^']*)'\", str(value)) or [\n        v.strip() for v in str(value).strip(\"[]\").split(\",\")\n    ]\n    viol = df[~df[field].isin(vals)].copy()\n    viol[\"dq_status\"] = f\"{field}:{check}:{value}\"\n    return viol\n</code></pre>"},{"location":"api/engine/engine-pandas/#sumeh.engine.pandas_engine.is_date_after","title":"<code>is_date_after(df, rule)</code>","text":"<p>Filters a DataFrame to return rows where a specified date field is earlier than a given target date.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing the data to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - field (str): The name of the column in the DataFrame to check. - check (str): A descriptive label for the check being performed. - date_str (str): The target date as a string in a format parsable by <code>pd.to_datetime</code>.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing rows where the date in the specified field is earlier</p> <code>DataFrame</code> <p>than the target date. An additional column <code>dq_status</code> is added to indicate the rule that</p> <code>DataFrame</code> <p>was violated in the format \"{field}:{check}:{date_str}\".</p> Source code in <code>sumeh/engine/pandas_engine.py</code> <pre><code>def is_date_after(df: pd.DataFrame, rule: dict) -&gt; pd.DataFrame:\n    \"\"\"\n    Filters a DataFrame to return rows where a specified date field is earlier than a given target date.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame containing the data to be checked.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - field (str): The name of the column in the DataFrame to check.\n            - check (str): A descriptive label for the check being performed.\n            - date_str (str): The target date as a string in a format parsable by `pd.to_datetime`.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing rows where the date in the specified field is earlier\n        than the target date. An additional column `dq_status` is added to indicate the rule that\n        was violated in the format \"{field}:{check}:{date_str}\".\n    \"\"\"\n    field, check, date_str = __extract_params(rule)\n    target = pd.to_datetime(date_str)\n    dates = pd.to_datetime(df[field], errors=\"coerce\")\n    viol = df[dates &lt; target].copy()\n    viol[\"dq_status\"] = f\"{field}:{check}:{date_str}\"\n    return viol\n</code></pre>"},{"location":"api/engine/engine-pandas/#sumeh.engine.pandas_engine.is_date_before","title":"<code>is_date_before(df, rule)</code>","text":"<p>Filters a DataFrame to identify rows where a date field is after a specified target date.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing the data to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - field (str): The name of the column in the DataFrame containing date values. - check (str): A descriptive label for the check being performed. - date_str (str): The target date as a string in a format parsable by <code>pd.to_datetime</code>.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing rows where the date in the specified field is after</p> <code>DataFrame</code> <p>the target date. An additional column <code>dq_status</code> is added to indicate the rule that was</p> <code>DataFrame</code> <p>violated in the format \"{field}:{check}:{date_str}\".</p> Source code in <code>sumeh/engine/pandas_engine.py</code> <pre><code>def is_date_before(df: pd.DataFrame, rule: dict) -&gt; pd.DataFrame:\n    \"\"\"\n    Filters a DataFrame to identify rows where a date field is after a specified target date.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame containing the data to be checked.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - field (str): The name of the column in the DataFrame containing date values.\n            - check (str): A descriptive label for the check being performed.\n            - date_str (str): The target date as a string in a format parsable by `pd.to_datetime`.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing rows where the date in the specified field is after\n        the target date. An additional column `dq_status` is added to indicate the rule that was\n        violated in the format \"{field}:{check}:{date_str}\".\n    \"\"\"\n    field, check, date_str = __extract_params(rule)\n    target = pd.to_datetime(date_str)\n    dates = pd.to_datetime(df[field], errors=\"coerce\")\n    viol = df[dates &gt; target].copy()\n    viol[\"dq_status\"] = f\"{field}:{check}:{date_str}\"\n    return viol\n</code></pre>"},{"location":"api/engine/engine-pandas/#sumeh.engine.pandas_engine.is_date_between","title":"<code>is_date_between(df, rule)</code>","text":"<p>Filters rows in a DataFrame where the values in a specified date column are not within a given date range.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing the data to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected          to include the following:          - field: The name of the column to check.          - check: A string representing the type of check (used for                   status annotation).          - raw: A string representing the date range in the format                 '[start_date, end_date]'.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing the rows where the date values in           the specified column are outside the given range. An           additional column 'dq_status' is added to indicate the           rule that was violated.</p> Source code in <code>sumeh/engine/pandas_engine.py</code> <pre><code>def is_date_between(df: pd.DataFrame, rule: dict) -&gt; pd.DataFrame:\n    \"\"\"\n    Filters rows in a DataFrame where the values in a specified date column\n    are not within a given date range.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame containing the data to be checked.\n        rule (dict): A dictionary containing the rule parameters. It is expected\n                     to include the following:\n                     - field: The name of the column to check.\n                     - check: A string representing the type of check (used for\n                              status annotation).\n                     - raw: A string representing the date range in the format\n                            '[start_date, end_date]'.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing the rows where the date values in\n                      the specified column are outside the given range. An\n                      additional column 'dq_status' is added to indicate the\n                      rule that was violated.\n    \"\"\"\n    field, check, raw = __extract_params(rule)\n    start_str, end_str = [s.strip() for s in raw.strip(\"[]\").split(\",\")]\n    start = pd.to_datetime(start_str)\n    end = pd.to_datetime(end_str)\n    dates = pd.to_datetime(df[field], errors=\"coerce\")\n    mask = ~dates.between(start, end)\n    viol = df[mask].copy()\n    viol[\"dq_status\"] = f\"{field}:{check}:{raw}\"\n    return viol\n</code></pre>"},{"location":"api/engine/engine-pandas/#sumeh.engine.pandas_engine.is_equal","title":"<code>is_equal(df, rule)</code>","text":"<p>Filters a DataFrame to identify rows where the value in a specified field does not match a given value, and annotates these rows with a data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The column name in the DataFrame to check. - 'check': A string describing the check being performed (e.g., \"is_equal\"). - 'value': The value to compare against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing rows that do not satisfy the equality check.</p> <code>DataFrame</code> <p>An additional column 'dq_status' is added to indicate the data quality status</p> <code>DataFrame</code> <p>in the format \"{field}:{check}:{value}\".</p> Source code in <code>sumeh/engine/pandas_engine.py</code> <pre><code>def is_equal(df: pd.DataFrame, rule: dict) -&gt; pd.DataFrame:\n    \"\"\"\n    Filters a DataFrame to identify rows where the value in a specified field\n    does not match a given value, and annotates these rows with a data quality status.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame to be checked.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field': The column name in the DataFrame to check.\n            - 'check': A string describing the check being performed (e.g., \"is_equal\").\n            - 'value': The value to compare against.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing rows that do not satisfy the equality check.\n        An additional column 'dq_status' is added to indicate the data quality status\n        in the format \"{field}:{check}:{value}\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    viol = df[df[field] != value].copy()\n    viol[\"dq_status\"] = f\"{field}:{check}:{value}\"\n    return viol\n</code></pre>"},{"location":"api/engine/engine-pandas/#sumeh.engine.pandas_engine.is_equal_than","title":"<code>is_equal_than(df, rule)</code>","text":"<p>Compares the values in a DataFrame against a specified rule and returns the result.</p> <p>This function acts as a wrapper for the <code>is_equal</code> function, passing the given DataFrame and rule to it.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The DataFrame to be evaluated.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the comparison rule.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame indicating the result of the comparison.</p> Source code in <code>sumeh/engine/pandas_engine.py</code> <pre><code>def is_equal_than(df: pd.DataFrame, rule: dict) -&gt; pd.DataFrame:\n    \"\"\"\n    Compares the values in a DataFrame against a specified rule and returns the result.\n\n    This function acts as a wrapper for the `is_equal` function, passing the given\n    DataFrame and rule to it.\n\n    Args:\n        df (pd.DataFrame): The DataFrame to be evaluated.\n        rule (dict): A dictionary containing the comparison rule.\n\n    Returns:\n        pd.DataFrame: A DataFrame indicating the result of the comparison.\n    \"\"\"\n    return is_equal(df, rule)\n</code></pre>"},{"location":"api/engine/engine-pandas/#sumeh.engine.pandas_engine.is_future_date","title":"<code>is_future_date(df, rule)</code>","text":"<p>Identifies rows in a DataFrame where the date in a specified field is in the future.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing the data to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to include          the field name to check and the check type.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing only the rows where the date in the specified           field is in the future. An additional column 'dq_status' is added to           indicate the field, check type, and the current date in ISO format.</p> Source code in <code>sumeh/engine/pandas_engine.py</code> <pre><code>def is_future_date(df: pd.DataFrame, rule: dict) -&gt; pd.DataFrame:\n    \"\"\"\n    Identifies rows in a DataFrame where the date in a specified field is in the future.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame containing the data to be checked.\n        rule (dict): A dictionary containing the rule parameters. It is expected to include\n                     the field name to check and the check type.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing only the rows where the date in the specified\n                      field is in the future. An additional column 'dq_status' is added to\n                      indicate the field, check type, and the current date in ISO format.\n    \"\"\"\n    field, check, _ = __extract_params(rule)\n    today = date.today()\n    dates = pd.to_datetime(df[field], errors=\"coerce\")\n    viol = df[dates &gt; today].copy()\n    viol[\"dq_status\"] = f\"{field}:{check}:{today.isoformat()}\"\n    return viol\n</code></pre>"},{"location":"api/engine/engine-pandas/#sumeh.engine.pandas_engine.is_greater_or_equal_than","title":"<code>is_greater_or_equal_than(df, rule)</code>","text":"<p>Filters a DataFrame to include only rows where the value in a specified field is greater than or equal to a given threshold. Adds a 'dq_status' column to indicate the rule applied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The column name to apply the rule on. - 'check' (str): The type of check being performed (e.g., 'greater_or_equal'). - 'value' (numeric): The threshold value for the comparison.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A new DataFrame containing only the rows that satisfy the rule,</p> <code>DataFrame</code> <p>with an additional 'dq_status' column describing the rule applied.</p> Source code in <code>sumeh/engine/pandas_engine.py</code> <pre><code>def is_greater_or_equal_than(df: pd.DataFrame, rule: dict) -&gt; pd.DataFrame:\n    \"\"\"\n    Filters a DataFrame to include only rows where the value in a specified field\n    is greater than or equal to a given threshold. Adds a 'dq_status' column to\n    indicate the rule applied.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame to be filtered.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field' (str): The column name to apply the rule on.\n            - 'check' (str): The type of check being performed (e.g., 'greater_or_equal').\n            - 'value' (numeric): The threshold value for the comparison.\n\n    Returns:\n        pd.DataFrame: A new DataFrame containing only the rows that satisfy the rule,\n        with an additional 'dq_status' column describing the rule applied.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    viol = df[df[field] &gt;= value].copy()\n    viol[\"dq_status\"] = f\"{field}:{check}:{value}\"\n    return viol\n</code></pre>"},{"location":"api/engine/engine-pandas/#sumeh.engine.pandas_engine.is_greater_than","title":"<code>is_greater_than(df, rule)</code>","text":"<p>Filters a DataFrame to return rows where a specified field's value is greater than a given threshold.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The column name in the DataFrame to be checked. - 'check' (str): The type of check being performed (e.g., 'greater_than'). - 'value' (numeric): The threshold value to compare against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A new DataFrame containing rows where the specified field's value is greater than the given threshold.           An additional column 'dq_status' is added to indicate the rule applied in the format \"field:check:value\".</p> Source code in <code>sumeh/engine/pandas_engine.py</code> <pre><code>def is_greater_than(df: pd.DataFrame, rule: dict) -&gt; pd.DataFrame:\n    \"\"\"\n    Filters a DataFrame to return rows where a specified field's value is greater than a given threshold.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame to be filtered.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field' (str): The column name in the DataFrame to be checked.\n            - 'check' (str): The type of check being performed (e.g., 'greater_than').\n            - 'value' (numeric): The threshold value to compare against.\n\n    Returns:\n        pd.DataFrame: A new DataFrame containing rows where the specified field's value is greater than the given threshold.\n                      An additional column 'dq_status' is added to indicate the rule applied in the format \"field:check:value\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    viol = df[df[field] &gt; value].copy()\n    viol[\"dq_status\"] = f\"{field}:{check}:{value}\"\n    return viol\n</code></pre>"},{"location":"api/engine/engine-pandas/#sumeh.engine.pandas_engine.is_in","title":"<code>is_in(df, rule)</code>","text":"<p>Checks if the values in a DataFrame satisfy a given rule by delegating the operation to the <code>is_contained_in</code> function.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to be evaluated.</p> required <code>rule</code> <code>dict</code> <p>A dictionary defining the rule to check against the DataFrame.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame indicating whether each element satisfies the rule.</p> Source code in <code>sumeh/engine/pandas_engine.py</code> <pre><code>def is_in(df: pd.DataFrame, rule: dict) -&gt; pd.DataFrame:\n    \"\"\"\n    Checks if the values in a DataFrame satisfy a given rule by delegating\n    the operation to the `is_contained_in` function.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame to be evaluated.\n        rule (dict): A dictionary defining the rule to check against the DataFrame.\n\n    Returns:\n        pd.DataFrame: A DataFrame indicating whether each element satisfies the rule.\n    \"\"\"\n    return is_contained_in(df, rule)\n</code></pre>"},{"location":"api/engine/engine-pandas/#sumeh.engine.pandas_engine.is_in_billions","title":"<code>is_in_billions(df, rule)</code>","text":"<p>Filters a DataFrame to include only rows where the specified field's value is greater than or equal to one billion, and adds a data quality status column.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - field (str): The column name to check. - check (str): The type of check being performed (used for status annotation). - value (any): The value associated with the rule (used for status annotation).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A new DataFrame containing rows where the specified field's</p> <code>DataFrame</code> <p>value is greater than or equal to one billion. Includes an additional</p> <code>DataFrame</code> <p>column <code>dq_status</code> with the format \"{field}:{check}:{value}\".</p> Source code in <code>sumeh/engine/pandas_engine.py</code> <pre><code>def is_in_billions(df: pd.DataFrame, rule: dict) -&gt; pd.DataFrame:\n    \"\"\"\n    Filters a DataFrame to include only rows where the specified field's value\n    is greater than or equal to one billion, and adds a data quality status column.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame to filter.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - field (str): The column name to check.\n            - check (str): The type of check being performed (used for status annotation).\n            - value (any): The value associated with the rule (used for status annotation).\n\n    Returns:\n        pd.DataFrame: A new DataFrame containing rows where the specified field's\n        value is greater than or equal to one billion. Includes an additional\n        column `dq_status` with the format \"{field}:{check}:{value}\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    out = df[df[field] &lt; 1_000_000_000].copy()\n    out[\"dq_status\"] = f\"{field}:{check}:{value}\"\n    return out\n</code></pre>"},{"location":"api/engine/engine-pandas/#sumeh.engine.pandas_engine.is_in_millions","title":"<code>is_in_millions(df, rule)</code>","text":"<p>Filters rows in the DataFrame where the specified field's value is greater than or equal to one million and adds a \"dq_status\" column with a formatted string indicating the rule applied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to include: - field (str): The column name to check. - check (str): The type of check being performed (e.g., \"greater_than\"). - value (any): The value associated with the rule (not used in this function).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A new DataFrame containing rows where the specified field's value is &gt;= 1,000,000.           Includes an additional \"dq_status\" column with the rule details.</p> Source code in <code>sumeh/engine/pandas_engine.py</code> <pre><code>def is_in_millions(df: pd.DataFrame, rule: dict) -&gt; pd.DataFrame:\n    \"\"\"\n    Filters rows in the DataFrame where the specified field's value is greater than or equal to one million\n    and adds a \"dq_status\" column with a formatted string indicating the rule applied.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame to filter.\n        rule (dict): A dictionary containing the rule parameters. It is expected to include:\n            - field (str): The column name to check.\n            - check (str): The type of check being performed (e.g., \"greater_than\").\n            - value (any): The value associated with the rule (not used in this function).\n\n    Returns:\n        pd.DataFrame: A new DataFrame containing rows where the specified field's value is &gt;= 1,000,000.\n                      Includes an additional \"dq_status\" column with the rule details.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    out = df[df[field] &lt; 1_000_000].copy()\n    out[\"dq_status\"] = f\"{field}:{check}:{value}\"\n    return out\n</code></pre>"},{"location":"api/engine/engine-pandas/#sumeh.engine.pandas_engine.is_legit","title":"<code>is_legit(df, rule)</code>","text":"<p>Validates a DataFrame against a specified rule and identifies rows that violate the rule.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to validate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the validation rule. It is expected to have          keys that define the field to check, the type of check, and the value          to validate against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing rows that violate the rule. An additional           column 'dq_status' is added to indicate the field, check, and value           that caused the violation in the format \"{field}:{check}:{value}\".</p> Source code in <code>sumeh/engine/pandas_engine.py</code> <pre><code>def is_legit(df: pd.DataFrame, rule: dict) -&gt; pd.DataFrame:\n    \"\"\"\n    Validates a DataFrame against a specified rule and identifies rows that violate the rule.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame to validate.\n        rule (dict): A dictionary containing the validation rule. It is expected to have\n                     keys that define the field to check, the type of check, and the value\n                     to validate against.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing rows that violate the rule. An additional\n                      column 'dq_status' is added to indicate the field, check, and value\n                      that caused the violation in the format \"{field}:{check}:{value}\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    mask = df[field].notna() &amp; df[field].astype(str).str.contains(r\"^\\S+$\", na=False)\n    viol = df[~mask].copy()\n    viol[\"dq_status\"] = f\"{field}:{check}:{value}\"\n    return viol\n</code></pre>"},{"location":"api/engine/engine-pandas/#sumeh.engine.pandas_engine.is_less_or_equal_than","title":"<code>is_less_or_equal_than(df, rule)</code>","text":"<p>Filters rows in a DataFrame where the value in a specified field is less than or equal to a given value.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The column name in the DataFrame to apply the rule on. - 'check' (str): A descriptive label for the check being performed. - 'value' (numeric): The threshold value to compare against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A new DataFrame containing only the rows that satisfy the condition.           An additional column 'dq_status' is added to indicate the rule applied           in the format \"{field}:{check}:{value}\".</p> Source code in <code>sumeh/engine/pandas_engine.py</code> <pre><code>def is_less_or_equal_than(df: pd.DataFrame, rule: dict) -&gt; pd.DataFrame:\n    \"\"\"\n    Filters rows in a DataFrame where the value in a specified field is less than or equal to a given value.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame to be filtered.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field' (str): The column name in the DataFrame to apply the rule on.\n            - 'check' (str): A descriptive label for the check being performed.\n            - 'value' (numeric): The threshold value to compare against.\n\n    Returns:\n        pd.DataFrame: A new DataFrame containing only the rows that satisfy the condition.\n                      An additional column 'dq_status' is added to indicate the rule applied\n                      in the format \"{field}:{check}:{value}\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    viol = df[df[field] &lt;= value].copy()\n    viol[\"dq_status\"] = f\"{field}:{check}:{value}\"\n    return viol\n</code></pre>"},{"location":"api/engine/engine-pandas/#sumeh.engine.pandas_engine.is_less_than","title":"<code>is_less_than(df, rule)</code>","text":"<p>Filters a DataFrame to return rows where a specified field's value is less than a given threshold.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The column name in the DataFrame to be checked. - 'check' (str): A descriptive string for the check (e.g., \"less_than\"). - 'value' (numeric): The threshold value to compare against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A new DataFrame containing only the rows where the specified field's value</p> <code>DataFrame</code> <p>is less than the given threshold. An additional column 'dq_status' is added to indicate</p> <code>DataFrame</code> <p>the rule applied in the format \"field:check:value\".</p> Source code in <code>sumeh/engine/pandas_engine.py</code> <pre><code>def is_less_than(df: pd.DataFrame, rule: dict) -&gt; pd.DataFrame:\n    \"\"\"\n    Filters a DataFrame to return rows where a specified field's value is less than a given threshold.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame to be filtered.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field' (str): The column name in the DataFrame to be checked.\n            - 'check' (str): A descriptive string for the check (e.g., \"less_than\").\n            - 'value' (numeric): The threshold value to compare against.\n\n    Returns:\n        pd.DataFrame: A new DataFrame containing only the rows where the specified field's value\n        is less than the given threshold. An additional column 'dq_status' is added to indicate\n        the rule applied in the format \"field:check:value\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    viol = df[df[field] &lt; value].copy()\n    viol[\"dq_status\"] = f\"{field}:{check}:{value}\"\n    return viol\n</code></pre>"},{"location":"api/engine/engine-pandas/#sumeh.engine.pandas_engine.is_negative","title":"<code>is_negative(df, rule)</code>","text":"<p>Filters a DataFrame to identify rows where a specified field does not satisfy a \"negative\" condition.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to include: - 'field': The column name in the DataFrame to check. - 'check': The type of check being performed (e.g., \"negative\"). - 'value': Additional value associated with the rule (not used in this function).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A new DataFrame containing rows where the specified field is non-negative (&gt;= 0).           An additional column 'dq_status' is added to indicate the rule violation in the format           \"{field}:{check}:{value}\".</p> Source code in <code>sumeh/engine/pandas_engine.py</code> <pre><code>def is_negative(df: pd.DataFrame, rule: dict) -&gt; pd.DataFrame:\n    \"\"\"\n    Filters a DataFrame to identify rows where a specified field does not satisfy a \"negative\" condition.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame to be checked.\n        rule (dict): A dictionary containing the rule parameters. It is expected to include:\n            - 'field': The column name in the DataFrame to check.\n            - 'check': The type of check being performed (e.g., \"negative\").\n            - 'value': Additional value associated with the rule (not used in this function).\n\n    Returns:\n        pd.DataFrame: A new DataFrame containing rows where the specified field is non-negative (&gt;= 0).\n                      An additional column 'dq_status' is added to indicate the rule violation in the format\n                      \"{field}:{check}:{value}\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    viol = df[df[field] &gt;= 0].copy()\n    viol[\"dq_status\"] = f\"{field}:{check}:{value}\"\n    return viol\n</code></pre>"},{"location":"api/engine/engine-pandas/#sumeh.engine.pandas_engine.is_on_friday","title":"<code>is_on_friday(df, rule)</code>","text":"<p>Filters the rows of a DataFrame based on whether a specific date column corresponds to a Friday.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing the data to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rules or parameters for filtering.          It should specify the column to check for the day of the week.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A filtered DataFrame containing only the rows where the specified date column corresponds to a Friday.</p> Source code in <code>sumeh/engine/pandas_engine.py</code> <pre><code>def is_on_friday(df: pd.DataFrame, rule: dict) -&gt; pd.DataFrame:\n    \"\"\"\n    Filters the rows of a DataFrame based on whether a specific date column corresponds to a Friday.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame containing the data to be filtered.\n        rule (dict): A dictionary containing the rules or parameters for filtering.\n                     It should specify the column to check for the day of the week.\n\n    Returns:\n        pd.DataFrame: A filtered DataFrame containing only the rows where the specified date column corresponds to a Friday.\n    \"\"\"\n    return _day_of_week(df, rule, 4)\n</code></pre>"},{"location":"api/engine/engine-pandas/#sumeh.engine.pandas_engine.is_on_monday","title":"<code>is_on_monday(df, rule)</code>","text":"<p>Filters the rows of a DataFrame based on whether a specific date column corresponds to a Monday.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing the data to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the filtering rules, including the column to check.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A filtered DataFrame containing only the rows where the specified date column corresponds to a Monday.</p> Source code in <code>sumeh/engine/pandas_engine.py</code> <pre><code>def is_on_monday(df: pd.DataFrame, rule: dict) -&gt; pd.DataFrame:\n    \"\"\"\n    Filters the rows of a DataFrame based on whether a specific date column corresponds to a Monday.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame containing the data to be filtered.\n        rule (dict): A dictionary containing the filtering rules, including the column to check.\n\n    Returns:\n        pd.DataFrame: A filtered DataFrame containing only the rows where the specified date column corresponds to a Monday.\n    \"\"\"\n    return _day_of_week(df, rule, 0)\n</code></pre>"},{"location":"api/engine/engine-pandas/#sumeh.engine.pandas_engine.is_on_saturday","title":"<code>is_on_saturday(df, rule)</code>","text":"<p>Filters a DataFrame to include only rows where the date corresponds to a Saturday.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing date information.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing rules or parameters for filtering.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A filtered DataFrame containing only rows where the date is a Saturday.</p> Source code in <code>sumeh/engine/pandas_engine.py</code> <pre><code>def is_on_saturday(df: pd.DataFrame, rule: dict) -&gt; pd.DataFrame:\n    \"\"\"\n    Filters a DataFrame to include only rows where the date corresponds to a Saturday.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame containing date information.\n        rule (dict): A dictionary containing rules or parameters for filtering.\n\n    Returns:\n        pd.DataFrame: A filtered DataFrame containing only rows where the date is a Saturday.\n    \"\"\"\n    return _day_of_week(df, rule, 5)\n</code></pre>"},{"location":"api/engine/engine-pandas/#sumeh.engine.pandas_engine.is_on_sunday","title":"<code>is_on_sunday(df, rule)</code>","text":"<p>Determines whether the dates in a given DataFrame fall on a Sunday.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing date-related data.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing rules or parameters for the operation.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame indicating whether each date falls on a Sunday.</p> Source code in <code>sumeh/engine/pandas_engine.py</code> <pre><code>def is_on_sunday(df: pd.DataFrame, rule: dict) -&gt; pd.DataFrame:\n    \"\"\"\n    Determines whether the dates in a given DataFrame fall on a Sunday.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame containing date-related data.\n        rule (dict): A dictionary containing rules or parameters for the operation.\n\n    Returns:\n        pd.DataFrame: A DataFrame indicating whether each date falls on a Sunday.\n    \"\"\"\n    return _day_of_week(df, rule, 6)\n</code></pre>"},{"location":"api/engine/engine-pandas/#sumeh.engine.pandas_engine.is_on_thursday","title":"<code>is_on_thursday(df, rule)</code>","text":"<p>Filters the rows of a DataFrame based on whether a date column corresponds to a Thursday.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing the data to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the filtering rules, including the column to check.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A filtered DataFrame containing only the rows where the specified date column           corresponds to a Thursday.</p> Source code in <code>sumeh/engine/pandas_engine.py</code> <pre><code>def is_on_thursday(df: pd.DataFrame, rule: dict) -&gt; pd.DataFrame:\n    \"\"\"\n    Filters the rows of a DataFrame based on whether a date column corresponds to a Thursday.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame containing the data to be filtered.\n        rule (dict): A dictionary containing the filtering rules, including the column to check.\n\n    Returns:\n        pd.DataFrame: A filtered DataFrame containing only the rows where the specified date column\n                      corresponds to a Thursday.\n    \"\"\"\n    return _day_of_week(df, rule, 3)\n</code></pre>"},{"location":"api/engine/engine-pandas/#sumeh.engine.pandas_engine.is_on_tuesday","title":"<code>is_on_tuesday(df, rule)</code>","text":"<p>Filters the rows of a DataFrame based on whether a specific date column corresponds to a Tuesday.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing the data to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the filtering rules, including the column to check.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A filtered DataFrame containing only the rows where the specified date column corresponds to a Tuesday.</p> Source code in <code>sumeh/engine/pandas_engine.py</code> <pre><code>def is_on_tuesday(df: pd.DataFrame, rule: dict) -&gt; pd.DataFrame:\n    \"\"\"\n    Filters the rows of a DataFrame based on whether a specific date column corresponds to a Tuesday.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame containing the data to be filtered.\n        rule (dict): A dictionary containing the filtering rules, including the column to check.\n\n    Returns:\n        pd.DataFrame: A filtered DataFrame containing only the rows where the specified date column corresponds to a Tuesday.\n    \"\"\"\n    return _day_of_week(df, rule, 1)\n</code></pre>"},{"location":"api/engine/engine-pandas/#sumeh.engine.pandas_engine.is_on_wednesday","title":"<code>is_on_wednesday(df, rule)</code>","text":"<p>Filters the rows of a DataFrame based on whether a date column corresponds to Wednesday.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing the data to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule configuration.          It is expected to specify the column to evaluate.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A filtered DataFrame containing only the rows where the specified date column           corresponds to Wednesday.</p> Source code in <code>sumeh/engine/pandas_engine.py</code> <pre><code>def is_on_wednesday(df: pd.DataFrame, rule: dict) -&gt; pd.DataFrame:\n    \"\"\"\n    Filters the rows of a DataFrame based on whether a date column corresponds to Wednesday.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame containing the data to be filtered.\n        rule (dict): A dictionary containing the rule configuration.\n                     It is expected to specify the column to evaluate.\n\n    Returns:\n        pd.DataFrame: A filtered DataFrame containing only the rows where the specified date column\n                      corresponds to Wednesday.\n    \"\"\"\n    return _day_of_week(df, rule, 2)\n</code></pre>"},{"location":"api/engine/engine-pandas/#sumeh.engine.pandas_engine.is_on_weekday","title":"<code>is_on_weekday(df, rule)</code>","text":"<p>Filters a DataFrame to include only rows where the specified date field falls on a weekday (Monday to Friday) and adds a \"dq_status\" column indicating the rule applied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing the data to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - field (str): The name of the date column to check. - check (str): A descriptive string for the check being performed. - value (str): A value associated with the rule for documentation purposes.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A filtered DataFrame containing only rows where the specified date field</p> <code>DataFrame</code> <p>falls on a weekday, with an additional \"dq_status\" column describing the rule applied.</p> Source code in <code>sumeh/engine/pandas_engine.py</code> <pre><code>def is_on_weekday(df: pd.DataFrame, rule: dict) -&gt; pd.DataFrame:\n    \"\"\"\n    Filters a DataFrame to include only rows where the specified date field falls on a weekday\n    (Monday to Friday) and adds a \"dq_status\" column indicating the rule applied.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame containing the data to be filtered.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - field (str): The name of the date column to check.\n            - check (str): A descriptive string for the check being performed.\n            - value (str): A value associated with the rule for documentation purposes.\n\n    Returns:\n        pd.DataFrame: A filtered DataFrame containing only rows where the specified date field\n        falls on a weekday, with an additional \"dq_status\" column describing the rule applied.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    mask = ~df[field].dt.dayofweek.between(0, 4)\n    out = df[mask].copy()\n    out[\"dq_status\"] = f\"{field}:{check}:{value}\"\n    return out\n</code></pre>"},{"location":"api/engine/engine-pandas/#sumeh.engine.pandas_engine.is_on_weekend","title":"<code>is_on_weekend(df, rule)</code>","text":"<p>Filters a DataFrame to include only rows where the specified date field falls on a weekend (Saturday or Sunday) and adds a \"dq_status\" column indicating the rule applied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing the data to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to include: - field (str): The name of the date column to check. - check (str): A descriptive string for the type of check being performed. - value (str): A value associated with the rule for documentation purposes.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A new DataFrame containing only the rows where the specified date field</p> <code>DataFrame</code> <p>falls on a weekend. Includes an additional \"dq_status\" column with the rule details.</p> Source code in <code>sumeh/engine/pandas_engine.py</code> <pre><code>def is_on_weekend(df: pd.DataFrame, rule: dict) -&gt; pd.DataFrame:\n    \"\"\"\n    Filters a DataFrame to include only rows where the specified date field falls on a weekend\n    (Saturday or Sunday) and adds a \"dq_status\" column indicating the rule applied.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame containing the data to be filtered.\n        rule (dict): A dictionary containing the rule parameters. It is expected to include:\n            - field (str): The name of the date column to check.\n            - check (str): A descriptive string for the type of check being performed.\n            - value (str): A value associated with the rule for documentation purposes.\n\n    Returns:\n        pd.DataFrame: A new DataFrame containing only the rows where the specified date field\n        falls on a weekend. Includes an additional \"dq_status\" column with the rule details.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    mask = ~df[field].dt.dayofweek.isin([5, 6])\n    out = df[mask].copy()\n    out[\"dq_status\"] = f\"{field}:{check}:{value}\"\n    return out\n</code></pre>"},{"location":"api/engine/engine-pandas/#sumeh.engine.pandas_engine.is_past_date","title":"<code>is_past_date(df, rule)</code>","text":"<p>Identifies rows in a DataFrame where the date in a specified column is in the past.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing the data to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to include          the field name to check and the check type.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing the rows where the date in the specified column           is earlier than the current date. An additional column 'dq_status' is           added to indicate the field, check type, and the current date.</p> Notes <ul> <li>The function uses <code>pd.to_datetime</code> to convert the specified column to datetime format.   Any invalid date entries will be coerced to NaT (Not a Time).</li> <li>Rows with invalid or missing dates are excluded from the result.</li> </ul> Source code in <code>sumeh/engine/pandas_engine.py</code> <pre><code>def is_past_date(df: pd.DataFrame, rule: dict) -&gt; pd.DataFrame:\n    \"\"\"\n    Identifies rows in a DataFrame where the date in a specified column is in the past.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame containing the data to be checked.\n        rule (dict): A dictionary containing the rule parameters. It is expected to include\n                     the field name to check and the check type.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing the rows where the date in the specified column\n                      is earlier than the current date. An additional column 'dq_status' is\n                      added to indicate the field, check type, and the current date.\n\n    Notes:\n        - The function uses `pd.to_datetime` to convert the specified column to datetime format.\n          Any invalid date entries will be coerced to NaT (Not a Time).\n        - Rows with invalid or missing dates are excluded from the result.\n    \"\"\"\n    field, check, _ = __extract_params(rule)\n    today = date.today()\n    dates = pd.to_datetime(df[field], errors=\"coerce\")\n    viol = df[dates &lt; today].copy()\n    viol[\"dq_status\"] = f\"{field}:{check}:{today.isoformat()}\"\n    return viol\n</code></pre>"},{"location":"api/engine/engine-pandas/#sumeh.engine.pandas_engine.is_positive","title":"<code>is_positive(df, rule)</code>","text":"<p>Identifies rows in a DataFrame where the specified field contains negative values.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to include: - 'field': The column name in the DataFrame to check. - 'check': A descriptive label for the type of check being performed. - 'value': A value associated with the rule (not directly used in this function).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing only the rows where the specified field has negative values.           An additional column 'dq_status' is added to indicate the rule violation in the format           \"{field}:{check}:{value}\".</p> Source code in <code>sumeh/engine/pandas_engine.py</code> <pre><code>def is_positive(df: pd.DataFrame, rule: dict) -&gt; pd.DataFrame:\n    \"\"\"\n    Identifies rows in a DataFrame where the specified field contains negative values.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame to be checked.\n        rule (dict): A dictionary containing the rule parameters. It is expected to include:\n            - 'field': The column name in the DataFrame to check.\n            - 'check': A descriptive label for the type of check being performed.\n            - 'value': A value associated with the rule (not directly used in this function).\n\n    Returns:\n        pd.DataFrame: A DataFrame containing only the rows where the specified field has negative values.\n                      An additional column 'dq_status' is added to indicate the rule violation in the format\n                      \"{field}:{check}:{value}\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    viol = df[df[field] &lt; 0].copy()\n    viol[\"dq_status\"] = f\"{field}:{check}:{value}\"\n    return viol\n</code></pre>"},{"location":"api/engine/engine-pandas/#sumeh.engine.pandas_engine.is_t_minus_2","title":"<code>is_t_minus_2(df, rule)</code>","text":"<p>Filters a DataFrame to include only rows where the specified date field matches the date two days prior to the current date.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing the data to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to include the field name, check type, and value.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A filtered DataFrame containing only the rows where the</p> <code>DataFrame</code> <p>specified date field matches the target date (two days prior). An</p> <code>DataFrame</code> <p>additional column \"dq_status\" is added to indicate the rule applied.</p> Source code in <code>sumeh/engine/pandas_engine.py</code> <pre><code>def is_t_minus_2(df: pd.DataFrame, rule: dict) -&gt; pd.DataFrame:\n    \"\"\"\n    Filters a DataFrame to include only rows where the specified date field\n    matches the date two days prior to the current date.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame containing the data to filter.\n        rule (dict): A dictionary containing the rule parameters. It is expected\n            to include the field name, check type, and value.\n\n    Returns:\n        pd.DataFrame: A filtered DataFrame containing only the rows where the\n        specified date field matches the target date (two days prior). An\n        additional column \"dq_status\" is added to indicate the rule applied.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    target = pd.Timestamp(date.today() - timedelta(days=2))\n    mask = df[field].dt.normalize() != target\n    out = df[mask].copy()\n    out[\"dq_status\"] = f\"{field}:{check}:{value}\"\n    return out\n</code></pre>"},{"location":"api/engine/engine-pandas/#sumeh.engine.pandas_engine.is_t_minus_3","title":"<code>is_t_minus_3(df, rule)</code>","text":"<p>Filters a DataFrame to include only rows where the specified date field matches the date three days prior to the current date.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing the data to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. The rule should include the field to check, the type of check, and the value.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A filtered DataFrame containing only the rows where the</p> <code>DataFrame</code> <p>specified date field matches the target date (three days prior). An</p> <code>DataFrame</code> <p>additional column \"dq_status\" is added to indicate the rule applied.</p> Source code in <code>sumeh/engine/pandas_engine.py</code> <pre><code>def is_t_minus_3(df: pd.DataFrame, rule: dict) -&gt; pd.DataFrame:\n    \"\"\"\n    Filters a DataFrame to include only rows where the specified date field\n    matches the date three days prior to the current date.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame containing the data to filter.\n        rule (dict): A dictionary containing the rule parameters. The rule\n            should include the field to check, the type of check, and the value.\n\n    Returns:\n        pd.DataFrame: A filtered DataFrame containing only the rows where the\n        specified date field matches the target date (three days prior). An\n        additional column \"dq_status\" is added to indicate the rule applied.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    target = pd.Timestamp(date.today() - timedelta(days=3))\n    mask = df[field].dt.normalize() != target\n    out = df[mask].copy()\n    out[\"dq_status\"] = f\"{field}:{check}:{value}\"\n    return out\n</code></pre>"},{"location":"api/engine/engine-pandas/#sumeh.engine.pandas_engine.is_today","title":"<code>is_today(df, rule)</code>","text":"<p>Filters a DataFrame to include only rows where the specified date field matches today's date.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing the data to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to include          the field name, a check operation, and a value.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A new DataFrame containing only the rows where the specified date field           matches today's date. An additional column \"dq_status\" is added to indicate           the rule applied in the format \"{field}:{check}:{value}\".</p> Source code in <code>sumeh/engine/pandas_engine.py</code> <pre><code>def is_today(df: pd.DataFrame, rule: dict) -&gt; pd.DataFrame:\n    \"\"\"\n    Filters a DataFrame to include only rows where the specified date field matches today's date.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame containing the data to filter.\n        rule (dict): A dictionary containing the rule parameters. It is expected to include\n                     the field name, a check operation, and a value.\n\n    Returns:\n        pd.DataFrame: A new DataFrame containing only the rows where the specified date field\n                      matches today's date. An additional column \"dq_status\" is added to indicate\n                      the rule applied in the format \"{field}:{check}:{value}\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    today = pd.Timestamp(date.today())\n    mask = df[field].dt.normalize() != today\n    out = df[mask].copy()\n    out[\"dq_status\"] = f\"{field}:{check}:{value}\"\n    return out\n</code></pre>"},{"location":"api/engine/engine-pandas/#sumeh.engine.pandas_engine.is_unique","title":"<code>is_unique(df, rule)</code>","text":"<p>Checks for duplicate values in a specified field of a DataFrame based on a rule.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to check for duplicates.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to          include the field to check, the type of check, and a value.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing the rows with duplicate values in the           specified field. An additional column 'dq_status' is added           to indicate the field, check type, and value associated with           the rule.</p> Source code in <code>sumeh/engine/pandas_engine.py</code> <pre><code>def is_unique(df: pd.DataFrame, rule: dict) -&gt; pd.DataFrame:\n    \"\"\"\n    Checks for duplicate values in a specified field of a DataFrame based on a rule.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame to check for duplicates.\n        rule (dict): A dictionary containing the rule parameters. It is expected to\n                     include the field to check, the type of check, and a value.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing the rows with duplicate values in the\n                      specified field. An additional column 'dq_status' is added\n                      to indicate the field, check type, and value associated with\n                      the rule.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    dup = df[field].duplicated(keep=False)\n    viol = df[dup].copy()\n    viol[\"dq_status\"] = f\"{field}:{check}:{value}\"\n    return viol\n</code></pre>"},{"location":"api/engine/engine-pandas/#sumeh.engine.pandas_engine.is_yesterday","title":"<code>is_yesterday(df, rule)</code>","text":"<p>Filters a DataFrame to include only rows where the specified date field matches yesterday's date.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing the data to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to have          keys that allow <code>__extract_params(rule)</code> to return the field name,          check type, and value.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A filtered DataFrame containing only rows where the specified date field           matches yesterday's date. An additional column <code>dq_status</code> is added to           indicate the data quality status in the format \"{field}:{check}:{value}\".</p> Source code in <code>sumeh/engine/pandas_engine.py</code> <pre><code>def is_yesterday(df: pd.DataFrame, rule: dict) -&gt; pd.DataFrame:\n    \"\"\"\n    Filters a DataFrame to include only rows where the specified date field matches yesterday's date.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame containing the data to filter.\n        rule (dict): A dictionary containing the rule parameters. It is expected to have\n                     keys that allow `__extract_params(rule)` to return the field name,\n                     check type, and value.\n\n    Returns:\n        pd.DataFrame: A filtered DataFrame containing only rows where the specified date field\n                      matches yesterday's date. An additional column `dq_status` is added to\n                      indicate the data quality status in the format \"{field}:{check}:{value}\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    target = pd.Timestamp(date.today() - timedelta(days=1))\n    mask = df[field].dt.normalize() != target\n    out = df[mask].copy()\n    out[\"dq_status\"] = f\"{field}:{check}:{value}\"\n    return out\n</code></pre>"},{"location":"api/engine/engine-pandas/#sumeh.engine.pandas_engine.not_contained_in","title":"<code>not_contained_in(df, rule)</code>","text":"<p>Filters a DataFrame to return rows where the specified field contains values that are not allowed according to the provided rule.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The column name in the DataFrame to check. - 'check': The type of check being performed (used for status annotation). - 'value': A list or string representation of values that are not allowed.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing rows that violate the rule. An additional</p> <code>DataFrame</code> <p>column 'dq_status' is added to indicate the rule violation in the format</p> <code>DataFrame</code> <p>\"{field}:{check}:{value}\".</p> Source code in <code>sumeh/engine/pandas_engine.py</code> <pre><code>def not_contained_in(df: pd.DataFrame, rule: dict) -&gt; pd.DataFrame:\n    \"\"\"\n    Filters a DataFrame to return rows where the specified field contains values\n    that are not allowed according to the provided rule.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame to be filtered.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field': The column name in the DataFrame to check.\n            - 'check': The type of check being performed (used for status annotation).\n            - 'value': A list or string representation of values that are not allowed.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing rows that violate the rule. An additional\n        column 'dq_status' is added to indicate the rule violation in the format\n        \"{field}:{check}:{value}\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    vals = re.findall(r\"'([^']*)'\", str(value)) or [\n        v.strip() for v in str(value).strip(\"[]\").split(\",\")\n    ]\n    viol = df[df[field].isin(vals)].copy()\n    viol[\"dq_status\"] = f\"{field}:{check}:{value}\"\n    return viol\n</code></pre>"},{"location":"api/engine/engine-pandas/#sumeh.engine.pandas_engine.not_in","title":"<code>not_in(df, rule)</code>","text":"<p>Filters a DataFrame by excluding rows that match the specified rule.</p> <p>This function is a wrapper around the <code>not_contained_in</code> function, which performs the actual filtering logic.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary specifying the filtering criteria.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A new DataFrame with rows that do not match the rule.</p> Source code in <code>sumeh/engine/pandas_engine.py</code> <pre><code>def not_in(df: pd.DataFrame, rule: dict) -&gt; pd.DataFrame:\n    \"\"\"\n    Filters a DataFrame by excluding rows that match the specified rule.\n\n    This function is a wrapper around the `not_contained_in` function,\n    which performs the actual filtering logic.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame to be filtered.\n        rule (dict): A dictionary specifying the filtering criteria.\n\n    Returns:\n        pd.DataFrame: A new DataFrame with rows that do not match the rule.\n    \"\"\"\n    return not_contained_in(df, rule)\n</code></pre>"},{"location":"api/engine/engine-pandas/#sumeh.engine.pandas_engine.satisfies","title":"<code>satisfies(df, rule)</code>","text":"<p>Filters a DataFrame based on a rule and returns rows that do not satisfy the rule.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to be evaluated.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule to be applied. It is expected to contain parameters that can be extracted using the <code>__extract_params</code> function.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing rows that do not satisfy the rule. An additional</p> <code>DataFrame</code> <p>column <code>dq_status</code> is added to indicate the field, check, and expression that failed.</p> Source code in <code>sumeh/engine/pandas_engine.py</code> <pre><code>def satisfies(df: pd.DataFrame, rule: dict) -&gt; pd.DataFrame:\n    \"\"\"\n    Filters a DataFrame based on a rule and returns rows that do not satisfy the rule.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame to be evaluated.\n        rule (dict): A dictionary containing the rule to be applied. It is expected\n            to contain parameters that can be extracted using the `__extract_params` function.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing rows that do not satisfy the rule. An additional\n        column `dq_status` is added to indicate the field, check, and expression that failed.\n    \"\"\"\n    field, check, expr = __extract_params(rule)\n    mask = df.eval(expr)\n    viol = df[~mask].copy()\n    viol[\"dq_status\"] = f\"{field}:{check}:{expr}\"\n    return viol\n</code></pre>"},{"location":"api/engine/engine-pandas/#sumeh.engine.pandas_engine.summarize","title":"<code>summarize(qc_df, rules, total_rows)</code>","text":"<p>Summarizes quality check results for a given DataFrame based on specified rules.</p> <p>Parameters:</p> Name Type Description Default <code>qc_df</code> <code>DataFrame</code> <p>The input DataFrame containing a 'dq_status' column with quality check results in the format 'column:rule:value', separated by semicolons.</p> required <code>rules</code> <code>list[dict]</code> <p>A list of dictionaries representing the quality check rules. Each dictionary should define the 'column', 'rule', 'value', and 'pass_threshold'.</p> required <code>total_rows</code> <code>int</code> <p>The total number of rows in the original dataset.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame summarizing the quality check results with the following columns: - 'id': A unique identifier for each rule. - 'timestamp': The timestamp of the summary generation. - 'check': The type of check performed (e.g., 'Quality Check'). - 'level': The severity level of the check (e.g., 'WARNING'). - 'column': The column name associated with the rule. - 'rule': The rule being checked. - 'value': The value associated with the rule. - 'rows': The total number of rows in the dataset. - 'violations': The number of rows that violated the rule. - 'pass_rate': The proportion of rows that passed the rule. - 'pass_threshold': The threshold for passing the rule. - 'status': The status of the rule ('PASS' or 'FAIL') based on the pass rate.</p> Notes <ul> <li>The function calculates the number of violations for each rule and merges it with the   provided rules to compute the pass rate and status.</li> <li>The 'timestamp' column is set to the current time with seconds and microseconds set to zero.</li> </ul> Source code in <code>sumeh/engine/pandas_engine.py</code> <pre><code>def summarize(qc_df: pd.DataFrame, rules: list[dict], total_rows: int) -&gt; pd.DataFrame:\n    \"\"\"\n    Summarizes quality check results for a given DataFrame based on specified rules.\n\n    Args:\n        qc_df (pd.DataFrame): The input DataFrame containing a 'dq_status' column with\n            quality check results in the format 'column:rule:value', separated by semicolons.\n        rules (list[dict]): A list of dictionaries representing the quality check rules.\n            Each dictionary should define the 'column', 'rule', 'value', and 'pass_threshold'.\n        total_rows (int): The total number of rows in the original dataset.\n\n    Returns:\n        pd.DataFrame: A DataFrame summarizing the quality check results with the following columns:\n            - 'id': A unique identifier for each rule.\n            - 'timestamp': The timestamp of the summary generation.\n            - 'check': The type of check performed (e.g., 'Quality Check').\n            - 'level': The severity level of the check (e.g., 'WARNING').\n            - 'column': The column name associated with the rule.\n            - 'rule': The rule being checked.\n            - 'value': The value associated with the rule.\n            - 'rows': The total number of rows in the dataset.\n            - 'violations': The number of rows that violated the rule.\n            - 'pass_rate': The proportion of rows that passed the rule.\n            - 'pass_threshold': The threshold for passing the rule.\n            - 'status': The status of the rule ('PASS' or 'FAIL') based on the pass rate.\n\n    Notes:\n        - The function calculates the number of violations for each rule and merges it with the\n          provided rules to compute the pass rate and status.\n        - The 'timestamp' column is set to the current time with seconds and microseconds set to zero.\n    \"\"\"\n    split = qc_df[\"dq_status\"].str.split(\";\").explode().dropna()\n    parts = split.str.split(\":\", expand=True)\n    parts.columns = [\"column\", \"rule\", \"value\"]\n    viol_count = (\n        parts.groupby([\"column\", \"rule\", \"value\"]).size().reset_index(name=\"violations\")\n    )\n    rules_df = __build_rules_df(rules)\n    df = rules_df.merge(viol_count, on=[\"column\", \"rule\", \"value\"], how=\"left\")\n    df[\"violations\"] = df[\"violations\"].fillna(0).astype(int)\n    df[\"rows\"] = total_rows\n    df[\"pass_rate\"] = (total_rows - df[\"violations\"]) / total_rows\n    df[\"status\"] = np.where(df[\"pass_rate\"] &gt;= df[\"pass_threshold\"], \"PASS\", \"FAIL\")\n    df[\"timestamp\"] = datetime.now().replace(second=0, microsecond=0)\n    df[\"check\"] = \"Quality Check\"\n    df[\"level\"] = \"WARNING\"\n    df.insert(0, \"id\", np.array([uuid.uuid4() for _ in range(len(df))], dtype=\"object\"))\n    return df[\n        [\n            \"id\",\n            \"timestamp\",\n            \"check\",\n            \"level\",\n            \"column\",\n            \"rule\",\n            \"value\",\n            \"rows\",\n            \"violations\",\n            \"pass_rate\",\n            \"pass_threshold\",\n            \"status\",\n        ]\n    ]\n</code></pre>"},{"location":"api/engine/engine-pandas/#sumeh.engine.pandas_engine.validate","title":"<code>validate(df, rules)</code>","text":"<p>Validates a pandas DataFrame against a set of rules and returns the processed DataFrame along with a DataFrame containing validation violations.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to validate.</p> required <code>rules</code> <code>list[dict]</code> <p>A list of dictionaries, where each dictionary represents a validation rule. Each rule should contain the following keys: - 'check_type' (str): The type of validation to perform. This should correspond to a   function name available in the global scope. Special cases include 'is_primary_key'   and 'is_composite_key', which map to 'is_unique' and 'are_unique', respectively. - 'execute' (bool, optional): Whether to execute the rule. Defaults to True.</p> required <p>Returns:</p> Type Description <code>Tuple[DataFrame, DataFrame]</code> <p>Tuple[pd.DataFrame, pd.DataFrame]: A tuple containing: - The processed DataFrame with validation statuses merged. - A DataFrame containing rows that violated the validation rules.</p> Notes <ul> <li>The input DataFrame is copied and reset to ensure the original data is not modified.</li> <li>An '_id' column is temporarily added to track row indices during validation.</li> <li>If a rule's 'check_type' does not correspond to a known function, a warning is issued.</li> <li>The 'dq_status' column in the violations DataFrame summarizes validation issues for   each row.</li> </ul> Source code in <code>sumeh/engine/pandas_engine.py</code> <pre><code>def validate(df: pd.DataFrame, rules: list[dict]) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Validates a pandas DataFrame against a set of rules and returns the processed DataFrame\n    along with a DataFrame containing validation violations.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame to validate.\n        rules (list[dict]): A list of dictionaries, where each dictionary represents a validation\n            rule. Each rule should contain the following keys:\n            - 'check_type' (str): The type of validation to perform. This should correspond to a\n              function name available in the global scope. Special cases include 'is_primary_key'\n              and 'is_composite_key', which map to 'is_unique' and 'are_unique', respectively.\n            - 'execute' (bool, optional): Whether to execute the rule. Defaults to True.\n\n    Returns:\n        Tuple[pd.DataFrame, pd.DataFrame]: A tuple containing:\n            - The processed DataFrame with validation statuses merged.\n            - A DataFrame containing rows that violated the validation rules.\n\n    Notes:\n        - The input DataFrame is copied and reset to ensure the original data is not modified.\n        - An '_id' column is temporarily added to track row indices during validation.\n        - If a rule's 'check_type' does not correspond to a known function, a warning is issued.\n        - The 'dq_status' column in the violations DataFrame summarizes validation issues for\n          each row.\n    \"\"\"\n    df = df.copy().reset_index(drop=True)\n    df[\"_id\"] = df.index\n    raw_list = []\n    for rule in rules:\n        if not rule.get(\"execute\", True):\n            continue\n        rt = rule[\"check_type\"]\n        fn = globals().get(\n            rt\n            if rt not in (\"is_primary_key\", \"is_composite_key\")\n            else (\"is_unique\" if rt == \"is_primary_key\" else \"are_unique\")\n        )\n        if fn is None:\n            warnings.warn(f\"Unknown rule: {rt}\")\n            continue\n        viol = fn(df, rule)\n        raw_list.append(viol)\n    raw = (\n        pd.concat(raw_list, ignore_index=True)\n        if raw_list\n        else pd.DataFrame(columns=df.columns)\n    )\n    summary = raw.groupby(\"_id\")[\"dq_status\"].agg(\";\".join).reset_index()\n    out = df.merge(summary, on=\"_id\", how=\"left\").drop(columns=[\"_id\"])\n    return out, raw\n</code></pre>"},{"location":"api/engine/engine-pandas/#sumeh.engine.pandas_engine.validate_date_format","title":"<code>validate_date_format(df, rule)</code>","text":"<p>Validates the date format of a specified field in a DataFrame against a given format.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing the data to validate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the validation rule. It should include: - 'field': The name of the column to validate. - 'check': A description or identifier for the validation check. - 'fmt': The expected date format to validate against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing rows that violate the date format rule.           An additional column 'dq_status' is added to indicate the           validation status in the format \"{field}:{check}:{fmt}\".</p> Source code in <code>sumeh/engine/pandas_engine.py</code> <pre><code>def validate_date_format(df: pd.DataFrame, rule: dict) -&gt; pd.DataFrame:\n    \"\"\"\n    Validates the date format of a specified field in a DataFrame against a given format.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame containing the data to validate.\n        rule (dict): A dictionary containing the validation rule. It should include:\n            - 'field': The name of the column to validate.\n            - 'check': A description or identifier for the validation check.\n            - 'fmt': The expected date format to validate against.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing rows that violate the date format rule.\n                      An additional column 'dq_status' is added to indicate the\n                      validation status in the format \"{field}:{check}:{fmt}\".\n    \"\"\"\n    field, check, fmt = __extract_params(rule)\n    pattern = __transform_date_format_in_pattern(fmt)\n    mask = ~df[field].astype(str).str.match(pattern, na=False) | df[field].isna()\n    viol = df[mask].copy()\n    viol[\"dq_status\"] = f\"{field}:{check}:{fmt}\"\n    return viol\n</code></pre>"},{"location":"api/engine/engine-pandas/#sumeh.engine.pandas_engine.validate_schema","title":"<code>validate_schema(df, expected)</code>","text":"<p>Validates the schema of a given DataFrame against an expected schema.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <p>The DataFrame whose schema needs to be validated.</p> required <code>expected</code> <p>The expected schema, represented as a list of tuples where each tuple       contains the column name and its data type.</p> required <p>Returns:</p> Type Description <code>Tuple[bool, List[Tuple[str, str]]]</code> <p>Tuple[bool, List[Tuple[str, str]]]: A tuple containing: - A boolean indicating whether the schema matches the expected schema. - A list of tuples representing the errors, where each tuple contains   the column name and a description of the mismatch.</p> Source code in <code>sumeh/engine/pandas_engine.py</code> <pre><code>def validate_schema(df, expected) -&gt; Tuple[bool, List[Tuple[str, str]]]:\n    \"\"\"\n    Validates the schema of a given DataFrame against an expected schema.\n\n    Args:\n        df: The DataFrame whose schema needs to be validated.\n        expected: The expected schema, represented as a list of tuples where each tuple\n                  contains the column name and its data type.\n\n    Returns:\n        Tuple[bool, List[Tuple[str, str]]]: A tuple containing:\n            - A boolean indicating whether the schema matches the expected schema.\n            - A list of tuples representing the errors, where each tuple contains\n              the column name and a description of the mismatch.\n    \"\"\"\n    actual = __pandas_schema_to_list(df)\n    result, errors = __compare_schemas(actual, expected)\n    return result, errors\n</code></pre>"},{"location":"api/engine/engine-polars/","title":"Module <code>sumeh.engine.polars_engine</code>","text":"<p>This module provides a set of data quality validation functions using the Polars library. It includes various checks for data validation, such as completeness, uniqueness, range checks, pattern matching, and schema validation.</p> <p>Functions:</p> Name Description <code>is_positive</code> <p>Filters rows where the specified field is less than zero.</p> <code>is_negative</code> <p>Filters rows where the specified field is greater than or equal to zero.</p> <code>is_complete</code> <p>Filters rows where the specified field is null.</p> <code>is_unique</code> <p>Filters rows with duplicate values in the specified field.</p> <code>are_complete</code> <p>Filters rows where any of the specified fields are null.</p> <code>are_unique</code> <p>Filters rows with duplicate combinations of the specified fields.</p> <code>is_greater_than</code> <p>Filters rows where the specified field is less than or equal to the given value.</p> <code>is_greater_or_equal_than</code> <p>Filters rows where the specified field is less than the given value.</p> <code>is_less_than</code> <p>Filters rows where the specified field is greater than or equal to the given value.</p> <code>is_less_or_equal_than</code> <p>Filters rows where the specified field is greater than the given value.</p> <code>is_equal</code> <p>Filters rows where the specified field is not equal to the given value.</p> <code>is_equal_than</code> <p>Alias for <code>is_equal</code>.</p> <code>is_in_millions</code> <p>Retains rows where the field value is less than 1,000,000 and flags them with dq_status.</p> <code>is_in_billions</code> <p>Retains rows where the field value is less than 1,000,000,000 and flags them with dq_status.</p> <code>is_t_minus_1</code> <p>Retains rows where the date field not equals yesterday (T-1) and flags them with dq_status.</p> <code>is_t_minus_2</code> <p>Retains rows where the date field not equals two days ago (T-2) and flags them with dq_status.</p> <code>is_t_minus_3</code> <p>Retains rows where the date field not equals three days ago (T-3) and flags them with dq_status.</p> <code>is_today</code> <p>Retains rows where the date field not equals today and flags them with dq_status.</p> <code>is_yesterday</code> <p>Retains rows where the date field not equals yesterday and flags them with dq_status.</p> <code>is_on_weekday</code> <p>Retains rows where the date field not falls on a weekday (Mon-Fri) and flags them with dq_status.</p> <code>is_on_weekend</code> <p>Retains rows where the date field is not on a weekend (Sat-Sun) and flags them with dq_status.</p> <code>is_on_monday</code> <p>Retains rows where the date field is not on Monday and flags them with dq_status.</p> <code>is_on_tuesday</code> <p>Retains rows where the date field is not on Tuesday and flags them with dq_status.</p> <code>is_on_wednesday</code> <p>Retains rows where the date field is not on Wednesday and flags them with dq_status.</p> <code>is_on_thursday</code> <p>Retains rows where the date field is not on Thursday and flags them with dq_status.</p> <code>is_on_friday</code> <p>Retains rows where the date field is not on Friday and flags them with dq_status.</p> <code>is_on_saturday</code> <p>Retains rows where the date field is not on Saturday and flags them with dq_status.</p> <code>is_on_sunday</code> <p>Retains rows where the date field is not on Sunday and flags them with dq_status.</p> <code>is_contained_in</code> <p>Filters rows where the specified field is not in the given list of values.</p> <code>not_contained_in</code> <p>Filters rows where the specified field is in the given list of values.</p> <code>is_between</code> <p>Filters rows where the specified field is not within the given range.</p> <code>has_pattern</code> <p>Filters rows where the specified field does not match the given regex pattern.</p> <code>is_legit</code> <p>Filters rows where the specified field is null or contains whitespace.</p> <code>has_max</code> <p>Filters rows where the specified field exceeds the given maximum value.</p> <code>has_min</code> <p>Filters rows where the specified field is below the given minimum value.</p> <code>has_std</code> <p>Checks if the standard deviation of the specified field exceeds the given value.</p> <code>has_mean</code> <p>Checks if the mean of the specified field exceeds the given value.</p> <code>has_sum</code> <p>Checks if the sum of the specified field exceeds the given value.</p> <code>has_cardinality</code> <p>Checks if the cardinality (number of unique values) of the specified field exceeds the given value.</p> <code>has_infogain</code> <p>Placeholder for information gain validation (currently uses cardinality).</p> <code>has_entropy</code> <p>Placeholder for entropy validation (currently uses cardinality).</p> <code>satisfies</code> <p>Filters rows that do not satisfy the given SQL condition.</p> <code>validate_date_format</code> <p>Filters rows where the specified field does not match the expected date format or is null.</p> <code>is_future_date</code> <p>Filters rows where the specified date field is after today.</p> <code>is_past_date</code> <p>Filters rows where the specified date field is before today.</p> <code>is_date_between</code> <p>Filters rows where the specified date field is not within the given [start,end] range.</p> <code>is_date_after</code> <p>Filters rows where the specified date field is before the given date.</p> <code>is_date_before</code> <p>Filters rows where the specified date field is after the given date.</p> <code>all_date_checks</code> <p>Alias for <code>is_past_date</code> (checks date against today).</p> <code>validate</code> <p>Validates a DataFrame against a list of rules and returns the original DataFrame with data quality status and a DataFrame of violations.</p> <code>__build_rules_df</code> <p>Converts a list of rules into a Polars DataFrame for summarization.</p> <code>summarize</code> <p>Summarizes the results of data quality checks, including pass rates and statuses.</p> <code>__polars_schema_to_list</code> <p>Converts a Polars DataFrame schema into a list of dictionaries.</p> <code>validate_schema</code> <p>Validates the schema of a DataFrame against an expected schema and returns a boolean result and a list of errors.</p>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.__build_rules_df","title":"<code>__build_rules_df(rules)</code>","text":"<p>Builds a Polars DataFrame from a list of rule dictionaries.</p> <p>This function processes a list of rule dictionaries, filters out rules that are not marked for execution, and constructs a DataFrame with the relevant rule information. It ensures uniqueness of rows based on specific columns and casts the data to appropriate types.</p> <p>Parameters:</p> Name Type Description Default <code>rules</code> <code>list[dict]</code> <p>A list of dictionaries, where each dictionary represents a rule. Each rule dictionary may contain the following keys: - \"field\" (str or list): The column(s) the rule applies to. - \"check_type\" (str): The type of rule or check. - \"threshold\" (float, optional): The pass threshold for the rule. Defaults to 1.0. - \"value\" (any, optional): Additional value associated with the rule. - \"execute\" (bool, optional): Whether the rule should be executed. Defaults to True.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A Polars DataFrame containing the processed rules with the following columns: - \"column\" (str): The column(s) the rule applies to, joined by commas if multiple. - \"rule\" (str): The type of rule or check. - \"pass_threshold\" (float): The pass threshold for the rule. - \"value\" (str): The value associated with the rule, or an empty string if not provided.</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def __build_rules_df(rules: list[dict]) -&gt; pl.DataFrame:\n    \"\"\"\n    Builds a Polars DataFrame from a list of rule dictionaries.\n\n    This function processes a list of rule dictionaries, filters out rules\n    that are not marked for execution, and constructs a DataFrame with the\n    relevant rule information. It ensures uniqueness of rows based on\n    specific columns and casts the data to appropriate types.\n\n    Args:\n        rules (list[dict]): A list of dictionaries, where each dictionary\n            represents a rule. Each rule dictionary may contain the following keys:\n            - \"field\" (str or list): The column(s) the rule applies to.\n            - \"check_type\" (str): The type of rule or check.\n            - \"threshold\" (float, optional): The pass threshold for the rule. Defaults to 1.0.\n            - \"value\" (any, optional): Additional value associated with the rule.\n            - \"execute\" (bool, optional): Whether the rule should be executed. Defaults to True.\n\n    Returns:\n        pl.DataFrame: A Polars DataFrame containing the processed rules with the following columns:\n            - \"column\" (str): The column(s) the rule applies to, joined by commas if multiple.\n            - \"rule\" (str): The type of rule or check.\n            - \"pass_threshold\" (float): The pass threshold for the rule.\n            - \"value\" (str): The value associated with the rule, or an empty string if not provided.\n    \"\"\"\n    rules_df = (\n        pl.DataFrame(\n            [\n                {\n                    \"column\": (\n                        \",\".join(r[\"field\"])\n                        if isinstance(r[\"field\"], list)\n                        else r[\"field\"]\n                    ),\n                    \"rule\": r[\"check_type\"],\n                    \"pass_threshold\": float(r.get(\"threshold\") or 1.0),\n                    \"value\": r.get(\"value\"),\n                }\n                for r in rules\n                if r.get(\"execute\", True)\n            ]\n        )\n        .unique(subset=[\"column\", \"rule\", \"value\"])\n        .with_columns(\n            [\n                pl.col(\"column\").cast(str),\n                pl.col(\"rule\").cast(str),\n                pl.col(\"value\").cast(str),\n            ]\n        )\n    ).with_columns(pl.col(\"value\").fill_null(\"\").alias(\"value\"))\n\n    return rules_df\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.__compare_schemas","title":"<code>__compare_schemas(actual, expected)</code>","text":"<p>Compare two lists of schema definitions and identify discrepancies.</p> <p>Parameters:</p> Name Type Description Default <code>actual</code> <code>List[SchemaDef]</code> <p>The list of actual schema definitions.</p> required <code>expected</code> <code>List[SchemaDef]</code> <p>The list of expected schema definitions.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>Tuple[bool, List[Tuple[str, str]]]: A tuple where the first element is a boolean indicating</p> <code>List[Tuple[str, str]]</code> <p>whether the schemas match (True if they match, False otherwise), and the second element</p> <code>Tuple[bool, List[Tuple[str, str]]]</code> <p>is a list of tuples describing the discrepancies. Each tuple contains: - The field name (str). - A description of the discrepancy (str), such as \"missing\", \"type mismatch\",   \"nullable but expected non-nullable\", or \"extra column\".</p> Notes <ul> <li>A field is considered \"missing\" if it exists in the expected schema but not in the actual schema.</li> <li>A \"type mismatch\" occurs if the data type of a field in the actual schema does not match   the expected data type.</li> <li>A field is considered \"nullable but expected non-nullable\" if it is nullable in the actual   schema but not nullable in the expected schema.</li> <li>An \"extra column\" is a field that exists in the actual schema but not in the expected schema.</li> </ul> Source code in <code>sumeh/services/utils.py</code> <pre><code>def __compare_schemas(\n    actual: List[SchemaDef],\n    expected: List[SchemaDef],\n) -&gt; Tuple[bool, List[Tuple[str, str]]]:\n    \"\"\"\n    Compare two lists of schema definitions and identify discrepancies.\n\n    Args:\n        actual (List[SchemaDef]): The list of actual schema definitions.\n        expected (List[SchemaDef]): The list of expected schema definitions.\n\n    Returns:\n        Tuple[bool, List[Tuple[str, str]]]: A tuple where the first element is a boolean indicating\n        whether the schemas match (True if they match, False otherwise), and the second element\n        is a list of tuples describing the discrepancies. Each tuple contains:\n            - The field name (str).\n            - A description of the discrepancy (str), such as \"missing\", \"type mismatch\",\n              \"nullable but expected non-nullable\", or \"extra column\".\n\n    Notes:\n        - A field is considered \"missing\" if it exists in the expected schema but not in the actual schema.\n        - A \"type mismatch\" occurs if the data type of a field in the actual schema does not match\n          the expected data type.\n        - A field is considered \"nullable but expected non-nullable\" if it is nullable in the actual\n          schema but not nullable in the expected schema.\n        - An \"extra column\" is a field that exists in the actual schema but not in the expected schema.\n    \"\"\"\n\n    exp_map = {c[\"field\"]: c for c in expected}\n    act_map = {c[\"field\"]: c for c in actual}\n\n    erros: List[Tuple[str, str]] = []\n\n    for fld, exp in exp_map.items():\n        if fld not in act_map:\n            erros.append((fld, \"missing\"))\n            continue\n        act = act_map[fld]\n        if act[\"data_type\"] != exp[\"data_type\"]:\n            erros.append(\n                (\n                    fld,\n                    f\"type mismatch (got {act['data_type']}, expected {exp['data_type']})\",\n                )\n            )\n\n        if act[\"nullable\"] and not exp[\"nullable\"]:\n            erros.append((fld, \"nullable but expected non-nullable\"))\n\n        if exp.get(\"max_length\") is not None:\n            pass\n\n    # 2. campos extras (se quiser)\n    extras = set(act_map) - set(exp_map)\n    for fld in extras:\n        erros.append((fld, \"extra column\"))\n\n    return len(erros) == 0, erros\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.__convert_value","title":"<code>__convert_value(value)</code>","text":"<p>Converts the provided value to the appropriate type (date, float, or int).</p> <p>Depending on the format of the input value, it will be converted to a datetime object, a floating-point number (float), or an integer (int).</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>The value to be converted, represented as a string.</p> required <p>Returns:</p> Type Description <p>Union[datetime, float, int]: The converted value, which can be a datetime object, float, or int.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the value does not match an expected format.</p> Source code in <code>sumeh/services/utils.py</code> <pre><code>def __convert_value(value):\n    \"\"\"\n    Converts the provided value to the appropriate type (date, float, or int).\n\n    Depending on the format of the input value, it will be converted to a datetime object,\n    a floating-point number (float), or an integer (int).\n\n    Args:\n        value (str): The value to be converted, represented as a string.\n\n    Returns:\n        Union[datetime, float, int]: The converted value, which can be a datetime object, float, or int.\n\n    Raises:\n        ValueError: If the value does not match an expected format.\n    \"\"\"\n    from datetime import datetime\n\n    value = value.strip()\n    try:\n        if \"-\" in value:\n            return datetime.strptime(value, \"%Y-%m-%d\")\n        else:\n            return datetime.strptime(value, \"%d/%m/%Y\")\n    except ValueError:\n        if \".\" in value:\n            return float(value)\n        return int(value)\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.__extract_params","title":"<code>__extract_params(rule)</code>","text":"Source code in <code>sumeh/services/utils.py</code> <pre><code>def __extract_params(rule: dict) -&gt; tuple:\n    rule_name = rule[\"check_type\"]\n    field = rule[\"field\"]\n    raw_value = rule.get(\"value\")\n    if isinstance(raw_value, str) and raw_value not in (None, \"\", \"NULL\"):\n        try:\n            value = __convert_value(raw_value)\n        except ValueError:\n            value = raw_value\n    else:\n        value = raw_value\n    value = value if value not in (None, \"\", \"NULL\") else \"\"\n    return field, rule_name, value\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.__polars_schema_to_list","title":"<code>__polars_schema_to_list(df)</code>","text":"<p>Converts the schema of a Polars DataFrame into a list of dictionaries, where each dictionary represents a field in the schema.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The Polars DataFrame whose schema is to be converted.</p> required <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List[Dict[str, Any]]: A list of dictionaries, each containing the following keys: - \"field\" (str): The name of the field. - \"data_type\" (str): The data type of the field, converted to lowercase. - \"nullable\" (bool): Always set to True, as Polars does not expose nullability in the schema. - \"max_length\" (None): Always set to None, as max length is not applicable.</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def __polars_schema_to_list(df: pl.DataFrame) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Converts the schema of a Polars DataFrame into a list of dictionaries,\n    where each dictionary represents a field in the schema.\n\n    Args:\n        df (pl.DataFrame): The Polars DataFrame whose schema is to be converted.\n\n    Returns:\n        List[Dict[str, Any]]: A list of dictionaries, each containing the following keys:\n            - \"field\" (str): The name of the field.\n            - \"data_type\" (str): The data type of the field, converted to lowercase.\n            - \"nullable\" (bool): Always set to True, as Polars does not expose nullability in the schema.\n            - \"max_length\" (None): Always set to None, as max length is not applicable.\n    \"\"\"\n    return [\n        {\n            \"field\": name,\n            \"data_type\": str(dtype).lower(),\n            \"nullable\": True,  # Polars n\u00e3o exp\u00f5e nullability no schema\n            \"max_length\": None,\n        }\n        for name, dtype in df.schema.items()\n    ]\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.__transform_date_format_in_pattern","title":"<code>__transform_date_format_in_pattern(date_format)</code>","text":"Source code in <code>sumeh/services/utils.py</code> <pre><code>def __transform_date_format_in_pattern(date_format):\n    date_patterns = {\n        \"DD\": \"(0[1-9]|[12][0-9]|3[01])\",\n        \"MM\": \"(0[1-9]|1[012])\",\n        \"YYYY\": \"(19|20)\\\\d\\\\d\",\n        \"YY\": \"\\\\d\\\\d\",\n        \" \": \"\\\\s\",\n        \".\": \"\\\\.\",\n    }\n\n    date_pattern = date_format\n    for single_format, pattern in date_patterns.items():\n        date_pattern = date_pattern.replace(single_format, pattern)\n\n    return date_pattern\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine._day_of_week","title":"<code>_day_of_week(df, rule, dow)</code>","text":"<p>Filters a Polars DataFrame to include only rows where the day of the week of a specified date column matches the given day of the week (dow). Adds a new column indicating the data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. The rule should include the field name, check type, and value.</p> required <code>dow</code> <code>int</code> <p>The target day of the week (0 = Monday, 6 = Sunday).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new DataFrame filtered by the specified day of the week</p> <code>DataFrame</code> <p>and with an additional \"dq_status\" column indicating the rule applied.</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def _day_of_week(df: pl.DataFrame, rule: dict, dow: int) -&gt; pl.DataFrame:\n    \"\"\"\n    Filters a Polars DataFrame to include only rows where the day of the week\n    of a specified date column matches the given day of the week (dow). Adds\n    a new column indicating the data quality status.\n\n    Args:\n        df (pl.DataFrame): The input Polars DataFrame.\n        rule (dict): A dictionary containing the rule parameters. The rule\n            should include the field name, check type, and value.\n        dow (int): The target day of the week (0 = Monday, 6 = Sunday).\n\n    Returns:\n        pl.DataFrame: A new DataFrame filtered by the specified day of the week\n        and with an additional \"dq_status\" column indicating the rule applied.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    return df.filter(\n        pl.col(field).str.strptime(pl.Date, \"%Y-%m-%d\").dt.weekday() == dow\n    ).with_columns(pl.lit(f\"{field}:{check}:{value}\").alias(\"dq_status\"))\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.all_date_checks","title":"<code>all_date_checks(df, rule)</code>","text":"<p>Applies all date-related validation checks on the given DataFrame based on the specified rule.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to validate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the validation rules to apply.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: The DataFrame after applying the date validation checks.</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def all_date_checks(df: pl.DataFrame, rule: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Applies all date-related validation checks on the given DataFrame based on the specified rule.\n\n    Args:\n        df (pl.DataFrame): The input DataFrame to validate.\n        rule (dict): A dictionary containing the validation rules to apply.\n\n    Returns:\n        pl.DataFrame: The DataFrame after applying the date validation checks.\n    \"\"\"\n    return is_past_date(df, rule)\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.are_complete","title":"<code>are_complete(df, rule)</code>","text":"<p>Filters a Polars DataFrame to identify rows where specified fields contain null values and tags them with a data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'fields': A list of column names to check for null values. - 'check': A string representing the type of check (e.g., \"is_null\"). - 'value': A value associated with the check (not used in this function).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A filtered DataFrame containing only rows where at least one of the</p> <code>DataFrame</code> <p>specified fields is null, with an additional column \"dq_status\" indicating the</p> <code>DataFrame</code> <p>data quality status.</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def are_complete(df: pl.DataFrame, rule: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Filters a Polars DataFrame to identify rows where specified fields contain null values\n    and tags them with a data quality status.\n\n    Args:\n        df (pl.DataFrame): The input Polars DataFrame to be checked.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'fields': A list of column names to check for null values.\n            - 'check': A string representing the type of check (e.g., \"is_null\").\n            - 'value': A value associated with the check (not used in this function).\n\n    Returns:\n        pl.DataFrame: A filtered DataFrame containing only rows where at least one of the\n        specified fields is null, with an additional column \"dq_status\" indicating the\n        data quality status.\n    \"\"\"\n    fields, check, value = __extract_params(rule)\n    cond = reduce(operator.or_, [pl.col(f).is_null() for f in fields])\n\n    tag = f\"{fields}:{check}:{value}\"\n    return df.filter(cond).with_columns(pl.lit(tag).alias(\"dq_status\"))\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.are_unique","title":"<code>are_unique(df, rule)</code>","text":"<p>Checks for duplicate combinations of specified fields in a Polars DataFrame and returns a DataFrame containing the rows with duplicates along with a data quality status column.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to check for duplicates.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected          to include the following keys:          - 'fields': A list of column names to check for uniqueness.          - 'check': A string representing the type of check (e.g., \"unique\").          - 'value': A value associated with the check (e.g., \"True\").</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A DataFrame containing rows with duplicate combinations of           the specified fields. An additional column, \"dq_status\",           is added to indicate the data quality status in the format           \"{fields}:{check}:{value}\".</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def are_unique(df: pl.DataFrame, rule: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Checks for duplicate combinations of specified fields in a Polars DataFrame\n    and returns a DataFrame containing the rows with duplicates along with a\n    data quality status column.\n\n    Args:\n        df (pl.DataFrame): The input Polars DataFrame to check for duplicates.\n        rule (dict): A dictionary containing the rule parameters. It is expected\n                     to include the following keys:\n                     - 'fields': A list of column names to check for uniqueness.\n                     - 'check': A string representing the type of check (e.g., \"unique\").\n                     - 'value': A value associated with the check (e.g., \"True\").\n\n    Returns:\n        pl.DataFrame: A DataFrame containing rows with duplicate combinations of\n                      the specified fields. An additional column, \"dq_status\",\n                      is added to indicate the data quality status in the format\n                      \"{fields}:{check}:{value}\".\n    \"\"\"\n    fields, check, value = __extract_params(rule)\n    combo = df.with_columns(\n        pl.concat_str([pl.col(f).cast(str) for f in fields], separator=\"|\").alias(\n            \"_combo\"\n        )\n    )\n    dupes = (\n        combo.group_by(\"_combo\")\n        .agg(pl.len().alias(\"cnt\"))\n        .filter(pl.col(\"cnt\") &gt; 1)\n        .select(\"_combo\")\n        .to_series()\n        .to_list()\n    )\n    return (\n        combo.filter(pl.col(\"_combo\").is_in(dupes))\n        .drop(\"_combo\")\n        .with_columns(pl.lit(f\"{fields}:{check}:{value}\").alias(\"dq_status\"))\n    )\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.has_cardinality","title":"<code>has_cardinality(df, rule)</code>","text":"<p>Checks if the cardinality (number of unique values) of a specified field in the given DataFrame satisfies a condition defined in the rule. If the cardinality exceeds the specified value, a new column \"dq_status\" is added to the DataFrame with a string indicating the rule violation. Otherwise, an empty DataFrame is returned.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to evaluate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - \"field\" (str): The column name to check. - \"check\" (str): The type of check (e.g., \"greater_than\"). - \"value\" (int): The threshold value for the cardinality.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: The original DataFrame with an added \"dq_status\" column if the rule is violated,           or an empty DataFrame if the rule is not violated.</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def has_cardinality(df: pl.DataFrame, rule: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Checks if the cardinality (number of unique values) of a specified field in the given DataFrame\n    satisfies a condition defined in the rule. If the cardinality exceeds the specified value,\n    a new column \"dq_status\" is added to the DataFrame with a string indicating the rule violation.\n    Otherwise, an empty DataFrame is returned.\n\n    Args:\n        df (pl.DataFrame): The input DataFrame to evaluate.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - \"field\" (str): The column name to check.\n            - \"check\" (str): The type of check (e.g., \"greater_than\").\n            - \"value\" (int): The threshold value for the cardinality.\n\n    Returns:\n        pl.DataFrame: The original DataFrame with an added \"dq_status\" column if the rule is violated,\n                      or an empty DataFrame if the rule is not violated.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    card = df.select(pl.col(field).n_unique()).to_numpy()[0] or 0\n    if card &gt; value:\n        return df.with_columns(pl.lit(f\"{field}:{check}:{value}\").alias(\"dq_status\"))\n    return df.head(0)\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.has_entropy","title":"<code>has_entropy(df, rule)</code>","text":"<p>Evaluates the entropy of a specified field in a Polars DataFrame based on a given rule.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to evaluate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The column name in the DataFrame to evaluate. - 'check' (str): The type of check to perform (not used directly in this function). - 'value' (float): The threshold value for entropy comparison.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: - If the entropy of the specified field exceeds the given threshold (<code>value</code>),   returns the original DataFrame with an additional column <code>dq_status</code> indicating   the rule that was applied. - If the entropy does not exceed the threshold, returns an empty DataFrame with   the same schema as the input DataFrame.</p> Notes <ul> <li>The entropy is calculated as the number of unique values in the specified field.</li> <li>The <code>dq_status</code> column contains a string in the format \"{field}:{check}:{value}\".</li> </ul> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def has_entropy(df: pl.DataFrame, rule: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Evaluates the entropy of a specified field in a Polars DataFrame based on a given rule.\n\n    Parameters:\n        df (pl.DataFrame): The input Polars DataFrame to evaluate.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field' (str): The column name in the DataFrame to evaluate.\n            - 'check' (str): The type of check to perform (not used directly in this function).\n            - 'value' (float): The threshold value for entropy comparison.\n\n    Returns:\n        pl.DataFrame:\n            - If the entropy of the specified field exceeds the given threshold (`value`),\n              returns the original DataFrame with an additional column `dq_status` indicating\n              the rule that was applied.\n            - If the entropy does not exceed the threshold, returns an empty DataFrame with\n              the same schema as the input DataFrame.\n\n    Notes:\n        - The entropy is calculated as the number of unique values in the specified field.\n        - The `dq_status` column contains a string in the format \"{field}:{check}:{value}\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    ent = df.select(pl.col(field).n_unique()).to_numpy()[0] or 0.0\n    if ent &gt; value:\n        return df.with_columns(pl.lit(f\"{field}:{check}:{value}\").alias(\"dq_status\"))\n    return df.head(0)\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.has_infogain","title":"<code>has_infogain(df, rule)</code>","text":"<p>Evaluates whether a given DataFrame satisfies an information gain condition based on a specified rule. If the condition is met, a new column indicating the rule is added; otherwise, an empty DataFrame is returned.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to evaluate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include the following keys: - 'field': The column name to evaluate. - 'check': The type of check to perform (not used directly in this function). - 'value': The threshold value for the information gain.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: The original DataFrame with an additional column named</p> <code>DataFrame</code> <p>\"dq_status\" if the condition is met, or an empty DataFrame if the</p> <code>DataFrame</code> <p>condition is not met.</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def has_infogain(df: pl.DataFrame, rule: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Evaluates whether a given DataFrame satisfies an information gain condition\n    based on a specified rule. If the condition is met, a new column indicating\n    the rule is added; otherwise, an empty DataFrame is returned.\n\n    Args:\n        df (pl.DataFrame): The input DataFrame to evaluate.\n        rule (dict): A dictionary containing the rule parameters. It should\n            include the following keys:\n            - 'field': The column name to evaluate.\n            - 'check': The type of check to perform (not used directly in this function).\n            - 'value': The threshold value for the information gain.\n\n    Returns:\n        pl.DataFrame: The original DataFrame with an additional column named\n        \"dq_status\" if the condition is met, or an empty DataFrame if the\n        condition is not met.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    ig = df.select(pl.col(field).n_unique()).to_numpy()[0] or 0.0\n    if ig &gt; value:\n        return df.with_columns(pl.lit(f\"{field}:{check}:{value}\").alias(\"dq_status\"))\n    return df.head(0)\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.has_max","title":"<code>has_max(df, rule)</code>","text":"<p>Filters a Polars DataFrame to include only rows where the value in a specified column exceeds a given threshold, and adds a new column indicating the rule applied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The column name to apply the filter on. - 'check' (str): The type of check being performed (e.g., \"max\"). - 'value' (numeric): The threshold value to compare against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new DataFrame containing only the rows that satisfy the condition,</p> <code>DataFrame</code> <p>with an additional column named \"dq_status\" that describes the applied rule.</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def has_max(df: pl.DataFrame, rule: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Filters a Polars DataFrame to include only rows where the value in a specified\n    column exceeds a given threshold, and adds a new column indicating the rule applied.\n\n    Args:\n        df (pl.DataFrame): The input Polars DataFrame to be filtered.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field' (str): The column name to apply the filter on.\n            - 'check' (str): The type of check being performed (e.g., \"max\").\n            - 'value' (numeric): The threshold value to compare against.\n\n    Returns:\n        pl.DataFrame: A new DataFrame containing only the rows that satisfy the condition,\n        with an additional column named \"dq_status\" that describes the applied rule.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    return df.filter(pl.col(field) &gt; value).with_columns(\n        pl.lit(f\"{field}:{check}:{value}\").alias(\"dq_status\")\n    )\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.has_mean","title":"<code>has_mean(df, rule)</code>","text":"<p>Checks if the mean value of a specified column in a Polars DataFrame satisfies a given condition.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The name of the column to calculate the mean for. - 'check' (str): The condition to check (e.g., 'greater than'). - 'value' (float): The threshold value to compare the mean against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: - If the mean value of the specified column is greater than the threshold value,   returns the original DataFrame with an additional column \"dq_status\" containing   a string in the format \"{field}:{check}:{value}\". - If the condition is not met, returns an empty DataFrame with the same schema as the input.</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def has_mean(df: pl.DataFrame, rule: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Checks if the mean value of a specified column in a Polars DataFrame satisfies a given condition.\n\n    Args:\n        df (pl.DataFrame): The input Polars DataFrame.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field' (str): The name of the column to calculate the mean for.\n            - 'check' (str): The condition to check (e.g., 'greater than').\n            - 'value' (float): The threshold value to compare the mean against.\n\n    Returns:\n        pl.DataFrame:\n            - If the mean value of the specified column is greater than the threshold value,\n              returns the original DataFrame with an additional column \"dq_status\" containing\n              a string in the format \"{field}:{check}:{value}\".\n            - If the condition is not met, returns an empty DataFrame with the same schema as the input.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    mean_val = df.select(pl.col(field).mean()).to_numpy()[0] or 0.0\n    if mean_val &gt; value:\n        return df.with_columns(pl.lit(f\"{field}:{check}:{value}\").alias(\"dq_status\"))\n    return df.head(0)\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.has_min","title":"<code>has_min(df, rule)</code>","text":"<p>Filters a Polars DataFrame to include only rows where the value of a specified column is less than a given threshold and adds a new column indicating the data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The name of the column to apply the filter on. - 'check': A string representing the type of check (e.g., 'min'). - 'value': The threshold value for the filter.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new Polars DataFrame containing only the rows that satisfy</p> <code>DataFrame</code> <p>the condition, with an additional column named \"dq_status\" indicating the</p> <code>DataFrame</code> <p>applied rule in the format \"field:check:value\".</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def has_min(df: pl.DataFrame, rule: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Filters a Polars DataFrame to include only rows where the value of a specified\n    column is less than a given threshold and adds a new column indicating the\n    data quality status.\n\n    Args:\n        df (pl.DataFrame): The input Polars DataFrame to be filtered.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field': The name of the column to apply the filter on.\n            - 'check': A string representing the type of check (e.g., 'min').\n            - 'value': The threshold value for the filter.\n\n    Returns:\n        pl.DataFrame: A new Polars DataFrame containing only the rows that satisfy\n        the condition, with an additional column named \"dq_status\" indicating the\n        applied rule in the format \"field:check:value\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    return df.filter(pl.col(field) &lt; value).with_columns(\n        pl.lit(f\"{field}:{check}:{value}\").alias(\"dq_status\")\n    )\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.has_pattern","title":"<code>has_pattern(df, rule)</code>","text":"<p>Filters a Polars DataFrame based on a pattern-matching rule and adds a data quality status column.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The column name in the DataFrame to apply the pattern check. - 'check': A descriptive label for the check being performed. - 'pattern': The regex pattern to match against the column values.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new DataFrame with rows not matching the pattern removed and an additional</p> <code>DataFrame</code> <p>column named \"dq_status\" indicating the rule applied in the format \"field:check:pattern\".</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def has_pattern(df: pl.DataFrame, rule: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Filters a Polars DataFrame based on a pattern-matching rule and adds a data quality status column.\n\n    Args:\n        df (pl.DataFrame): The input Polars DataFrame to be filtered.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field': The column name in the DataFrame to apply the pattern check.\n            - 'check': A descriptive label for the check being performed.\n            - 'pattern': The regex pattern to match against the column values.\n\n    Returns:\n        pl.DataFrame: A new DataFrame with rows not matching the pattern removed and an additional\n        column named \"dq_status\" indicating the rule applied in the format \"field:check:pattern\".\n    \"\"\"\n    field, check, pattern = __extract_params(rule)\n    return df.filter(~pl.col(field).str.contains(pattern, literal=False)).with_columns(\n        pl.lit(f\"{field}:{check}:{pattern}\").alias(\"dq_status\")\n    )\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.has_std","title":"<code>has_std(df, rule)</code>","text":"<p>Evaluates whether the standard deviation of a specified column in a Polars DataFrame exceeds a given threshold and returns a modified DataFrame accordingly.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to evaluate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The name of the column to calculate the standard deviation for. - 'check' (str): A descriptive label for the check being performed. - 'value' (float): The threshold value for the standard deviation.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A modified DataFrame. If the standard deviation of the specified column</p> <code>DataFrame</code> <p>exceeds the threshold, the DataFrame will include a new column <code>dq_status</code> with a</p> <code>DataFrame</code> <p>descriptive string. Otherwise, an empty DataFrame with the <code>dq_status</code> column is returned.</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def has_std(df: pl.DataFrame, rule: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Evaluates whether the standard deviation of a specified column in a Polars DataFrame\n    exceeds a given threshold and returns a modified DataFrame accordingly.\n\n    Args:\n        df (pl.DataFrame): The input Polars DataFrame to evaluate.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field' (str): The name of the column to calculate the standard deviation for.\n            - 'check' (str): A descriptive label for the check being performed.\n            - 'value' (float): The threshold value for the standard deviation.\n\n    Returns:\n        pl.DataFrame: A modified DataFrame. If the standard deviation of the specified column\n        exceeds the threshold, the DataFrame will include a new column `dq_status` with a\n        descriptive string. Otherwise, an empty DataFrame with the `dq_status` column is returned.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    std_val = df.select(pl.col(field).std()).to_numpy()[0] or 0.0\n    if std_val &gt; value:\n        return df.with_columns(pl.lit(f\"{field}:{check}:{value}\").alias(\"dq_status\"))\n    return df.head(0).with_columns(pl.lit(\"dq_status\").alias(\"dq_status\")).head(0)\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.has_sum","title":"<code>has_sum(df, rule)</code>","text":"<p>Checks if the sum of a specified column in a Polars DataFrame exceeds a given value.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The name of the column to sum. - 'check': A string representing the check type (not used in this function). - 'value': The threshold value to compare the sum against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: If the sum of the specified column exceeds the given value,</p> <code>DataFrame</code> <p>returns the original DataFrame with an additional column <code>dq_status</code> containing</p> <code>DataFrame</code> <p>a string in the format \"{field}:{check}:{value}\". Otherwise, returns an empty DataFrame.</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def has_sum(df: pl.DataFrame, rule: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Checks if the sum of a specified column in a Polars DataFrame exceeds a given value.\n\n    Args:\n        df (pl.DataFrame): The input Polars DataFrame.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field': The name of the column to sum.\n            - 'check': A string representing the check type (not used in this function).\n            - 'value': The threshold value to compare the sum against.\n\n    Returns:\n        pl.DataFrame: If the sum of the specified column exceeds the given value,\n        returns the original DataFrame with an additional column `dq_status` containing\n        a string in the format \"{field}:{check}:{value}\". Otherwise, returns an empty DataFrame.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    sum_val = df.select(pl.col(field).sum()).to_numpy()[0] or 0.0\n    if sum_val &gt; value:\n        return df.with_columns(pl.lit(f\"{field}:{check}:{value}\").alias(\"dq_status\"))\n    return df.head(0)\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.is_between","title":"<code>is_between(df, rule)</code>","text":"<p>Filters a Polars DataFrame to exclude rows where the specified field's value falls within a given range, and adds a column indicating the data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The name of the column to check. - 'check': The type of check being performed (e.g., \"is_between\"). - 'value': A string representing the range in the format \"[lo,hi]\".</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new Polars DataFrame with rows outside the specified range</p> <code>DataFrame</code> <p>and an additional column named \"dq_status\" indicating the rule applied.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the 'value' parameter is not in the expected format \"[lo,hi]\".</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def is_between(df: pl.DataFrame, rule: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Filters a Polars DataFrame to exclude rows where the specified field's value\n    falls within a given range, and adds a column indicating the data quality status.\n\n    Args:\n        df (pl.DataFrame): The input Polars DataFrame to filter.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field': The name of the column to check.\n            - 'check': The type of check being performed (e.g., \"is_between\").\n            - 'value': A string representing the range in the format \"[lo,hi]\".\n\n    Returns:\n        pl.DataFrame: A new Polars DataFrame with rows outside the specified range\n        and an additional column named \"dq_status\" indicating the rule applied.\n\n    Raises:\n        ValueError: If the 'value' parameter is not in the expected format \"[lo,hi]\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    lo, hi = value.strip(\"[]\").split(\",\")\n    lo, hi = __convert_value(lo), __convert_value(hi)\n    return df.filter(~pl.col(field).is_between(lo, hi)).with_columns(\n        pl.lit(f\"{field}:{check}:{value}\").alias(\"dq_status\")\n    )\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.is_complete","title":"<code>is_complete(df, rule)</code>","text":"<p>Filters a Polars DataFrame to include only rows where the specified field is not null and appends a new column indicating the data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to be filtered and modified.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The name of the column to check for non-null values. - 'check' (str): A descriptive string for the type of check being performed. - 'value' (str): A value associated with the rule for status annotation.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new Polars DataFrame with rows filtered based on the rule and</p> <code>DataFrame</code> <p>an additional column named \"dq_status\" containing the data quality status.</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def is_complete(df: pl.DataFrame, rule: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Filters a Polars DataFrame to include only rows where the specified field is not null\n    and appends a new column indicating the data quality status.\n\n    Args:\n        df (pl.DataFrame): The input Polars DataFrame to be filtered and modified.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field' (str): The name of the column to check for non-null values.\n            - 'check' (str): A descriptive string for the type of check being performed.\n            - 'value' (str): A value associated with the rule for status annotation.\n\n    Returns:\n        pl.DataFrame: A new Polars DataFrame with rows filtered based on the rule and\n        an additional column named \"dq_status\" containing the data quality status.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    return df.filter(pl.col(field).is_not_null()).with_columns(\n        pl.lit(f\"{field}:{check}:{value}\").alias(\"dq_status\")\n    )\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.is_composite_key","title":"<code>is_composite_key(df, rule)</code>","text":"<p>Determines if the given DataFrame satisfies the composite key condition based on the provided rule.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to evaluate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary defining the rule to check for composite key uniqueness.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A DataFrame indicating whether the composite key condition is met.</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def is_composite_key(df: pl.DataFrame, rule: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Determines if the given DataFrame satisfies the composite key condition based on the provided rule.\n\n    Args:\n        df (pl.DataFrame): The input DataFrame to evaluate.\n        rule (dict): A dictionary defining the rule to check for composite key uniqueness.\n\n    Returns:\n        pl.DataFrame: A DataFrame indicating whether the composite key condition is met.\n    \"\"\"\n    return are_unique(df, rule)\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.is_contained_in","title":"<code>is_contained_in(df, rule)</code>","text":"<p>Filters a Polars DataFrame to exclude rows where the specified field's value is contained in a given list of values, and adds a new column indicating the rule applied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The column name to check. - 'check': The type of check being performed (e.g., \"is_contained_in\"). - 'value': A string representation of a list of values to check against,   e.g., \"[value1, value2, value3]\".</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new DataFrame with rows filtered based on the rule and an</p> <code>DataFrame</code> <p>additional column \"dq_status\" indicating the rule applied.</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def is_contained_in(df: pl.DataFrame, rule: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Filters a Polars DataFrame to exclude rows where the specified field's value is\n    contained in a given list of values, and adds a new column indicating the rule applied.\n\n    Args:\n        df (pl.DataFrame): The input Polars DataFrame to be filtered.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field': The column name to check.\n            - 'check': The type of check being performed (e.g., \"is_contained_in\").\n            - 'value': A string representation of a list of values to check against,\n              e.g., \"[value1, value2, value3]\".\n\n    Returns:\n        pl.DataFrame: A new DataFrame with rows filtered based on the rule and an\n        additional column \"dq_status\" indicating the rule applied.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    lst = [v.strip() for v in value.strip(\"[]\").split(\",\")]\n    return df.filter(~pl.col(field).is_in(lst)).with_columns(\n        pl.lit(f\"{field}:{check}:{value}\").alias(\"dq_status\")\n    )\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.is_date_after","title":"<code>is_date_after(df, rule)</code>","text":"<p>Filters a Polars DataFrame to include only rows where the specified date field is earlier than a given date, and adds a new column indicating the data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The name of the column containing date strings. - 'check' (str): A descriptive label for the check being performed. - 'date_str' (str): The date string in the format \"%Y-%m-%d\" to compare against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new Polars DataFrame with rows filtered based on the date condition</p> <code>DataFrame</code> <p>and an additional column named \"dq_status\" indicating the applied rule.</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def is_date_after(df: pl.DataFrame, rule: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Filters a Polars DataFrame to include only rows where the specified date field\n    is earlier than a given date, and adds a new column indicating the data quality status.\n\n    Args:\n        df (pl.DataFrame): The input Polars DataFrame to filter.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field' (str): The name of the column containing date strings.\n            - 'check' (str): A descriptive label for the check being performed.\n            - 'date_str' (str): The date string in the format \"%Y-%m-%d\" to compare against.\n\n    Returns:\n        pl.DataFrame: A new Polars DataFrame with rows filtered based on the date condition\n        and an additional column named \"dq_status\" indicating the applied rule.\n    \"\"\"\n    field, check, date_str = __extract_params(rule)\n    return df.filter(\n        pl.col(field).str.strptime(pl.Date, \"%Y-%m-%d\") &lt; date_str\n    ).with_columns(pl.lit(f\"{field}:{check}:{date_str}\").alias(\"dq_status\"))\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.is_date_before","title":"<code>is_date_before(df, rule)</code>","text":"<p>Filters a Polars DataFrame to include only rows where the specified date field is after a given date, and adds a new column indicating the data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The name of the column to check. - 'check' (str): A descriptive label for the check being performed. - 'date_str' (str): The date string in the format \"%Y-%m-%d\" to compare against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new Polars DataFrame with rows filtered based on the date condition</p> <code>DataFrame</code> <p>and an additional column named \"dq_status\" indicating the applied rule.</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def is_date_before(df: pl.DataFrame, rule: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Filters a Polars DataFrame to include only rows where the specified date field\n    is after a given date, and adds a new column indicating the data quality status.\n\n    Args:\n        df (pl.DataFrame): The input Polars DataFrame to filter.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field' (str): The name of the column to check.\n            - 'check' (str): A descriptive label for the check being performed.\n            - 'date_str' (str): The date string in the format \"%Y-%m-%d\" to compare against.\n\n    Returns:\n        pl.DataFrame: A new Polars DataFrame with rows filtered based on the date condition\n        and an additional column named \"dq_status\" indicating the applied rule.\n    \"\"\"\n    field, check, date_str = __extract_params(rule)\n    return df.filter(\n        pl.col(field).str.strptime(pl.Date, \"%Y-%m-%d\") &gt; date_str\n    ).with_columns(pl.lit(f\"{field}:{check}:{date_str}\").alias(\"dq_status\"))\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.is_date_between","title":"<code>is_date_between(df, rule)</code>","text":"<p>Filters a Polars DataFrame to exclude rows where the specified date field is within a given range.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the filtering rule. It should include: - 'field': The name of the column to check. - 'check': A string representing the type of check (e.g., \"is_date_between\"). - 'value': A string representing the date range in the format \"[YYYY-MM-DD,YYYY-MM-DD]\".</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new DataFrame excluding rows where the date in the specified field           falls within the given inclusive range, with an additional column           \"dq_status\" indicating the rule applied.</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def is_date_between(df: pl.DataFrame, rule: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Filters a Polars DataFrame to exclude rows where the specified date field is within a given range.\n\n    Args:\n        df (pl.DataFrame): The input Polars DataFrame to filter.\n        rule (dict): A dictionary containing the filtering rule. It should include:\n            - 'field': The name of the column to check.\n            - 'check': A string representing the type of check (e.g., \"is_date_between\").\n            - 'value': A string representing the date range in the format \"[YYYY-MM-DD,YYYY-MM-DD]\".\n\n    Returns:\n        pl.DataFrame: A new DataFrame excluding rows where the date in the specified field\n                      falls within the given inclusive range, with an additional column\n                      \"dq_status\" indicating the rule applied.\n    \"\"\"\n    field, check, raw = __extract_params(rule)\n    start_str, end_str = [s.strip() for s in raw.strip(\"[]\").split(\",\")]\n\n    # build literal date expressions\n    start_expr = pl.lit(start_str).str.strptime(pl.Date, \"%Y-%m-%d\")\n    end_expr = pl.lit(end_str).str.strptime(pl.Date, \"%Y-%m-%d\")\n\n    return df.filter(\n        ~pl.col(field)\n        .str.strptime(pl.Date, \"%Y-%m-%d\")\n        .is_between(start_expr, end_expr)\n    ).with_columns(pl.lit(f\"{field}:{check}:{raw}\").alias(\"dq_status\"))\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.is_equal","title":"<code>is_equal(df, rule)</code>","text":"<p>Filters rows in a Polars DataFrame that do not match a specified equality condition and adds a column indicating the data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The column name to apply the equality check on. - 'check': The type of check (expected to be 'eq' for equality). - 'value': The value to compare against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new DataFrame with rows filtered based on the rule and an</p> <code>DataFrame</code> <p>additional column named \"dq_status\" indicating the rule applied.</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def is_equal(df: pl.DataFrame, rule: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Filters rows in a Polars DataFrame that do not match a specified equality condition\n    and adds a column indicating the data quality status.\n\n    Args:\n        df (pl.DataFrame): The input Polars DataFrame to be filtered.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field': The column name to apply the equality check on.\n            - 'check': The type of check (expected to be 'eq' for equality).\n            - 'value': The value to compare against.\n\n    Returns:\n        pl.DataFrame: A new DataFrame with rows filtered based on the rule and an\n        additional column named \"dq_status\" indicating the rule applied.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    return df.filter(~pl.col(field).eq(value)).with_columns(\n        pl.lit(f\"{field}:{check}:{value}\").alias(\"dq_status\")\n    )\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.is_equal_than","title":"<code>is_equal_than(df, rule)</code>","text":"<p>Filters rows in a Polars DataFrame where the specified field is not equal to a given value and adds a new column indicating the data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The name of the column to check. - 'check': The type of check (expected to be 'equal' for this function). - 'value': The value to compare against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new Polars DataFrame with rows filtered based on the rule and an</p> <code>DataFrame</code> <p>additional column named \"dq_status\" indicating the applied rule.</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def is_equal_than(df: pl.DataFrame, rule: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Filters rows in a Polars DataFrame where the specified field is not equal to a given value\n    and adds a new column indicating the data quality status.\n\n    Args:\n        df (pl.DataFrame): The input Polars DataFrame to be filtered.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field': The name of the column to check.\n            - 'check': The type of check (expected to be 'equal' for this function).\n            - 'value': The value to compare against.\n\n    Returns:\n        pl.DataFrame: A new Polars DataFrame with rows filtered based on the rule and an\n        additional column named \"dq_status\" indicating the applied rule.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    return df.filter(~pl.col(field).eq(value)).with_columns(\n        pl.lit(f\"{field}:{check}:{value}\").alias(\"dq_status\")\n    )\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.is_future_date","title":"<code>is_future_date(df, rule)</code>","text":"<p>Filters a Polars DataFrame to include only rows where the specified date field contains a future date, based on the current date.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to include the field name to check, the check type, and additional parameters (ignored in this function).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new DataFrame containing only rows where the specified</p> <code>DataFrame</code> <p>date field is in the future. An additional column \"dq_status\" is added</p> <code>DataFrame</code> <p>to indicate the field, check type, and today's date in the format</p> <code>DataFrame</code> <p>\"field:check:today\".</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def is_future_date(df: pl.DataFrame, rule: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Filters a Polars DataFrame to include only rows where the specified date field\n    contains a future date, based on the current date.\n\n    Args:\n        df (pl.DataFrame): The input Polars DataFrame to filter.\n        rule (dict): A dictionary containing the rule parameters. It is expected\n            to include the field name to check, the check type, and additional\n            parameters (ignored in this function).\n\n    Returns:\n        pl.DataFrame: A new DataFrame containing only rows where the specified\n        date field is in the future. An additional column \"dq_status\" is added\n        to indicate the field, check type, and today's date in the format\n        \"field:check:today\".\n    \"\"\"\n    field, check, _ = __extract_params(rule)\n    today = _dt.today().isoformat()\n    return df.filter(\n        pl.col(field).str.strptime(pl.Date, \"%Y-%m-%d\") &gt; pl.lit(today).cast(pl.Date)\n    ).with_columns(pl.lit(f\"{field}:{check}:{today}\").alias(\"dq_status\"))\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.is_greater_or_equal_than","title":"<code>is_greater_or_equal_than(df, rule)</code>","text":"<p>Filters a Polars DataFrame to include only rows where the specified field is greater than or equal to a given value, and adds a new column indicating the data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the filtering rule. It should include the following keys: - 'field': The name of the column to be checked. - 'check': The type of check being performed (e.g., \"greater_or_equal\"). - 'value': The threshold value for the comparison.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new Polars DataFrame with rows filtered based on the</p> <code>DataFrame</code> <p>specified rule and an additional column named \"dq_status\" indicating</p> <code>DataFrame</code> <p>the data quality status in the format \"field:check:value\".</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def is_greater_or_equal_than(df: pl.DataFrame, rule: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Filters a Polars DataFrame to include only rows where the specified field\n    is greater than or equal to a given value, and adds a new column indicating\n    the data quality status.\n\n    Args:\n        df (pl.DataFrame): The input Polars DataFrame to be filtered.\n        rule (dict): A dictionary containing the filtering rule. It should\n            include the following keys:\n            - 'field': The name of the column to be checked.\n            - 'check': The type of check being performed (e.g., \"greater_or_equal\").\n            - 'value': The threshold value for the comparison.\n\n    Returns:\n        pl.DataFrame: A new Polars DataFrame with rows filtered based on the\n        specified rule and an additional column named \"dq_status\" indicating\n        the data quality status in the format \"field:check:value\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    return df.filter(pl.col(field) &lt; value).with_columns(\n        pl.lit(f\"{field}:{check}:{value}\").alias(\"dq_status\")\n    )\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.is_greater_than","title":"<code>is_greater_than(df, rule)</code>","text":"<p>Filters a Polars DataFrame to include only rows where the specified field's value is less than or equal to a given value, and adds a new column indicating the data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The Polars DataFrame to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the filtering rule. It should include: - 'field': The name of the column to apply the filter on. - 'check': A string describing the check (e.g., \"greater_than\"). - 'value': The value to compare against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new DataFrame with rows filtered based on the rule and an</p> <code>DataFrame</code> <p>additional column named \"dq_status\" indicating the applied rule.</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def is_greater_than(df: pl.DataFrame, rule: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Filters a Polars DataFrame to include only rows where the specified field's value\n    is less than or equal to a given value, and adds a new column indicating the\n    data quality status.\n\n    Args:\n        df (pl.DataFrame): The Polars DataFrame to filter.\n        rule (dict): A dictionary containing the filtering rule. It should include:\n            - 'field': The name of the column to apply the filter on.\n            - 'check': A string describing the check (e.g., \"greater_than\").\n            - 'value': The value to compare against.\n\n    Returns:\n        pl.DataFrame: A new DataFrame with rows filtered based on the rule and an\n        additional column named \"dq_status\" indicating the applied rule.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    return df.filter(pl.col(field) &lt;= value).with_columns(\n        pl.lit(f\"{field}:{check}:{value}\").alias(\"dq_status\")\n    )\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.is_in","title":"<code>is_in(df, rule)</code>","text":"<p>Checks if the rows in the given DataFrame satisfy the conditions specified in the rule.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to evaluate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary specifying the conditions to check against the DataFrame.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A DataFrame containing rows that satisfy the specified conditions.</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def is_in(df: pl.DataFrame, rule: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Checks if the rows in the given DataFrame satisfy the conditions specified in the rule.\n\n    Args:\n        df (pl.DataFrame): The input DataFrame to evaluate.\n        rule (dict): A dictionary specifying the conditions to check against the DataFrame.\n\n    Returns:\n        pl.DataFrame: A DataFrame containing rows that satisfy the specified conditions.\n    \"\"\"\n    return is_contained_in(df, rule)\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.is_in_billions","title":"<code>is_in_billions(df, rule)</code>","text":"<p>Filters a Polars DataFrame to include only rows where the specified field's value is less than one billion and adds a new column indicating the data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - field (str): The name of the column to check. - check (str): The type of check being performed (e.g., \"less_than\"). - value (any): The value associated with the rule (not used in this function).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new DataFrame with rows filtered based on the rule and an</p> <code>DataFrame</code> <p>additional column named \"dq_status\" containing a string in the format</p> <code>DataFrame</code> <p>\"{field}:{check}:{value}\".</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def is_in_billions(df: pl.DataFrame, rule: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Filters a Polars DataFrame to include only rows where the specified field's value\n    is less than one billion and adds a new column indicating the data quality status.\n\n    Args:\n        df (pl.DataFrame): The input Polars DataFrame to be filtered.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - field (str): The name of the column to check.\n            - check (str): The type of check being performed (e.g., \"less_than\").\n            - value (any): The value associated with the rule (not used in this function).\n\n    Returns:\n        pl.DataFrame: A new DataFrame with rows filtered based on the rule and an\n        additional column named \"dq_status\" containing a string in the format\n        \"{field}:{check}:{value}\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    return df.filter(pl.col(field) &lt; 1_000_000_000).with_columns(\n        pl.lit(f\"{field}:{check}:{value}\").alias(\"dq_status\")\n    )\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.is_in_millions","title":"<code>is_in_millions(df, rule)</code>","text":"<p>Filters a Polars DataFrame to include only rows where the specified field's value is less than one million and adds a new column indicating the data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The name of the column to check. - 'check': A string describing the check being performed. - 'value': A value associated with the rule (used for status annotation).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new Polars DataFrame with rows filtered based on the rule and</p> <code>DataFrame</code> <p>an additional column named \"dq_status\" containing the data quality status.</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def is_in_millions(df: pl.DataFrame, rule: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Filters a Polars DataFrame to include only rows where the specified field's value\n    is less than one million and adds a new column indicating the data quality status.\n\n    Args:\n        df (pl.DataFrame): The input Polars DataFrame to be filtered.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field': The name of the column to check.\n            - 'check': A string describing the check being performed.\n            - 'value': A value associated with the rule (used for status annotation).\n\n    Returns:\n        pl.DataFrame: A new Polars DataFrame with rows filtered based on the rule and\n        an additional column named \"dq_status\" containing the data quality status.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n\n    return df.filter(pl.col(field) &lt; 1_000_000).with_columns(\n        pl.lit(f\"{field}:{check}:{value}\").alias(\"dq_status\")\n    )\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.is_legit","title":"<code>is_legit(df, rule)</code>","text":"<p>Filters a Polars DataFrame based on a validation rule and appends a data quality status column.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to validate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the validation rule. It should include: - 'field': The name of the column to validate. - 'check': The type of validation check (e.g., regex, condition). - 'value': The value or pattern to validate against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new DataFrame containing rows that failed the validation,</p> <code>DataFrame</code> <p>with an additional column 'dq_status' indicating the validation rule applied.</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def is_legit(df: pl.DataFrame, rule: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Filters a Polars DataFrame based on a validation rule and appends a data quality status column.\n\n    Args:\n        df (pl.DataFrame): The input Polars DataFrame to validate.\n        rule (dict): A dictionary containing the validation rule. It should include:\n            - 'field': The name of the column to validate.\n            - 'check': The type of validation check (e.g., regex, condition).\n            - 'value': The value or pattern to validate against.\n\n    Returns:\n        pl.DataFrame: A new DataFrame containing rows that failed the validation,\n        with an additional column 'dq_status' indicating the validation rule applied.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    mask = pl.col(field).is_not_null() &amp; pl.col(field).str.contains(r\"^\\S+$\")\n    return df.filter(~mask).with_columns(\n        pl.lit(f\"{field}:{check}:{value}\").alias(\"dq_status\")\n    )\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.is_less_or_equal_than","title":"<code>is_less_or_equal_than(df, rule)</code>","text":"<p>Filters a Polars DataFrame to include only rows where the specified field's value is greater than the given value, and adds a new column indicating the rule applied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The name of the column to apply the filter on. - 'check': The type of check being performed (e.g., 'less_or_equal_than'). - 'value': The value to compare against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new DataFrame with rows filtered based on the rule and an</p> <code>DataFrame</code> <p>additional column named \"dq_status\" indicating the rule applied.</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def is_less_or_equal_than(df: pl.DataFrame, rule: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Filters a Polars DataFrame to include only rows where the specified field's value\n    is greater than the given value, and adds a new column indicating the rule applied.\n\n    Args:\n        df (pl.DataFrame): The input Polars DataFrame to be filtered.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field': The name of the column to apply the filter on.\n            - 'check': The type of check being performed (e.g., 'less_or_equal_than').\n            - 'value': The value to compare against.\n\n    Returns:\n        pl.DataFrame: A new DataFrame with rows filtered based on the rule and an\n        additional column named \"dq_status\" indicating the rule applied.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    return df.filter(pl.col(field) &gt; value).with_columns(\n        pl.lit(f\"{field}:{check}:{value}\").alias(\"dq_status\")\n    )\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.is_less_than","title":"<code>is_less_than(df, rule)</code>","text":"<p>Filters a Polars DataFrame to include only rows where the specified field is greater than or equal to a given value. Adds a new column indicating the data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the filtering rule. It should include the following keys: - 'field': The name of the column to apply the filter on. - 'check': A string representing the type of check (not used in logic). - 'value': The threshold value for the filter.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new Polars DataFrame with rows filtered based on the</p> <code>DataFrame</code> <p>condition and an additional column named \"dq_status\" containing the</p> <code>DataFrame</code> <p>rule description in the format \"field:check:value\".</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def is_less_than(df: pl.DataFrame, rule: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Filters a Polars DataFrame to include only rows where the specified field\n    is greater than or equal to a given value. Adds a new column indicating\n    the data quality status.\n\n    Args:\n        df (pl.DataFrame): The input Polars DataFrame to filter.\n        rule (dict): A dictionary containing the filtering rule. It should\n            include the following keys:\n            - 'field': The name of the column to apply the filter on.\n            - 'check': A string representing the type of check (not used in logic).\n            - 'value': The threshold value for the filter.\n\n    Returns:\n        pl.DataFrame: A new Polars DataFrame with rows filtered based on the\n        condition and an additional column named \"dq_status\" containing the\n        rule description in the format \"field:check:value\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    return df.filter(pl.col(field) &gt;= value).with_columns(\n        pl.lit(f\"{field}:{check}:{value}\").alias(\"dq_status\")\n    )\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.is_negative","title":"<code>is_negative(df, rule)</code>","text":"<p>Filters a Polars DataFrame to exclude rows where the specified field is negative and adds a new column indicating the data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The name of the column to check. - 'check': The type of check being performed (e.g., \"is_negative\"). - 'value': The value associated with the rule (not used in this function).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new DataFrame with rows where the specified field is non-negative</p> <code>DataFrame</code> <p>and an additional column named \"dq_status\" containing the rule details.</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def is_negative(df: pl.DataFrame, rule: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Filters a Polars DataFrame to exclude rows where the specified field is negative\n    and adds a new column indicating the data quality status.\n\n    Args:\n        df (pl.DataFrame): The input Polars DataFrame to be filtered.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field': The name of the column to check.\n            - 'check': The type of check being performed (e.g., \"is_negative\").\n            - 'value': The value associated with the rule (not used in this function).\n\n    Returns:\n        pl.DataFrame: A new DataFrame with rows where the specified field is non-negative\n        and an additional column named \"dq_status\" containing the rule details.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    return df.filter(pl.col(field) &gt;= 0).with_columns(\n        [pl.lit(f\"{field}:{check}:{value}\").alias(\"dq_status\")]\n    )\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.is_on_friday","title":"<code>is_on_friday(df, rule)</code>","text":"<p>Filters a Polars DataFrame to include only rows where the date corresponds to a Friday.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame containing the data to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing filtering rules or parameters.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new Polars DataFrame containing only the rows where the date is a Friday.</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def is_on_friday(df: pl.DataFrame, rule: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Filters a Polars DataFrame to include only rows where the date corresponds to a Friday.\n\n    Args:\n        df (pl.DataFrame): The input Polars DataFrame containing the data to filter.\n        rule (dict): A dictionary containing filtering rules or parameters.\n\n    Returns:\n        pl.DataFrame: A new Polars DataFrame containing only the rows where the date is a Friday.\n    \"\"\"\n    return _day_of_week(df, rule, 4)\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.is_on_monday","title":"<code>is_on_monday(df, rule)</code>","text":"<p>Filters the given DataFrame to include only rows where the date corresponds to a Monday.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing rules or parameters for filtering.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new DataFrame containing only the rows where the date is a Monday.</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def is_on_monday(df: pl.DataFrame, rule: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Filters the given DataFrame to include only rows where the date corresponds to a Monday.\n\n    Args:\n        df (pl.DataFrame): The input DataFrame to filter.\n        rule (dict): A dictionary containing rules or parameters for filtering.\n\n    Returns:\n        pl.DataFrame: A new DataFrame containing only the rows where the date is a Monday.\n    \"\"\"\n    return _day_of_week(df, rule, 0)\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.is_on_saturday","title":"<code>is_on_saturday(df, rule)</code>","text":"<p>Determines if the dates in the given DataFrame fall on a Saturday.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing date information.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing rules or parameters for the operation.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A DataFrame with the result of the operation, indicating whether each date is on a Saturday.</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def is_on_saturday(df: pl.DataFrame, rule: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Determines if the dates in the given DataFrame fall on a Saturday.\n\n    Args:\n        df (pl.DataFrame): The input DataFrame containing date information.\n        rule (dict): A dictionary containing rules or parameters for the operation.\n\n    Returns:\n        pl.DataFrame: A DataFrame with the result of the operation, indicating whether each date is on a Saturday.\n    \"\"\"\n    return _day_of_week(df, rule, 5)\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.is_on_sunday","title":"<code>is_on_sunday(df, rule)</code>","text":"<p>Filters the given DataFrame to include only rows where the date corresponds to Sunday.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing date-related data.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing rules or parameters for filtering.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A filtered DataFrame containing only rows where the date is a Sunday.</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def is_on_sunday(df: pl.DataFrame, rule: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Filters the given DataFrame to include only rows where the date corresponds to Sunday.\n\n    Args:\n        df (pl.DataFrame): The input DataFrame containing date-related data.\n        rule (dict): A dictionary containing rules or parameters for filtering.\n\n    Returns:\n        pl.DataFrame: A filtered DataFrame containing only rows where the date is a Sunday.\n    \"\"\"\n    return _day_of_week(df, rule, 6)\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.is_on_thursday","title":"<code>is_on_thursday(df, rule)</code>","text":"<p>Filters a Polars DataFrame to include only rows where the date corresponds to a Thursday.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame containing the data to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing filtering rules or parameters.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new Polars DataFrame containing only the rows where the date is a Thursday.</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def is_on_thursday(df: pl.DataFrame, rule: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Filters a Polars DataFrame to include only rows where the date corresponds to a Thursday.\n\n    Args:\n        df (pl.DataFrame): The input Polars DataFrame containing the data to filter.\n        rule (dict): A dictionary containing filtering rules or parameters.\n\n    Returns:\n        pl.DataFrame: A new Polars DataFrame containing only the rows where the date is a Thursday.\n    \"\"\"\n    return _day_of_week(df, rule, 3)\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.is_on_tuesday","title":"<code>is_on_tuesday(df, rule)</code>","text":"<p>Filters the given DataFrame to include only rows where the day of the week matches Tuesday.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing rules or parameters for filtering.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new DataFrame containing only rows where the day of the week is Tuesday.</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def is_on_tuesday(df: pl.DataFrame, rule: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Filters the given DataFrame to include only rows where the day of the week matches Tuesday.\n\n    Args:\n        df (pl.DataFrame): The input DataFrame to filter.\n        rule (dict): A dictionary containing rules or parameters for filtering.\n\n    Returns:\n        pl.DataFrame: A new DataFrame containing only rows where the day of the week is Tuesday.\n    \"\"\"\n    return _day_of_week(df, rule, 1)\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.is_on_wednesday","title":"<code>is_on_wednesday(df, rule)</code>","text":"<p>Filters the given DataFrame to include only rows where the day of the week matches Wednesday.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing rules or parameters for filtering.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A filtered DataFrame containing only rows corresponding to Wednesday.</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def is_on_wednesday(df: pl.DataFrame, rule: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Filters the given DataFrame to include only rows where the day of the week matches Wednesday.\n\n    Args:\n        df (pl.DataFrame): The input DataFrame to filter.\n        rule (dict): A dictionary containing rules or parameters for filtering.\n\n    Returns:\n        pl.DataFrame: A filtered DataFrame containing only rows corresponding to Wednesday.\n    \"\"\"\n    return _day_of_week(df, rule, 2)\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.is_on_weekday","title":"<code>is_on_weekday(df, rule)</code>","text":"<p>Filters a Polars DataFrame to include only rows where the specified date field falls on a weekday (Monday to Friday). Adds a new column indicating the rule applied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to have          keys that can be extracted using the <code>__extract_params</code> function.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new DataFrame filtered to include only rows where the date field           falls on a weekday, with an additional column named \"dq_status\"           indicating the applied rule in the format \"field:check:value\".</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def is_on_weekday(df: pl.DataFrame, rule: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Filters a Polars DataFrame to include only rows where the specified date field\n    falls on a weekday (Monday to Friday). Adds a new column indicating the rule applied.\n\n    Args:\n        df (pl.DataFrame): The input Polars DataFrame.\n        rule (dict): A dictionary containing the rule parameters. It is expected to have\n                     keys that can be extracted using the `__extract_params` function.\n\n    Returns:\n        pl.DataFrame: A new DataFrame filtered to include only rows where the date field\n                      falls on a weekday, with an additional column named \"dq_status\"\n                      indicating the applied rule in the format \"field:check:value\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    return df.filter(\n        pl.col(field).str.strptime(pl.Date, \"%Y-%m-%d\").dt.weekday() &lt; 5\n    ).with_columns(pl.lit(f\"{field}:{check}:{value}\").alias(\"dq_status\"))\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.is_on_weekend","title":"<code>is_on_weekend(df, rule)</code>","text":"<p>Filters a Polars DataFrame to include only rows where the specified date field falls on a weekend (Saturday or Sunday). Adds a new column indicating the data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to include the following keys: - 'field': The name of the column containing date strings. - 'check': A string representing the type of check being performed. - 'value': A value associated with the rule (not used in the logic).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new Polars DataFrame filtered to include only rows where</p> <code>DataFrame</code> <p>the specified date field falls on a weekend. The resulting DataFrame also</p> <code>DataFrame</code> <p>includes an additional column named \"dq_status\" with a string indicating</p> <code>DataFrame</code> <p>the rule applied.</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def is_on_weekend(df: pl.DataFrame, rule: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Filters a Polars DataFrame to include only rows where the specified date field\n    falls on a weekend (Saturday or Sunday). Adds a new column indicating the\n    data quality status.\n\n    Args:\n        df (pl.DataFrame): The input Polars DataFrame.\n        rule (dict): A dictionary containing the rule parameters. It is expected\n            to include the following keys:\n            - 'field': The name of the column containing date strings.\n            - 'check': A string representing the type of check being performed.\n            - 'value': A value associated with the rule (not used in the logic).\n\n    Returns:\n        pl.DataFrame: A new Polars DataFrame filtered to include only rows where\n        the specified date field falls on a weekend. The resulting DataFrame also\n        includes an additional column named \"dq_status\" with a string indicating\n        the rule applied.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    return df.filter(\n        pl.col(field).str.strptime(pl.Date, \"%Y-%m-%d\").dt.weekday() &gt;= 5\n    ).with_columns(pl.lit(f\"{field}:{check}:{value}\").alias(\"dq_status\"))\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.is_past_date","title":"<code>is_past_date(df, rule)</code>","text":"<p>Filters a Polars DataFrame to include only rows where the specified date field contains a date earlier than today. Adds a new column indicating the data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to include          the field name to check, a check identifier, and additional parameters.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new DataFrame containing only rows where the specified date field           is in the past, with an additional column named \"dq_status\" that           contains a string in the format \"{field}:{check}:{today}\".</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def is_past_date(df: pl.DataFrame, rule: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Filters a Polars DataFrame to include only rows where the specified date field\n    contains a date earlier than today. Adds a new column indicating the data quality status.\n\n    Args:\n        df (pl.DataFrame): The input Polars DataFrame to filter.\n        rule (dict): A dictionary containing the rule parameters. It is expected to include\n                     the field name to check, a check identifier, and additional parameters.\n\n    Returns:\n        pl.DataFrame: A new DataFrame containing only rows where the specified date field\n                      is in the past, with an additional column named \"dq_status\" that\n                      contains a string in the format \"{field}:{check}:{today}\".\n    \"\"\"\n    field, check, _ = __extract_params(rule)\n    today = _dt.today().isoformat()\n    return df.filter(\n        pl.col(field).str.strptime(pl.Date, \"%Y-%m-%d\") &lt; pl.lit(today).cast(pl.Date)\n    ).with_columns(pl.lit(f\"{field}:{check}:{today}\").alias(\"dq_status\"))\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.is_positive","title":"<code>is_positive(df, rule)</code>","text":"<p>Filters a Polars DataFrame to identify rows where the specified field contains negative values and appends a new column indicating the data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to include the following keys: - 'field': The name of the column to check. - 'check': The type of check being performed (e.g., \"is_positive\"). - 'value': The reference value for the check.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new Polars DataFrame containing only the rows where</p> <code>DataFrame</code> <p>the specified field has negative values, with an additional column</p> <code>DataFrame</code> <p>named \"dq_status\" that describes the rule applied.</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def is_positive(df: pl.DataFrame, rule: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Filters a Polars DataFrame to identify rows where the specified field\n    contains negative values and appends a new column indicating the data\n    quality status.\n\n    Args:\n        df (pl.DataFrame): The input Polars DataFrame to be filtered.\n        rule (dict): A dictionary containing the rule parameters. It is\n            expected to include the following keys:\n            - 'field': The name of the column to check.\n            - 'check': The type of check being performed (e.g., \"is_positive\").\n            - 'value': The reference value for the check.\n\n    Returns:\n        pl.DataFrame: A new Polars DataFrame containing only the rows where\n        the specified field has negative values, with an additional column\n        named \"dq_status\" that describes the rule applied.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    return df.filter(pl.col(field) &lt; 0).with_columns(\n        [pl.lit(f\"{field}:{check}:{value}\").alias(\"dq_status\")]\n    )\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.is_primary_key","title":"<code>is_primary_key(df, rule)</code>","text":"<p>Checks if the specified rule identifies a primary key in the given DataFrame.</p> <p>A primary key is a set of columns in a DataFrame that uniquely identifies each row. This function delegates the check to the <code>is_unique</code> function.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The DataFrame to check for primary key uniqueness.</p> required <code>rule</code> <code>dict</code> <p>A dictionary specifying the rule or criteria to determine the primary key.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A DataFrame indicating whether the rule satisfies the primary key condition.</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def is_primary_key(df: pl.DataFrame, rule: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Checks if the specified rule identifies a primary key in the given DataFrame.\n\n    A primary key is a set of columns in a DataFrame that uniquely identifies each row.\n    This function delegates the check to the `is_unique` function.\n\n    Args:\n        df (pl.DataFrame): The DataFrame to check for primary key uniqueness.\n        rule (dict): A dictionary specifying the rule or criteria to determine the primary key.\n\n    Returns:\n        pl.DataFrame: A DataFrame indicating whether the rule satisfies the primary key condition.\n    \"\"\"\n    return is_unique(df, rule)\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.is_t_minus_1","title":"<code>is_t_minus_1(df, rule)</code>","text":"<p>Filters a Polars DataFrame to include only rows where the specified field matches the date of \"yesterday\" (T-1) and appends a new column indicating the data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to include the following keys: - 'field': The name of the column to check. - 'check': A string representing the type of check (used for metadata). - 'value': A value associated with the check (used for metadata).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new Polars DataFrame filtered to include only rows where</p> <code>DataFrame</code> <p>the specified field matches the date of yesterday (T-1). The resulting</p> <code>DataFrame</code> <p>DataFrame also includes an additional column named \"dq_status\" that</p> <code>DataFrame</code> <p>contains metadata about the rule applied.</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def is_t_minus_1(df: pl.DataFrame, rule: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Filters a Polars DataFrame to include only rows where the specified field\n    matches the date of \"yesterday\" (T-1) and appends a new column indicating\n    the data quality status.\n\n    Args:\n        df (pl.DataFrame): The input Polars DataFrame to filter.\n        rule (dict): A dictionary containing the rule parameters. It is expected\n            to include the following keys:\n            - 'field': The name of the column to check.\n            - 'check': A string representing the type of check (used for metadata).\n            - 'value': A value associated with the check (used for metadata).\n\n    Returns:\n        pl.DataFrame: A new Polars DataFrame filtered to include only rows where\n        the specified field matches the date of yesterday (T-1). The resulting\n        DataFrame also includes an additional column named \"dq_status\" that\n        contains metadata about the rule applied.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    target = (_dt.today() - timedelta(days=1)).isoformat()\n    return df.filter(\n        pl.col(field).str.strptime(pl.Date, \"%Y-%m-%d\") == pl.lit(target).cast(pl.Date)\n    ).with_columns(pl.lit(f\"{field}:{check}:{value}\").alias(\"dq_status\"))\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.is_t_minus_2","title":"<code>is_t_minus_2(df, rule)</code>","text":"<p>Filters a Polars DataFrame to include only rows where the specified date field matches the date two days prior to the current date. Adds a new column indicating the data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to include the following keys: - 'field': The name of the date field to check. - 'check': A string representing the type of check (not used in filtering). - 'value': A value associated with the rule (not used in filtering).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new Polars DataFrame filtered to include only rows where the</p> <code>DataFrame</code> <p>specified date field matches the date two days ago. The resulting DataFrame</p> <code>DataFrame</code> <p>includes an additional column named \"dq_status\" with a string indicating the</p> <code>DataFrame</code> <p>rule applied.</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def is_t_minus_2(df: pl.DataFrame, rule: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Filters a Polars DataFrame to include only rows where the specified date field\n    matches the date two days prior to the current date. Adds a new column indicating\n    the data quality status.\n\n    Args:\n        df (pl.DataFrame): The input Polars DataFrame to filter.\n        rule (dict): A dictionary containing the rule parameters. It is expected to\n            include the following keys:\n            - 'field': The name of the date field to check.\n            - 'check': A string representing the type of check (not used in filtering).\n            - 'value': A value associated with the rule (not used in filtering).\n\n    Returns:\n        pl.DataFrame: A new Polars DataFrame filtered to include only rows where the\n        specified date field matches the date two days ago. The resulting DataFrame\n        includes an additional column named \"dq_status\" with a string indicating the\n        rule applied.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    target = (_dt.today() - timedelta(days=2)).isoformat()\n    return df.filter(\n        pl.col(field).str.strptime(pl.Date, \"%Y-%m-%d\") == pl.lit(target).cast(pl.Date)\n    ).with_columns(pl.lit(f\"{field}:{check}:{value}\").alias(\"dq_status\"))\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.is_t_minus_3","title":"<code>is_t_minus_3(df, rule)</code>","text":"<p>Filters a Polars DataFrame to include only rows where the specified date field matches the date three days prior to the current date. Additionally, adds a new column indicating the data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The name of the date column to check. - 'check': A string representing the type of check (used for status annotation). - 'value': A value associated with the rule (used for status annotation).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A filtered Polars DataFrame with an additional column named</p> <code>DataFrame</code> <p>\"dq_status\" that contains a string in the format \"{field}:{check}:{value}\".</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def is_t_minus_3(df: pl.DataFrame, rule: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Filters a Polars DataFrame to include only rows where the specified date field\n    matches the date three days prior to the current date. Additionally, adds a\n    new column indicating the data quality status.\n\n    Args:\n        df (pl.DataFrame): The input Polars DataFrame to filter.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field': The name of the date column to check.\n            - 'check': A string representing the type of check (used for status annotation).\n            - 'value': A value associated with the rule (used for status annotation).\n\n    Returns:\n        pl.DataFrame: A filtered Polars DataFrame with an additional column named\n        \"dq_status\" that contains a string in the format \"{field}:{check}:{value}\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    target = (_dt.today() - timedelta(days=3)).isoformat()\n    return df.filter(\n        pl.col(field).str.strptime(pl.Date, \"%Y-%m-%d\") == pl.lit(target).cast(pl.Date)\n    ).with_columns(pl.lit(f\"{field}:{check}:{value}\").alias(\"dq_status\"))\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.is_today","title":"<code>is_today(df, rule)</code>","text":"<p>Filters a Polars DataFrame to include only rows where the specified date field matches today's date. Additionally, adds a new column \"dq_status\" with a formatted string indicating the rule applied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to have the following keys: - field (str): The name of the column to check. - check (str): A descriptive string for the type of check (used in the \"dq_status\" column). - value (str): A value associated with the rule (used in the \"dq_status\" column).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A filtered Polars DataFrame with rows matching today's date in the specified field</p> <code>DataFrame</code> <p>and an additional \"dq_status\" column describing the rule applied.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the rule dictionary does not contain the required keys or if the date parsing fails.</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def is_today(df: pl.DataFrame, rule: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Filters a Polars DataFrame to include only rows where the specified date field matches today's date.\n    Additionally, adds a new column \"dq_status\" with a formatted string indicating the rule applied.\n\n    Args:\n        df (pl.DataFrame): The input Polars DataFrame to filter.\n        rule (dict): A dictionary containing the rule parameters. It is expected to have the following keys:\n            - field (str): The name of the column to check.\n            - check (str): A descriptive string for the type of check (used in the \"dq_status\" column).\n            - value (str): A value associated with the rule (used in the \"dq_status\" column).\n\n    Returns:\n        pl.DataFrame: A filtered Polars DataFrame with rows matching today's date in the specified field\n        and an additional \"dq_status\" column describing the rule applied.\n\n    Raises:\n        ValueError: If the rule dictionary does not contain the required keys or if the date parsing fails.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    today = _dt.today().isoformat()\n    return df.filter(\n        pl.col(field).str.strptime(pl.Date, \"%Y-%m-%d\") == pl.lit(today).cast(pl.Date)\n    ).with_columns(pl.lit(f\"{field}:{check}:{value}\").alias(\"dq_status\"))\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.is_unique","title":"<code>is_unique(df, rule)</code>","text":"<p>Checks for duplicate values in a specified field of a Polars DataFrame and returns a filtered DataFrame containing only the rows with duplicate values. Additionally, it adds a new column 'dq_status' with a formatted string indicating the field, check type, and value.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to check for duplicates.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected          to have keys that allow extraction of the field to check,          the type of check, and a value.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A filtered DataFrame containing rows with duplicate values           in the specified field, along with an additional column           'dq_status' describing the rule applied.</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def is_unique(df: pl.DataFrame, rule: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Checks for duplicate values in a specified field of a Polars DataFrame and\n    returns a filtered DataFrame containing only the rows with duplicate values.\n    Additionally, it adds a new column 'dq_status' with a formatted string\n    indicating the field, check type, and value.\n\n    Args:\n        df (pl.DataFrame): The input Polars DataFrame to check for duplicates.\n        rule (dict): A dictionary containing the rule parameters. It is expected\n                     to have keys that allow extraction of the field to check,\n                     the type of check, and a value.\n\n    Returns:\n        pl.DataFrame: A filtered DataFrame containing rows with duplicate values\n                      in the specified field, along with an additional column\n                      'dq_status' describing the rule applied.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    dup_vals = (\n        df.group_by(field)\n        .agg(pl.len().alias(\"cnt\"))\n        .filter(pl.col(\"cnt\") &gt; 1)\n        .select(field)\n        .to_series()\n        .to_list()\n    )\n    return df.filter(pl.col(field).is_in(dup_vals)).with_columns(\n        pl.lit(f\"{field}:{check}:{value}\").alias(\"dq_status\")\n    )\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.not_contained_in","title":"<code>not_contained_in(df, rule)</code>","text":"<p>Filters a Polars DataFrame to include only rows where the specified field's value is in a given list, and adds a new column indicating the data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the filtering rule. It should include: - 'field': The column name to apply the filter on. - 'check': A string representing the type of check (not used in logic). - 'value': A string representation of a list of values (e.g., \"[value1, value2]\").</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new Polars DataFrame with rows filtered based on the rule and</p> <code>DataFrame</code> <p>an additional column \"dq_status\" indicating the applied rule.</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def not_contained_in(df: pl.DataFrame, rule: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Filters a Polars DataFrame to include only rows where the specified field's value\n    is in a given list, and adds a new column indicating the data quality status.\n\n    Args:\n        df (pl.DataFrame): The input Polars DataFrame to filter.\n        rule (dict): A dictionary containing the filtering rule. It should include:\n            - 'field': The column name to apply the filter on.\n            - 'check': A string representing the type of check (not used in logic).\n            - 'value': A string representation of a list of values (e.g., \"[value1, value2]\").\n\n    Returns:\n        pl.DataFrame: A new Polars DataFrame with rows filtered based on the rule and\n        an additional column \"dq_status\" indicating the applied rule.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    lst = [v.strip() for v in value.strip(\"[]\").split(\",\")]\n    return df.filter(pl.col(field).is_in(lst)).with_columns(\n        pl.lit(f\"{field}:{check}:{value}\").alias(\"dq_status\")\n    )\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.not_in","title":"<code>not_in(df, rule)</code>","text":"<p>Filters a Polars DataFrame by excluding rows where the specified rule applies.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary specifying the filtering rule. The structure and expected keys of this dictionary depend on the implementation of the <code>not_contained_in</code> function.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new DataFrame with rows excluded based on the given rule.</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def not_in(df: pl.DataFrame, rule: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Filters a Polars DataFrame by excluding rows where the specified rule applies.\n\n    Args:\n        df (pl.DataFrame): The input Polars DataFrame to filter.\n        rule (dict): A dictionary specifying the filtering rule. The structure and\n            expected keys of this dictionary depend on the implementation of the\n            `not_contained_in` function.\n\n    Returns:\n        pl.DataFrame: A new DataFrame with rows excluded based on the given rule.\n    \"\"\"\n    return not_contained_in(df, rule)\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.satisfies","title":"<code>satisfies(df, rule)</code>","text":"<p>Evaluates a given rule against a Polars DataFrame and returns rows that do not satisfy the rule.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to be evaluated.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule to be applied. The rule should include          the following keys:          - 'field': The column name in the DataFrame to be checked.          - 'check': The type of check or condition to be applied.          - 'value': The value or expression to validate against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A DataFrame containing rows that do not satisfy the rule, with an additional           column <code>dq_status</code> indicating the rule that was violated in the format           \"field:check:value\".</p> Example <p>rule = {\"field\": \"age\", \"check\": \"&gt;\", \"value\": \"18\"} result = satisfies(df, rule)</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def satisfies(df: pl.DataFrame, rule: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Evaluates a given rule against a Polars DataFrame and returns rows that do not satisfy the rule.\n\n    Args:\n        df (pl.DataFrame): The input Polars DataFrame to be evaluated.\n        rule (dict): A dictionary containing the rule to be applied. The rule should include\n                     the following keys:\n                     - 'field': The column name in the DataFrame to be checked.\n                     - 'check': The type of check or condition to be applied.\n                     - 'value': The value or expression to validate against.\n\n    Returns:\n        pl.DataFrame: A DataFrame containing rows that do not satisfy the rule, with an additional\n                      column `dq_status` indicating the rule that was violated in the format\n                      \"field:check:value\".\n\n    Example:\n        rule = {\"field\": \"age\", \"check\": \"&gt;\", \"value\": \"18\"}\n        result = satisfies(df, rule)\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    ctx = pl.SQLContext(sumeh=df)\n    viol = ctx.execute(\n        f\"\"\"\n        SELECT *\n        FROM sumeh\n        WHERE NOT ({value})\n        \"\"\",\n        eager=True,\n    )\n    return viol.with_columns(pl.lit(f\"{field}:{check}:{value}\").alias(\"dq_status\"))\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.summarize","title":"<code>summarize(qc_df, rules, total_rows)</code>","text":"<p>Summarizes quality check results by processing a DataFrame containing data quality statuses and comparing them against defined rules.</p> <p>Parameters:</p> Name Type Description Default <code>qc_df</code> <code>DataFrame</code> <p>A Polars DataFrame containing a column <code>dq_status</code> with semicolon-separated strings representing data quality statuses in the format \"column:rule:value\".</p> required <code>rules</code> <code>list[dict]</code> <p>A list of dictionaries where each dictionary defines a rule with keys such as \"column\", \"rule\", \"value\", and \"pass_threshold\".</p> required <code>total_rows</code> <code>int</code> <p>The total number of rows in the original dataset, used to calculate the pass rate.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A summarized DataFrame containing the following columns: - id: A unique identifier for each rule. - timestamp: The timestamp when the summary was generated. - check: A label indicating the type of check (e.g., \"Quality Check\"). - level: The severity level of the check (e.g., \"WARNING\"). - column: The column name associated with the rule. - rule: The rule being evaluated. - value: The specific value associated with the rule. - rows: The total number of rows in the dataset. - violations: The number of rows that violated the rule. - pass_rate: The proportion of rows that passed the rule. - pass_threshold: The threshold for passing the rule. - status: The status of the rule evaluation (\"PASS\" or \"FAIL\").</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def summarize(qc_df: pl.DataFrame, rules: list[dict], total_rows: int) -&gt; pl.DataFrame:\n    \"\"\"\n    Summarizes quality check results by processing a DataFrame containing\n    data quality statuses and comparing them against defined rules.\n\n    Args:\n        qc_df (pl.DataFrame): A Polars DataFrame containing a column `dq_status`\n            with semicolon-separated strings representing data quality statuses\n            in the format \"column:rule:value\".\n        rules (list[dict]): A list of dictionaries where each dictionary defines\n            a rule with keys such as \"column\", \"rule\", \"value\", and \"pass_threshold\".\n        total_rows (int): The total number of rows in the original dataset, used\n            to calculate the pass rate.\n\n    Returns:\n        pl.DataFrame: A summarized DataFrame containing the following columns:\n            - id: A unique identifier for each rule.\n            - timestamp: The timestamp when the summary was generated.\n            - check: A label indicating the type of check (e.g., \"Quality Check\").\n            - level: The severity level of the check (e.g., \"WARNING\").\n            - column: The column name associated with the rule.\n            - rule: The rule being evaluated.\n            - value: The specific value associated with the rule.\n            - rows: The total number of rows in the dataset.\n            - violations: The number of rows that violated the rule.\n            - pass_rate: The proportion of rows that passed the rule.\n            - pass_threshold: The threshold for passing the rule.\n            - status: The status of the rule evaluation (\"PASS\" or \"FAIL\").\n    \"\"\"\n    exploded = (\n        qc_df.select(\n            pl.col(\"dq_status\").str.split(\";\").list.explode().alias(\"dq_status\")\n        )\n        .filter(pl.col(\"dq_status\") != \"\")\n        .with_columns(\n            [\n                pl.col(\"dq_status\").str.split(\":\").list.get(0).alias(\"column\"),\n                pl.col(\"dq_status\").str.split(\":\").list.get(1).alias(\"rule\"),\n                pl.col(\"dq_status\").str.split(\":\").list.get(2).alias(\"value\"),\n            ]\n        )\n    ).drop(\"dq_status\")\n    viol_count = exploded.group_by([\"column\", \"rule\", \"value\"]).agg(\n        pl.len().alias(\"violations\")\n    )\n\n    rules_df = __build_rules_df(rules)\n\n    viol_count2 = viol_count.with_columns(pl.col(\"value\").fill_null(\"\").alias(\"value\"))\n\n    step1 = rules_df.join(\n        viol_count2,\n        on=[\"column\", \"rule\", \"value\"],\n        how=\"left\",\n    )\n\n    step2 = step1.with_columns([pl.col(\"violations\").fill_null(0).alias(\"violations\")])\n\n    step3 = step2.with_columns(\n        [\n            ((pl.lit(total_rows) - pl.col(\"violations\")) / pl.lit(total_rows)).alias(\n                \"pass_rate\"\n            )\n        ]\n    )\n\n    now = datetime.now().replace(second=0, microsecond=0)\n    step4 = step3.with_columns(\n        [\n            pl.lit(total_rows).alias(\"rows\"),\n            pl.when(pl.col(\"pass_rate\") &gt;= pl.col(\"pass_threshold\"))\n            .then(pl.lit(\"PASS\"))\n            .otherwise(pl.lit(\"FAIL\"))\n            .alias(\"status\"),\n            pl.lit(now).alias(\"timestamp\"),\n            pl.lit(\"Quality Check\").alias(\"check\"),\n            pl.lit(\"WARNING\").alias(\"level\"),\n        ]\n    )\n\n    uuids = np.array([uuid.uuid4() for _ in range(len(step4))], dtype=\"object\")\n\n    summary = step4.with_columns(pl.Series(uuids).alias(\"id\")).select(\n        [\n            \"id\",\n            \"timestamp\",\n            \"check\",\n            \"level\",\n            \"column\",\n            \"rule\",\n            \"value\",\n            \"rows\",\n            \"violations\",\n            \"pass_rate\",\n            \"pass_threshold\",\n            \"status\",\n        ]\n    )\n\n    return summary\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.validate","title":"<code>validate(df, rules)</code>","text":"<p>Validates a Polars DataFrame against a set of rules and returns the updated DataFrame with validation statuses and a DataFrame containing the validation violations.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to validate.</p> required <code>rules</code> <code>list[dict]</code> <p>A list of dictionaries representing validation rules. Each rule should contain the following keys: - \"check_type\" (str): The type of validation to perform (e.g., \"is_primary_key\",     \"is_composite_key\", \"has_pattern\", etc.). - \"value\" (optional): The value to validate against, depending on the rule type. - \"execute\" (bool, optional): Whether to execute the rule. Defaults to True.</p> required <p>Returns:</p> Type Description <code>Tuple[DataFrame, DataFrame]</code> <p>Tuple[pl.DataFrame, pl.DataFrame]: A tuple containing: - The original DataFrame with an additional \"dq_status\" column indicating the     validation status for each row. - A DataFrame containing rows that violated the validation rules, including     details of the violations.</p> Notes <ul> <li>The function dynamically resolves validation functions based on the \"check_type\"     specified in the rules.</li> <li>If a rule's \"check_type\" is unknown, a warning is issued, and the rule is skipped.</li> <li>The \"__id\" column is temporarily added to the DataFrame for internal processing     and is removed in the final output.</li> </ul> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def validate(df: pl.DataFrame, rules: list[dict]) -&gt; Tuple[pl.DataFrame, pl.DataFrame]:\n    \"\"\"\n    Validates a Polars DataFrame against a set of rules and returns the updated DataFrame\n    with validation statuses and a DataFrame containing the validation violations.\n\n    Args:\n        df (pl.DataFrame): The input Polars DataFrame to validate.\n        rules (list[dict]): A list of dictionaries representing validation rules. Each rule\n            should contain the following keys:\n            - \"check_type\" (str): The type of validation to perform (e.g., \"is_primary_key\",\n                \"is_composite_key\", \"has_pattern\", etc.).\n            - \"value\" (optional): The value to validate against, depending on the rule type.\n            - \"execute\" (bool, optional): Whether to execute the rule. Defaults to True.\n\n    Returns:\n        Tuple[pl.DataFrame, pl.DataFrame]: A tuple containing:\n            - The original DataFrame with an additional \"dq_status\" column indicating the\n                validation status for each row.\n            - A DataFrame containing rows that violated the validation rules, including\n                details of the violations.\n\n    Notes:\n        - The function dynamically resolves validation functions based on the \"check_type\"\n            specified in the rules.\n        - If a rule's \"check_type\" is unknown, a warning is issued, and the rule is skipped.\n        - The \"__id\" column is temporarily added to the DataFrame for internal processing\n            and is removed in the final output.\n    \"\"\"\n    df = df.with_columns(pl.arange(0, pl.len()).alias(\"__id\"))\n    df_with_dq = df.with_columns(pl.lit(\"\").alias(\"dq_status\"))\n    result = df_with_dq.head(0)\n    for rule in rules:\n        if not rule.get(\"execute\", True):\n            continue\n        rule_name = rule[\"check_type\"]\n        if rule_name == \"is_primary_key\":\n            rule_name = \"is_unique\"\n        elif rule_name == \"is_composite_key\":\n            rule_name = \"are_unique\"\n\n        func = globals().get(rule_name)\n        if func is None:\n            warnings.warn(f\"Unknown rule: {rule_name}\")\n            continue\n\n        raw_value = rule.get(\"value\")\n        if rule_name in (\"has_pattern\", \"satisfies\"):\n            value = raw_value\n        else:\n            try:\n                value = (\n                    __convert_value(raw_value)\n                    if isinstance(raw_value, str) and raw_value not in (\"\", \"NULL\")\n                    else raw_value\n                )\n            except ValueError:\n                value = raw_value\n\n        viol = func(df_with_dq, rule)\n        result = pl.concat([result, viol]) if not result.is_empty() else viol\n\n    summary = (\n        result.group_by(\"__id\", maintain_order=True)\n        .agg(\"dq_status\")\n        .with_columns(pl.col(\"dq_status\").list.join(\";\").alias(\"dq_status\"))\n    )\n    out = df.join(summary, on=\"__id\", how=\"left\").drop(\"__id\")\n\n    return out, result\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.validate_date_format","title":"<code>validate_date_format(df, rule)</code>","text":"<p>Validates the date format of a specified field in a Polars DataFrame based on a given rule.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to validate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the validation rule. It should include: - field (str): The name of the column to validate. - check (str): The name of the validation check. - fmt (str): The expected date format to validate against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new DataFrame containing only the rows where the specified field</p> <code>DataFrame</code> <p>does not match the expected date format or is null. An additional column</p> <code>DataFrame</code> <p>\"dq_status\" is added to indicate the validation status in the format</p> <code>DataFrame</code> <p>\"{field}:{check}:{fmt}\".</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def validate_date_format(df: pl.DataFrame, rule: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Validates the date format of a specified field in a Polars DataFrame based on a given rule.\n\n    Args:\n        df (pl.DataFrame): The input Polars DataFrame to validate.\n        rule (dict): A dictionary containing the validation rule. It should include:\n            - field (str): The name of the column to validate.\n            - check (str): The name of the validation check.\n            - fmt (str): The expected date format to validate against.\n\n    Returns:\n        pl.DataFrame: A new DataFrame containing only the rows where the specified field\n        does not match the expected date format or is null. An additional column\n        \"dq_status\" is added to indicate the validation status in the format\n        \"{field}:{check}:{fmt}\".\n    \"\"\"\n    field, check, fmt = __extract_params(rule)\n    regex = __transform_date_format_in_pattern(fmt)\n    return df.filter(\n        ~pl.col(field).str.contains(regex, literal=False) | pl.col(field).is_null()\n    ).with_columns(pl.lit(f\"{field}:{check}:{fmt}\").alias(\"dq_status\"))\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.validate_schema","title":"<code>validate_schema(df, expected)</code>","text":"<p>Validates the schema of a given DataFrame against an expected schema.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <p>The DataFrame whose schema needs to be validated.</p> required <code>expected</code> <p>The expected schema, represented as a list of tuples where each tuple       contains the column name and its data type.</p> required <p>Returns:</p> Type Description <code>Tuple[bool, List[Tuple[str, str]]]</code> <p>Tuple[bool, List[Tuple[str, str]]]: A tuple containing: - A boolean indicating whether the schema matches the expected schema. - A list of tuples representing the errors, where each tuple contains   the column name and a description of the mismatch.</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def validate_schema(df, expected) -&gt; Tuple[bool, List[Tuple[str, str]]]:\n    \"\"\"\n    Validates the schema of a given DataFrame against an expected schema.\n\n    Args:\n        df: The DataFrame whose schema needs to be validated.\n        expected: The expected schema, represented as a list of tuples where each tuple\n                  contains the column name and its data type.\n\n    Returns:\n        Tuple[bool, List[Tuple[str, str]]]: A tuple containing:\n            - A boolean indicating whether the schema matches the expected schema.\n            - A list of tuples representing the errors, where each tuple contains\n              the column name and a description of the mismatch.\n    \"\"\"\n    actual = __polars_schema_to_list(df)\n    result, errors = __compare_schemas(actual, expected)\n    return result, errors\n</code></pre>"},{"location":"api/engine/engine-pyspark/","title":"Module <code>sumeh.engine.pyspark_engine</code>","text":"<p>This module provides a set of functions for performing data quality checks on PySpark DataFrames. It includes various validation rules, schema validation, and summarization utilities.</p> <p>Functions:</p> Name Description <code>is_positive</code> <p>Filters rows where the specified field is negative and adds a data quality status column.</p> <code>is_negative</code> <p>Filters rows where the specified field is non-negative and adds a data quality status column.</p> <code>is_in_millions</code> <p>Retains rows where the field value is at least 1,000,000 and flags them with dq_status.</p> <code>is_positive</code> <p>Filters rows where the specified field is negative and adds a data quality status column.</p> <code>is_negative</code> <p>Filters rows where the specified field is non-negative and adds a data quality status column.</p> <code>is_in_millions</code> <p>Retains rows where the field value is at least 1,000,000 and flags them with dq_status.</p> <code>is_in_billions</code> <p>Retains rows where the field value is at least 1,000,000,000 and flags them with dq_status.</p> <code>is_t_minus_1</code> <p>Retains rows where the date field equals yesterday (T-1) and flags them with dq_status.</p> <code>is_t_minus_2</code> <p>Retains rows where the date field equals two days ago (T-2) and flags them with dq_status.</p> <code>is_t_minus_3</code> <p>Retains rows where the date field equals three days ago (T-3) and flags them with dq_status.</p> <code>is_today</code> <p>Retains rows where the date field equals today and flags them with dq_status.</p> <code>is_yesterday</code> <p>Retains rows where the date field equals yesterday and flags them with dq_status.</p> <code>is_on_weekday</code> <p>Retains rows where the date field falls on a weekday (Mon-Fri) and flags them with dq_status.</p> <code>is_on_weekend</code> <p>Retains rows where the date field is on a weekend (Sat-Sun) and flags them with dq_status.</p> <code>is_on_monday</code> <p>Retains rows where the date field is on Monday and flags them with dq_status.</p> <code>is_on_tuesday</code> <p>Retains rows where the date field is on Tuesday and flags them with dq_status.</p> <code>is_on_wednesday</code> <p>Retains rows where the date field is on Wednesday and flags them with dq_status.</p> <code>is_on_thursday</code> <p>Retains rows where the date field is on Thursday and flags them with dq_status.</p> <code>is_on_friday</code> <p>Retains rows where the date field is on Friday and flags them with dq_status.</p> <code>is_on_saturday</code> <p>Retains rows where the date field is on Saturday and flags them with dq_status.</p> <code>is_on_sunday</code> <p>Retains rows where the date field is on Sunday and flags them with dq_status.</p> <code>is_complete</code> <p>Filters rows where the specified field is null and adds a data quality status column.</p> <code>is_unique</code> <p>Identifies duplicate rows based on the specified field and adds a data quality status column.</p> <code>are_complete</code> <p>Filters rows where any of the specified fields are null and adds a data quality status column.</p> <code>are_unique</code> <p>Identifies duplicate rows based on a combination of specified fields and adds a data quality status column.</p> <code>is_greater_than</code> <p>Filters rows where the specified field is less than or equal to the given value.</p> <code>is_greater_or_equal_than</code> <p>Filters rows where the specified field is less than the given value.</p> <code>is_less_than</code> <p>Filters rows where the specified field is greater than or equal to the given value.</p> <code>is_less_or_equal_than</code> <p>Filters rows where the specified field is greater than the given value.</p> <code>is_equal</code> <p>Filters rows where the specified field is not equal to the given value.</p> <code>is_equal_than</code> <p>Alias for <code>is_equal</code>.</p> <code>is_contained_in</code> <p>Filters rows where the specified field is not in the given list of values.</p> <code>not_contained_in</code> <p>Filters rows where the specified field is in the given list of values.</p> <code>is_between</code> <p>Filters rows where the specified field is not within the given range.</p> <code>has_pattern</code> <p>Filters rows where the specified field does not match the given regex pattern.</p> <code>is_legit</code> <p>Filters rows where the specified field is null or does not match a non-whitespace pattern.</p> <code>is_primary_key</code> <p>DataFrame, rule: dict):</p> <code>is_composite_key</code> <p>DataFrame, rule: dict):</p> <code>has_max</code> <p>Filters rows where the specified field exceeds the given maximum value.</p> <code>has_min</code> <p>Filters rows where the specified field is below the given minimum value.</p> <code>has_std</code> <p>Checks if the standard deviation of the specified field exceeds the given value.</p> <code>has_mean</code> <p>Checks if the mean of the specified field exceeds the given value.</p> <code>has_sum</code> <p>Checks if the sum of the specified field exceeds the given value.</p> <code>has_cardinality</code> <p>Checks if the cardinality (distinct count) of the specified field exceeds the given value.</p> <code>has_infogain</code> <p>Checks if the information gain (distinct count) of the specified field exceeds the given value.</p> <code>has_entropy</code> <p>Checks if the entropy (distinct count) of the specified field exceeds the given value.</p> <code>all_date_checks</code> <p>Filters rows where the specified date field is earlier than the current date.</p> <code>satisfies</code> <p>Filters rows where the specified field matches the given regex pattern.</p> <code>validate</code> <p>Applies a list of validation rules to the DataFrame and returns the results.</p> <code>summarize</code> <p>Summarizes the results of data quality checks, including pass rates and violations.</p> <code>validate_schema</code> <p>Validates the schema of the DataFrame against the expected schema.</p> <code>__rules_to_df</code> <p>Converts a list of rules into a DataFrame for further processing.</p> <code>__pyspark_schema_to_list</code> <p>Converts the schema of a DataFrame into a list of dictionaries for comparison.</p>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.__compare_schemas","title":"<code>__compare_schemas(actual, expected)</code>","text":"<p>Compare two lists of schema definitions and identify discrepancies.</p> <p>Parameters:</p> Name Type Description Default <code>actual</code> <code>List[SchemaDef]</code> <p>The list of actual schema definitions.</p> required <code>expected</code> <code>List[SchemaDef]</code> <p>The list of expected schema definitions.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>Tuple[bool, List[Tuple[str, str]]]: A tuple where the first element is a boolean indicating</p> <code>List[Tuple[str, str]]</code> <p>whether the schemas match (True if they match, False otherwise), and the second element</p> <code>Tuple[bool, List[Tuple[str, str]]]</code> <p>is a list of tuples describing the discrepancies. Each tuple contains: - The field name (str). - A description of the discrepancy (str), such as \"missing\", \"type mismatch\",   \"nullable but expected non-nullable\", or \"extra column\".</p> Notes <ul> <li>A field is considered \"missing\" if it exists in the expected schema but not in the actual schema.</li> <li>A \"type mismatch\" occurs if the data type of a field in the actual schema does not match   the expected data type.</li> <li>A field is considered \"nullable but expected non-nullable\" if it is nullable in the actual   schema but not nullable in the expected schema.</li> <li>An \"extra column\" is a field that exists in the actual schema but not in the expected schema.</li> </ul> Source code in <code>sumeh/services/utils.py</code> <pre><code>def __compare_schemas(\n    actual: List[SchemaDef],\n    expected: List[SchemaDef],\n) -&gt; Tuple[bool, List[Tuple[str, str]]]:\n    \"\"\"\n    Compare two lists of schema definitions and identify discrepancies.\n\n    Args:\n        actual (List[SchemaDef]): The list of actual schema definitions.\n        expected (List[SchemaDef]): The list of expected schema definitions.\n\n    Returns:\n        Tuple[bool, List[Tuple[str, str]]]: A tuple where the first element is a boolean indicating\n        whether the schemas match (True if they match, False otherwise), and the second element\n        is a list of tuples describing the discrepancies. Each tuple contains:\n            - The field name (str).\n            - A description of the discrepancy (str), such as \"missing\", \"type mismatch\",\n              \"nullable but expected non-nullable\", or \"extra column\".\n\n    Notes:\n        - A field is considered \"missing\" if it exists in the expected schema but not in the actual schema.\n        - A \"type mismatch\" occurs if the data type of a field in the actual schema does not match\n          the expected data type.\n        - A field is considered \"nullable but expected non-nullable\" if it is nullable in the actual\n          schema but not nullable in the expected schema.\n        - An \"extra column\" is a field that exists in the actual schema but not in the expected schema.\n    \"\"\"\n\n    exp_map = {c[\"field\"]: c for c in expected}\n    act_map = {c[\"field\"]: c for c in actual}\n\n    erros: List[Tuple[str, str]] = []\n\n    for fld, exp in exp_map.items():\n        if fld not in act_map:\n            erros.append((fld, \"missing\"))\n            continue\n        act = act_map[fld]\n        if act[\"data_type\"] != exp[\"data_type\"]:\n            erros.append(\n                (\n                    fld,\n                    f\"type mismatch (got {act['data_type']}, expected {exp['data_type']})\",\n                )\n            )\n\n        if act[\"nullable\"] and not exp[\"nullable\"]:\n            erros.append((fld, \"nullable but expected non-nullable\"))\n\n        if exp.get(\"max_length\") is not None:\n            pass\n\n    # 2. campos extras (se quiser)\n    extras = set(act_map) - set(exp_map)\n    for fld in extras:\n        erros.append((fld, \"extra column\"))\n\n    return len(erros) == 0, erros\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.__convert_value","title":"<code>__convert_value(value)</code>","text":"<p>Converts the provided value to the appropriate type (date, float, or int).</p> <p>Depending on the format of the input value, it will be converted to a datetime object, a floating-point number (float), or an integer (int).</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>The value to be converted, represented as a string.</p> required <p>Returns:</p> Type Description <p>Union[datetime, float, int]: The converted value, which can be a datetime object, float, or int.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the value does not match an expected format.</p> Source code in <code>sumeh/services/utils.py</code> <pre><code>def __convert_value(value):\n    \"\"\"\n    Converts the provided value to the appropriate type (date, float, or int).\n\n    Depending on the format of the input value, it will be converted to a datetime object,\n    a floating-point number (float), or an integer (int).\n\n    Args:\n        value (str): The value to be converted, represented as a string.\n\n    Returns:\n        Union[datetime, float, int]: The converted value, which can be a datetime object, float, or int.\n\n    Raises:\n        ValueError: If the value does not match an expected format.\n    \"\"\"\n    from datetime import datetime\n\n    value = value.strip()\n    try:\n        if \"-\" in value:\n            return datetime.strptime(value, \"%Y-%m-%d\")\n        else:\n            return datetime.strptime(value, \"%d/%m/%Y\")\n    except ValueError:\n        if \".\" in value:\n            return float(value)\n        return int(value)\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.__extract_params","title":"<code>__extract_params(rule)</code>","text":"Source code in <code>sumeh/services/utils.py</code> <pre><code>def __extract_params(rule: dict) -&gt; tuple:\n    rule_name = rule[\"check_type\"]\n    field = rule[\"field\"]\n    raw_value = rule.get(\"value\")\n    if isinstance(raw_value, str) and raw_value not in (None, \"\", \"NULL\"):\n        try:\n            value = __convert_value(raw_value)\n        except ValueError:\n            value = raw_value\n    else:\n        value = raw_value\n    value = value if value not in (None, \"\", \"NULL\") else \"\"\n    return field, rule_name, value\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.__pyspark_schema_to_list","title":"<code>__pyspark_schema_to_list(df)</code>","text":"<p>Convert the schema of a PySpark DataFrame into a list of dictionaries.</p> <p>Each dictionary in the output list represents a field in the DataFrame schema and contains the following keys:     - \"field\": The name of the field.     - \"data_type\": The data type of the field as a lowercase string.     - \"nullable\": A boolean indicating whether the field allows null values.     - \"max_length\": Always set to None (reserved for future use).</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The PySpark DataFrame whose schema is to be converted.</p> required <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List[Dict[str, Any]]: A list of dictionaries representing the schema of the DataFrame.</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def __pyspark_schema_to_list(df: DataFrame) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Convert the schema of a PySpark DataFrame into a list of dictionaries.\n\n    Each dictionary in the output list represents a field in the DataFrame schema\n    and contains the following keys:\n        - \"field\": The name of the field.\n        - \"data_type\": The data type of the field as a lowercase string.\n        - \"nullable\": A boolean indicating whether the field allows null values.\n        - \"max_length\": Always set to None (reserved for future use).\n\n    Args:\n        df (DataFrame): The PySpark DataFrame whose schema is to be converted.\n\n    Returns:\n        List[Dict[str, Any]]: A list of dictionaries representing the schema of the DataFrame.\n    \"\"\"\n    out: List[Dict[str, Any]] = []\n    for f in df.schema.fields:\n        out.append(\n            {\n                \"field\": f.name,\n                \"data_type\": f.dataType.simpleString().lower(),\n                \"nullable\": f.nullable,\n                \"max_length\": None,\n            }\n        )\n    return out\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.__rules_to_df","title":"<code>__rules_to_df(rules)</code>","text":"<p>Converts a list of rule dictionaries into a PySpark DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>rules</code> <code>List[Dict]</code> <p>A list of dictionaries where each dictionary represents a rule. Each rule dictionary should contain the following keys:     - \"field\" (str or list): The name of the field or a list of field names.     - \"check_type\" (str): The type of rule or check to be applied.     - \"threshold\" (float, optional): The threshold value for the rule. Defaults to 1.0 if not provided.     - \"value\" (str, optional): The value associated with the rule. Defaults to \"N/A\" if not provided.     - \"execute\" (bool, optional): A flag indicating whether the rule should be executed. Defaults to True.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A PySpark DataFrame containing the following columns: - \"column\" (str): The name of the field. - \"rule\" (str): The type of rule or check. - \"pass_threshold\" (float): The threshold value for the rule. - \"value\" (str): The value associated with the rule.</p> Notes <ul> <li>Rows with \"execute\" set to False are skipped.</li> <li>Duplicate rows based on the \"column\" and \"rule\" columns are removed.</li> </ul> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def __rules_to_df(rules: List[Dict]) -&gt; DataFrame:\n    \"\"\"\n    Converts a list of rule dictionaries into a PySpark DataFrame.\n\n    Args:\n        rules (List[Dict]): A list of dictionaries where each dictionary represents a rule.\n            Each rule dictionary should contain the following keys:\n                - \"field\" (str or list): The name of the field or a list of field names.\n                - \"check_type\" (str): The type of rule or check to be applied.\n                - \"threshold\" (float, optional): The threshold value for the rule. Defaults to 1.0 if not provided.\n                - \"value\" (str, optional): The value associated with the rule. Defaults to \"N/A\" if not provided.\n                - \"execute\" (bool, optional): A flag indicating whether the rule should be executed. Defaults to True.\n\n    Returns:\n        DataFrame: A PySpark DataFrame containing the following columns:\n            - \"column\" (str): The name of the field.\n            - \"rule\" (str): The type of rule or check.\n            - \"pass_threshold\" (float): The threshold value for the rule.\n            - \"value\" (str): The value associated with the rule.\n\n    Notes:\n        - Rows with \"execute\" set to False are skipped.\n        - Duplicate rows based on the \"column\" and \"rule\" columns are removed.\n    \"\"\"\n    from pyspark.sql import SparkSession\n\n    spark = SparkSession.builder.getOrCreate()\n    rows = []\n    for r in rules:\n        if not r.get(\"execute\", True):\n            continue\n        col_name = str(r[\"field\"]) if isinstance(r[\"field\"], list) else r[\"field\"]\n        rows.append(\n            Row(\n                column=col_name.strip(),\n                rule=r[\"check_type\"],\n                pass_threshold=float(r.get(\"threshold\") or 1.0),\n                value=r.get(\"value\", \"N/A\") or \"N/A\",\n            )\n        )\n    return spark.createDataFrame(rows).dropDuplicates([\"column\", \"rule\"])\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.__transform_date_format_in_pattern","title":"<code>__transform_date_format_in_pattern(date_format)</code>","text":"Source code in <code>sumeh/services/utils.py</code> <pre><code>def __transform_date_format_in_pattern(date_format):\n    date_patterns = {\n        \"DD\": \"(0[1-9]|[12][0-9]|3[01])\",\n        \"MM\": \"(0[1-9]|1[012])\",\n        \"YYYY\": \"(19|20)\\\\d\\\\d\",\n        \"YY\": \"\\\\d\\\\d\",\n        \" \": \"\\\\s\",\n        \".\": \"\\\\.\",\n    }\n\n    date_pattern = date_format\n    for single_format, pattern in date_patterns.items():\n        date_pattern = date_pattern.replace(single_format, pattern)\n\n    return date_pattern\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.all_date_checks","title":"<code>all_date_checks(df, rule)</code>","text":"<p>Filters the input DataFrame based on a date-related rule and adds a data quality status column.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The name of the column to apply the rule on. - 'check': The type of check to perform (e.g., comparison operator). - 'value': The value to be used in the check.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame filtered based on the rule, with an additional column</p> <code>DataFrame</code> <p>\"dq_status\" indicating the data quality status in the format \"field:check:value\".</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def all_date_checks(df: DataFrame, rule: dict) -&gt; DataFrame:\n    \"\"\"\n    Filters the input DataFrame based on a date-related rule and adds a data quality status column.\n\n    Args:\n        df (DataFrame): The input PySpark DataFrame to be filtered.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field': The name of the column to apply the rule on.\n            - 'check': The type of check to perform (e.g., comparison operator).\n            - 'value': The value to be used in the check.\n\n    Returns:\n        DataFrame: A new DataFrame filtered based on the rule, with an additional column\n        \"dq_status\" indicating the data quality status in the format \"field:check:value\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    return df.filter((col(field) &lt; current_date())).withColumn(\n        \"dq_status\", concat(lit(field), lit(\":\"), lit(check), lit(\":\"), lit(value))\n    )\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.are_complete","title":"<code>are_complete(df, rule)</code>","text":"<p>Filters rows in a DataFrame that do not meet the completeness rule and adds a data quality status column.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - \"fields\" (list): A list of column names to check for completeness (non-null values). - \"check\" (str): A descriptive label for the type of check being performed. - \"value\" (str): A descriptive value associated with the check.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame containing only the rows that fail the completeness check,</p> <code>DataFrame</code> <p>with an additional column \"dq_status\" describing the failed rule.</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def are_complete(df: DataFrame, rule: dict) -&gt; DataFrame:\n    \"\"\"\n    Filters rows in a DataFrame that do not meet the completeness rule and adds a data quality status column.\n\n    Args:\n        df (DataFrame): The input PySpark DataFrame to be checked.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - \"fields\" (list): A list of column names to check for completeness (non-null values).\n            - \"check\" (str): A descriptive label for the type of check being performed.\n            - \"value\" (str): A descriptive value associated with the check.\n\n    Returns:\n        DataFrame: A new DataFrame containing only the rows that fail the completeness check,\n        with an additional column \"dq_status\" describing the failed rule.\n    \"\"\"\n    fields, check, value = __extract_params(rule)\n    predicate = reduce(operator.and_, [col(field).isNotNull() for field in fields])\n    return df.filter(~predicate).withColumn(\n        \"dq_status\",\n        concat(lit(str(fields)), lit(\":\"), lit(check), lit(\":\"), lit(value)),\n    )\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.are_unique","title":"<code>are_unique(df, rule)</code>","text":"<p>Checks for uniqueness of specified fields in a PySpark DataFrame based on the provided rule.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'fields': A list of column names to check for uniqueness. - 'check': A string representing the type of check (e.g., \"unique\"). - 'value': A value associated with the rule for logging or identification.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A DataFrame containing rows that violate the uniqueness rule.</p> <code>DataFrame</code> <p>The resulting DataFrame includes an additional column <code>dq_status</code> that</p> <code>DataFrame</code> <p>describes the rule violation in the format: \"[fields]:[check]:[value]\".</p> Notes <ul> <li>The function concatenates the specified fields into a single column   and checks for duplicate values within that column.</li> <li>Rows that do not meet the uniqueness criteria are returned, while   rows that satisfy the criteria are excluded from the result.</li> </ul> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def are_unique(df: DataFrame, rule: dict) -&gt; DataFrame:\n    \"\"\"\n    Checks for uniqueness of specified fields in a PySpark DataFrame based on the provided rule.\n\n    Args:\n        df (DataFrame): The input PySpark DataFrame to be checked.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'fields': A list of column names to check for uniqueness.\n            - 'check': A string representing the type of check (e.g., \"unique\").\n            - 'value': A value associated with the rule for logging or identification.\n\n    Returns:\n        DataFrame: A DataFrame containing rows that violate the uniqueness rule.\n        The resulting DataFrame includes an additional column `dq_status` that\n        describes the rule violation in the format: \"[fields]:[check]:[value]\".\n\n    Notes:\n        - The function concatenates the specified fields into a single column\n          and checks for duplicate values within that column.\n        - Rows that do not meet the uniqueness criteria are returned, while\n          rows that satisfy the criteria are excluded from the result.\n    \"\"\"\n    fields, check, value = __extract_params(rule)\n    combined_col = concat_ws(\"|\", *[coalesce(col(f), lit(\"\")) for f in fields])\n    window = Window.partitionBy(combined_col)\n    result = (\n        df.withColumn(\"_count\", count(\"*\").over(window))\n        .filter(col(\"_count\") &gt; 1)\n        .drop(\"_count\")\n        .withColumn(\n            \"dq_status\",\n            concat(lit(str(fields)), lit(\":\"), lit(check), lit(\":\"), lit(value)),\n        )\n    )\n    return result\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.has_cardinality","title":"<code>has_cardinality(df, rule)</code>","text":"<p>Checks the cardinality of a specified field in a DataFrame against a given rule.</p> <p>This function evaluates whether the distinct count of values in a specified column (field) of the DataFrame exceeds a given threshold (value) as defined in the rule. If the cardinality exceeds the threshold, a new column <code>dq_status</code> is added to the DataFrame with information about the rule violation. Otherwise, an empty DataFrame is returned.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The name of the column to check. - 'check': The type of check being performed (e.g., \"cardinality\"). - 'value': The threshold value for the cardinality.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A DataFrame with the <code>dq_status</code> column added if the cardinality</p> <code>DataFrame</code> <p>exceeds the threshold, or an empty DataFrame if the condition is not met.</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def has_cardinality(df: DataFrame, rule: dict) -&gt; DataFrame:\n    \"\"\"\n    Checks the cardinality of a specified field in a DataFrame against a given rule.\n\n    This function evaluates whether the distinct count of values in a specified column\n    (field) of the DataFrame exceeds a given threshold (value) as defined in the rule.\n    If the cardinality exceeds the threshold, a new column `dq_status` is added to the\n    DataFrame with information about the rule violation. Otherwise, an empty DataFrame\n    is returned.\n\n    Args:\n        df (DataFrame): The input PySpark DataFrame to be checked.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field': The name of the column to check.\n            - 'check': The type of check being performed (e.g., \"cardinality\").\n            - 'value': The threshold value for the cardinality.\n\n    Returns:\n        DataFrame: A DataFrame with the `dq_status` column added if the cardinality\n        exceeds the threshold, or an empty DataFrame if the condition is not met.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    card_val = df.select(countDistinct(col(field))).first()[0] or 0\n    if card_val &gt; value:\n        return df.withColumn(\n            \"dq_status\", concat(lit(field), lit(\":\"), lit(check), lit(\":\"), lit(value))\n        )\n    return df.limit(0)\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.has_entropy","title":"<code>has_entropy(df, rule)</code>","text":"<p>Evaluates the entropy of a specified field in a DataFrame and applies a rule to determine whether the DataFrame should be processed further or filtered out.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to evaluate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The name of the column to evaluate. - 'check' (str): The type of check being performed (e.g., \"entropy\"). - 'value' (float): The threshold value for the entropy check.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>If the entropy of the specified field exceeds the given value, returns the</p> <code>DataFrame</code> <p>original DataFrame with an additional column \"dq_status\" indicating the rule applied.</p> <code>DataFrame</code> <p>Otherwise, returns an empty DataFrame with the same schema as the input.</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def has_entropy(df: DataFrame, rule: dict) -&gt; DataFrame:\n    \"\"\"\n    Evaluates the entropy of a specified field in a DataFrame and applies a rule to determine\n    whether the DataFrame should be processed further or filtered out.\n\n    Parameters:\n        df (DataFrame): The input PySpark DataFrame to evaluate.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field' (str): The name of the column to evaluate.\n            - 'check' (str): The type of check being performed (e.g., \"entropy\").\n            - 'value' (float): The threshold value for the entropy check.\n\n    Returns:\n        DataFrame: If the entropy of the specified field exceeds the given value, returns the\n        original DataFrame with an additional column \"dq_status\" indicating the rule applied.\n        Otherwise, returns an empty DataFrame with the same schema as the input.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    entropy_val = df.select(countDistinct(col(field))).first()[0] or 0.0\n    if entropy_val &gt; value:\n        return df.withColumn(\n            \"dq_status\", concat(lit(field), lit(\":\"), lit(check), lit(\":\"), lit(value))\n        )\n    return df.limit(0)\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.has_infogain","title":"<code>has_infogain(df, rule)</code>","text":"<p>Evaluates whether a given DataFrame satisfies an information gain condition based on the provided rule. If the condition is met, it appends a column indicating the status; otherwise, it returns an empty DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to evaluate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should          include the following keys:          - 'field': The column name to evaluate.          - 'check': The condition type (not used directly in the logic).          - 'value': The threshold value for information gain.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A DataFrame with an additional \"dq_status\" column if the        information gain condition is met, or an empty DataFrame        if the condition is not satisfied.</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def has_infogain(df: DataFrame, rule: dict) -&gt; DataFrame:\n    \"\"\"\n    Evaluates whether a given DataFrame satisfies an information gain condition\n    based on the provided rule. If the condition is met, it appends a column\n    indicating the status; otherwise, it returns an empty DataFrame.\n\n    Args:\n        df (DataFrame): The input PySpark DataFrame to evaluate.\n        rule (dict): A dictionary containing the rule parameters. It should\n                     include the following keys:\n                     - 'field': The column name to evaluate.\n                     - 'check': The condition type (not used directly in the logic).\n                     - 'value': The threshold value for information gain.\n\n    Returns:\n        DataFrame: A DataFrame with an additional \"dq_status\" column if the\n                   information gain condition is met, or an empty DataFrame\n                   if the condition is not satisfied.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    info_gain = df.select(countDistinct(col(field))).first()[0] or 0.0\n    if info_gain &gt; value:\n        return df.withColumn(\n            \"dq_status\", concat(lit(field), lit(\":\"), lit(check), lit(\":\"), lit(value))\n        )\n    return df.limit(0)\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.has_max","title":"<code>has_max(df, rule)</code>","text":"<p>Filters a PySpark DataFrame to include only rows where the value of a specified field is greater than a given threshold. Adds a new column 'dq_status' to indicate the rule applied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The name of the column to apply the rule on. - 'check' (str): The type of check being performed (e.g., 'max'). - 'value' (numeric): The threshold value to compare against.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame filtered based on the rule, with an additional column 'dq_status'</p> <code>DataFrame</code> <p>describing the rule applied in the format \"field:check:value\".</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def has_max(df: DataFrame, rule: dict) -&gt; DataFrame:\n    \"\"\"\n    Filters a PySpark DataFrame to include only rows where the value of a specified field\n    is greater than a given threshold. Adds a new column 'dq_status' to indicate the rule applied.\n\n    Args:\n        df (DataFrame): The input PySpark DataFrame to be filtered.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field' (str): The name of the column to apply the rule on.\n            - 'check' (str): The type of check being performed (e.g., 'max').\n            - 'value' (numeric): The threshold value to compare against.\n\n    Returns:\n        DataFrame: A new DataFrame filtered based on the rule, with an additional column 'dq_status'\n        describing the rule applied in the format \"field:check:value\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    return df.filter(col(field) &gt; value).withColumn(\n        \"dq_status\", concat(lit(field), lit(\":\"), lit(check), lit(\":\"), lit(value))\n    )\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.has_mean","title":"<code>has_mean(df, rule)</code>","text":"<p>Evaluates whether the mean value of a specified column in a DataFrame satisfies a given rule.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to evaluate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The name of the column to calculate the mean for. - 'check' (str): The type of check being performed (e.g., 'greater_than'). - 'value' (float): The threshold value to compare the mean against.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>If the mean value of the specified column exceeds the threshold,</p> <code>DataFrame</code> <p>returns the original DataFrame with an additional column <code>dq_status</code> indicating</p> <code>DataFrame</code> <p>the rule violation. If the mean value satisfies the rule, returns an empty DataFrame.</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def has_mean(df: DataFrame, rule: dict) -&gt; DataFrame:\n    \"\"\"\n    Evaluates whether the mean value of a specified column in a DataFrame satisfies a given rule.\n\n    Args:\n        df (DataFrame): The input PySpark DataFrame to evaluate.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field' (str): The name of the column to calculate the mean for.\n            - 'check' (str): The type of check being performed (e.g., 'greater_than').\n            - 'value' (float): The threshold value to compare the mean against.\n\n    Returns:\n        DataFrame: If the mean value of the specified column exceeds the threshold,\n        returns the original DataFrame with an additional column `dq_status` indicating\n        the rule violation. If the mean value satisfies the rule, returns an empty DataFrame.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    mean_val = (df.select(avg(col(field))).first()[0]) or 0.0\n    if mean_val &gt; value:  # regra falhou\n        return df.withColumn(\n            \"dq_status\", concat(lit(field), lit(\":\"), lit(check), lit(\":\"), lit(value))\n        )\n    else:  # passou\n        return df.limit(0)\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.has_min","title":"<code>has_min(df, rule)</code>","text":"<p>Filters rows in a DataFrame where the value of a specified field is less than a given threshold and adds a new column indicating the data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The name of the column to check. - 'check' (str): The type of check being performed (e.g., \"min\"). - 'value' (numeric): The threshold value for the check.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame with rows filtered based on the rule and an additional</p> <code>DataFrame</code> <p>\"dq_status\" column containing a string representation of the rule applied.</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def has_min(df: DataFrame, rule: dict) -&gt; DataFrame:\n    \"\"\"\n    Filters rows in a DataFrame where the value of a specified field is less than a given threshold\n    and adds a new column indicating the data quality status.\n\n    Args:\n        df (DataFrame): The input PySpark DataFrame to be filtered.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field' (str): The name of the column to check.\n            - 'check' (str): The type of check being performed (e.g., \"min\").\n            - 'value' (numeric): The threshold value for the check.\n\n    Returns:\n        DataFrame: A new DataFrame with rows filtered based on the rule and an additional\n        \"dq_status\" column containing a string representation of the rule applied.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    return df.filter(col(field) &lt; value).withColumn(\n        \"dq_status\", concat(lit(field), lit(\":\"), lit(check), lit(\":\"), lit(value))\n    )\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.has_pattern","title":"<code>has_pattern(df, rule)</code>","text":"<p>Filters a PySpark DataFrame based on a pattern match and adds a data quality status column.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The name of the column to apply the pattern check. - 'check': A descriptive label for the type of check being performed. - 'value': The regex pattern to match against the column values.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame with rows that do not match the pattern filtered out.        Additionally, a \"dq_status\" column is added, containing a string        representation of the rule applied in the format \"field:check:value\".</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def has_pattern(df: DataFrame, rule: dict) -&gt; DataFrame:\n    \"\"\"\n    Filters a PySpark DataFrame based on a pattern match and adds a data quality status column.\n\n    Args:\n        df (DataFrame): The input PySpark DataFrame to be filtered.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field': The name of the column to apply the pattern check.\n            - 'check': A descriptive label for the type of check being performed.\n            - 'value': The regex pattern to match against the column values.\n\n    Returns:\n        DataFrame: A new DataFrame with rows that do not match the pattern filtered out.\n                   Additionally, a \"dq_status\" column is added, containing a string\n                   representation of the rule applied in the format \"field:check:value\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    return df.filter(~col(field).rlike(value)).withColumn(\n        \"dq_status\", concat(lit(field), lit(\":\"), lit(check), lit(\":\"), lit(value))\n    )\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.has_std","title":"<code>has_std(df, rule)</code>","text":"<p>Checks if the standard deviation of a specified field in a DataFrame exceeds a given value.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The name of the column to calculate the standard deviation for. - 'check' (str): A descriptive label for the check being performed. - 'value' (float): The threshold value for the standard deviation.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>If the standard deviation of the specified field exceeds the given value,</p> <code>DataFrame</code> <p>returns the original DataFrame with an additional column \"dq_status\" indicating the</p> <code>DataFrame</code> <p>field, check, and value. Otherwise, returns an empty DataFrame.</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def has_std(df: DataFrame, rule: dict) -&gt; DataFrame:\n    \"\"\"\n    Checks if the standard deviation of a specified field in a DataFrame exceeds a given value.\n\n    Args:\n        df (DataFrame): The input PySpark DataFrame.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field' (str): The name of the column to calculate the standard deviation for.\n            - 'check' (str): A descriptive label for the check being performed.\n            - 'value' (float): The threshold value for the standard deviation.\n\n    Returns:\n        DataFrame: If the standard deviation of the specified field exceeds the given value,\n        returns the original DataFrame with an additional column \"dq_status\" indicating the\n        field, check, and value. Otherwise, returns an empty DataFrame.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    std_val = df.select(stddev(col(field))).first()[0]\n    std_val = std_val or 0.0\n    if std_val &gt; value:\n        return df.withColumn(\n            \"dq_status\", concat(lit(field), lit(\":\"), lit(check), lit(\":\"), lit(value))\n        )\n    else:\n        return df.limit(0)\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.has_sum","title":"<code>has_sum(df, rule)</code>","text":"<p>Checks if the sum of values in a specified column of a DataFrame exceeds a given threshold.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to evaluate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The name of the column to sum. - 'check' (str): A descriptive label for the check being performed. - 'value' (float): The threshold value to compare the sum against.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>If the sum of the specified column exceeds the threshold, returns the original</p> <code>DataFrame</code> <p>DataFrame with an additional column <code>dq_status</code> indicating the rule details. If the sum</p> <code>DataFrame</code> <p>does not exceed the threshold, returns an empty DataFrame.</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def has_sum(df: DataFrame, rule: dict) -&gt; DataFrame:\n    \"\"\"\n    Checks if the sum of values in a specified column of a DataFrame exceeds a given threshold.\n\n    Args:\n        df (DataFrame): The input PySpark DataFrame to evaluate.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field' (str): The name of the column to sum.\n            - 'check' (str): A descriptive label for the check being performed.\n            - 'value' (float): The threshold value to compare the sum against.\n\n    Returns:\n        DataFrame: If the sum of the specified column exceeds the threshold, returns the original\n        DataFrame with an additional column `dq_status` indicating the rule details. If the sum\n        does not exceed the threshold, returns an empty DataFrame.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    sum_val = (df.select(sum(col(field))).first()[0]) or 0.0\n    if sum_val &gt; value:\n        return df.withColumn(\n            \"dq_status\", concat(lit(field), lit(\":\"), lit(check), lit(\":\"), lit(value))\n        )\n    return df.limit(0)\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.is_between","title":"<code>is_between(df, rule)</code>","text":"<p>Filters rows in a PySpark DataFrame where the value of a specified field is not within a given range. Adds a new column 'dq_status' to indicate the rule that was applied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The name of the column to check. - 'check': A string representing the type of check (e.g., \"between\"). - 'value': A string representing the range in the format \"[min_value,max_value]\".</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame with rows filtered based on the rule and an additional</p> <code>DataFrame</code> <p>'dq_status' column indicating the applied rule.</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def is_between(df: DataFrame, rule: dict) -&gt; DataFrame:\n    \"\"\"\n    Filters rows in a PySpark DataFrame where the value of a specified field is not within a given range.\n    Adds a new column 'dq_status' to indicate the rule that was applied.\n\n    Args:\n        df (DataFrame): The input PySpark DataFrame.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field': The name of the column to check.\n            - 'check': A string representing the type of check (e.g., \"between\").\n            - 'value': A string representing the range in the format \"[min_value,max_value]\".\n\n    Returns:\n        DataFrame: A new DataFrame with rows filtered based on the rule and an additional\n        'dq_status' column indicating the applied rule.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    min_value, max_value = value.strip(\"[]\").split(\",\")\n    return df.filter(~col(field).between(min_value, max_value)).withColumn(\n        \"dq_status\", concat(lit(field), lit(\":\"), lit(check), lit(\":\"), lit(value))\n    )\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.is_complete","title":"<code>is_complete(df, rule)</code>","text":"<p>Filters a DataFrame to identify rows where a specified field is null and adds a \"dq_status\" column indicating the data quality rule applied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the data quality rule. It should include: - \"field\" (str): The name of the field to check for null values. - \"check\" (str): A description of the check being performed. - \"value\" (str): Additional information about the rule.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame filtered to include only rows where the specified</p> <code>DataFrame</code> <p>field is null, with an additional \"dq_status\" column describing the rule.</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def is_complete(df: DataFrame, rule: dict) -&gt; DataFrame:\n    \"\"\"\n    Filters a DataFrame to identify rows where a specified field is null and adds a\n    \"dq_status\" column indicating the data quality rule applied.\n\n    Args:\n        df (DataFrame): The input PySpark DataFrame to be checked.\n        rule (dict): A dictionary containing the data quality rule. It should include:\n            - \"field\" (str): The name of the field to check for null values.\n            - \"check\" (str): A description of the check being performed.\n            - \"value\" (str): Additional information about the rule.\n\n    Returns:\n        DataFrame: A new DataFrame filtered to include only rows where the specified\n        field is null, with an additional \"dq_status\" column describing the rule.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    return df.filter(col(field).isNull()).withColumn(\n        \"dq_status\", concat(lit(field), lit(\":\"), lit(check), lit(\":\"), lit(value))\n    )\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.is_composite_key","title":"<code>is_composite_key(df, rule)</code>","text":"<p>Determines if the given DataFrame satisfies the composite key condition based on the provided rule.</p> <p>A composite key is a combination of two or more columns in a DataFrame that uniquely identify a row.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The PySpark DataFrame to be evaluated.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rules or criteria to determine the composite key.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <p>True if the DataFrame satisfies the composite key condition, False otherwise.</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def is_composite_key(df: DataFrame, rule: dict):\n    \"\"\"\n    Determines if the given DataFrame satisfies the composite key condition based on the provided rule.\n\n    A composite key is a combination of two or more columns in a DataFrame that uniquely identify a row.\n\n    Args:\n        df (DataFrame): The PySpark DataFrame to be evaluated.\n        rule (dict): A dictionary containing the rules or criteria to determine the composite key.\n\n    Returns:\n        bool: True if the DataFrame satisfies the composite key condition, False otherwise.\n    \"\"\"\n    return are_unique(df, rule)\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.is_contained_in","title":"<code>is_contained_in(df, rule)</code>","text":"<p>Filters rows in a PySpark DataFrame based on whether a specified column's value is not contained in a given list of values. Adds a new column 'dq_status' to indicate the rule applied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The name of the column to check. - 'check': The type of check being performed (e.g., \"is_contained_in\"). - 'value': A string representation of a list of values (e.g., \"[value1,value2]\").</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new PySpark DataFrame with rows filtered based on the rule</p> <code>DataFrame</code> <p>and an additional column 'dq_status' describing the rule applied.</p> Example <p>rule = {\"field\": \"column_name\", \"check\": \"is_contained_in\", \"value\": \"[value1,value2]\"} result_df = is_contained_in(input_df, rule)</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def is_contained_in(df: DataFrame, rule: dict) -&gt; DataFrame:\n    \"\"\"\n    Filters rows in a PySpark DataFrame based on whether a specified column's value\n    is not contained in a given list of values. Adds a new column 'dq_status' to\n    indicate the rule applied.\n\n    Args:\n        df (DataFrame): The input PySpark DataFrame to filter.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field': The name of the column to check.\n            - 'check': The type of check being performed (e.g., \"is_contained_in\").\n            - 'value': A string representation of a list of values (e.g., \"[value1,value2]\").\n\n    Returns:\n        DataFrame: A new PySpark DataFrame with rows filtered based on the rule\n        and an additional column 'dq_status' describing the rule applied.\n\n    Example:\n        rule = {\"field\": \"column_name\", \"check\": \"is_contained_in\", \"value\": \"[value1,value2]\"}\n        result_df = is_contained_in(input_df, rule)\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    positive_list = value.strip(\"[]\").split(\",\")\n    return df.filter(~col(field).isin(positive_list)).withColumn(\n        \"dq_status\", concat(lit(field), lit(\":\"), lit(check), lit(\":\"), lit(value))\n    )\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.is_date_after","title":"<code>is_date_after(df, rule)</code>","text":"<p>Filters a DataFrame to identify rows where a specified field has a date lower than the date informed in the rule and adds a \"dq_status\" column indicating the data quality rule applied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the data quality rule. It should include: - \"field\" (str): The name of the field to check for null values. - \"check\" (str): A description of the check being performed. - \"value\" (str): Additional information about the rule.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame filtered to include only rows where the specified</p> <code>DataFrame</code> <p>field is null, with an additional \"dq_status\" column describing the rule.</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def is_date_after(df: DataFrame, rule: dict) -&gt; DataFrame:\n    \"\"\"\n    Filters a DataFrame to identify rows where a specified field has a date lower than the date informed in the rule\n    and adds a \"dq_status\" column indicating the data quality rule applied.\n\n    Args:\n        df (DataFrame): The input PySpark DataFrame to be checked.\n        rule (dict): A dictionary containing the data quality rule. It should include:\n            - \"field\" (str): The name of the field to check for null values.\n            - \"check\" (str): A description of the check being performed.\n            - \"value\" (str): Additional information about the rule.\n\n    Returns:\n        DataFrame: A new DataFrame filtered to include only rows where the specified\n        field is null, with an additional \"dq_status\" column describing the rule.\n    \"\"\"\n\n    field, check, value = __extract_params(rule)\n    return df.filter(col(field) &lt; value).withColumn(\n        \"dq_status\", concat(lit(field), lit(\":\"), lit(check), lit(\":\"), lit(value))\n    )\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.is_date_before","title":"<code>is_date_before(df, rule)</code>","text":"<p>Filters a DataFrame to identify rows where a specified field has a date greater than the date informed in the rule and adds a \"dq_status\" column indicating the data quality rule applied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the data quality rule. It should include: - \"field\" (str): The name of the field to check for null values. - \"check\" (str): A description of the check being performed. - \"value\" (str): Additional information about the rule.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame filtered to include only rows where the specified</p> <code>DataFrame</code> <p>field is null, with an additional \"dq_status\" column describing the rule.</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def is_date_before(df: DataFrame, rule: dict) -&gt; DataFrame:\n    \"\"\"\n    Filters a DataFrame to identify rows where a specified field has a date greater than the date informed in the rule\n    and adds a \"dq_status\" column indicating the data quality rule applied.\n\n    Args:\n        df (DataFrame): The input PySpark DataFrame to be checked.\n        rule (dict): A dictionary containing the data quality rule. It should include:\n            - \"field\" (str): The name of the field to check for null values.\n            - \"check\" (str): A description of the check being performed.\n            - \"value\" (str): Additional information about the rule.\n\n    Returns:\n        DataFrame: A new DataFrame filtered to include only rows where the specified\n        field is null, with an additional \"dq_status\" column describing the rule.\n    \"\"\"\n\n    field, check, value = __extract_params(rule)\n    return df.filter(col(field) &gt; value).withColumn(\n        \"dq_status\", concat(lit(field), lit(\":\"), lit(check), lit(\":\"), lit(value))\n    )\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.is_date_between","title":"<code>is_date_between(df, rule)</code>","text":"<p>Filters a DataFrame to identify rows where a specified field has a date between two dates passed in the rule using the format: \"[, ]\" and adds a \"dq_status\" column indicating the data quality rule applied. <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the data quality rule. It should include: - \"field\" (str): The name of the field to check for null values. - \"check\" (str): A description of the check being performed. - \"value\" (str): Additional information about the rule.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame filtered to include only rows where the specified</p> <code>DataFrame</code> <p>field is null, with an additional \"dq_status\" column describing the rule.</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def is_date_between(df: DataFrame, rule: dict) -&gt; DataFrame:\n    \"\"\"\n    Filters a DataFrame to identify rows where a specified field has a date between two dates passed in the rule using\n    the format: \"[&lt;initial_date&gt;, &lt;final_date&gt;]\" and adds a \"dq_status\" column indicating the data quality rule applied.\n\n    Args:\n        df (DataFrame): The input PySpark DataFrame to be checked.\n        rule (dict): A dictionary containing the data quality rule. It should include:\n            - \"field\" (str): The name of the field to check for null values.\n            - \"check\" (str): A description of the check being performed.\n            - \"value\" (str): Additional information about the rule.\n\n    Returns:\n        DataFrame: A new DataFrame filtered to include only rows where the specified\n        field is null, with an additional \"dq_status\" column describing the rule.\n    \"\"\"\n\n    field, check, value = __extract_params(rule)\n    start_date, end_date = value.strip(\"[]\").split(\",\")\n    return df.filter(~col(field).between(start_date, end_date)).withColumn(\n        \"dq_status\", concat(lit(field), lit(\":\"), lit(check), lit(\":\"), lit(value))\n    )\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.is_equal","title":"<code>is_equal(df, rule)</code>","text":"<p>Filters a PySpark DataFrame based on a rule that checks for equality between a specified field and a given value. Rows that do not satisfy the equality condition are retained, and a new column \"dq_status\" is added to indicate the rule applied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - \"field\" (str): The name of the column to check. - \"check\" (str): The type of check (e.g., \"equal\"). This is used for logging purposes. - \"value\" (Any): The value to compare against.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame with rows that do not satisfy the equality condition and an</p> <code>DataFrame</code> <p>additional \"dq_status\" column describing the rule applied.</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def is_equal(df: DataFrame, rule: dict) -&gt; DataFrame:\n    \"\"\"\n    Filters a PySpark DataFrame based on a rule that checks for equality between a specified field\n    and a given value. Rows that do not satisfy the equality condition are retained, and a new\n    column \"dq_status\" is added to indicate the rule applied.\n\n    Args:\n        df (DataFrame): The input PySpark DataFrame to be filtered.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - \"field\" (str): The name of the column to check.\n            - \"check\" (str): The type of check (e.g., \"equal\"). This is used for logging purposes.\n            - \"value\" (Any): The value to compare against.\n\n    Returns:\n        DataFrame: A new DataFrame with rows that do not satisfy the equality condition and an\n        additional \"dq_status\" column describing the rule applied.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    return df.filter(~col(field).eqNullSafe(value)).withColumn(\n        \"dq_status\", concat(lit(field), lit(\":\"), lit(check), lit(\":\"), lit(value))\n    )\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.is_equal_than","title":"<code>is_equal_than(df, rule)</code>","text":"<p>Filters rows in a PySpark DataFrame that do not satisfy an equality condition specified in the rule dictionary and adds a \"dq_status\" column with details about the rule applied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - \"field\" (str): The name of the column to check. - \"check\" (str): The type of check being performed (e.g., \"equal\"). - \"value\" (Any): The value to compare against.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame with rows filtered based on the rule and an</p> <code>DataFrame</code> <p>additional \"dq_status\" column describing the rule applied.</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def is_equal_than(df: DataFrame, rule: dict) -&gt; DataFrame:\n    \"\"\"\n    Filters rows in a PySpark DataFrame that do not satisfy an equality condition\n    specified in the rule dictionary and adds a \"dq_status\" column with details\n    about the rule applied.\n\n    Args:\n        df (DataFrame): The input PySpark DataFrame to be filtered.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - \"field\" (str): The name of the column to check.\n            - \"check\" (str): The type of check being performed (e.g., \"equal\").\n            - \"value\" (Any): The value to compare against.\n\n    Returns:\n        DataFrame: A new DataFrame with rows filtered based on the rule and an\n        additional \"dq_status\" column describing the rule applied.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    return df.filter(~col(field).eqNullSafe(value)).withColumn(\n        \"dq_status\", concat(lit(field), lit(\":\"), lit(check), lit(\":\"), lit(value))\n    )\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.is_future_date","title":"<code>is_future_date(df, rule)</code>","text":"<p>Filters a DataFrame to identify rows where a specified field has a date greater than the current date and adds a \"dq_status\" column indicating the data quality rule applied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the data quality rule. It should include: - \"field\" (str): The name of the field to check for null values. - \"check\" (str): A description of the check being performed. - \"value\" (str): Additional information about the rule.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame filtered to include only rows where the specified</p> <code>DataFrame</code> <p>field is null, with an additional \"dq_status\" column describing the rule.</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def is_future_date(df: DataFrame, rule: dict) -&gt; DataFrame:\n    \"\"\"\n    Filters a DataFrame to identify rows where a specified field has a date greater than the current date and\n    adds a \"dq_status\" column indicating the data quality rule applied.\n\n    Args:\n        df (DataFrame): The input PySpark DataFrame to be checked.\n        rule (dict): A dictionary containing the data quality rule. It should include:\n            - \"field\" (str): The name of the field to check for null values.\n            - \"check\" (str): A description of the check being performed.\n            - \"value\" (str): Additional information about the rule.\n\n    Returns:\n        DataFrame: A new DataFrame filtered to include only rows where the specified\n        field is null, with an additional \"dq_status\" column describing the rule.\n    \"\"\"\n\n    field, check, value = __extract_params(rule)\n    return df.filter(col(field) &gt; current_date()).withColumn(\n        \"dq_status\", concat(lit(field), lit(\":\"), lit(check), lit(\":\"), lit(value))\n    )\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.is_greater_or_equal_than","title":"<code>is_greater_or_equal_than(df, rule)</code>","text":"<p>Filters rows in a DataFrame where the value of a specified field is less than a given value and adds a new column \"dq_status\" with a formatted string indicating the rule applied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - \"field\" (str): The name of the column to check. - \"check\" (str): A descriptive string for the check (e.g., \"greater_or_equal\"). - \"value\" (numeric): The threshold value for the comparison.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame with rows filtered based on the rule and an additional</p> <code>DataFrame</code> <p>\"dq_status\" column describing the rule applied.</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def is_greater_or_equal_than(df: DataFrame, rule: dict) -&gt; DataFrame:\n    \"\"\"\n    Filters rows in a DataFrame where the value of a specified field is less than a given value\n    and adds a new column \"dq_status\" with a formatted string indicating the rule applied.\n\n    Args:\n        df (DataFrame): The input PySpark DataFrame to filter.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - \"field\" (str): The name of the column to check.\n            - \"check\" (str): A descriptive string for the check (e.g., \"greater_or_equal\").\n            - \"value\" (numeric): The threshold value for the comparison.\n\n    Returns:\n        DataFrame: A new DataFrame with rows filtered based on the rule and an additional\n        \"dq_status\" column describing the rule applied.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    return df.filter(col(field) &lt; value).withColumn(\n        \"dq_status\", concat(lit(field), lit(\":\"), lit(check), lit(\":\"), lit(value))\n    )\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.is_greater_than","title":"<code>is_greater_than(df, rule)</code>","text":"<p>Filters rows in a DataFrame where the value of a specified field is less than or equal to a given threshold and adds a new column indicating the rule applied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The name of the column to apply the rule on. - 'check' (str): A descriptive string for the rule (e.g., \"greater_than\"). - 'value' (int or float): The threshold value for the comparison.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame with rows filtered based on the rule and an</p> <code>DataFrame</code> <p>additional column \"dq_status\" describing the rule applied.</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def is_greater_than(df: DataFrame, rule: dict) -&gt; DataFrame:\n    \"\"\"\n    Filters rows in a DataFrame where the value of a specified field is less than\n    or equal to a given threshold and adds a new column indicating the rule applied.\n\n    Args:\n        df (DataFrame): The input PySpark DataFrame to filter.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field' (str): The name of the column to apply the rule on.\n            - 'check' (str): A descriptive string for the rule (e.g., \"greater_than\").\n            - 'value' (int or float): The threshold value for the comparison.\n\n    Returns:\n        DataFrame: A new DataFrame with rows filtered based on the rule and an\n        additional column \"dq_status\" describing the rule applied.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    return df.filter(col(field) &lt;= value).withColumn(\n        \"dq_status\", concat(lit(field), lit(\":\"), lit(check), lit(\":\"), lit(value))\n    )\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.is_in","title":"<code>is_in(df, rule)</code>","text":"<p>Checks if the values in the specified column of a DataFrame are contained within a given set of values.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to evaluate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule for the check. It should specify the column name          and the set of values to check against.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A DataFrame with the applied rule, typically filtered or modified based on the check.</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def is_in(df: DataFrame, rule: dict) -&gt; DataFrame:\n    \"\"\"\n    Checks if the values in the specified column of a DataFrame are contained within a given set of values.\n\n    Args:\n        df (DataFrame): The input DataFrame to evaluate.\n        rule (dict): A dictionary containing the rule for the check. It should specify the column name\n                     and the set of values to check against.\n\n    Returns:\n        DataFrame: A DataFrame with the applied rule, typically filtered or modified based on the check.\n    \"\"\"\n    return is_contained_in(df, rule)\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.is_in_billions","title":"<code>is_in_billions(df, rule)</code>","text":"<p>Filters a PySpark DataFrame to include only rows where the specified field's value is greater than or equal to one billion, and adds a \"dq_status\" column with a formatted string indicating the field, check, and value.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The name of the column to check. - 'check': The type of check being performed (e.g., \"greater_than\"). - 'value': The threshold value for the check.</p> required <p>Returns:</p> Type Description <p>pyspark.sql.DataFrame: A new DataFrame filtered by the rule and with an</p> <p>additional \"dq_status\" column.</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def is_in_billions(df, rule: dict):\n    \"\"\"\n    Filters a PySpark DataFrame to include only rows where the specified field's value\n    is greater than or equal to one billion, and adds a \"dq_status\" column with a\n    formatted string indicating the field, check, and value.\n\n    Args:\n        df (pyspark.sql.DataFrame): The input PySpark DataFrame to filter.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field': The name of the column to check.\n            - 'check': The type of check being performed (e.g., \"greater_than\").\n            - 'value': The threshold value for the check.\n\n    Returns:\n        pyspark.sql.DataFrame: A new DataFrame filtered by the rule and with an\n        additional \"dq_status\" column.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    return df.filter(col(field) &lt; lit(1_000_000_000)).withColumn(\n        \"dq_status\", concat(lit(field), lit(\":\"), lit(check), lit(\":\"), lit(value))\n    )\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.is_in_millions","title":"<code>is_in_millions(df, rule)</code>","text":"<p>Filters a DataFrame to include only rows where the specified field's value is greater than or equal to 1,000,000 and adds a \"dq_status\" column with a formatted string indicating the rule applied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to filter and modify.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should          include the field to check, the check type, and the value.</p> required <p>Returns:</p> Type Description <p>pyspark.sql.DataFrame: A new DataFrame with rows filtered based on the</p> <p>rule and an additional \"dq_status\" column describing the rule applied.</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def is_in_millions(df, rule: dict):\n    \"\"\"\n    Filters a DataFrame to include only rows where the specified field's value\n    is greater than or equal to 1,000,000 and adds a \"dq_status\" column with\n    a formatted string indicating the rule applied.\n\n    Args:\n        df (pyspark.sql.DataFrame): The input DataFrame to filter and modify.\n        rule (dict): A dictionary containing the rule parameters. It should\n                     include the field to check, the check type, and the value.\n\n    Returns:\n        pyspark.sql.DataFrame: A new DataFrame with rows filtered based on the\n        rule and an additional \"dq_status\" column describing the rule applied.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    return df.filter(col(field) &lt; lit(1_000_000)).withColumn(\n        \"dq_status\", concat(lit(field), lit(\":\"), lit(check), lit(\":\"), lit(value))\n    )\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.is_legit","title":"<code>is_legit(df, rule)</code>","text":"<p>Filters a PySpark DataFrame to identify rows that do not meet a specified rule and appends a column indicating the data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be validated.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The name of the column to validate. - 'check': The type of check being performed (e.g., \"is_legit\"). - 'value': The expected value or condition for the validation.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame containing only the rows that fail the validation</p> <code>DataFrame</code> <p>rule, with an additional column \"dq_status\" describing the validation status</p> <code>DataFrame</code> <p>in the format \"field:check:value\".</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def is_legit(df: DataFrame, rule: dict) -&gt; DataFrame:\n    \"\"\"\n    Filters a PySpark DataFrame to identify rows that do not meet a specified rule\n    and appends a column indicating the data quality status.\n\n    Args:\n        df (DataFrame): The input PySpark DataFrame to be validated.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field': The name of the column to validate.\n            - 'check': The type of check being performed (e.g., \"is_legit\").\n            - 'value': The expected value or condition for the validation.\n\n    Returns:\n        DataFrame: A new DataFrame containing only the rows that fail the validation\n        rule, with an additional column \"dq_status\" describing the validation status\n        in the format \"field:check:value\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    pattern_legit = \"\\S*\"\n    return df.filter(\n        ~(col(field).isNotNull() &amp; col(field).rlike(pattern_legit))\n    ).withColumn(\n        \"dq_status\", concat(lit(field), lit(\":\"), lit(check), lit(\":\"), lit(value))\n    )\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.is_less_or_equal_than","title":"<code>is_less_or_equal_than(df, rule)</code>","text":"<p>Filters rows in a PySpark DataFrame where the value of a specified field is greater than a given value and adds a new column \"dq_status\" with a formatted string indicating the rule applied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - \"field\" (str): The name of the column to evaluate. - \"check\" (str): A descriptive string for the check being performed. - \"value\" (numeric): The threshold value to compare against.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new PySpark DataFrame with rows filtered based on the rule and an additional</p> <code>DataFrame</code> <p>\"dq_status\" column describing the rule applied.</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def is_less_or_equal_than(df: DataFrame, rule: dict) -&gt; DataFrame:\n    \"\"\"\n    Filters rows in a PySpark DataFrame where the value of a specified field is greater than a given value\n    and adds a new column \"dq_status\" with a formatted string indicating the rule applied.\n\n    Args:\n        df (DataFrame): The input PySpark DataFrame to filter.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - \"field\" (str): The name of the column to evaluate.\n            - \"check\" (str): A descriptive string for the check being performed.\n            - \"value\" (numeric): The threshold value to compare against.\n\n    Returns:\n        DataFrame: A new PySpark DataFrame with rows filtered based on the rule and an additional\n        \"dq_status\" column describing the rule applied.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    return df.filter(col(field) &gt; value).withColumn(\n        \"dq_status\", concat(lit(field), lit(\":\"), lit(check), lit(\":\"), lit(value))\n    )\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.is_less_than","title":"<code>is_less_than(df, rule)</code>","text":"<p>Filters rows in a PySpark DataFrame where the specified field is greater than or equal to a given value and adds a new column indicating the rule applied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The name of the column to apply the filter on. - 'check' (str): A descriptive string for the rule (e.g., \"less_than\"). - 'value' (int, float, or str): The value to compare the column against.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame with rows filtered based on the rule and an</p> <code>DataFrame</code> <p>additional column \"dq_status\" describing the rule applied.</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def is_less_than(df: DataFrame, rule: dict) -&gt; DataFrame:\n    \"\"\"\n    Filters rows in a PySpark DataFrame where the specified field is greater than\n    or equal to a given value and adds a new column indicating the rule applied.\n\n    Args:\n        df (DataFrame): The input PySpark DataFrame to filter.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field' (str): The name of the column to apply the filter on.\n            - 'check' (str): A descriptive string for the rule (e.g., \"less_than\").\n            - 'value' (int, float, or str): The value to compare the column against.\n\n    Returns:\n        DataFrame: A new DataFrame with rows filtered based on the rule and an\n        additional column \"dq_status\" describing the rule applied.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    return df.filter(col(field) &gt;= value).withColumn(\n        \"dq_status\", concat(lit(field), lit(\":\"), lit(check), lit(\":\"), lit(value))\n    )\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.is_negative","title":"<code>is_negative(df, rule)</code>","text":"<p>Filters rows in the given DataFrame where the specified field is non-negative and adds a new column \"dq_status\" containing a formatted string with rule details.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be filtered and modified.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The name of the column to check. - 'check' (str): A descriptive string for the check being performed. - 'value' (any): The value associated with the rule.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame with rows filtered based on the rule and an</p> <code>DataFrame</code> <p>additional \"dq_status\" column describing the rule applied.</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def is_negative(df: DataFrame, rule: dict) -&gt; DataFrame:\n    \"\"\"\n    Filters rows in the given DataFrame where the specified field is non-negative\n    and adds a new column \"dq_status\" containing a formatted string with rule details.\n\n    Args:\n        df (DataFrame): The input PySpark DataFrame to be filtered and modified.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field' (str): The name of the column to check.\n            - 'check' (str): A descriptive string for the check being performed.\n            - 'value' (any): The value associated with the rule.\n\n    Returns:\n        DataFrame: A new DataFrame with rows filtered based on the rule and an\n        additional \"dq_status\" column describing the rule applied.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    return df.filter(col(field) &gt;= 0).withColumn(\n        \"dq_status\", concat(lit(field), lit(\":\"), lit(check), lit(\":\"), lit(value))\n    )\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.is_on_friday","title":"<code>is_on_friday(df, rule)</code>","text":"<p>Filters a PySpark DataFrame to include only rows where the specified date field falls on a Friday.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to have the following keys: - 'field': The name of the column in the DataFrame to check. - 'check': A string representing the type of check (not used in this function but included for consistency). - 'value': A value associated with the rule (not used in this function but included for consistency).</p> required <p>Returns:</p> Type Description <p>pyspark.sql.DataFrame: A new DataFrame filtered to include only rows where the specified date field</p> <p>corresponds to a Friday. Additionally, a new column <code>dq_status</code> is added, which contains a string</p> <p>representation of the rule applied in the format \"field:check:value\".</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def is_on_friday(df, rule: dict):\n    \"\"\"\n    Filters a PySpark DataFrame to include only rows where the specified date field falls on a Friday.\n\n    Args:\n        df (pyspark.sql.DataFrame): The input DataFrame to filter.\n        rule (dict): A dictionary containing the rule parameters. It is expected to have the following keys:\n            - 'field': The name of the column in the DataFrame to check.\n            - 'check': A string representing the type of check (not used in this function but included for consistency).\n            - 'value': A value associated with the rule (not used in this function but included for consistency).\n\n    Returns:\n        pyspark.sql.DataFrame: A new DataFrame filtered to include only rows where the specified date field\n        corresponds to a Friday. Additionally, a new column `dq_status` is added, which contains a string\n        representation of the rule applied in the format \"field:check:value\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    return df.filter(dayofweek(col(field)) != 6).withColumn(\n        \"dq_status\", concat(lit(field), lit(\":\"), lit(check), lit(\":\"), lit(value))\n    )\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.is_on_monday","title":"<code>is_on_monday(df, rule)</code>","text":"<p>Filters a PySpark DataFrame to include only rows where the specified date field falls on a Monday.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing rule parameters. It is expected to include: - 'field': The name of the column to check. - 'check': A string representing the type of check (not used in this function). - 'value': A value associated with the rule (not used in this function).</p> required <p>Returns:</p> Type Description <p>pyspark.sql.DataFrame: A new DataFrame filtered to include only rows where the specified</p> <p>date field corresponds to a Monday. Additionally, a new column \"dq_status\" is added,</p> <p>containing a concatenated string of the field, check, and value.</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def is_on_monday(df, rule: dict):\n    \"\"\"\n    Filters a PySpark DataFrame to include only rows where the specified date field falls on a Monday.\n\n    Args:\n        df (pyspark.sql.DataFrame): The input PySpark DataFrame.\n        rule (dict): A dictionary containing rule parameters. It is expected to include:\n            - 'field': The name of the column to check.\n            - 'check': A string representing the type of check (not used in this function).\n            - 'value': A value associated with the rule (not used in this function).\n\n    Returns:\n        pyspark.sql.DataFrame: A new DataFrame filtered to include only rows where the specified\n        date field corresponds to a Monday. Additionally, a new column \"dq_status\" is added,\n        containing a concatenated string of the field, check, and value.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    return df.filter(dayofweek(col(field)) != 2).withColumn(\n        \"dq_status\", concat(lit(field), lit(\":\"), lit(check), lit(\":\"), lit(value))\n    )\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.is_on_saturday","title":"<code>is_on_saturday(df, rule)</code>","text":"<p>Filters a PySpark DataFrame to include only rows where the specified date field falls on a Saturday.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing rule parameters. The function expects the rule to include: - 'field': The name of the column to check. - 'check': A string representing the check being performed (not used in logic, but included in the output column). - 'value': A value to include in the output column (not used in logic, but included in the output column).</p> required <p>Returns:</p> Type Description <p>pyspark.sql.DataFrame: A new DataFrame filtered to include only rows where the specified field falls on a Saturday.</p> <p>Additionally, a new column \"dq_status\" is added, containing a string in the format \"field:check:value\".</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def is_on_saturday(df, rule: dict):\n    \"\"\"\n    Filters a PySpark DataFrame to include only rows where the specified date field falls on a Saturday.\n\n    Args:\n        df (pyspark.sql.DataFrame): The input DataFrame to filter.\n        rule (dict): A dictionary containing rule parameters. The function expects the rule to include:\n            - 'field': The name of the column to check.\n            - 'check': A string representing the check being performed (not used in logic, but included in the output column).\n            - 'value': A value to include in the output column (not used in logic, but included in the output column).\n\n    Returns:\n        pyspark.sql.DataFrame: A new DataFrame filtered to include only rows where the specified field falls on a Saturday.\n        Additionally, a new column \"dq_status\" is added, containing a string in the format \"field:check:value\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    return df.filter(dayofweek(col(field)) != 7).withColumn(\n        \"dq_status\", concat(lit(field), lit(\":\"), lit(check), lit(\":\"), lit(value))\n    )\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.is_on_sunday","title":"<code>is_on_sunday(df, rule)</code>","text":"<p>Filters a PySpark DataFrame to include only rows where the specified date field falls on a Sunday.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to include: - field (str): The name of the column to check. - check (str): A descriptive string for the check being performed. - value (str): A value to include in the \"dq_status\" column for context.</p> required <p>Returns:</p> Type Description <p>pyspark.sql.DataFrame: A new DataFrame filtered to include only rows where the specified</p> <p>date field corresponds to a Sunday. Additionally, a \"dq_status\" column is added to the</p> <p>DataFrame, containing a string in the format \"field:check:value\".</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def is_on_sunday(df, rule: dict):\n    \"\"\"\n    Filters a PySpark DataFrame to include only rows where the specified date field falls on a Sunday.\n\n    Args:\n        df (pyspark.sql.DataFrame): The input DataFrame to filter.\n        rule (dict): A dictionary containing the rule parameters. It is expected to include:\n            - field (str): The name of the column to check.\n            - check (str): A descriptive string for the check being performed.\n            - value (str): A value to include in the \"dq_status\" column for context.\n\n    Returns:\n        pyspark.sql.DataFrame: A new DataFrame filtered to include only rows where the specified\n        date field corresponds to a Sunday. Additionally, a \"dq_status\" column is added to the\n        DataFrame, containing a string in the format \"field:check:value\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    return df.filter(dayofweek(col(field)) != 1).withColumn(\n        \"dq_status\", concat(lit(field), lit(\":\"), lit(check), lit(\":\"), lit(value))\n    )\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.is_on_thursday","title":"<code>is_on_thursday(df, rule)</code>","text":"<p>Filters a PySpark DataFrame to include only rows where the specified date column falls on a Thursday.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The PySpark DataFrame to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to include: - 'field': The name of the column to check. - 'check': A string representing the type of check (not used in the filtering logic). - 'value': A value associated with the rule (not used in the filtering logic).</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <p>A new PySpark DataFrame filtered to include only rows where the specified column's day of the week is Thursday.        Additionally, a new column \"dq_status\" is added, containing a concatenated string of the field, check, and value.</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def is_on_thursday(df, rule: dict):\n    \"\"\"\n    Filters a PySpark DataFrame to include only rows where the specified date column falls on a Thursday.\n\n    Args:\n        df (DataFrame): The PySpark DataFrame to filter.\n        rule (dict): A dictionary containing the rule parameters. It is expected to include:\n            - 'field': The name of the column to check.\n            - 'check': A string representing the type of check (not used in the filtering logic).\n            - 'value': A value associated with the rule (not used in the filtering logic).\n\n    Returns:\n        DataFrame: A new PySpark DataFrame filtered to include only rows where the specified column's day of the week is Thursday.\n                   Additionally, a new column \"dq_status\" is added, containing a concatenated string of the field, check, and value.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    return df.filter(dayofweek(col(field)) != 5).withColumn(\n        \"dq_status\", concat(lit(field), lit(\":\"), lit(check), lit(\":\"), lit(value))\n    )\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.is_on_tuesday","title":"<code>is_on_tuesday(df, rule)</code>","text":"<p>Filters a PySpark DataFrame to include only rows where the day of the week for a specified date column is Tuesday. Adds a new column 'dq_status' to indicate the validation status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to include: - 'field': The name of the column to check. - 'check': A string describing the check being performed. - 'value': A value associated with the check.</p> required <p>Returns:</p> Type Description <p>pyspark.sql.DataFrame: A new DataFrame filtered to include only rows</p> <p>where the specified column corresponds to Tuesday, with an additional</p> <p>'dq_status' column describing the validation status.</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def is_on_tuesday(df, rule: dict):\n    \"\"\"\n    Filters a PySpark DataFrame to include only rows where the day of the week\n    for a specified date column is Tuesday. Adds a new column 'dq_status' to\n    indicate the validation status.\n\n    Args:\n        df (pyspark.sql.DataFrame): The input PySpark DataFrame.\n        rule (dict): A dictionary containing the rule parameters. It is expected\n            to include:\n            - 'field': The name of the column to check.\n            - 'check': A string describing the check being performed.\n            - 'value': A value associated with the check.\n\n    Returns:\n        pyspark.sql.DataFrame: A new DataFrame filtered to include only rows\n        where the specified column corresponds to Tuesday, with an additional\n        'dq_status' column describing the validation status.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    return df.filter(dayofweek(col(field)) != 3).withColumn(\n        \"dq_status\", concat(lit(field), lit(\":\"), lit(check), lit(\":\"), lit(value))\n    )\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.is_on_wednesday","title":"<code>is_on_wednesday(df, rule)</code>","text":"<p>Filters a PySpark DataFrame to include only rows where the specified date field falls on a Wednesday.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to have the following keys: - 'field': The name of the column in the DataFrame to check. - 'check': A string representing the type of check (not used in the logic but included for status reporting). - 'value': A value associated with the rule (not used in the logic but included for status reporting).</p> required <p>Returns:</p> Type Description <p>pyspark.sql.DataFrame: A new DataFrame filtered to include only rows where the specified field corresponds to a Wednesday.</p> <p>Additionally, a new column 'dq_status' is added, which contains a string in the format \"field:check:value\".</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def is_on_wednesday(df, rule: dict):\n    \"\"\"\n    Filters a PySpark DataFrame to include only rows where the specified date field falls on a Wednesday.\n\n    Args:\n        df (pyspark.sql.DataFrame): The input PySpark DataFrame.\n        rule (dict): A dictionary containing the rule parameters. It is expected to have the following keys:\n            - 'field': The name of the column in the DataFrame to check.\n            - 'check': A string representing the type of check (not used in the logic but included for status reporting).\n            - 'value': A value associated with the rule (not used in the logic but included for status reporting).\n\n    Returns:\n        pyspark.sql.DataFrame: A new DataFrame filtered to include only rows where the specified field corresponds to a Wednesday.\n        Additionally, a new column 'dq_status' is added, which contains a string in the format \"field:check:value\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    return df.filter(dayofweek(col(field)) != 4).withColumn(\n        \"dq_status\", concat(lit(field), lit(\":\"), lit(check), lit(\":\"), lit(value))\n    )\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.is_on_weekday","title":"<code>is_on_weekday(df, rule)</code>","text":"<p>Filters a PySpark DataFrame to include only rows where the specified date field falls on a weekday (Monday to Friday). Adds a new column 'dq_status' to indicate the rule applied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to include the following keys: - 'field': The name of the column to check. - 'check': A string representing the type of check (used for logging). - 'value': A value associated with the rule (used for logging).</p> required <p>Returns:</p> Type Description <p>pyspark.sql.DataFrame: A new DataFrame filtered to include only rows where</p> <p>the specified date field is a weekday, with an additional 'dq_status' column</p> <p>describing the rule applied.</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def is_on_weekday(df, rule: dict):\n    \"\"\"\n    Filters a PySpark DataFrame to include only rows where the specified date field\n    falls on a weekday (Monday to Friday). Adds a new column 'dq_status' to indicate\n    the rule applied.\n\n    Args:\n        df (pyspark.sql.DataFrame): The input PySpark DataFrame.\n        rule (dict): A dictionary containing the rule parameters. It is expected to\n            include the following keys:\n            - 'field': The name of the column to check.\n            - 'check': A string representing the type of check (used for logging).\n            - 'value': A value associated with the rule (used for logging).\n\n    Returns:\n        pyspark.sql.DataFrame: A new DataFrame filtered to include only rows where\n        the specified date field is a weekday, with an additional 'dq_status' column\n        describing the rule applied.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    return df.filter(\n        (dayofweek(col(field)) == 1) | (dayofweek(col(field)) == 7)\n    ).withColumn(\n        \"dq_status\", concat(lit(field), lit(\":\"), lit(check), lit(\":\"), lit(value))\n    )\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.is_on_weekend","title":"<code>is_on_weekend(df, rule)</code>","text":"<p>Filters a PySpark DataFrame to include only rows where the specified date field falls on a weekend (Saturday or Sunday). Additionally, adds a new column 'dq_status' to indicate the rule applied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected          to have the following keys:          - 'field': The name of the date column to check.          - 'check': A string representing the type of check (not used in logic).          - 'value': A string representing the value to include in the 'dq_status' column.</p> required <p>Returns:</p> Type Description <p>pyspark.sql.DataFrame: A new DataFrame filtered to include only rows where</p> <p>the specified date field is on a weekend, with an additional 'dq_status' column.</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def is_on_weekend(df, rule: dict):\n    \"\"\"\n    Filters a PySpark DataFrame to include only rows where the specified date field\n    falls on a weekend (Saturday or Sunday). Additionally, adds a new column\n    'dq_status' to indicate the rule applied.\n\n    Args:\n        df (pyspark.sql.DataFrame): The input PySpark DataFrame.\n        rule (dict): A dictionary containing the rule parameters. It is expected\n                     to have the following keys:\n                     - 'field': The name of the date column to check.\n                     - 'check': A string representing the type of check (not used in logic).\n                     - 'value': A string representing the value to include in the 'dq_status' column.\n\n    Returns:\n        pyspark.sql.DataFrame: A new DataFrame filtered to include only rows where\n        the specified date field is on a weekend, with an additional 'dq_status' column.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    return df.filter(\n        (dayofweek(col(field)) != 1) | (dayofweek(col(field)) != 7)\n    ).withColumn(\n        \"dq_status\", concat(lit(field), lit(\":\"), lit(check), lit(\":\"), lit(value))\n    )\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.is_past_date","title":"<code>is_past_date(df, rule)</code>","text":"<p>Filters a DataFrame to identify rows where a specified field has a date lower than the current date and adds a \"dq_status\" column indicating the data quality rule applied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the data quality rule. It should include: - \"field\" (str): The name of the field to check for null values. - \"check\" (str): A description of the check being performed. - \"value\" (str): Additional information about the rule.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame filtered to include only rows where the specified</p> <code>DataFrame</code> <p>field is null, with an additional \"dq_status\" column describing the rule.</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def is_past_date(df: DataFrame, rule: dict) -&gt; DataFrame:\n    \"\"\"\n    Filters a DataFrame to identify rows where a specified field has a date lower than the current date and\n    adds a \"dq_status\" column indicating the data quality rule applied.\n\n    Args:\n        df (DataFrame): The input PySpark DataFrame to be checked.\n        rule (dict): A dictionary containing the data quality rule. It should include:\n            - \"field\" (str): The name of the field to check for null values.\n            - \"check\" (str): A description of the check being performed.\n            - \"value\" (str): Additional information about the rule.\n\n    Returns:\n        DataFrame: A new DataFrame filtered to include only rows where the specified\n        field is null, with an additional \"dq_status\" column describing the rule.\n    \"\"\"\n\n    field, check, value = __extract_params(rule)\n    return df.filter(col(field) &lt; current_date()).withColumn(\n        \"dq_status\", concat(lit(field), lit(\":\"), lit(check), lit(\":\"), lit(value))\n    )\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.is_positive","title":"<code>is_positive(df, rule)</code>","text":"<p>Filters a DataFrame to identify rows where the specified field does not satisfy a positive check and adds a \"dq_status\" column with details of the rule applied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - \"field\" (str): The name of the column to check. - \"check\" (str): The type of check being performed (e.g., \"positive\"). - \"value\" (any): The value associated with the rule (not directly used in this function).</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame filtered to include only rows where the specified field is less than 0,</p> <code>DataFrame</code> <p>with an additional \"dq_status\" column describing the rule applied.</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def is_positive(df: DataFrame, rule: dict) -&gt; DataFrame:\n    \"\"\"\n    Filters a DataFrame to identify rows where the specified field does not satisfy a positive check\n    and adds a \"dq_status\" column with details of the rule applied.\n\n    Args:\n        df (DataFrame): The input PySpark DataFrame to be filtered.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - \"field\" (str): The name of the column to check.\n            - \"check\" (str): The type of check being performed (e.g., \"positive\").\n            - \"value\" (any): The value associated with the rule (not directly used in this function).\n\n    Returns:\n        DataFrame: A new DataFrame filtered to include only rows where the specified field is less than 0,\n        with an additional \"dq_status\" column describing the rule applied.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    return df.filter(col(field) &lt; 0).withColumn(\n        \"dq_status\", concat(lit(field), lit(\":\"), lit(check), lit(\":\"), lit(value))\n    )\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.is_primary_key","title":"<code>is_primary_key(df, rule)</code>","text":"<p>Determines if a given DataFrame column or set of columns satisfies the primary key constraint.</p> <p>A primary key constraint requires that the specified column(s) in the DataFrame have unique values.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The PySpark DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rules or specifications for identifying the primary key.          Typically, this includes the column(s) to be checked for uniqueness.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <p>True if the specified column(s) in the DataFrame satisfy the primary key constraint, False otherwise.</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def is_primary_key(df: DataFrame, rule: dict):\n    \"\"\"\n    Determines if a given DataFrame column or set of columns satisfies the primary key constraint.\n\n    A primary key constraint requires that the specified column(s) in the DataFrame have unique values.\n\n    Args:\n        df (DataFrame): The PySpark DataFrame to be checked.\n        rule (dict): A dictionary containing the rules or specifications for identifying the primary key.\n                     Typically, this includes the column(s) to be checked for uniqueness.\n\n    Returns:\n        bool: True if the specified column(s) in the DataFrame satisfy the primary key constraint, False otherwise.\n    \"\"\"\n    return is_unique(df, rule)\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.is_t_minus_1","title":"<code>is_t_minus_1(df, rule)</code>","text":"<p>Filters the input DataFrame to include only rows where the specified field matches the date corresponding to \"T-1\" (yesterday). Adds a new column \"dq_status\" to indicate the rule applied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The name of the column to be checked. - 'check': The type of check being performed (not used in filtering but included in \"dq_status\"). - 'value': The value associated with the check (not used in filtering but included in \"dq_status\").</p> required <p>Returns:</p> Type Description <p>pyspark.sql.DataFrame: A new DataFrame filtered by the rule and with an additional \"dq_status\" column.</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def is_t_minus_1(df, rule: dict):\n    \"\"\"\n    Filters the input DataFrame to include only rows where the specified field matches the date\n    corresponding to \"T-1\" (yesterday). Adds a new column \"dq_status\" to indicate the rule applied.\n\n    Args:\n        df (pyspark.sql.DataFrame): The input DataFrame to be filtered.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field': The name of the column to be checked.\n            - 'check': The type of check being performed (not used in filtering but included in \"dq_status\").\n            - 'value': The value associated with the check (not used in filtering but included in \"dq_status\").\n\n    Returns:\n        pyspark.sql.DataFrame: A new DataFrame filtered by the rule and with an additional \"dq_status\" column.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    target = date_sub(current_date(), 1)\n    return df.filter(col(field) != target).withColumn(\n        \"dq_status\", concat(lit(field), lit(\":\"), lit(check), lit(\":\"), lit(value))\n    )\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.is_t_minus_2","title":"<code>is_t_minus_2(df, rule)</code>","text":"<p>Filters the input DataFrame to include only rows where the specified field matches the date that is two days prior to the current date. Adds a new column 'dq_status' to indicate the data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The name of the column to be checked. - 'check': A string representing the type of check (not used in filtering). - 'value': A value associated with the check (not used in filtering).</p> required <p>Returns:</p> Type Description <p>pyspark.sql.DataFrame: A new DataFrame filtered by the rule and with an additional</p> <p>'dq_status' column indicating the field, check, and value.</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def is_t_minus_2(df, rule: dict):\n    \"\"\"\n    Filters the input DataFrame to include only rows where the specified field matches the date\n    that is two days prior to the current date. Adds a new column 'dq_status' to indicate the\n    data quality status.\n\n    Args:\n        df (pyspark.sql.DataFrame): The input DataFrame to be filtered.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field': The name of the column to be checked.\n            - 'check': A string representing the type of check (not used in filtering).\n            - 'value': A value associated with the check (not used in filtering).\n\n    Returns:\n        pyspark.sql.DataFrame: A new DataFrame filtered by the rule and with an additional\n        'dq_status' column indicating the field, check, and value.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    target = date_sub(current_date(), 2)\n    return df.filter(col(field) != target).withColumn(\n        \"dq_status\", concat(lit(field), lit(\":\"), lit(check), lit(\":\"), lit(value))\n    )\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.is_t_minus_3","title":"<code>is_t_minus_3(df, rule)</code>","text":"<p>Filters the input DataFrame to include only rows where the specified field matches the date that is three days prior to the current date. Adds a new column 'dq_status' to indicate the data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The name of the column to be checked. - 'check': A string representing the type of check (not used in filtering). - 'value': A value associated with the rule (not used in filtering).</p> required <p>Returns:</p> Type Description <p>pyspark.sql.DataFrame: A new DataFrame filtered by the rule and with an</p> <p>additional 'dq_status' column.</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def is_t_minus_3(df, rule: dict):\n    \"\"\"\n    Filters the input DataFrame to include only rows where the specified field matches\n    the date that is three days prior to the current date. Adds a new column 'dq_status'\n    to indicate the data quality status.\n\n    Args:\n        df (pyspark.sql.DataFrame): The input DataFrame to be filtered.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field': The name of the column to be checked.\n            - 'check': A string representing the type of check (not used in filtering).\n            - 'value': A value associated with the rule (not used in filtering).\n\n    Returns:\n        pyspark.sql.DataFrame: A new DataFrame filtered by the rule and with an\n        additional 'dq_status' column.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    target = date_sub(current_date(), 3)\n    return df.filter(col(field) != target).withColumn(\n        \"dq_status\", concat(lit(field), lit(\":\"), lit(check), lit(\":\"), lit(value))\n    )\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.is_today","title":"<code>is_today(df, rule)</code>","text":"<p>Filters a DataFrame to include only rows where the specified field matches the current date.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to have          the following keys:          - 'field': The name of the column to check.          - 'check': A string representing the type of check (not used in this function).          - 'value': A value associated with the rule (not used in this function).</p> required <p>Returns:</p> Type Description <p>pyspark.sql.DataFrame: A new DataFrame filtered by the current date and with an additional                    column \"dq_status\" indicating the rule applied in the format                    \"field:check:value\".</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def is_today(df, rule: dict):\n    \"\"\"\n    Filters a DataFrame to include only rows where the specified field matches the current date.\n\n    Args:\n        df (pyspark.sql.DataFrame): The input DataFrame to filter.\n        rule (dict): A dictionary containing the rule parameters. It is expected to have\n                     the following keys:\n                     - 'field': The name of the column to check.\n                     - 'check': A string representing the type of check (not used in this function).\n                     - 'value': A value associated with the rule (not used in this function).\n\n    Returns:\n        pyspark.sql.DataFrame: A new DataFrame filtered by the current date and with an additional\n                               column \"dq_status\" indicating the rule applied in the format\n                               \"field:check:value\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    today = current_date()\n    return df.filter(col(field) != today).withColumn(\n        \"dq_status\", concat(lit(field), lit(\":\"), lit(check), lit(\":\"), lit(value))\n    )\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.is_unique","title":"<code>is_unique(df, rule)</code>","text":"<p>Checks for uniqueness of a specified field in a PySpark DataFrame based on the given rule.</p> <p>This function identifies rows where the specified field is not unique within the DataFrame. It adds a new column <code>dq_status</code> to the resulting DataFrame, which contains information about the field, the check type, and the value from the rule.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to check for uniqueness.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - <code>field</code> (str): The name of the field to check for uniqueness. - <code>check</code> (str): The type of check being performed (e.g., \"unique\"). - <code>value</code> (str): Additional value or metadata related to the check.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame containing rows where the specified field is not unique.</p> <code>DataFrame</code> <p>The resulting DataFrame includes a <code>dq_status</code> column with details about the rule violation.</p> Example <p>rule = {\"field\": \"column_name\", \"check\": \"unique\", \"value\": \"some_value\"} result_df = is_unique(input_df, rule)</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def is_unique(df: DataFrame, rule: dict) -&gt; DataFrame:\n    \"\"\"\n    Checks for uniqueness of a specified field in a PySpark DataFrame based on the given rule.\n\n    This function identifies rows where the specified field is not unique within the DataFrame.\n    It adds a new column `dq_status` to the resulting DataFrame, which contains information\n    about the field, the check type, and the value from the rule.\n\n    Args:\n        df (DataFrame): The input PySpark DataFrame to check for uniqueness.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - `field` (str): The name of the field to check for uniqueness.\n            - `check` (str): The type of check being performed (e.g., \"unique\").\n            - `value` (str): Additional value or metadata related to the check.\n\n    Returns:\n        DataFrame: A new DataFrame containing rows where the specified field is not unique.\n        The resulting DataFrame includes a `dq_status` column with details about the rule violation.\n\n    Example:\n        rule = {\"field\": \"column_name\", \"check\": \"unique\", \"value\": \"some_value\"}\n        result_df = is_unique(input_df, rule)\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    window = Window.partitionBy(col(field))\n    df_with_count = df.withColumn(\"count\", count(col(field)).over(window))\n    res = (\n        df_with_count.filter(col(\"count\") &gt; 1)\n        .withColumn(\n            \"dq_status\", concat(lit(field), lit(\":\"), lit(check), lit(\":\"), lit(value))\n        )\n        .drop(\"count\")\n    )\n    return res\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.is_yesterday","title":"<code>is_yesterday(df, rule)</code>","text":"<p>Filters a PySpark DataFrame to include only rows where the specified field matches yesterday's date. Adds a new column 'dq_status' to indicate the data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The name of the column to check. - 'check': The type of check being performed (used for status message). - 'value': Additional value information (used for status message).</p> required <p>Returns:</p> Type Description <p>pyspark.sql.DataFrame: A filtered DataFrame with an additional 'dq_status' column.</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def is_yesterday(df, rule: dict):\n    \"\"\"\n    Filters a PySpark DataFrame to include only rows where the specified field matches yesterday's date.\n    Adds a new column 'dq_status' to indicate the data quality status.\n\n    Args:\n        df (pyspark.sql.DataFrame): The input PySpark DataFrame.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field': The name of the column to check.\n            - 'check': The type of check being performed (used for status message).\n            - 'value': Additional value information (used for status message).\n\n    Returns:\n        pyspark.sql.DataFrame: A filtered DataFrame with an additional 'dq_status' column.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    yesterday = date_sub(current_date(), 1)\n    return df.filter(col(field) != yesterday).withColumn(\n        \"dq_status\", concat(lit(field), lit(\":\"), lit(check), lit(\":\"), lit(value))\n    )\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.not_contained_in","title":"<code>not_contained_in(df, rule)</code>","text":"<p>Filters rows in a DataFrame where the specified field's value is in a given list and adds a column indicating the data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The name of the column to check. - 'check': A string representing the type of check (e.g., \"not_contained_in\"). - 'value': A string representation of a list (e.g., \"[value1,value2,...]\")   containing the values to check against.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame with rows filtered based on the rule and an</p> <code>DataFrame</code> <p>additional column \"dq_status\" indicating the data quality status in the</p> <code>DataFrame</code> <p>format \"field:check:value\".</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def not_contained_in(df: DataFrame, rule: dict) -&gt; DataFrame:\n    \"\"\"\n    Filters rows in a DataFrame where the specified field's value is in a given list\n    and adds a column indicating the data quality status.\n\n    Args:\n        df (DataFrame): The input PySpark DataFrame to filter.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field': The name of the column to check.\n            - 'check': A string representing the type of check (e.g., \"not_contained_in\").\n            - 'value': A string representation of a list (e.g., \"[value1,value2,...]\")\n              containing the values to check against.\n\n    Returns:\n        DataFrame: A new DataFrame with rows filtered based on the rule and an\n        additional column \"dq_status\" indicating the data quality status in the\n        format \"field:check:value\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    negative_list = value.strip(\"[]\").split(\",\")\n    return df.filter(col(field).isin(negative_list)).withColumn(\n        \"dq_status\", concat(lit(field), lit(\":\"), lit(check), lit(\":\"), lit(value))\n    )\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.not_in","title":"<code>not_in(df, rule)</code>","text":"<p>Filters rows in a DataFrame where the specified rule is not contained.</p> <p>This function delegates the operation to the <code>not_contained_in</code> function.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary specifying the rule to apply for filtering.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame with rows that do not match the specified rule.</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def not_in(df: DataFrame, rule: dict) -&gt; DataFrame:\n    \"\"\"\n    Filters rows in a DataFrame where the specified rule is not contained.\n\n    This function delegates the operation to the `not_contained_in` function.\n\n    Args:\n        df (DataFrame): The input DataFrame to be filtered.\n        rule (dict): A dictionary specifying the rule to apply for filtering.\n\n    Returns:\n        DataFrame: A new DataFrame with rows that do not match the specified rule.\n    \"\"\"\n    return not_contained_in(df, rule)\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.satisfies","title":"<code>satisfies(df, rule)</code>","text":"<p>Filters a PySpark DataFrame based on a rule and adds a data quality status column.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the filtering rule. It should include: - 'field': The name of the column to apply the filter on. - 'check': The type of check to perform (currently unused in this implementation). - 'value': The expression in the pattern of pyspark.sql.functions.expr.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame filtered based on the rule, with an additional column</p> <code>DataFrame</code> <p>\"dq_status\" that describes the rule applied in the format \"field:check:value\".</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def satisfies(df: DataFrame, rule: dict) -&gt; DataFrame:\n    \"\"\"\n    Filters a PySpark DataFrame based on a rule and adds a data quality status column.\n\n    Args:\n        df (DataFrame): The input PySpark DataFrame to be filtered.\n        rule (dict): A dictionary containing the filtering rule. It should include:\n            - 'field': The name of the column to apply the filter on.\n            - 'check': The type of check to perform (currently unused in this implementation).\n            - 'value': The expression in the pattern of pyspark.sql.functions.expr.\n\n    Returns:\n        DataFrame: A new DataFrame filtered based on the rule, with an additional column\n        \"dq_status\" that describes the rule applied in the format \"field:check:value\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    expression = expr(value)\n    return df.filter(~expression).withColumn(\n        \"dq_status\", concat(lit(field), lit(\":\"), lit(check), lit(\":\"), lit(value))\n    )\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.summarize","title":"<code>summarize(df, rules, total_rows)</code>","text":"<p>Summarizes data quality results based on provided rules and total rows.</p> <p>This function processes a DataFrame containing data quality statuses, applies rules to calculate violations, and generates a summary DataFrame with metrics such as pass rate, status, and other relevant information.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing a column <code>dq_status</code> with data quality statuses in the format \"column:rule:value\".</p> required <code>rules</code> <code>List[Dict]</code> <p>A list of dictionaries representing the data quality rules. Each dictionary should define the <code>column</code>, <code>rule</code>, and optional <code>value</code> and <code>pass_threshold</code>.</p> required <code>total_rows</code> <code>int</code> <p>The total number of rows in the input DataFrame.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A summary DataFrame containing the following columns: - id: A unique identifier for each row. - timestamp: The timestamp when the summary was generated. - check: The type of check performed (e.g., \"Quality Check\"). - level: The severity level of the check (e.g., \"WARNING\"). - column: The column name associated with the rule. - rule: The rule applied to the column. - value: The value associated with the rule. - rows: The total number of rows in the input DataFrame. - violations: The number of rows that violated the rule. - pass_rate: The percentage of rows that passed the rule. - pass_threshold: The threshold for passing the rule. - status: The overall status of the rule (e.g., \"PASS\" or \"FAIL\").</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def summarize(df: DataFrame, rules: List[Dict], total_rows) -&gt; DataFrame:\n    \"\"\"\n    Summarizes data quality results based on provided rules and total rows.\n\n    This function processes a DataFrame containing data quality statuses, applies\n    rules to calculate violations, and generates a summary DataFrame with metrics\n    such as pass rate, status, and other relevant information.\n\n    Args:\n        df (DataFrame): The input DataFrame containing a column `dq_status` with\n            data quality statuses in the format \"column:rule:value\".\n        rules (List[Dict]): A list of dictionaries representing the data quality\n            rules. Each dictionary should define the `column`, `rule`, and optional\n            `value` and `pass_threshold`.\n        total_rows (int): The total number of rows in the input DataFrame.\n\n    Returns:\n        DataFrame: A summary DataFrame containing the following columns:\n            - id: A unique identifier for each row.\n            - timestamp: The timestamp when the summary was generated.\n            - check: The type of check performed (e.g., \"Quality Check\").\n            - level: The severity level of the check (e.g., \"WARNING\").\n            - column: The column name associated with the rule.\n            - rule: The rule applied to the column.\n            - value: The value associated with the rule.\n            - rows: The total number of rows in the input DataFrame.\n            - violations: The number of rows that violated the rule.\n            - pass_rate: The percentage of rows that passed the rule.\n            - pass_threshold: The threshold for passing the rule.\n            - status: The overall status of the rule (e.g., \"PASS\" or \"FAIL\").\n    \"\"\"\n    now_ts = current_timestamp()\n\n    viol_df = (\n        df.filter(trim(col(\"dq_status\")) != lit(\"\"))\n        .withColumn(\"dq_status\", split(trim(col(\"dq_status\")), \":\"))\n        .withColumn(\"column\", col(\"dq_status\")[0])\n        .withColumn(\"rule\", col(\"dq_status\")[1])\n        .withColumn(\"value\", col(\"dq_status\")[2])\n        .groupBy(\"column\", \"rule\", \"value\")\n        .agg(count(\"*\").alias(\"violations\"))\n        .withColumn(\n            \"value\",\n            coalesce(\n                when(col(\"value\") == \"\", None).otherwise(col(\"value\")), lit(\"N/A\")\n            ),\n        )\n    )\n\n    rules_df = __rules_to_df(rules).withColumn(\n        \"value\", coalesce(col(\"value\"), lit(\"N/A\"))\n    )\n\n    base = (\n        broadcast(rules_df)\n        .join(viol_df, [\"column\", \"rule\", \"value\"], how=\"left\")\n        .withColumn(\"violations\", coalesce(col(\"violations\"), lit(0)))\n    )\n\n    summary = (\n        base.withColumn(\"rows\", lit(total_rows))\n        .withColumn(\n            \"pass_rate\", (lit(total_rows) - col(\"violations\")) / lit(total_rows)\n        )\n        .withColumn(\n            \"status\",\n            when(col(\"pass_rate\") &gt;= col(\"pass_threshold\"), \"PASS\").otherwise(\"FAIL\"),\n        )\n        .withColumn(\"timestamp\", now_ts)\n        .withColumn(\"check\", lit(\"Quality Check\"))\n        .withColumn(\"level\", lit(\"WARNING\"))\n    )\n\n    summary = summary.withColumn(\"id\", expr(\"uuid()\"))\n    summary = summary.select(\n        \"id\",\n        \"timestamp\",\n        \"check\",\n        \"level\",\n        \"column\",\n        \"rule\",\n        \"value\",\n        \"rows\",\n        \"violations\",\n        \"pass_rate\",\n        \"pass_threshold\",\n        \"status\",\n    )\n\n    return summary\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.validate","title":"<code>validate(df, rules)</code>","text":"<p>Validates a DataFrame against a set of rules and returns the validation results.</p> <p>This function applies a series of validation rules to the input DataFrame. Each rule is expected to be a dictionary containing the parameters required for validation. The function generates two DataFrames as output: 1. A summarized result DataFrame with aggregated validation statuses. 2. A raw result DataFrame containing detailed validation results.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to validate.</p> required <code>rules</code> <code>list[dict]</code> <p>A list of dictionaries, where each dictionary defines a validation rule. Each rule should include the following keys: - <code>field</code> (str): The column name to validate. - <code>rule_name</code> (str): The name of the validation function to apply. - <code>value</code> (any): The value or parameter required by the validation function.</p> required <p>Returns:</p> Type Description <code>Tuple[DataFrame, DataFrame]</code> <p>Tuple[DataFrame, DataFrame]: A tuple containing: - result (DataFrame): A DataFrame with aggregated validation statuses. - raw_result (DataFrame): A DataFrame with detailed validation results.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If a rule references a validation function that does not exist in the global scope.</p> Notes <ul> <li>The <code>dq_status</code> column is used to store validation statuses.</li> <li>The function assumes that the validation functions are defined in the global scope   and are accessible by their names.</li> <li>The <code>concat_ws</code> function is used to concatenate multiple validation statuses   into a single string for each record in the summarized result.</li> </ul> Example <p>from pyspark.sql import SparkSession spark = SparkSession.builder.getOrCreate() df = spark.createDataFrame([(1, \"Alice\"), (2, \"Bob\")], [\"id\", \"name\"]) rules = [{\"field\": \"id\", \"rule_name\": \"validate_positive\", \"value\": None}] result, raw_result = validate(df, rules)</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def validate(df: DataFrame, rules: list[dict]) -&gt; Tuple[DataFrame, DataFrame]:\n    \"\"\"\n    Validates a DataFrame against a set of rules and returns the validation results.\n\n    This function applies a series of validation rules to the input DataFrame. Each rule\n    is expected to be a dictionary containing the parameters required for validation.\n    The function generates two DataFrames as output:\n    1. A summarized result DataFrame with aggregated validation statuses.\n    2. A raw result DataFrame containing detailed validation results.\n\n    Args:\n        df (DataFrame): The input PySpark DataFrame to validate.\n        rules (list[dict]): A list of dictionaries, where each dictionary defines a validation rule.\n            Each rule should include the following keys:\n            - `field` (str): The column name to validate.\n            - `rule_name` (str): The name of the validation function to apply.\n            - `value` (any): The value or parameter required by the validation function.\n\n    Returns:\n        Tuple[DataFrame, DataFrame]: A tuple containing:\n            - result (DataFrame): A DataFrame with aggregated validation statuses.\n            - raw_result (DataFrame): A DataFrame with detailed validation results.\n\n    Raises:\n        KeyError: If a rule references a validation function that does not exist in the global scope.\n\n    Warnings:\n        If a rule references an unknown validation function, a warning is issued.\n\n    Notes:\n        - The `dq_status` column is used to store validation statuses.\n        - The function assumes that the validation functions are defined in the global scope\n          and are accessible by their names.\n        - The `concat_ws` function is used to concatenate multiple validation statuses\n          into a single string for each record in the summarized result.\n\n    Example:\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt; df = spark.createDataFrame([(1, \"Alice\"), (2, \"Bob\")], [\"id\", \"name\"])\n        &gt;&gt;&gt; rules = [{\"field\": \"id\", \"rule_name\": \"validate_positive\", \"value\": None}]\n        &gt;&gt;&gt; result, raw_result = validate(df, rules)\n    \"\"\"\n    df = df.withColumn(\"dq_status\", lit(\"\"))\n    raw_result = df.limit(0)\n    for rule in rules:\n        field, rule_name, value = __extract_params(rule)\n        try:\n            rule_func = globals()[rule_name]\n            raw_result = raw_result.unionByName(rule_func(df, rule))\n        except KeyError:\n            warnings.warn(f\"Unknown rule name: {rule_name}, {field}\")\n    group_columns = [c for c in df.columns if c != \"dq_status\"]\n    result = raw_result.groupBy(*group_columns).agg(\n        concat_ws(\";\", collect_list(\"dq_status\")).alias(\"dq_status\")\n    )\n    return result, raw_result\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.validate_date_format","title":"<code>validate_date_format(df, rule)</code>","text":"<p>Filters a DataFrame to identify rows where a specified field has wrong date format based in the format from the rule and adds a \"dq_status\" column indicating the data quality rule applied.</p> <p>YYYY = full year, ex: 2012; YY = only second part of the year, ex: 12; MM = Month number (1-12); DD = Day (1-31);</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the data quality rule. It should include: - \"field\" (str): The name of the field to check for null values. - \"check\" (str): A description of the check being performed. - \"value\" (str): Additional information about the rule.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame filtered to include only rows where the specified</p> <code>DataFrame</code> <p>field is null, with an additional \"dq_status\" column describing the rule.</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def validate_date_format(df: DataFrame, rule: dict) -&gt; DataFrame:\n    \"\"\"\n    Filters a DataFrame to identify rows where a specified field has wrong date format based in the format from the rule\n    and adds a \"dq_status\" column indicating the data quality rule applied.\n\n    YYYY = full year, ex: 2012;\n    YY = only second part of the year, ex: 12;\n    MM = Month number (1-12);\n    DD = Day (1-31);\n\n    Args:\n        df (DataFrame): The input PySpark DataFrame to be checked.\n        rule (dict): A dictionary containing the data quality rule. It should include:\n            - \"field\" (str): The name of the field to check for null values.\n            - \"check\" (str): A description of the check being performed.\n            - \"value\" (str): Additional information about the rule.\n\n    Returns:\n        DataFrame: A new DataFrame filtered to include only rows where the specified\n        field is null, with an additional \"dq_status\" column describing the rule.\n    \"\"\"\n\n    field, check, date_format = __extract_params(rule)\n\n    date_regex = __transform_date_format_in_pattern(date_format)\n\n    return df.filter(~col(field).rlike(date_regex) | col(field).isNull()).withColumn(\n        \"dq_status\",\n        concat(lit(field), lit(\":\"), lit(check), lit(\":\"), lit(date_format)),\n    )\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.validate_schema","title":"<code>validate_schema(df, expected)</code>","text":"<p>Validates the schema of a PySpark DataFrame against an expected schema.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The PySpark DataFrame whose schema is to be validated.</p> required <code>expected</code> <code>list</code> <p>The expected schema represented as a list of tuples,              where each tuple contains the column name and its data type              and a boolean, if the column is nullable or not.</p> required <p>Returns:</p> Type Description <code>Tuple[bool, List[Tuple[str, str]]]</code> <p>Tuple[bool, List[Tuple[str, str]]]: A tuple containing: - A boolean indicating whether the schema matches the expected schema. - A list of tuples representing the mismatched columns, where each tuple   contains the column name and the reason for the mismatch.</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def validate_schema(df: DataFrame, expected) -&gt; Tuple[bool, List[Tuple[str, str]]]:\n    \"\"\"\n    Validates the schema of a PySpark DataFrame against an expected schema.\n\n    Args:\n        df (DataFrame): The PySpark DataFrame whose schema is to be validated.\n        expected (list): The expected schema represented as a list of tuples,\n                         where each tuple contains the column name and its data type\n                         and a boolean, if the column is nullable or not.\n\n    Returns:\n        Tuple[bool, List[Tuple[str, str]]]: A tuple containing:\n            - A boolean indicating whether the schema matches the expected schema.\n            - A list of tuples representing the mismatched columns, where each tuple\n              contains the column name and the reason for the mismatch.\n    \"\"\"\n    actual = __pyspark_schema_to_list(df)\n    result, errors = __compare_schemas(actual, expected)\n    return result, errors\n</code></pre>"},{"location":"api/engine/engines/","title":"Module <code>sumeh.engine</code> - Overview","text":"<p>This package provides engine-specific implementations for data quality checks, schema validation, and result summarization across multiple backends.</p> <p>\ud83d\udce6 File Structure</p> <pre><code>sumeh/engine/\n\u251c\u2500\u2500 __init__.py         # \u21e8 engine registry and dynamic import logic\n\u251c\u2500\u2500 bigquery_engine.py  # \u21e8 BigQuery schema introspection &amp; validation\n\u251c\u2500\u2500 dask_engine.py      # \u21e8 Dask DataFrame rule execution, validation &amp; summarization\n\u251c\u2500\u2500 duckdb_engine.py    # \u21e8 DuckDB SQL builder for rule validation &amp; summarization\n\u251c\u2500\u2500 pandas_engine.py    # \u21e8 Pandas DataFrame rule execution, validation &amp; summarization\n\u251c\u2500\u2500 polars_engine.py    # \u21e8 Polars DataFrame rule execution, validation &amp; summarization\n\u2514\u2500\u2500 pyspark_engine.py   # \u21e8 PySpark DataFrame rule execution, validation &amp; summarization\n</code></pre> <ul> <li> <p><code>__init__.py</code>   Detects the active engine based on the DataFrame type and dynamically dispatches to the appropriate module.</p> </li> <li> <p><code>bigquery_engine.py</code>   Converts a BigQuery table schema into a unified format and compares it against an expected schema.</p> </li> <li> <p><code>dask_engine.py</code>   Implements all data quality checks (completeness, uniqueness, value ranges, patterns, etc.) on Dask DataFrames, plus functions to aggregate violations and produce a summary report.</p> </li> <li> <p><code>duckdb_engine.py</code>   Builds SQL expressions for each rule, executes them in DuckDB as UNION ALL queries, and returns both raw violations and an aggregated result, along with schema validation via PRAGMA introspection.</p> </li> <li> <p><code>pandas_engine.py</code>   Executes the full suite of data quality checks directly against a Pandas DataFrame, annotating each violation in a <code>dq_status</code> column, and provides functions to aggregate raw results and generate a pass/fail summary. Also includes schema comparison utilities via Pandas dtypes.</p> </li> <li> <p><code>polars_engine.py</code>   Mirrors the full suite of quality checks for Polars DataFrames, annotating violations in a <code>dq_status</code> column and providing summarization and schema-comparison utilities.</p> </li> <li> <p><code>pyspark_engine.py</code>   Leverages PySpark SQL functions and window operations to apply the same rule set to Spark DataFrames, including logic for schema validation and summarization.</p> </li> </ul>"},{"location":"api/services/services-config/","title":"Module <code>sumeh.services.config</code>","text":"<p>This module provides a set of utility functions to retrieve and parse configuration data from various data sources, including S3, MySQL, PostgreSQL, BigQuery, CSV files, AWS Glue Data Catalog, DuckDB, and Databricks. Additionally, it includes functions to infer schema information from these sources.</p> <p>Functions:</p> Name Description <code>get_config_from_s3</code> <p>str, delimiter: Optional[str] = \",\") -&gt; List[Dict[str, Any]]:</p> <code>get_config_from_mysql</code> <code>get_config_from_postgresql</code> <code>get_config_from_bigquery</code> <code>get_config_from_csv</code> <p>str, delimiter: Optional[str] = \",\") -&gt; List[Dict[str, str]]: Retrieves configuration data from a local CSV file.</p> <code>get_config_from_glue_data_catalog</code> <code>get_config_from_duckdb</code> <p>Retrieves configuration data from a DuckDB database.</p> <code>get_config_from_databricks</code> <p>Retrieves configuration data from a Databricks table.</p> <code>get_schema_from_csv</code> <p>str, delimiter: str = \",\", sample_size: int = 1_000) -&gt; List[Dict[str, Any]]: Infers the schema of a CSV file based on its content.</p> <code>get_schema_from_s3</code> <p>str, **kwargs) -&gt; List[Dict[str, Any]]: Infers the schema of a CSV file stored in S3.</p> <code>get_schema_from_mysql</code> <p>Retrieves schema information from a MySQL database table.</p> <code>get_schema_from_postgresql</code> <p>Retrieves schema information from a PostgreSQL database table.</p> <code>get_schema_from_bigquery</code> <p>Retrieves schema information from a Google BigQuery table.</p> <code>get_schema_from_glue</code> <p>Retrieves schema information from AWS Glue Data Catalog.</p> <code>get_schema_from_duckdb</code> <p>Retrieves schema information from a DuckDB database table.</p> <code>get_schema_from_databricks</code> <p>Retrieves schema information from a Databricks table.</p> <code>__read_s3_file</code> <p>str) -&gt; Optional[str]:</p> <code>__parse_s3_path</code> <p>str) -&gt; Tuple[str, str]:</p> <code>__read_local_file</code> <p>str) -&gt; str:</p> <code>__read_csv_file</code> <p>str, delimiter: Optional[str] = \",\") -&gt; List[Dict[str, str]]:</p> <code>__parse_data</code> <p>list[dict]) -&gt; list[dict]: Parses the configuration data into a structured format.</p> <code>__create_connection</code> <code>infer_basic_type</code> <p>str) -&gt; str: Infers the basic data type of a given value.</p>"},{"location":"api/services/services-config/#sumeh.services.config.__create_connection","title":"<code>__create_connection(connect_func, host, user, password, database, port)</code>","text":"<p>Helper function to create a database connection.</p> <p>Parameters:</p> Name Type Description Default <code>connect_func</code> <p>A connection function (e.g., <code>mysql.connector.connect</code> or <code>psycopg2.connect</code>).</p> required <code>host</code> <code>str</code> <p>The host of the database server.</p> required <code>user</code> <code>str</code> <p>The username for the database.</p> required <code>password</code> <code>str</code> <p>The password for the database.</p> required <code>database</code> <code>str</code> <p>The name of the database.</p> required <code>port</code> <code>int</code> <p>The port to connect to.</p> required <p>Returns:</p> Name Type Description <code>Connection</code> <code>Any</code> <p>A connection object for the database.</p> <p>Raises:</p> Type Description <code>ConnectionError</code> <p>If there is an error establishing the connection.</p> Source code in <code>sumeh/services/config.py</code> <pre><code>def __create_connection(connect_func, host, user, password, database, port) -&gt; Any:\n    \"\"\"\n    Helper function to create a database connection.\n\n    Args:\n        connect_func: A connection function (e.g., `mysql.connector.connect` or `psycopg2.connect`).\n        host (str): The host of the database server.\n        user (str): The username for the database.\n        password (str): The password for the database.\n        database (str): The name of the database.\n        port (int): The port to connect to.\n\n    Returns:\n        Connection: A connection object for the database.\n\n    Raises:\n        ConnectionError: If there is an error establishing the connection.\n    \"\"\"\n    try:\n        return connect_func(\n            host=host, user=user, password=password, database=database, port=port\n        )\n    except Exception as e:\n        raise ConnectionError(f\"Error creating connection: {e}\")\n</code></pre>"},{"location":"api/services/services-config/#sumeh.services.config.__parse_data","title":"<code>__parse_data(data)</code>","text":"<p>Parse the configuration data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>List[Dict[str, str]]</code> <p>The raw data to be parsed.</p> required <p>Returns:</p> Type Description <code>list[dict]</code> <p>List[Dict[str, str]]: A list of parsed configuration data.</p> Source code in <code>sumeh/services/config.py</code> <pre><code>def __parse_data(data: list[dict]) -&gt; list[dict]:\n    \"\"\"\n    Parse the configuration data.\n\n    Args:\n        data (List[Dict[str, str]]): The raw data to be parsed.\n\n    Returns:\n        List[Dict[str, str]]: A list of parsed configuration data.\n    \"\"\"\n    parsed_data = []\n\n    for row in data:\n        parsed_row = {\n            \"field\": (\n                row[\"field\"].strip(\"[]\").split(\",\")\n                if \"[\" in row[\"field\"]\n                else row[\"field\"]\n            ),\n            \"check_type\": row[\"check_type\"],\n            \"value\": None if row[\"value\"] == \"NULL\" else row[\"value\"],\n            \"threshold\": (\n                None if row[\"threshold\"] == \"NULL\" else float(row[\"threshold\"])\n            ),\n            \"execute\": (\n                row[\"execute\"].lower() == \"true\"\n                if isinstance(row[\"execute\"], str)\n                else row[\"execute\"] is True\n            ),\n            \"updated_at\": parser.parse(row[\"updated_at\"]),\n        }\n        parsed_data.append(parsed_row)\n\n    return parsed_data\n</code></pre>"},{"location":"api/services/services-config/#sumeh.services.config.__parse_s3_path","title":"<code>__parse_s3_path(s3_path)</code>","text":"<p>Parses an S3 path into its bucket and key components.</p> <p>Parameters:</p> Name Type Description Default <code>s3_path</code> <code>str</code> <p>The S3 path to parse. Must start with \"s3://\".</p> required <p>Returns:</p> Type Description <code>Tuple[str, str]</code> <p>Tuple[str, str]: A tuple containing the bucket name and the key.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the S3 path does not start with \"s3://\", or if the path         format is invalid and cannot be split into bucket and key.</p> Source code in <code>sumeh/services/config.py</code> <pre><code>def __parse_s3_path(s3_path: str) -&gt; Tuple[str, str]:\n    \"\"\"\n    Parses an S3 path into its bucket and key components.\n\n    Args:\n        s3_path (str): The S3 path to parse. Must start with \"s3://\".\n\n    Returns:\n        Tuple[str, str]: A tuple containing the bucket name and the key.\n\n    Raises:\n        ValueError: If the S3 path does not start with \"s3://\", or if the path\n                    format is invalid and cannot be split into bucket and key.\n    \"\"\"\n    try:\n        if not s3_path.startswith(\"s3://\"):\n            raise ValueError(\"S3 path must start with 's3://'\")\n\n        s3_path = s3_path[5:]\n        bucket, key = s3_path.split(\"/\", 1)\n        return bucket, key\n\n    except ValueError as e:\n        raise ValueError(\n            f\"Invalid S3 path format: '{s3_path}'. Expected format 's3://bucket/key'. Details: {e}\"\n        ) from e\n</code></pre>"},{"location":"api/services/services-config/#sumeh.services.config.__read_csv_file","title":"<code>__read_csv_file(file_content, delimiter=',')</code>","text":"<p>Parses the content of a CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>The content of the CSV file as a string.</p> required <code>delimiter</code> <code>str</code> <p>The delimiter used in the CSV file.</p> <code>','</code> <p>Returns:</p> Type Description <code>List[Dict[str, str]]</code> <p>List[Dict[str, str]]: A list of dictionaries representing the parsed CSV data.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If there is an error parsing the CSV content.</p> Source code in <code>sumeh/services/config.py</code> <pre><code>def __read_csv_file(\n    file_content: str, delimiter: Optional[str] = \",\"\n) -&gt; List[Dict[str, str]]:\n    \"\"\"\n    Parses the content of a CSV file.\n\n    Args:\n        content (str): The content of the CSV file as a string.\n        delimiter (str): The delimiter used in the CSV file.\n\n    Returns:\n        List[Dict[str, str]]: A list of dictionaries representing the parsed CSV data.\n\n    Raises:\n        ValueError: If there is an error parsing the CSV content.\n    \"\"\"\n    import csv\n\n    try:\n        reader = csv.DictReader(StringIO(file_content), delimiter=delimiter)\n        # next(reader, None)  # Skip the header row\n        return [dict(row) for row in reader]\n    except csv.Error as e:\n        raise ValueError(f\"Error: Could not parse CSV content. Details: {e}\") from e\n</code></pre>"},{"location":"api/services/services-config/#sumeh.services.config.__read_local_file","title":"<code>__read_local_file(file_path)</code>","text":"<p>Reads the content of a local file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The local file path to be read.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The content of the file.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the file is not found.</p> Source code in <code>sumeh/services/config.py</code> <pre><code>def __read_local_file(file_path: str) -&gt; str:\n    \"\"\"\n    Reads the content of a local file.\n\n    Args:\n        file_path (str): The local file path to be read.\n\n    Returns:\n        str: The content of the file.\n\n    Raises:\n        FileNotFoundError: If the file is not found.\n    \"\"\"\n    try:\n        with open(file_path, mode=\"r\", encoding=\"utf-8\") as file:\n            return file.read()\n    except FileNotFoundError as e:\n        raise FileNotFoundError(\n            f\"Error: The file at '{file_path}' was not found.\"\n        ) from e\n    except IOError as e:\n        raise IOError(f\"Error: Could not read file '{file_path}'. Details: {e}\") from e\n</code></pre>"},{"location":"api/services/services-config/#sumeh.services.config.__read_s3_file","title":"<code>__read_s3_file(s3_path)</code>","text":"<p>Reads the content of a file stored in S3.</p> <p>Parameters:</p> Name Type Description Default <code>s3_path</code> <code>str</code> <p>The S3 path of the file.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Optional[str]</code> <p>The content of the S3 file.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If there is an error retrieving the file from S3.</p> Source code in <code>sumeh/services/config.py</code> <pre><code>def __read_s3_file(s3_path: str) -&gt; Optional[str]:\n    \"\"\"\n    Reads the content of a file stored in S3.\n\n    Args:\n        s3_path (str): The S3 path of the file.\n\n    Returns:\n        str: The content of the S3 file.\n\n    Raises:\n        RuntimeError: If there is an error retrieving the file from S3.\n    \"\"\"\n    import boto3\n    from botocore.exceptions import BotoCoreError, ClientError\n\n    try:\n        s3 = boto3.client(\"s3\")\n        bucket, key = __parse_s3_path(s3_path)\n\n        response = s3.get_object(Bucket=bucket, Key=key)\n        return response[\"Body\"].read().decode(\"utf-8\")\n\n    except (BotoCoreError, ClientError) as e:\n        raise RuntimeError(\n            f\"Failed to read file from S3. Path: '{s3_path}'. Error: {e}\"\n        ) from e\n\n    except UnicodeDecodeError as e:\n        raise ValueError(\n            f\"Failed to decode file content from S3 path '{s3_path}' as UTF-8. Error: {e}\"\n        ) from e\n</code></pre>"},{"location":"api/services/services-config/#sumeh.services.config.get_config_from_bigquery","title":"<code>get_config_from_bigquery(project_id, dataset_id, table_id, credentials_path=None, query=None)</code>","text":"<p>Retrieves configuration data from a Google BigQuery table.</p> <p>Parameters:</p> Name Type Description Default <code>project_id</code> <code>str</code> <p>Google Cloud project ID.</p> required <code>dataset_id</code> <code>str</code> <p>BigQuery dataset ID.</p> required <code>table_id</code> <code>str</code> <p>BigQuery table ID.</p> required <code>credentials_path</code> <code>Optional[str]</code> <p>Path to service account credentials file (if not provided, defaults to default credentials).</p> <code>None</code> <code>query</code> <code>Optional[str]</code> <p>Custom SQL query to fetch data (if not provided, defaults to SELECT *).</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Dict[str, str]]</code> <p>List[Dict[str, str]]: A list of dictionaries representing the parsed configuration data.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If there is an error while querying BigQuery.</p> Source code in <code>sumeh/services/config.py</code> <pre><code>def get_config_from_bigquery(\n    project_id: str,\n    dataset_id: str,\n    table_id: str,\n    credentials_path: Optional[str] = None,\n    query: Optional[str] = None,\n) -&gt; List[Dict[str, str]]:\n    \"\"\"\n    Retrieves configuration data from a Google BigQuery table.\n\n    Args:\n        project_id (str): Google Cloud project ID.\n        dataset_id (str): BigQuery dataset ID.\n        table_id (str): BigQuery table ID.\n        credentials_path (Optional[str]): Path to service account credentials file (if not provided, defaults to default credentials).\n        query (Optional[str]): Custom SQL query to fetch data (if not provided, defaults to SELECT *).\n\n    Returns:\n        List[Dict[str, str]]: A list of dictionaries representing the parsed configuration data.\n\n    Raises:\n        RuntimeError: If there is an error while querying BigQuery.\n    \"\"\"\n    from google.cloud import bigquery\n    from google.auth.exceptions import DefaultCredentialsError\n\n    if query is None:\n        query = f\"SELECT * FROM `{project_id}.{dataset_id}.{table_id}`\"\n\n    try:\n        client = bigquery.Client(\n            project=project_id,\n            credentials=(\n                None\n                if credentials_path is None\n                else bigquery.Credentials.from_service_account_file(credentials_path)\n            ),\n        )\n\n        # Execute the query and convert the result to a pandas DataFrame\n        data = client.query(query).to_dataframe()\n\n        # Convert the DataFrame to a list of dictionaries\n        data_dict = data.to_dict(orient=\"records\")\n\n        # Parse the data and return the result\n        return __parse_data(data_dict)\n\n    except DefaultCredentialsError as e:\n        raise RuntimeError(f\"Credentials error: {e}\") from e\n\n    except Exception as e:\n        raise RuntimeError(f\"Error occurred while querying BigQuery: {e}\") from e\n</code></pre>"},{"location":"api/services/services-config/#sumeh.services.config.get_config_from_csv","title":"<code>get_config_from_csv(file_path, delimiter=',')</code>","text":"<p>Retrieves configuration data from a CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The local file path to the CSV file.</p> required <code>delimiter</code> <code>Optional[str]</code> <p>The delimiter used in the CSV file (default is \",\").</p> <code>','</code> <p>Returns:</p> Type Description <code>List[Dict[str, str]]</code> <p>List[Dict[str, str]]: A list of dictionaries representing the parsed configuration data.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If there is an error reading or processing the file.</p> Source code in <code>sumeh/services/config.py</code> <pre><code>def get_config_from_csv(\n    file_path: str, delimiter: Optional[str] = \",\"\n) -&gt; List[Dict[str, str]]:\n    \"\"\"\n    Retrieves configuration data from a CSV file.\n\n    Args:\n        file_path (str): The local file path to the CSV file.\n        delimiter (Optional[str]): The delimiter used in the CSV file (default is \",\").\n\n    Returns:\n        List[Dict[str, str]]: A list of dictionaries representing the parsed configuration data.\n\n    Raises:\n        RuntimeError: If there is an error reading or processing the file.\n    \"\"\"\n    try:\n        file_content = __read_local_file(file_path)\n        result = __read_csv_file(file_content, delimiter)\n\n        return __parse_data(result)\n\n    except FileNotFoundError as e:\n        raise RuntimeError(f\"File '{file_path}' not found. Error: {e}\") from e\n\n    except ValueError as e:\n        raise ValueError(\n            f\"Error while parsing CSV file '{file_path}'. Error: {e}\"\n        ) from e\n\n    except Exception as e:\n        # Catch any unexpected exceptions\n        raise RuntimeError(\n            f\"Unexpected error while processing CSV file '{file_path}'. Error: {e}\"\n        ) from e\n</code></pre>"},{"location":"api/services/services-config/#sumeh.services.config.get_config_from_databricks","title":"<code>get_config_from_databricks(catalog, schema, table, **kwargs)</code>","text":"<p>Retrieves configuration data from a Databricks table and returns it as a list of dictionaries.</p> <p>Parameters:</p> Name Type Description Default <code>catalog</code> <code>Optional[str]</code> <p>The catalog name in Databricks. If provided, it will be included in the table's full path.</p> required <code>schema</code> <code>Optional[str]</code> <p>The schema name in Databricks. If provided, it will be included in the table's full path.</p> required <code>table</code> <code>str</code> <p>The name of the table to retrieve data from.</p> required <code>**kwargs</code> <p>Additional keyword arguments (currently unused).</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List[Dict[str, Any]]: A list of dictionaries, where each dictionary represents a row of data from the table.</p> Source code in <code>sumeh/services/config.py</code> <pre><code>def get_config_from_databricks(\n    catalog: Optional[str], schema: Optional[str], table: str, **kwargs\n) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Retrieves configuration data from a Databricks table and returns it as a list of dictionaries.\n\n    Args:\n        catalog (Optional[str]): The catalog name in Databricks. If provided, it will be included in the table's full path.\n        schema (Optional[str]): The schema name in Databricks. If provided, it will be included in the table's full path.\n        table (str): The name of the table to retrieve data from.\n        **kwargs: Additional keyword arguments (currently unused).\n\n    Returns:\n        List[Dict[str, Any]]: A list of dictionaries, where each dictionary represents a row of data from the table.\n    \"\"\"\n    from pyspark.sql import SparkSession\n\n    spark = SparkSession.builder.getOrCreate()\n    if catalog and schema:\n        full = f\"{catalog}.{schema}.{table}\"\n    elif schema:\n        full = f\"{schema}.{table}\"\n    else:\n        full = table\n    if \"query\" in kwargs.keys():\n        df = spark.sql(f\"select * from {full} where {kwargs['query']}\")\n    else:\n        df = spark.table(full)\n    return [row.asDict() for row in df.collect()]\n</code></pre>"},{"location":"api/services/services-config/#sumeh.services.config.get_config_from_duckdb","title":"<code>get_config_from_duckdb(db_path, table=None, query=None, conn=None)</code>","text":"<p>Retrieve configuration data from a DuckDB database.</p> <p>This function fetches data from a DuckDB database either by executing a custom SQL query or by selecting all rows from a specified table. The data is then parsed into a list of dictionaries.</p> <p>Parameters:</p> Name Type Description Default <code>db_path</code> <code>str</code> <p>The path to the DuckDB database file.</p> required <code>table</code> <code>str</code> <p>The name of the table to fetch data from. Defaults to None.</p> <code>None</code> <code>query</code> <code>str</code> <p>A custom SQL query to execute. Defaults to None.</p> <code>None</code> <code>conn</code> <p>A valid DuckDB connection object.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List[Dict[str, Any]]: A list of dictionaries representing the fetched data.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If neither <code>table</code> nor <code>query</code> is provided, or if a valid <code>conn</code> is not supplied.</p> Example <p>import duckdb conn = duckdb.connect('my_db.duckdb') config = get_config_from_duckdb('my_db.duckdb', table='rules', conn=conn)</p> Source code in <code>sumeh/services/config.py</code> <pre><code>def get_config_from_duckdb(\n    db_path: str, table: str = None, query: str = None, conn=None\n) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Retrieve configuration data from a DuckDB database.\n\n    This function fetches data from a DuckDB database either by executing a custom SQL query\n    or by selecting all rows from a specified table. The data is then parsed into a list of\n    dictionaries.\n\n    Args:\n        db_path (str): The path to the DuckDB database file.\n        table (str, optional): The name of the table to fetch data from. Defaults to None.\n        query (str, optional): A custom SQL query to execute. Defaults to None.\n        conn: A valid DuckDB connection object.\n\n    Returns:\n        List[Dict[str, Any]]: A list of dictionaries representing the fetched data.\n\n    Raises:\n        ValueError: If neither `table` nor `query` is provided, or if a valid `conn` is not supplied.\n\n    Example:\n        &gt;&gt;&gt; import duckdb\n        &gt;&gt;&gt; conn = duckdb.connect('my_db.duckdb')\n        &gt;&gt;&gt; config = get_config_from_duckdb('my_db.duckdb', table='rules', conn=conn)\n    \"\"\"\n\n    if query:\n        df = conn.execute(query).fetchdf()\n    elif table:\n        df = conn.execute(f\"SELECT * FROM {table}\").fetchdf()\n    else:\n        raise ValueError(\n            \"DuckDB configuration requires:\\n\"\n            \"1. Either a `table` name or custom `query`\\n\"\n            \"2. A valid database `conn` connection object\\n\"\n            \"Example: get_config('duckdb', table='rules', conn=duckdb.connect('my_db.duckdb'))\"\n        )\n\n    return __parse_data(df.to_dict(orient=\"records\"))\n</code></pre>"},{"location":"api/services/services-config/#sumeh.services.config.get_config_from_glue_data_catalog","title":"<code>get_config_from_glue_data_catalog(glue_context, database_name, table_name, query=None)</code>","text":"<p>Retrieves configuration data from AWS Glue Data Catalog.</p> <p>Parameters:</p> Name Type Description Default <code>glue_context</code> <p>An instance of <code>GlueContext</code>.</p> required <code>database_name</code> <code>str</code> <p>Glue database name.</p> required <code>table_name</code> <code>str</code> <p>Glue table name.</p> required <code>query</code> <code>Optional[str]</code> <p>Custom SQL query to fetch data (if provided).</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Dict[str, str]]</code> <p>List[Dict[str, str]]: A list of dictionaries representing the parsed configuration data.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If there is an error querying Glue Data Catalog.</p> Source code in <code>sumeh/services/config.py</code> <pre><code>def get_config_from_glue_data_catalog(\n    glue_context, database_name: str, table_name: str, query: Optional[str] = None\n) -&gt; List[Dict[str, str]]:\n    \"\"\"\n    Retrieves configuration data from AWS Glue Data Catalog.\n\n    Args:\n        glue_context: An instance of `GlueContext`.\n        database_name (str): Glue database name.\n        table_name (str): Glue table name.\n        query (Optional[str]): Custom SQL query to fetch data (if provided).\n\n    Returns:\n        List[Dict[str, str]]: A list of dictionaries representing the parsed configuration data.\n\n    Raises:\n        RuntimeError: If there is an error querying Glue Data Catalog.\n    \"\"\"\n    from awsglue.context import GlueContext\n\n    if not isinstance(glue_context, GlueContext):\n        raise ValueError(\"The provided context is not a valid GlueContext.\")\n\n    spark = glue_context.spark_session\n\n    try:\n        dynamic_frame = glue_context.create_dynamic_frame.from_catalog(\n            database=database_name, table_name=table_name\n        )\n\n        data_frame = dynamic_frame.toDF()\n\n        if query:\n            data_frame.createOrReplaceTempView(\"table_name\")\n            data_frame = spark.sql(query)\n\n        data_dict = [row.asDict() for row in data_frame.collect()]\n\n        return __parse_data(data_dict)\n\n    except Exception as e:\n        raise RuntimeError(\n            f\"Error occurred while querying Glue Data Catalog: {e}\"\n        ) from e\n</code></pre>"},{"location":"api/services/services-config/#sumeh.services.config.get_config_from_mysql","title":"<code>get_config_from_mysql(connection=None, host=None, user=None, password=None, database=None, port=3306, schema=None, table=None, query=None)</code>","text":"<p>Retrieves configuration data from a MySQL database.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Optional</code> <p>An existing MySQL connection object.</p> <code>None</code> <code>host</code> <code>Optional[str]</code> <p>Host of the MySQL server.</p> <code>None</code> <code>user</code> <code>Optional[str]</code> <p>Username to connect to MySQL.</p> <code>None</code> <code>password</code> <code>Optional[str]</code> <p>Password for the MySQL user.</p> <code>None</code> <code>database</code> <code>Optional[str]</code> <p>Database name to query.</p> <code>None</code> <code>port</code> <code>Optional[int]</code> <p>The port for the MySQL connection (default is 3306).</p> <code>3306</code> <code>schema</code> <code>Optional[str]</code> <p>Schema name if query is not provided.</p> <code>None</code> <code>table</code> <code>Optional[str]</code> <p>Table name if query is not provided.</p> <code>None</code> <code>query</code> <code>Optional[str]</code> <p>Custom SQL query to fetch data (if not provided, <code>schema</code> and <code>table</code> must be given).</p> <code>None</code> <p>Returns:</p> Type Description <p>List[Dict[str, Any]]: A list of dictionaries representing the parsed configuration data.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If neither <code>query</code> nor both <code>schema</code> and <code>table</code> are provided.</p> <code>ConnectionError</code> <p>If there is an error connecting to MySQL.</p> <code>RuntimeError</code> <p>If there is an error executing the query or processing the data.</p> Source code in <code>sumeh/services/config.py</code> <pre><code>def get_config_from_mysql(\n    connection: Optional = None,\n    host: Optional[str] = None,\n    user: Optional[str] = None,\n    password: Optional[str] = None,\n    database: Optional[str] = None,\n    port: Optional[int] = 3306,\n    schema: Optional[str] = None,\n    table: Optional[str] = None,\n    query: Optional[str] = None,\n):\n    \"\"\"\n    Retrieves configuration data from a MySQL database.\n\n    Args:\n        connection (Optional): An existing MySQL connection object.\n        host (Optional[str]): Host of the MySQL server.\n        user (Optional[str]): Username to connect to MySQL.\n        password (Optional[str]): Password for the MySQL user.\n        database (Optional[str]): Database name to query.\n        port (Optional[int]): The port for the MySQL connection (default is 3306).\n        schema (Optional[str]): Schema name if query is not provided.\n        table (Optional[str]): Table name if query is not provided.\n        query (Optional[str]): Custom SQL query to fetch data (if not provided, `schema` and `table` must be given).\n\n    Returns:\n        List[Dict[str, Any]]: A list of dictionaries representing the parsed configuration data.\n\n    Raises:\n        ValueError: If neither `query` nor both `schema` and `table` are provided.\n        ConnectionError: If there is an error connecting to MySQL.\n        RuntimeError: If there is an error executing the query or processing the data.\n    \"\"\"\n    import mysql.connector\n    import pandas as pd\n\n    if query is None and (schema is None or table is None):\n        raise ValueError(\n            \"You must provide either a 'query' or both 'schema' and 'table'.\"\n        )\n\n    if query is None:\n        query = f\"SELECT * FROM {schema}.{table}\"\n\n    try:\n        connection = connection or __create_connection(\n            mysql.connector.connect, host, user, password, database, port\n        )\n        data = pd.read_sql(query, connection)\n        data_dict = data.to_dict(orient=\"records\")\n        return __parse_data(data_dict)\n\n    except mysql.connector.Error as e:\n        raise ConnectionError(f\"Error connecting to MySQL database: {e}\")\n\n    except Exception as e:\n        raise RuntimeError(f\"Error executing the query or processing data: {e}\")\n\n    finally:\n        if connection and host is not None:\n            connection.close()\n</code></pre>"},{"location":"api/services/services-config/#sumeh.services.config.get_config_from_postgresql","title":"<code>get_config_from_postgresql(connection=None, host=None, user=None, password=None, database=None, port=5432, schema=None, table=None, query=None)</code>","text":"<p>Retrieves configuration data from a PostgreSQL database.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Optional</code> <p>An existing PostgreSQL connection object.</p> <code>None</code> <code>host</code> <code>Optional[str]</code> <p>Host of the PostgreSQL server.</p> <code>None</code> <code>user</code> <code>Optional[str]</code> <p>Username to connect to PostgreSQL.</p> <code>None</code> <code>password</code> <code>Optional[str]</code> <p>Password for the PostgreSQL user.</p> <code>None</code> <code>database</code> <code>Optional[str]</code> <p>Database name to query.</p> <code>None</code> <code>port</code> <code>Optional[int]</code> <p>The port for the PostgreSQL connection (default is 5432).</p> <code>5432</code> <code>schema</code> <code>Optional[str]</code> <p>Schema name if query is not provided.</p> <code>None</code> <code>table</code> <code>Optional[str]</code> <p>Table name if query is not provided.</p> <code>None</code> <code>query</code> <code>Optional[str]</code> <p>Custom SQL query to fetch data (if not provided, <code>schema</code> and <code>table</code> must be given).</p> <code>None</code> <p>Returns:</p> Type Description <code>list[dict]</code> <p>List[Dict[str, Any]]: A list of dictionaries representing the parsed configuration data.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If neither <code>query</code> nor both <code>schema</code> and <code>table</code> are provided.</p> <code>ConnectionError</code> <p>If there is an error connecting to PostgreSQL.</p> <code>RuntimeError</code> <p>If there is an error executing the query or processing the data.</p> Source code in <code>sumeh/services/config.py</code> <pre><code>def get_config_from_postgresql(\n    connection: Optional = None,\n    host: Optional[str] = None,\n    user: Optional[str] = None,\n    password: Optional[str] = None,\n    database: Optional[str] = None,\n    port: Optional[int] = 5432,\n    schema: Optional[str] = None,\n    table: Optional[str] = None,\n    query: Optional[str] = None,\n) -&gt; list[dict]:\n    \"\"\"\n    Retrieves configuration data from a PostgreSQL database.\n\n    Args:\n        connection (Optional): An existing PostgreSQL connection object.\n        host (Optional[str]): Host of the PostgreSQL server.\n        user (Optional[str]): Username to connect to PostgreSQL.\n        password (Optional[str]): Password for the PostgreSQL user.\n        database (Optional[str]): Database name to query.\n        port (Optional[int]): The port for the PostgreSQL connection (default is 5432).\n        schema (Optional[str]): Schema name if query is not provided.\n        table (Optional[str]): Table name if query is not provided.\n        query (Optional[str]): Custom SQL query to fetch data (if not provided, `schema` and `table` must be given).\n\n    Returns:\n        List[Dict[str, Any]]: A list of dictionaries representing the parsed configuration data.\n\n    Raises:\n        ValueError: If neither `query` nor both `schema` and `table` are provided.\n        ConnectionError: If there is an error connecting to PostgreSQL.\n        RuntimeError: If there is an error executing the query or processing the data.\n    \"\"\"\n    import psycopg2\n    import pandas as pd\n\n    if query is None and (schema is None or table is None):\n        raise ValueError(\n            \"You must provide either a 'query' or both 'schema' and 'table'.\"\n        )\n\n    if query is None:\n        query = f\"SELECT * FROM {schema}.{table}\"\n\n    try:\n        connection = connection or __create_connection(\n            psycopg2.connect, host, user, password, database, port\n        )\n\n        data = pd.read_sql(query, connection)\n\n        data_dict = data.to_dict(orient=\"records\")\n        return __parse_data(data_dict)\n\n    except psycopg2.Error as e:\n        raise ConnectionError(f\"Error connecting to PostgreSQL database: {e}\")\n\n    except Exception as e:\n        raise RuntimeError(f\"Error executing the query or processing data: {e}\")\n\n    finally:\n        if connection and host is not None:\n            connection.close()\n</code></pre>"},{"location":"api/services/services-config/#sumeh.services.config.get_config_from_s3","title":"<code>get_config_from_s3(s3_path, delimiter=',')</code>","text":"<p>Retrieves configuration data from a CSV file stored in an S3 bucket.</p> <p>Parameters:</p> Name Type Description Default <code>s3_path</code> <code>str</code> <p>The S3 path to the CSV file.</p> required <code>delimiter</code> <code>Optional[str]</code> <p>The delimiter used in the CSV file (default is \",\").</p> <code>','</code> <p>Returns:</p> Type Description <p>List[Dict[str, Any]]: A list of dictionaries representing the parsed configuration data.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If there is an error reading or processing the S3 file.</p> Source code in <code>sumeh/services/config.py</code> <pre><code>def get_config_from_s3(s3_path: str, delimiter: Optional[str] = \",\"):\n    \"\"\"\n    Retrieves configuration data from a CSV file stored in an S3 bucket.\n\n    Args:\n        s3_path (str): The S3 path to the CSV file.\n        delimiter (Optional[str]): The delimiter used in the CSV file (default is \",\").\n\n    Returns:\n        List[Dict[str, Any]]: A list of dictionaries representing the parsed configuration data.\n\n    Raises:\n        RuntimeError: If there is an error reading or processing the S3 file.\n    \"\"\"\n    try:\n        file_content = __read_s3_file(s3_path)\n        data = __read_csv_file(file_content, delimiter)\n        return __parse_data(data)\n\n    except Exception as e:\n        raise RuntimeError(f\"Error reading or processing the S3 file: {e}\")\n</code></pre>"},{"location":"api/services/services-config/#sumeh.services.config.get_schema_from_bigquery","title":"<code>get_schema_from_bigquery(project_id, dataset_id, table_id, credentials_path=None)</code>","text":"Source code in <code>sumeh/services/config.py</code> <pre><code>def get_schema_from_bigquery(\n    project_id: str, dataset_id: str, table_id: str, credentials_path: str = None\n) -&gt; List[Dict[str, Any]]:\n    from google.cloud import bigquery\n\n    client = bigquery.Client(\n        project=project_id,\n        credentials=(\n            None\n            if credentials_path is None\n            else bigquery.Credentials.from_service_account_file(credentials_path)\n        ),\n    )\n    table = client.get_table(f\"{project_id}.{dataset_id}.{table_id}\")\n    return [\n        {\n            \"field\": schema_field.name,\n            \"data_type\": schema_field.field_type.lower(),\n            \"nullable\": schema_field.is_nullable,\n            \"max_length\": None,\n        }\n        for schema_field in table.schema\n    ]\n</code></pre>"},{"location":"api/services/services-config/#sumeh.services.config.get_schema_from_csv","title":"<code>get_schema_from_csv(file_path, delimiter=',', sample_size=1000)</code>","text":"Source code in <code>sumeh/services/config.py</code> <pre><code>def get_schema_from_csv(\n    file_path: str, delimiter: str = \",\", sample_size: int = 1_000\n) -&gt; List[Dict[str, Any]]:\n    import csv\n\n    cols: Dict[str, Dict[str, Any]] = {}\n    with open(file_path, newline=\"\", encoding=\"utf-8\") as f:\n        reader = csv.DictReader(f, delimiter=delimiter)\n        for idx, row in enumerate(reader):\n            if idx &gt;= sample_size:\n                break\n            for name, raw in row.items():\n                info = cols.setdefault(\n                    name,\n                    {\n                        \"field\": name,\n                        \"nullable\": False,\n                        \"max_length\": 0,\n                        \"type_hints\": set(),\n                    },\n                )\n                if raw == \"\" or raw is None:\n                    info[\"nullable\"] = True\n                    continue\n                info[\"max_length\"] = max(info[\"max_length\"], len(raw))\n                info[\"type_hints\"].add(infer_basic_type(raw))\n\n    out: List[Dict[str, Any]] = []\n    for info in cols.values():\n        hints = info[\"type_hints\"]\n        if hints == {\"integer\"}:\n            dtype = \"integer\"\n        elif hints &lt;= {\"integer\", \"float\"}:\n            dtype = \"float\"\n        elif hints == {\"date\"}:\n            dtype = \"date\"\n        else:\n            dtype = \"string\"\n        out.append(\n            {\n                \"field\": info[\"field\"],\n                \"data_type\": dtype,\n                \"nullable\": info[\"nullable\"],\n                \"max_length\": info[\"max_length\"] or None,\n            }\n        )\n    return out\n</code></pre>"},{"location":"api/services/services-config/#sumeh.services.config.get_schema_from_databricks","title":"<code>get_schema_from_databricks(catalog, schema, table, **kwargs)</code>","text":"Source code in <code>sumeh/services/config.py</code> <pre><code>def get_schema_from_databricks(\n    catalog: Optional[str], schema: Optional[str], table: str, **kwargs\n) -&gt; List[Dict[str, Any]]:\n    from pyspark.sql import SparkSession\n\n    spark = SparkSession.builder.getOrCreate()\n    if catalog and schema:\n        full = f\"{catalog}.{schema}.{table}\"\n    elif schema:\n        full = f\"{schema}.{table}\"\n    else:\n        full = table\n    schema = spark.table(full).schema\n    result = []\n    for f in schema.fields:\n        result.append(\n            {\n                \"field\": f.name,\n                \"data_type\": f.dataType.simpleString(),\n                \"nullable\": f.nullable,\n                \"max_length\": None,\n            }\n        )\n    return result\n</code></pre>"},{"location":"api/services/services-config/#sumeh.services.config.get_schema_from_duckdb","title":"<code>get_schema_from_duckdb(db_path, table, conn)</code>","text":"Source code in <code>sumeh/services/config.py</code> <pre><code>def get_schema_from_duckdb(db_path: str, table: str, conn) -&gt; List[Dict[str, Any]]:\n    df = conn.execute(f\"PRAGMA table_info('{table}')\").fetchdf()\n    return [\n        {\n            \"field\": row[\"name\"],\n            \"data_type\": row[\"type\"].lower(),\n            \"nullable\": not bool(row[\"notnull\"]),\n            \"max_length\": None,\n        }\n        for _, row in df.iterrows()\n    ]\n</code></pre>"},{"location":"api/services/services-config/#sumeh.services.config.get_schema_from_glue","title":"<code>get_schema_from_glue(glue_context, database_name, table_name)</code>","text":"Source code in <code>sumeh/services/config.py</code> <pre><code>def get_schema_from_glue(\n    glue_context, database_name: str, table_name: str\n) -&gt; List[Dict[str, Any]]:\n    from awsglue.context import GlueContext\n\n    if not isinstance(glue_context, GlueContext):\n        raise ValueError(\"Informe um GlueContext v\u00e1lido\")\n    df = glue_context.spark_session.read.table(f\"{database_name}.{table_name}\")\n    return [\n        {\n            \"field\": field.name,\n            \"data_type\": field.dataType.simpleString(),\n            \"nullable\": field.nullable,\n            \"max_length\": None,\n        }\n        for field in df.schema.fields\n    ]\n</code></pre>"},{"location":"api/services/services-config/#sumeh.services.config.get_schema_from_mysql","title":"<code>get_schema_from_mysql(host, user, password, database, table, port=3306)</code>","text":"Source code in <code>sumeh/services/config.py</code> <pre><code>def get_schema_from_mysql(\n    host: str, user: str, password: str, database: str, table: str, port: int = 3306\n) -&gt; List[Dict[str, Any]]:\n    import mysql.connector\n\n    conn = mysql.connector.connect(\n        host=host, user=user, password=password, database=database, port=port\n    )\n    cursor = conn.cursor(dictionary=True)\n    cursor.execute(\n        f\"\"\"\n        SELECT \n            column_name AS field,\n            data_type,\n            is_nullable = 'YES' AS nullable,\n            character_maximum_length AS max_length\n        FROM information_schema.columns\n        WHERE table_schema = %s AND table_name = %s\n    \"\"\",\n        (database, table),\n    )\n    schema = cursor.fetchall()\n    cursor.close()\n    conn.close()\n    return schema\n</code></pre>"},{"location":"api/services/services-config/#sumeh.services.config.get_schema_from_postgresql","title":"<code>get_schema_from_postgresql(host, user, password, database, table, port=5432)</code>","text":"Source code in <code>sumeh/services/config.py</code> <pre><code>def get_schema_from_postgresql(\n    host: str, user: str, password: str, database: str, table: str, port: int = 5432\n) -&gt; List[Dict[str, Any]]:\n    import psycopg2\n\n    conn = psycopg2.connect(\n        host=host, user=user, password=password, dbname=database, port=port\n    )\n    cursor = conn.cursor()\n    cursor.execute(\n        f\"\"\"\n        SELECT\n            column_name AS field,\n            data_type,\n            is_nullable = 'YES' AS nullable,\n            character_maximum_length AS max_length\n        FROM information_schema.columns\n        WHERE table_schema = 'public' AND table_name = %s\n    \"\"\",\n        (table,),\n    )\n    cols = cursor.fetchall()\n    cursor.close()\n    conn.close()\n    return [\n        {\"field\": f, \"data_type\": dt, \"nullable\": nl, \"max_length\": ml}\n        for f, dt, nl, ml in cols\n    ]\n</code></pre>"},{"location":"api/services/services-config/#sumeh.services.config.get_schema_from_s3","title":"<code>get_schema_from_s3(s3_path, **kwargs)</code>","text":"Source code in <code>sumeh/services/config.py</code> <pre><code>def get_schema_from_s3(s3_path: str, **kwargs) -&gt; List[Dict[str, Any]]:\n\n    content = __read_s3_file(s3_path)\n    with open(\"/tmp/temp.csv\", \"w\") as f:\n        f.write(content)\n    return get_schema_from_csv(\"/tmp/temp.csv\", **kwargs)\n</code></pre>"},{"location":"api/services/services-config/#sumeh.services.config.infer_basic_type","title":"<code>infer_basic_type(val)</code>","text":"Source code in <code>sumeh/services/config.py</code> <pre><code>def infer_basic_type(val: str) -&gt; str:\n    try:\n        int(val)\n        return \"integer\"\n    except:\n        pass\n    try:\n        float(val)\n        return \"float\"\n    except:\n        pass\n    try:\n        _ = parser.parse(val)\n        return \"date\"\n    except:\n        pass\n    return \"string\"\n</code></pre>"},{"location":"api/services/services-utils/","title":"Module <code>sumeh.services.utils</code>","text":""},{"location":"api/services/services-utils/#sumeh.services.utils.SchemaDef","title":"<code>SchemaDef = Dict[str, Any]</code>  <code>module-attribute</code>","text":""},{"location":"api/services/services-utils/#sumeh.services.utils.__compare_schemas","title":"<code>__compare_schemas(actual, expected)</code>","text":"<p>Compare two lists of schema definitions and identify discrepancies.</p> <p>Parameters:</p> Name Type Description Default <code>actual</code> <code>List[SchemaDef]</code> <p>The list of actual schema definitions.</p> required <code>expected</code> <code>List[SchemaDef]</code> <p>The list of expected schema definitions.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>Tuple[bool, List[Tuple[str, str]]]: A tuple where the first element is a boolean indicating</p> <code>List[Tuple[str, str]]</code> <p>whether the schemas match (True if they match, False otherwise), and the second element</p> <code>Tuple[bool, List[Tuple[str, str]]]</code> <p>is a list of tuples describing the discrepancies. Each tuple contains: - The field name (str). - A description of the discrepancy (str), such as \"missing\", \"type mismatch\",   \"nullable but expected non-nullable\", or \"extra column\".</p> Notes <ul> <li>A field is considered \"missing\" if it exists in the expected schema but not in the actual schema.</li> <li>A \"type mismatch\" occurs if the data type of a field in the actual schema does not match   the expected data type.</li> <li>A field is considered \"nullable but expected non-nullable\" if it is nullable in the actual   schema but not nullable in the expected schema.</li> <li>An \"extra column\" is a field that exists in the actual schema but not in the expected schema.</li> </ul> Source code in <code>sumeh/services/utils.py</code> <pre><code>def __compare_schemas(\n    actual: List[SchemaDef],\n    expected: List[SchemaDef],\n) -&gt; Tuple[bool, List[Tuple[str, str]]]:\n    \"\"\"\n    Compare two lists of schema definitions and identify discrepancies.\n\n    Args:\n        actual (List[SchemaDef]): The list of actual schema definitions.\n        expected (List[SchemaDef]): The list of expected schema definitions.\n\n    Returns:\n        Tuple[bool, List[Tuple[str, str]]]: A tuple where the first element is a boolean indicating\n        whether the schemas match (True if they match, False otherwise), and the second element\n        is a list of tuples describing the discrepancies. Each tuple contains:\n            - The field name (str).\n            - A description of the discrepancy (str), such as \"missing\", \"type mismatch\",\n              \"nullable but expected non-nullable\", or \"extra column\".\n\n    Notes:\n        - A field is considered \"missing\" if it exists in the expected schema but not in the actual schema.\n        - A \"type mismatch\" occurs if the data type of a field in the actual schema does not match\n          the expected data type.\n        - A field is considered \"nullable but expected non-nullable\" if it is nullable in the actual\n          schema but not nullable in the expected schema.\n        - An \"extra column\" is a field that exists in the actual schema but not in the expected schema.\n    \"\"\"\n\n    exp_map = {c[\"field\"]: c for c in expected}\n    act_map = {c[\"field\"]: c for c in actual}\n\n    erros: List[Tuple[str, str]] = []\n\n    for fld, exp in exp_map.items():\n        if fld not in act_map:\n            erros.append((fld, \"missing\"))\n            continue\n        act = act_map[fld]\n        if act[\"data_type\"] != exp[\"data_type\"]:\n            erros.append(\n                (\n                    fld,\n                    f\"type mismatch (got {act['data_type']}, expected {exp['data_type']})\",\n                )\n            )\n\n        if act[\"nullable\"] and not exp[\"nullable\"]:\n            erros.append((fld, \"nullable but expected non-nullable\"))\n\n        if exp.get(\"max_length\") is not None:\n            pass\n\n    # 2. campos extras (se quiser)\n    extras = set(act_map) - set(exp_map)\n    for fld in extras:\n        erros.append((fld, \"extra column\"))\n\n    return len(erros) == 0, erros\n</code></pre>"},{"location":"api/services/services-utils/#sumeh.services.utils.__convert_value","title":"<code>__convert_value(value)</code>","text":"<p>Converts the provided value to the appropriate type (date, float, or int).</p> <p>Depending on the format of the input value, it will be converted to a datetime object, a floating-point number (float), or an integer (int).</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>The value to be converted, represented as a string.</p> required <p>Returns:</p> Type Description <p>Union[datetime, float, int]: The converted value, which can be a datetime object, float, or int.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the value does not match an expected format.</p> Source code in <code>sumeh/services/utils.py</code> <pre><code>def __convert_value(value):\n    \"\"\"\n    Converts the provided value to the appropriate type (date, float, or int).\n\n    Depending on the format of the input value, it will be converted to a datetime object,\n    a floating-point number (float), or an integer (int).\n\n    Args:\n        value (str): The value to be converted, represented as a string.\n\n    Returns:\n        Union[datetime, float, int]: The converted value, which can be a datetime object, float, or int.\n\n    Raises:\n        ValueError: If the value does not match an expected format.\n    \"\"\"\n    from datetime import datetime\n\n    value = value.strip()\n    try:\n        if \"-\" in value:\n            return datetime.strptime(value, \"%Y-%m-%d\")\n        else:\n            return datetime.strptime(value, \"%d/%m/%Y\")\n    except ValueError:\n        if \".\" in value:\n            return float(value)\n        return int(value)\n</code></pre>"},{"location":"api/services/services-utils/#sumeh.services.utils.__extract_params","title":"<code>__extract_params(rule)</code>","text":"Source code in <code>sumeh/services/utils.py</code> <pre><code>def __extract_params(rule: dict) -&gt; tuple:\n    rule_name = rule[\"check_type\"]\n    field = rule[\"field\"]\n    raw_value = rule.get(\"value\")\n    if isinstance(raw_value, str) and raw_value not in (None, \"\", \"NULL\"):\n        try:\n            value = __convert_value(raw_value)\n        except ValueError:\n            value = raw_value\n    else:\n        value = raw_value\n    value = value if value not in (None, \"\", \"NULL\") else \"\"\n    return field, rule_name, value\n</code></pre>"},{"location":"api/services/services-utils/#sumeh.services.utils.__parse_databricks_uri","title":"<code>__parse_databricks_uri(uri)</code>","text":"<p>Parses a Databricks URI into its catalog, schema, and table components.</p> <p>The URI is expected to follow the format <code>protocol://catalog.schema.table</code> or <code>protocol://schema.table</code>. If the catalog is not provided, it will be set to <code>None</code>. If the schema is not provided, the current database from the active Spark session will be used.</p> <p>Parameters:</p> Name Type Description Default <code>uri</code> <code>str</code> <p>The Databricks URI to parse.</p> required <p>Returns:</p> Type Description <code>Dict[str, Optional[str]]</code> <p>Dict[str, Optional[str]]: A dictionary containing the parsed components: - \"catalog\" (Optional[str]): The catalog name, or <code>None</code> if not provided. - \"schema\" (Optional[str]): The schema name, or the current database if not provided. - \"table\" (Optional[str]): The table name.</p> Source code in <code>sumeh/services/utils.py</code> <pre><code>def __parse_databricks_uri(uri: str) -&gt; Dict[str, Optional[str]]:\n    \"\"\"\n    Parses a Databricks URI into its catalog, schema, and table components.\n\n    The URI is expected to follow the format `protocol://catalog.schema.table` or\n    `protocol://schema.table`. If the catalog is not provided, it will be set to `None`.\n    If the schema is not provided, the current database from the active Spark session\n    will be used.\n\n    Args:\n        uri (str): The Databricks URI to parse.\n\n    Returns:\n        Dict[str, Optional[str]]: A dictionary containing the parsed components:\n            - \"catalog\" (Optional[str]): The catalog name, or `None` if not provided.\n            - \"schema\" (Optional[str]): The schema name, or the current database if not provided.\n            - \"table\" (Optional[str]): The table name.\n    \"\"\"\n    _, path = uri.split(\"://\", 1)\n    parts = path.split(\".\")\n    if len(parts) == 3:\n        catalog, schema, table = parts\n    elif len(parts) == 2:\n        catalog, schema, table = None, parts[0], parts[1]\n    else:\n        from pyspark.sql import SparkSession\n\n        spark = SparkSession.builder.getOrCreate()\n        catalog = None\n        schema = spark.catalog.currentDatabase()\n        table = parts[0]\n    return {\"catalog\": catalog, \"schema\": schema, \"table\": table}\n</code></pre>"},{"location":"api/services/services-utils/#sumeh.services.utils.__transform_date_format_in_pattern","title":"<code>__transform_date_format_in_pattern(date_format)</code>","text":"Source code in <code>sumeh/services/utils.py</code> <pre><code>def __transform_date_format_in_pattern(date_format):\n    date_patterns = {\n        \"DD\": \"(0[1-9]|[12][0-9]|3[01])\",\n        \"MM\": \"(0[1-9]|1[012])\",\n        \"YYYY\": \"(19|20)\\\\d\\\\d\",\n        \"YY\": \"\\\\d\\\\d\",\n        \" \": \"\\\\s\",\n        \".\": \"\\\\.\",\n    }\n\n    date_pattern = date_format\n    for single_format, pattern in date_patterns.items():\n        date_pattern = date_pattern.replace(single_format, pattern)\n\n    return date_pattern\n</code></pre>"},{"location":"api/services/services/","title":"Module <code>sumeh.services</code> - Overview","text":""},{"location":"api/services/services/#configuration-and-utilities-services-package-overview","title":"Configuration and Utilities Services Package \u2013 Overview","text":"<p>This package consolidates all the logic needed to load, parse, and infer both Data Quality rules (configurations) and schema metadata from multiple sources\u2014while also providing a lightweight web interface for initial interaction.</p> <p>\ud83d\udce6 File Structure</p> <pre><code>sumeh/services/\n\u251c\u2500\u2500 config.py       # \u21e8 loading and parsing of configurations and schemas\n\u251c\u2500\u2500 utils.py        # \u21e8 generic helper functions (conversion, comparison, extraction)\n\u2514\u2500\u2500 index.html      # \u21e8 static interface for configuration via browser\n</code></pre>"}]}