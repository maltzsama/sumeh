{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"home","text":""},{"location":"#sumeh-dq","title":"Sumeh DQ","text":"<p>Sumeh is a unified data quality validation framework supporting multiple backends (PySpark, Dask, Polars, DuckDB) with centralized rule configuration.</p>"},{"location":"#installation","title":"\ud83d\ude80 Installation","text":"<pre><code># Using pip\npip install sumeh\n\n# Or with conda-forge\nconda install -c conda-forge sumeh\n</code></pre> <p>Prerequisites: - Python 3.10+ - One or more of: <code>pyspark</code>, <code>dask[dataframe]</code>, <code>polars</code>, <code>duckdb</code>, <code>cuallee</code></p>"},{"location":"#core-api","title":"\ud83d\udd0d Core API","text":"<ul> <li><code>report(df, rules, name=\"Quality Check\")</code>   Apply your validation rules over any DataFrame (Pandas, Spark, Dask, Polars, or DuckDB).  </li> <li><code>validate(df, rules)</code> (per-engine)   Returns a DataFrame with a <code>dq_status</code> column listing violations.  </li> <li><code>summarize(qc_df, rules, total_rows)</code> (per-engine)   Consolidates violations into a summary report.</li> </ul>"},{"location":"#supported-engines","title":"\u2699\ufe0f Supported Engines","text":"<p>Each engine implements the <code>validate()</code> + <code>summarize()</code> pair:</p> Engine Module Status PySpark <code>sumeh.engine.pyspark_engine</code> \u2705 Fully implemented Dask <code>sumeh.engine.dask_engine</code> \u2705 Fully implemented Polars <code>sumeh.engine.polars_engine</code> \u2705 Fully implemented DuckDB <code>sumeh.engine.duckdb_engine</code> \u2705 Fully implemented Pandas <code>sumeh.engine.pandas_engine</code> \ud83d\udd27 Stub implementation BigQuery (SQL) <code>sumeh.engine.bigquery_engine</code> \ud83d\udd27 Stub implementation"},{"location":"#configuration-sources","title":"\ud83c\udfd7 Configuration Sources","text":"<p>Load rules from CSV, S3, MySQL, Postgres, BigQuery table, or AWS Glue:</p> <pre><code>from sumeh.services.config import (\n    get_config_from_csv,\n    get_config_from_s3,\n    get_config_from_mysql,\n    get_config_from_postgresql,\n    get_config_from_bigquery,\n    get_config_from_glue_data_catalog,\n)\n\nrules = get_config_from_csv(\"rules.csv\", delimiter=\";\")\n</code></pre>"},{"location":"#typical-workflow","title":"\ud83c\udfc3\u200d\u2642\ufe0f Typical Workflow","text":"<pre><code>from sumeh import report\nfrom sumeh.engine.polars_engine import validate, summarize\nimport polars as pl\n\n# 1) Load data\ndf = pl.read_csv(\"data.csv\")\n\n# 2) Run validation\nqc_df = validate(df, rules)\n\n# 3) Generate summary\ntotal = df.height\nreport = summarize(qc_df, rules, total)\nprint(report)\n</code></pre> <p>Or simply:</p> <pre><code>from sumeh import report\n\nreport = report(df, rules, name=\"My Check\")\n</code></pre>"},{"location":"#rule-definition-example","title":"\ud83d\udccb Rule Definition Example","text":"<pre><code>{\n  \"field\": \"customer_id\",\n  \"check_type\": \"is_complete\",\n  \"threshold\": 0.99,\n  \"value\": null,\n  \"execute\": true\n}\n</code></pre> <p>Supported Validation Rules</p> <p>The following data quality checks are available:</p> Test Description <code>is_positive</code> Filters rows where the specified column is less than zero. <code>is_negative</code> Filters rows where the specified column is greater than or equal to zero. <code>is_complete</code> Filters rows where the specified column is null. <code>validate_date_format</code> Filters rows where the specified column does not match the expected date format or is null. <code>is_future_date</code> Filters rows where the specified date column is after today\u2019s date. <code>is_past_date</code> Filters rows where the specified date column is before today\u2019s date. <code>is_date_between</code> Filters rows where the specified date column is not within the given start\u2013end range. <code>is_date_after</code> Filters rows where the specified date column is before the date provided in the rule. <code>is_date_before</code> Filters rows where the specified date column is after the date provided in the rule. <code>is_unique</code> Identifies rows with duplicate values in the specified column. <code>are_complete</code> Filters rows where any of the specified columns is null. <code>are_unique</code> Identifies rows with duplicate combinations of the specified columns. <code>is_greater_than</code> Filters rows where the specified column is less than or equal to the threshold value. <code>is_greater_or_equal_than</code> Filters rows where the specified column is less than the threshold value. <code>is_less_than</code> Filters rows where the specified column is greater than or equal to the threshold value. <code>is_less_or_equal_than</code> Filters rows where the specified column is greater than the threshold value. <code>is_equal</code> Filters rows where the specified column is not equal (null-safe) to the given value. <code>is_equal_than</code> Alias of <code>is_equal</code>. <code>is_contained_in</code> Filters rows where the specified column is not in the provided list of values. <code>not_contained_in</code> Filters rows where the specified column is in the provided list of values. <code>is_between</code> Filters rows where the specified column is not within the given numeric range. <code>has_pattern</code> Filters rows where the specified column does not match the given regular-expression pattern. <code>is_legit</code> Filters rows where the specified column is null or does not match a non-whitespace pattern (<code>\\S*</code>). <code>is_primary_key</code> Alias of <code>is_unique</code> (checks uniqueness of a single column). <code>is_composite_key</code> Alias of <code>are_unique</code> (checks uniqueness across multiple columns). <code>has_max</code> Filters rows where the specified column exceeds the maximum threshold. <code>has_min</code> Filters rows where the specified column is below the minimum threshold. <code>has_std</code> Returns all rows if the standard deviation of the specified column exceeds the threshold; otherwise empty. <code>has_mean</code> Returns all rows if the mean of the specified column exceeds the threshold; otherwise empty. <code>has_sum</code> Returns all rows if the sum of the specified column exceeds the threshold; otherwise empty. <code>has_cardinality</code> Returns all rows if the distinct count of the specified column exceeds the threshold; otherwise empty. <code>has_infogain</code> Uses distinct-count as a proxy for information gain; returns all rows if it exceeds the threshold; otherwise empty. <code>has_entropy</code> Uses distinct-count as a proxy for entropy; returns all rows if it exceeds the threshold; otherwise empty. <code>all_date_checks</code> Filters rows where the specified date column is before today\u2019s date (similar to <code>is_past_date</code>). <code>satisfies</code> Filters rows where the given SQL expression (via <code>expr(value)</code>) is not satisfied. <code>validate</code> Applies a list of named validation rules and returns aggregated and raw result DataFrames. <code>validate_schema</code> Compares the actual schema of a DataFrame against an expected schema and returns a match flag and errors."},{"location":"#supported-validation-rules","title":"Supported Validation Rules","text":"<p>Sumeh supports a wide variety of validation checks including: - Completeness checks (<code>is_complete</code>, <code>are_complete</code>) - Uniqueness checks (<code>is_unique</code>, <code>are_unique</code>, <code>is_primary_key</code>, <code>is_composite_key</code>) - Value comparisons (<code>is_greater_than</code>, <code>is_less_than</code>, <code>is_equal</code>, <code>is_between</code>) - Set operations (<code>is_contained_in</code>, <code>not_contained_in</code>) - Pattern matching (<code>has_pattern</code>) - Statistical checks (<code>has_min</code>, <code>has_max</code>, <code>has_mean</code>, <code>has_std</code>, <code>has_sum</code>) - Date validations (<code>is_today</code>, <code>is_yesterday</code>, <code>is_on_weekday</code>, etc.) - Custom expressions (<code>satisfies</code>)</p>"},{"location":"#project-layout","title":"\ud83d\udcc2 Project Layout","text":"<pre><code>sumeh/\n\u251c\u2500\u2500 poetry.lock\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 sumeh\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 cli.py\n\u2502   \u251c\u2500\u2500 core.py\n\u2502   \u251c\u2500\u2500 engine\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 bigquery_engine.py\n\u2502   \u2502   \u251c\u2500\u2500 dask_engine.py\n\u2502   \u2502   \u251c\u2500\u2500 duckdb_engine.py\n\u2502   \u2502   \u251c\u2500\u2500 polars_engine.py\n\u2502   \u2502   \u2514\u2500\u2500 pyspark_engine.py\n\u2502   \u2514\u2500\u2500 services\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u251c\u2500\u2500 config.py\n\u2502       \u251c\u2500\u2500 index.html\n\u2502       \u2514\u2500\u2500 utils.py\n\u2514\u2500\u2500 tests\n    \u251c\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 mock\n    \u2502   \u251c\u2500\u2500 config.csv\n    \u2502   \u2514\u2500\u2500 data.csv\n    \u251c\u2500\u2500 test_dask_engine.py\n    \u251c\u2500\u2500 test_duckdb_engine.py\n    \u251c\u2500\u2500 test_polars_engine.py\n    \u251c\u2500\u2500 test_pyspark_engine.py\n    \u2514\u2500\u2500 test_sumeh.py\n</code></pre>"},{"location":"#roadmap","title":"\ud83d\udcc8 Roadmap","text":"<ul> <li>[ ] Complete BigQuery engine implementation</li> <li>[ ] Complete Pandas engine implementation</li> <li>[ ] Enhanced documentation</li> <li>[ ] More validation rule types</li> <li>[ ] Performance optimizations</li> </ul>"},{"location":"#contributing","title":"\ud83e\udd1d Contributing","text":"<ol> <li>Fork &amp; create a feature branch  </li> <li>Implement new checks or engines, following existing signatures  </li> <li>Add tests under <code>tests/</code> </li> <li>Open a PR and ensure CI passes</li> </ol>"},{"location":"#license","title":"\ud83d\udcdc License","text":"<p>Licensed under the Apache License 2.0.</p>"},{"location":"quickstart/","title":"Quickstart \ud83d\ude80","text":"<p>A concise guide to get started with Sumeh\u2019s unified data quality framework.</p>"},{"location":"quickstart/#1-installation","title":"1. Installation \ud83d\udcbb","text":"<p>Install Sumeh via pip (recommended) or conda:</p> <pre><code>pip install sumeh\n# or\nconda install -c conda-forge sumeh\n</code></pre>"},{"location":"quickstart/#2-loading-rules-and-schema-configuration","title":"2. Loading Rules and Schema Configuration \u2699\ufe0f","text":"<p>Use <code>get_rules_config</code> and <code>get_schema_config</code> to fetch your validation rules and expected schema from various sources.</p> <pre><code>from sumeh import get_rules_config, get_schema_config\n\n# Load rules from CSV\nrules = get_rules_config(\"path/to/rules.csv\", delimiter=';')\n\n# Load expected schema from Glue Data Catalog\nschema = get_schema_config(\n    \"glue\",\n    catalog_name=\"my_catalog\",\n    database_name=\"my_db\",\n    table_name=\"my_table\"\n)\n</code></pre> <p>Supported rule/schema sources include:</p> <ul> <li><code>bigquery://project.dataset.table</code> \ud83c\udf10</li> <li><code>s3://bucket/path</code> \u2601\ufe0f</li> <li>Local CSV (<code>*.csv</code>) \ud83d\udcc4</li> <li>Relational (\"mysql\", \"postgresql\") via kwargs \ud83d\uddc4\ufe0f</li> <li>AWS Glue (<code>\"glue\"</code>) \ud83d\udd25</li> <li>DuckDB (<code>duckdb://db_path.table</code>) \ud83e\udd86</li> <li>Databricks (<code>databricks://catalog.schema.table</code>) \ud83d\udc8e</li> </ul>"},{"location":"quickstart/#3-schema-validation","title":"3. Schema Validation \ud83d\udcd0","text":"<p>Before validating data, ensure your DataFrame or connection matches the expected schema:</p> <pre><code>from sumeh import validate_schema\n\n# For a Spark DataFrame:\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\ndf = spark.read.parquet(\"data.parquet\")\n\nis_valid, errors = validate_schema(\n    df,\n    expected=schema,\n    engine=\"pyspark_engine\"\n)\nif not is_valid:\n    print(\"Schema mismatches:\", errors)\n</code></pre>"},{"location":"quickstart/#4-data-validation","title":"4. Data Validation \ud83d\udd0d","text":"<p>Apply your loaded rules to any supported DataFrame using <code>validate</code>:</p> <pre><code>from sumeh import validate\n\n# Example with Pandas:\nimport pandas as pd\ndf = pd.read_csv(\"data.csv\")\n\n# Validate (detects engine automatically)\nresult = validate(df, rules)\n# `result` structure depends on engine (e.g., CheckResult for cuallee engines)\n</code></pre>"},{"location":"quickstart/#5-summarization","title":"5. Summarization \ud83d\udcca","text":"<p>Generate a tabular summary of violations and pass rates with <code>summarize</code>:</p> <pre><code>from sumeh import summarize\n\n# For DataFrames requiring manual total_rows (e.g., Pandas):\ntotal = len(df)\nsummary_df = summarize(\n    df=result,       # could be validation output or raw DataFrame\n    rules=rules,\n    total_rows=total\n)\nprint(summary_df)\n</code></pre>"},{"location":"quickstart/#6-one-step-reporting","title":"6. One-Step Reporting \ud83d\udcdd","text":"<p>Use <code>report</code> for an end-to-end quality check and summary in one call:</p> <pre><code>from sumeh import report\n\nreport_df = report(\n    df,              # your DataFrame or connection\n    rules,\n    name=\"My Quality Check\"\n)\nprint(report_df)\n</code></pre> <p>For deeper customization and engine-specific options, explore the full API and examples in the Sumeh repository.</p>"},{"location":"api/cli/","title":"Module <code>sumeh.cli</code>","text":""},{"location":"api/cli/#sumeh.cli.serve_index","title":"<code>serve_index()</code>","text":"<p>Serves the index.html file for initial configuration and opens it in a browser.</p> <p>This function determines the path to the 'index.html' file, changes the working directory to the appropriate location, and starts a simple HTTP server to serve the file. It also automatically opens the served page in the default web browser.</p> <p>The server runs on localhost at port 8000. The process continues until interrupted by the user (via a KeyboardInterrupt), at which point the server shuts down.</p> <p>Raises:</p> Type Description <code>KeyboardInterrupt</code> <p>If the server is manually interrupted by the user.</p> Source code in <code>sumeh/cli.py</code> <pre><code>def serve_index():\n    \"\"\"\n    Serves the index.html file for initial configuration and opens it in a browser.\n\n    This function determines the path to the 'index.html' file, changes the\n    working directory to the appropriate location, and starts a simple HTTP server\n    to serve the file. It also automatically opens the served page in the default\n    web browser.\n\n    The server runs on localhost at port 8000. The process continues until\n    interrupted by the user (via a KeyboardInterrupt), at which point the server\n    shuts down.\n\n    Raises:\n        KeyboardInterrupt: If the server is manually interrupted by the user.\n    \"\"\"  # Determine the directory of the index.html file\n    base_dir = os.path.dirname(os.path.abspath(__file__))\n    html_path = os.path.join(base_dir, \"services\")\n\n    # Change the current working directory to the location of index.html\n    os.chdir(html_path)\n\n    # Define the port and URL\n    port = 8000\n    url = f\"http://localhost:{port}\"\n\n    # Serve the HTML file\n    with TCPServer((\"localhost\", port), SimpleHTTPRequestHandler) as httpd:\n        print(f\"Serving index.html at {url}\")\n\n        # Open the URL in the default web browser\n        webbrowser.open(url)\n\n        try:\n            httpd.serve_forever()\n        except KeyboardInterrupt:\n            print(\"\\nShutting down server.\")\n            httpd.server_close()\n</code></pre>"},{"location":"api/core/","title":"Module <code>sumeh.core</code>","text":"<p>This module provides a set of functions and utilities for data validation, schema  retrieval, and summarization. It supports multiple data sources and engines,  including BigQuery, S3, CSV files, MySQL, PostgreSQL, AWS Glue, DuckDB, and Databricks.</p> <p>Functions:</p> Name Description <code>get_rules_config</code> <p>str, **kwargs) -&gt; List[Dict[str, Any]]: Retrieves configuration rules based on the specified source.</p> <code>get_schema_config</code> <p>str, **kwargs) -&gt; List[Dict[str, Any]]: Retrieves the schema configuration based on the provided data source.</p> <code>__detect_engine</code> <code>validate_schema</code> <p>Any, expected: List[Dict[str, Any]], engine: str, **engine_kwargs) -&gt; Tuple[bool, List[Tuple[str, str]]]:</p> <code>validate</code> <code>summarize</code> <p>list[dict], **context):</p> <code>report</code> <p>list[dict], name: str = \"Quality Check\"):</p> Constants <p>_CONFIG_DISPATCH: A dictionary mapping data source types (e.g., \"mysql\", \"postgresql\")    to their respective configuration retrieval functions.</p> Imports <p>cuallee: Provides the <code>Check</code> and <code>CheckLevel</code> classes for data validation. warnings: Used to issue warnings for unknown rule names. importlib: Dynamically imports modules based on engine detection. typing: Provides type hints for function arguments and return values. re: Used for regular expression matching in source string parsing. sumeh.services.config: Contains functions for retrieving configurations and schemas    from various data sources. sumeh.services.utils: Provides utility functions for value conversion and URI parsing.</p> <p>The module uses Python's structural pattern matching (<code>match-case</code>) to handle  different data source types and validation rules. The <code>report</code> function supports a wide range of validation checks, including  completeness, uniqueness, value comparisons, patterns, and date-related checks. The <code>validate</code> and <code>summarize</code> functions dynamically detect the appropriate engine  based on the input DataFrame type and delegate the processing to the corresponding  engine module.</p>"},{"location":"api/core/#sumeh.core._CONFIG_DISPATCH","title":"<code>_CONFIG_DISPATCH = {'mysql': get_config_from_mysql, 'postgresql': get_config_from_postgresql}</code>  <code>module-attribute</code>","text":""},{"location":"api/core/#sumeh.core.__convert_value","title":"<code>__convert_value(value)</code>","text":"<p>Converts the provided value to the appropriate type (date, float, or int).</p> <p>Depending on the format of the input value, it will be converted to a datetime object, a floating-point number (float), or an integer (int).</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>The value to be converted, represented as a string.</p> required <p>Returns:</p> Type Description <p>Union[datetime, float, int]: The converted value, which can be a datetime object, float, or int.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the value does not match an expected format.</p> Source code in <code>sumeh/services/utils.py</code> <pre><code>def __convert_value(value):\n    \"\"\"\n    Converts the provided value to the appropriate type (date, float, or int).\n\n    Depending on the format of the input value, it will be converted to a datetime object,\n    a floating-point number (float), or an integer (int).\n\n    Args:\n        value (str): The value to be converted, represented as a string.\n\n    Returns:\n        Union[datetime, float, int]: The converted value, which can be a datetime object, float, or int.\n\n    Raises:\n        ValueError: If the value does not match an expected format.\n    \"\"\"\n    from datetime import datetime\n\n    value = value.strip()\n    try:\n        if \"-\" in value:\n            return datetime.strptime(value, \"%Y-%m-%d\")\n        else:\n            return datetime.strptime(value, \"%d/%m/%Y\")\n    except ValueError:\n        if \".\" in value:\n            return float(value)\n        return int(value)\n</code></pre>"},{"location":"api/core/#sumeh.core.__detect_engine","title":"<code>__detect_engine(df)</code>","text":"<p>Detects the engine type of the given DataFrame based on its module.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <p>The DataFrame object whose engine type is to be detected.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>A string representing the detected engine type. Possible values are: - \"pyspark_engine\" for PySpark DataFrames - \"dask_engine\" for Dask DataFrames - \"polars_engine\" for Polars DataFrames - \"pandas_engine\" for Pandas DataFrames - \"duckdb_engine\" for DuckDB or BigQuery DataFrames</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the DataFrame type is unsupported.</p> Source code in <code>sumeh/core.py</code> <pre><code>def __detect_engine(df):\n    \"\"\"\n    Detects the engine type of the given DataFrame based on its module.\n\n    Args:\n        df: The DataFrame object whose engine type is to be detected.\n\n    Returns:\n        str: A string representing the detected engine type. Possible values are:\n            - \"pyspark_engine\" for PySpark DataFrames\n            - \"dask_engine\" for Dask DataFrames\n            - \"polars_engine\" for Polars DataFrames\n            - \"pandas_engine\" for Pandas DataFrames\n            - \"duckdb_engine\" for DuckDB or BigQuery DataFrames\n\n    Raises:\n        TypeError: If the DataFrame type is unsupported.\n    \"\"\"\n    mod = df.__class__.__module__\n    match mod:\n        case m if m.startswith(\"pyspark\"):\n            return \"pyspark_engine\"\n        case m if m.startswith(\"dask\"):\n            return \"dask_engine\"\n        case m if m.startswith(\"polars\"):\n            return \"polars_engine\"\n        case m if m.startswith(\"pandas\"):\n            return \"pandas_engine\"\n        case m if m.startswith(\"duckdb\"):\n            return \"duckdb_engine\"\n        case m if m.startswith(\"bigquery\"):\n            return \"duckdb_engine\"\n        case _:\n            raise TypeError(f\"Unsupported DataFrame type: {type(df)}\")\n</code></pre>"},{"location":"api/core/#sumeh.core.__parse_databricks_uri","title":"<code>__parse_databricks_uri(uri)</code>","text":"<p>Parses a Databricks URI into its catalog, schema, and table components.</p> <p>The URI is expected to follow the format <code>protocol://catalog.schema.table</code> or  <code>protocol://schema.table</code>. If the catalog is not provided, it will be set to <code>None</code>.  If the schema is not provided, the current database from the active Spark session  will be used.</p> <p>Parameters:</p> Name Type Description Default <code>uri</code> <code>str</code> <p>The Databricks URI to parse.</p> required <p>Returns:</p> Type Description <code>Dict[str, Optional[str]]</code> <p>Dict[str, Optional[str]]: A dictionary containing the parsed components: - \"catalog\" (Optional[str]): The catalog name, or <code>None</code> if not provided. - \"schema\" (Optional[str]): The schema name, or the current database if not provided. - \"table\" (Optional[str]): The table name.</p> Source code in <code>sumeh/services/utils.py</code> <pre><code>def __parse_databricks_uri(uri: str) -&gt; Dict[str, Optional[str]]:\n    \"\"\"\n    Parses a Databricks URI into its catalog, schema, and table components.\n\n    The URI is expected to follow the format `protocol://catalog.schema.table` or \n    `protocol://schema.table`. If the catalog is not provided, it will be set to `None`. \n    If the schema is not provided, the current database from the active Spark session \n    will be used.\n\n    Args:\n        uri (str): The Databricks URI to parse.\n\n    Returns:\n        Dict[str, Optional[str]]: A dictionary containing the parsed components:\n            - \"catalog\" (Optional[str]): The catalog name, or `None` if not provided.\n            - \"schema\" (Optional[str]): The schema name, or the current database if not provided.\n            - \"table\" (Optional[str]): The table name.\n    \"\"\"\n    _, path = uri.split(\"://\", 1)\n    parts = path.split(\".\")\n    if len(parts) == 3:\n        catalog, schema, table = parts\n    elif len(parts) == 2:\n        catalog, schema, table = None, parts[0], parts[1]\n    else:\n        from pyspark.sql import SparkSession\n\n        spark = SparkSession.builder.getOrCreate()\n        catalog = None\n        schema = spark.catalog.currentDatabase()\n        table = parts[0]\n    return {\"catalog\": catalog, \"schema\": schema, \"table\": table}\n</code></pre>"},{"location":"api/core/#sumeh.core.get_config_from_bigquery","title":"<code>get_config_from_bigquery(project_id, dataset_id, table_id, credentials_path=None, query=None)</code>","text":"<p>Retrieves configuration data from a Google BigQuery table.</p> <p>Parameters:</p> Name Type Description Default <code>project_id</code> <code>str</code> <p>Google Cloud project ID.</p> required <code>dataset_id</code> <code>str</code> <p>BigQuery dataset ID.</p> required <code>table_id</code> <code>str</code> <p>BigQuery table ID.</p> required <code>credentials_path</code> <code>Optional[str]</code> <p>Path to service account credentials file (if not provided, defaults to default credentials).</p> <code>None</code> <code>query</code> <code>Optional[str]</code> <p>Custom SQL query to fetch data (if not provided, defaults to SELECT *).</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Dict[str, str]]</code> <p>List[Dict[str, str]]: A list of dictionaries representing the parsed configuration data.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If there is an error while querying BigQuery.</p> Source code in <code>sumeh/services/config.py</code> <pre><code>def get_config_from_bigquery(\n    project_id: str,\n    dataset_id: str,\n    table_id: str,\n    credentials_path: Optional[str] = None,\n    query: Optional[str] = None,\n) -&gt; List[Dict[str, str]]:\n    \"\"\"\n    Retrieves configuration data from a Google BigQuery table.\n\n    Args:\n        project_id (str): Google Cloud project ID.\n        dataset_id (str): BigQuery dataset ID.\n        table_id (str): BigQuery table ID.\n        credentials_path (Optional[str]): Path to service account credentials file (if not provided, defaults to default credentials).\n        query (Optional[str]): Custom SQL query to fetch data (if not provided, defaults to SELECT *).\n\n    Returns:\n        List[Dict[str, str]]: A list of dictionaries representing the parsed configuration data.\n\n    Raises:\n        RuntimeError: If there is an error while querying BigQuery.\n    \"\"\"\n    from google.cloud import bigquery\n    from google.auth.exceptions import DefaultCredentialsError\n\n    if query is None:\n        query = f\"SELECT * FROM `{project_id}.{dataset_id}.{table_id}`\"\n\n    try:\n        client = bigquery.Client(\n            project=project_id,\n            credentials=(\n                None\n                if credentials_path is None\n                else bigquery.Credentials.from_service_account_file(credentials_path)\n            ),\n        )\n\n        # Execute the query and convert the result to a pandas DataFrame\n        data = client.query(query).to_dataframe()\n\n        # Convert the DataFrame to a list of dictionaries\n        data_dict = data.to_dict(orient=\"records\")\n\n        # Parse the data and return the result\n        return __parse_data(data_dict)\n\n    except DefaultCredentialsError as e:\n        raise RuntimeError(f\"Credentials error: {e}\") from e\n\n    except Exception as e:\n        raise RuntimeError(f\"Error occurred while querying BigQuery: {e}\") from e\n</code></pre>"},{"location":"api/core/#sumeh.core.get_config_from_csv","title":"<code>get_config_from_csv(file_path, delimiter=',')</code>","text":"<p>Retrieves configuration data from a CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The local file path to the CSV file.</p> required <code>delimiter</code> <code>Optional[str]</code> <p>The delimiter used in the CSV file (default is \",\").</p> <code>','</code> <p>Returns:</p> Type Description <code>List[Dict[str, str]]</code> <p>List[Dict[str, str]]: A list of dictionaries representing the parsed configuration data.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If there is an error reading or processing the file.</p> Source code in <code>sumeh/services/config.py</code> <pre><code>def get_config_from_csv(\n    file_path: str, delimiter: Optional[str] = \",\"\n) -&gt; List[Dict[str, str]]:\n    \"\"\"\n    Retrieves configuration data from a CSV file.\n\n    Args:\n        file_path (str): The local file path to the CSV file.\n        delimiter (Optional[str]): The delimiter used in the CSV file (default is \",\").\n\n    Returns:\n        List[Dict[str, str]]: A list of dictionaries representing the parsed configuration data.\n\n    Raises:\n        RuntimeError: If there is an error reading or processing the file.\n    \"\"\"\n    try:\n        file_content = __read_local_file(file_path)\n        result = __read_csv_file(file_content, delimiter)\n\n        return __parse_data(result)\n\n    except FileNotFoundError as e:\n        raise RuntimeError(f\"File '{file_path}' not found. Error: {e}\") from e\n\n    except ValueError as e:\n        raise ValueError(\n            f\"Error while parsing CSV file '{file_path}'. Error: {e}\"\n        ) from e\n\n    except Exception as e:\n        # Catch any unexpected exceptions\n        raise RuntimeError(\n            f\"Unexpected error while processing CSV file '{file_path}'. Error: {e}\"\n        ) from e\n</code></pre>"},{"location":"api/core/#sumeh.core.get_config_from_databricks","title":"<code>get_config_from_databricks(catalog, schema, table, **kwargs)</code>","text":"<p>Retrieves configuration data from a Databricks table and returns it as a list of dictionaries.</p> <p>Parameters:</p> Name Type Description Default <code>catalog</code> <code>Optional[str]</code> <p>The catalog name in Databricks. If provided, it will be included in the table's full path.</p> required <code>schema</code> <code>Optional[str]</code> <p>The schema name in Databricks. If provided, it will be included in the table's full path.</p> required <code>table</code> <code>str</code> <p>The name of the table to retrieve data from.</p> required <code>**kwargs</code> <p>Additional keyword arguments (currently unused).</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List[Dict[str, Any]]: A list of dictionaries, where each dictionary represents a row of data from the table.</p> Source code in <code>sumeh/services/config.py</code> <pre><code>def get_config_from_databricks(catalog: Optional[str], schema: Optional[str], table: str, **kwargs) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Retrieves configuration data from a Databricks table and returns it as a list of dictionaries.\n\n    Args:\n        catalog (Optional[str]): The catalog name in Databricks. If provided, it will be included in the table's full path.\n        schema (Optional[str]): The schema name in Databricks. If provided, it will be included in the table's full path.\n        table (str): The name of the table to retrieve data from.\n        **kwargs: Additional keyword arguments (currently unused).\n\n    Returns:\n        List[Dict[str, Any]]: A list of dictionaries, where each dictionary represents a row of data from the table.\n    \"\"\"\n    from pyspark.sql import SparkSession\n\n    spark = SparkSession.builder.getOrCreate()\n    if catalog and schema:\n        full = f\"{catalog}.{schema}.{table}\"\n    elif schema:\n        full = f\"{schema}.{table}\"\n    else:\n        full = table\n    if 'query' in kwargs.keys():\n        df = spark.sql(f\"select * from {full} where {kwargs['query']}\")\n    else:\n        df = spark.table(full)\n    return [row.asDict() for row in df.collect()]\n</code></pre>"},{"location":"api/core/#sumeh.core.get_config_from_duckdb","title":"<code>get_config_from_duckdb(db_path, table=None, query=None, conn=None)</code>","text":"<p>Retrieve configuration data from a DuckDB database.</p> <p>This function fetches data from a DuckDB database either by executing a custom SQL query or by selecting all rows from a specified table. The data is then parsed into a list of dictionaries.</p> <p>Parameters:</p> Name Type Description Default <code>db_path</code> <code>str</code> <p>The path to the DuckDB database file.</p> required <code>table</code> <code>str</code> <p>The name of the table to fetch data from. Defaults to None.</p> <code>None</code> <code>query</code> <code>str</code> <p>A custom SQL query to execute. Defaults to None.</p> <code>None</code> <code>conn</code> <p>A valid DuckDB connection object.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List[Dict[str, Any]]: A list of dictionaries representing the fetched data.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If neither <code>table</code> nor <code>query</code> is provided, or if a valid <code>conn</code> is not supplied.</p> Example <p>import duckdb conn = duckdb.connect('my_db.duckdb') config = get_config_from_duckdb('my_db.duckdb', table='rules', conn=conn)</p> Source code in <code>sumeh/services/config.py</code> <pre><code>def get_config_from_duckdb(db_path: str, table: str = None, query: str = None, conn=None) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Retrieve configuration data from a DuckDB database.\n\n    This function fetches data from a DuckDB database either by executing a custom SQL query\n    or by selecting all rows from a specified table. The data is then parsed into a list of\n    dictionaries.\n\n    Args:\n        db_path (str): The path to the DuckDB database file.\n        table (str, optional): The name of the table to fetch data from. Defaults to None.\n        query (str, optional): A custom SQL query to execute. Defaults to None.\n        conn: A valid DuckDB connection object.\n\n    Returns:\n        List[Dict[str, Any]]: A list of dictionaries representing the fetched data.\n\n    Raises:\n        ValueError: If neither `table` nor `query` is provided, or if a valid `conn` is not supplied.\n\n    Example:\n        &gt;&gt;&gt; import duckdb\n        &gt;&gt;&gt; conn = duckdb.connect('my_db.duckdb')\n        &gt;&gt;&gt; config = get_config_from_duckdb('my_db.duckdb', table='rules', conn=conn)\n    \"\"\"\n\n    if query:\n        df = conn.execute(query).fetchdf()\n    elif table:\n        df = conn.execute(f\"SELECT * FROM {table}\").fetchdf()\n    else:\n        raise ValueError(\n            \"DuckDB configuration requires:\\n\"\n            \"1. Either a `table` name or custom `query`\\n\"\n            \"2. A valid database `conn` connection object\\n\"\n            \"Example: get_config('duckdb', table='rules', conn=duckdb.connect('my_db.duckdb'))\"\n        )\n\n    return __parse_data(df.to_dict(orient=\"records\"))\n</code></pre>"},{"location":"api/core/#sumeh.core.get_config_from_glue_data_catalog","title":"<code>get_config_from_glue_data_catalog(glue_context, database_name, table_name, query=None)</code>","text":"<p>Retrieves configuration data from AWS Glue Data Catalog.</p> <p>Parameters:</p> Name Type Description Default <code>glue_context</code> <p>An instance of <code>GlueContext</code>.</p> required <code>database_name</code> <code>str</code> <p>Glue database name.</p> required <code>table_name</code> <code>str</code> <p>Glue table name.</p> required <code>query</code> <code>Optional[str]</code> <p>Custom SQL query to fetch data (if provided).</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Dict[str, str]]</code> <p>List[Dict[str, str]]: A list of dictionaries representing the parsed configuration data.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If there is an error querying Glue Data Catalog.</p> Source code in <code>sumeh/services/config.py</code> <pre><code>def get_config_from_glue_data_catalog(\n    glue_context, database_name: str, table_name: str, query: Optional[str] = None\n) -&gt; List[Dict[str, str]]:\n    \"\"\"\n    Retrieves configuration data from AWS Glue Data Catalog.\n\n    Args:\n        glue_context: An instance of `GlueContext`.\n        database_name (str): Glue database name.\n        table_name (str): Glue table name.\n        query (Optional[str]): Custom SQL query to fetch data (if provided).\n\n    Returns:\n        List[Dict[str, str]]: A list of dictionaries representing the parsed configuration data.\n\n    Raises:\n        RuntimeError: If there is an error querying Glue Data Catalog.\n    \"\"\"\n    from awsglue.context import GlueContext\n\n    if not isinstance(glue_context, GlueContext):\n        raise ValueError(\"The provided context is not a valid GlueContext.\")\n\n    spark = glue_context.spark_session\n\n    try:\n        dynamic_frame = glue_context.create_dynamic_frame.from_catalog(\n            database=database_name, table_name=table_name\n        )\n\n        data_frame = dynamic_frame.toDF()\n\n        if query:\n            data_frame.createOrReplaceTempView(\"table_name\")\n            data_frame = spark.sql(query)\n\n        data_dict = [row.asDict() for row in data_frame.collect()]\n\n        return __parse_data(data_dict)\n\n    except Exception as e:\n        raise RuntimeError(\n            f\"Error occurred while querying Glue Data Catalog: {e}\"\n        ) from e\n</code></pre>"},{"location":"api/core/#sumeh.core.get_config_from_mysql","title":"<code>get_config_from_mysql(connection=None, host=None, user=None, password=None, database=None, port=3306, schema=None, table=None, query=None)</code>","text":"<p>Retrieves configuration data from a MySQL database.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Optional</code> <p>An existing MySQL connection object.</p> <code>None</code> <code>host</code> <code>Optional[str]</code> <p>Host of the MySQL server.</p> <code>None</code> <code>user</code> <code>Optional[str]</code> <p>Username to connect to MySQL.</p> <code>None</code> <code>password</code> <code>Optional[str]</code> <p>Password for the MySQL user.</p> <code>None</code> <code>database</code> <code>Optional[str]</code> <p>Database name to query.</p> <code>None</code> <code>port</code> <code>Optional[int]</code> <p>The port for the MySQL connection (default is 3306).</p> <code>3306</code> <code>schema</code> <code>Optional[str]</code> <p>Schema name if query is not provided.</p> <code>None</code> <code>table</code> <code>Optional[str]</code> <p>Table name if query is not provided.</p> <code>None</code> <code>query</code> <code>Optional[str]</code> <p>Custom SQL query to fetch data (if not provided, <code>schema</code> and <code>table</code> must be given).</p> <code>None</code> <p>Returns:</p> Type Description <p>List[Dict[str, Any]]: A list of dictionaries representing the parsed configuration data.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If neither <code>query</code> nor both <code>schema</code> and <code>table</code> are provided.</p> <code>ConnectionError</code> <p>If there is an error connecting to MySQL.</p> <code>RuntimeError</code> <p>If there is an error executing the query or processing the data.</p> Source code in <code>sumeh/services/config.py</code> <pre><code>def get_config_from_mysql(\n    connection: Optional = None,\n    host: Optional[str] = None,\n    user: Optional[str] = None,\n    password: Optional[str] = None,\n    database: Optional[str] = None,\n    port: Optional[int] = 3306,\n    schema: Optional[str] = None,\n    table: Optional[str] = None,\n    query: Optional[str] = None,\n):\n    \"\"\"\n    Retrieves configuration data from a MySQL database.\n\n    Args:\n        connection (Optional): An existing MySQL connection object.\n        host (Optional[str]): Host of the MySQL server.\n        user (Optional[str]): Username to connect to MySQL.\n        password (Optional[str]): Password for the MySQL user.\n        database (Optional[str]): Database name to query.\n        port (Optional[int]): The port for the MySQL connection (default is 3306).\n        schema (Optional[str]): Schema name if query is not provided.\n        table (Optional[str]): Table name if query is not provided.\n        query (Optional[str]): Custom SQL query to fetch data (if not provided, `schema` and `table` must be given).\n\n    Returns:\n        List[Dict[str, Any]]: A list of dictionaries representing the parsed configuration data.\n\n    Raises:\n        ValueError: If neither `query` nor both `schema` and `table` are provided.\n        ConnectionError: If there is an error connecting to MySQL.\n        RuntimeError: If there is an error executing the query or processing the data.\n    \"\"\"\n    import mysql.connector\n    import pandas as pd\n\n    if query is None and (schema is None or table is None):\n        raise ValueError(\n            \"You must provide either a 'query' or both 'schema' and 'table'.\"\n        )\n\n    if query is None:\n        query = f\"SELECT * FROM {schema}.{table}\"\n\n    try:\n        connection = connection or __create_connection(\n            mysql.connector.connect, host, user, password, database, port\n        )\n        data = pd.read_sql(query, connection)\n        data_dict = data.to_dict(orient=\"records\")\n        return __parse_data(data_dict)\n\n    except mysql.connector.Error as e:\n        raise ConnectionError(f\"Error connecting to MySQL database: {e}\")\n\n    except Exception as e:\n        raise RuntimeError(f\"Error executing the query or processing data: {e}\")\n\n    finally:\n        if connection and host is not None:\n            connection.close()\n</code></pre>"},{"location":"api/core/#sumeh.core.get_config_from_postgresql","title":"<code>get_config_from_postgresql(connection=None, host=None, user=None, password=None, database=None, port=5432, schema=None, table=None, query=None)</code>","text":"<p>Retrieves configuration data from a PostgreSQL database.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Optional</code> <p>An existing PostgreSQL connection object.</p> <code>None</code> <code>host</code> <code>Optional[str]</code> <p>Host of the PostgreSQL server.</p> <code>None</code> <code>user</code> <code>Optional[str]</code> <p>Username to connect to PostgreSQL.</p> <code>None</code> <code>password</code> <code>Optional[str]</code> <p>Password for the PostgreSQL user.</p> <code>None</code> <code>database</code> <code>Optional[str]</code> <p>Database name to query.</p> <code>None</code> <code>port</code> <code>Optional[int]</code> <p>The port for the PostgreSQL connection (default is 5432).</p> <code>5432</code> <code>schema</code> <code>Optional[str]</code> <p>Schema name if query is not provided.</p> <code>None</code> <code>table</code> <code>Optional[str]</code> <p>Table name if query is not provided.</p> <code>None</code> <code>query</code> <code>Optional[str]</code> <p>Custom SQL query to fetch data (if not provided, <code>schema</code> and <code>table</code> must be given).</p> <code>None</code> <p>Returns:</p> Type Description <code>list[dict]</code> <p>List[Dict[str, Any]]: A list of dictionaries representing the parsed configuration data.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If neither <code>query</code> nor both <code>schema</code> and <code>table</code> are provided.</p> <code>ConnectionError</code> <p>If there is an error connecting to PostgreSQL.</p> <code>RuntimeError</code> <p>If there is an error executing the query or processing the data.</p> Source code in <code>sumeh/services/config.py</code> <pre><code>def get_config_from_postgresql(\n    connection: Optional = None,\n    host: Optional[str] = None,\n    user: Optional[str] = None,\n    password: Optional[str] = None,\n    database: Optional[str] = None,\n    port: Optional[int] = 5432,\n    schema: Optional[str] = None,\n    table: Optional[str] = None,\n    query: Optional[str] = None,\n) -&gt; list[dict]:\n    \"\"\"\n    Retrieves configuration data from a PostgreSQL database.\n\n    Args:\n        connection (Optional): An existing PostgreSQL connection object.\n        host (Optional[str]): Host of the PostgreSQL server.\n        user (Optional[str]): Username to connect to PostgreSQL.\n        password (Optional[str]): Password for the PostgreSQL user.\n        database (Optional[str]): Database name to query.\n        port (Optional[int]): The port for the PostgreSQL connection (default is 5432).\n        schema (Optional[str]): Schema name if query is not provided.\n        table (Optional[str]): Table name if query is not provided.\n        query (Optional[str]): Custom SQL query to fetch data (if not provided, `schema` and `table` must be given).\n\n    Returns:\n        List[Dict[str, Any]]: A list of dictionaries representing the parsed configuration data.\n\n    Raises:\n        ValueError: If neither `query` nor both `schema` and `table` are provided.\n        ConnectionError: If there is an error connecting to PostgreSQL.\n        RuntimeError: If there is an error executing the query or processing the data.\n    \"\"\"\n    import psycopg2\n    import pandas as pd\n\n    if query is None and (schema is None or table is None):\n        raise ValueError(\n            \"You must provide either a 'query' or both 'schema' and 'table'.\"\n        )\n\n    if query is None:\n        query = f\"SELECT * FROM {schema}.{table}\"\n\n    try:\n        connection = connection or __create_connection(\n            psycopg2.connect, host, user, password, database, port\n        )\n\n        data = pd.read_sql(query, connection)\n\n        data_dict = data.to_dict(orient=\"records\")\n        return __parse_data(data_dict)\n\n    except psycopg2.Error as e:\n        raise ConnectionError(f\"Error connecting to PostgreSQL database: {e}\")\n\n    except Exception as e:\n        raise RuntimeError(f\"Error executing the query or processing data: {e}\")\n\n    finally:\n        if connection and host is not None:\n            connection.close()\n</code></pre>"},{"location":"api/core/#sumeh.core.get_config_from_s3","title":"<code>get_config_from_s3(s3_path, delimiter=',')</code>","text":"<p>Retrieves configuration data from a CSV file stored in an S3 bucket.</p> <p>Parameters:</p> Name Type Description Default <code>s3_path</code> <code>str</code> <p>The S3 path to the CSV file.</p> required <code>delimiter</code> <code>Optional[str]</code> <p>The delimiter used in the CSV file (default is \",\").</p> <code>','</code> <p>Returns:</p> Type Description <p>List[Dict[str, Any]]: A list of dictionaries representing the parsed configuration data.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If there is an error reading or processing the S3 file.</p> Source code in <code>sumeh/services/config.py</code> <pre><code>def get_config_from_s3(s3_path: str, delimiter: Optional[str] = \",\"):\n    \"\"\"\n    Retrieves configuration data from a CSV file stored in an S3 bucket.\n\n    Args:\n        s3_path (str): The S3 path to the CSV file.\n        delimiter (Optional[str]): The delimiter used in the CSV file (default is \",\").\n\n    Returns:\n        List[Dict[str, Any]]: A list of dictionaries representing the parsed configuration data.\n\n    Raises:\n        RuntimeError: If there is an error reading or processing the S3 file.\n    \"\"\"\n    try:\n        file_content = __read_s3_file(s3_path)\n        data = __read_csv_file(file_content, delimiter)\n        return __parse_data(data)\n\n    except Exception as e:\n        raise RuntimeError(f\"Error reading or processing the S3 file: {e}\")\n</code></pre>"},{"location":"api/core/#sumeh.core.get_rules_config","title":"<code>get_rules_config(source, **kwargs)</code>","text":"<p>Retrieve configuration rules based on the specified source.</p> <p>Dispatches to the appropriate loader according to the format of <code>source</code>, returning a list of parsed rule dictionaries.</p> Supported sources <ul> <li><code>bigquery://&lt;project&gt;.&lt;dataset&gt;.&lt;table&gt;</code></li> <li><code>s3://&lt;bucket&gt;/&lt;path&gt;</code></li> <li><code>&lt;file&gt;.csv</code></li> <li><code>\"mysql\"</code> or <code>\"postgresql\"</code> (requires host/user/etc. in kwargs)</li> <li><code>\"glue\"</code> (AWS Glue Data Catalog)</li> <li><code>duckdb://&lt;db_path&gt;.&lt;table&gt;</code></li> <li><code>databricks://&lt;catalog&gt;.&lt;schema&gt;.&lt;table&gt;</code></li> </ul> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>Identifier of the rules configuration location. Determines which handler is invoked.</p> required <code>**kwargs</code> <p>Loader-specific parameters (e.g. <code>host</code>, <code>user</code>, <code>password</code>, <code>connection</code>, <code>query</code>).</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List[Dict[str, Any]]: A list of dictionaries, each representing a validation rule with keys like <code>\"field\"</code>, <code>\"check_type\"</code>, <code>\"value\"</code>, <code>\"threshold\"</code>, and <code>\"execute\"</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>source</code> does not match any supported format.</p> Source code in <code>sumeh/core.py</code> <pre><code>def get_rules_config(source: str, **kwargs) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Retrieve configuration rules based on the specified source.\n\n    Dispatches to the appropriate loader according to the format of `source`,\n    returning a list of parsed rule dictionaries.\n\n    Supported sources:\n      - `bigquery://&lt;project&gt;.&lt;dataset&gt;.&lt;table&gt;`\n      - `s3://&lt;bucket&gt;/&lt;path&gt;`\n      - `&lt;file&gt;.csv`\n      - `\"mysql\"` or `\"postgresql\"` (requires host/user/etc. in kwargs)\n      - `\"glue\"` (AWS Glue Data Catalog)\n      - `duckdb://&lt;db_path&gt;.&lt;table&gt;`\n      - `databricks://&lt;catalog&gt;.&lt;schema&gt;.&lt;table&gt;`\n\n    Args:\n        source (str):\n            Identifier of the rules configuration location. Determines which\n            handler is invoked.\n        **kwargs:\n            Loader-specific parameters (e.g. `host`, `user`, `password`,\n            `connection`, `query`).\n\n    Returns:\n        List[Dict[str, Any]]:\n            A list of dictionaries, each representing a validation rule with keys\n            like `\"field\"`, `\"check_type\"`, `\"value\"`, `\"threshold\"`, and `\"execute\"`.\n\n    Raises:\n        ValueError:\n            If `source` does not match any supported format.\n    \"\"\"\n    match source:\n        case s if s.startswith(\"bigquery://\"):\n            _, path = s.split(\"://\", 1)\n            project, dataset, table = path.split(\".\")\n            return get_config_from_bigquery(\n                project_id=project,\n                dataset_id=dataset,\n                table_id=table,\n                **kwargs,\n            )\n\n        case s if s.startswith(\"s3://\"):\n            return get_config_from_s3(s, **kwargs)\n\n        case s if re.search(r\"\\.csv$\", s, re.IGNORECASE):\n            return get_config_from_csv(s, **kwargs)\n\n        case \"mysql\" | \"postgresql\" as driver:\n            loader = _CONFIG_DISPATCH[driver]\n            return loader(**kwargs)\n\n        case \"glue\":\n            return get_config_from_glue_data_catalog(**kwargs)\n\n        case s if s.startswith(\"duckdb://\"):\n            _, path = s.split(\"://\", 1)\n            db_path, table = path.rsplit(\".\", 1)\n            conn = kwargs.pop(\"conn\", None)\n            return get_config_from_duckdb(\n                conn=conn,\n                table=table,\n            )\n\n        case s if s.startswith(\"databricks://\"):\n            parts = __parse_databricks_uri(s)\n            return get_config_from_databricks(\n                catalog=parts[\"catalog\"],\n                schema=parts[\"schema\"],\n                table=parts[\"table\"],\n                **kwargs,\n            )\n\n        case _:\n            raise ValueError(f\"Unknow source: {source}\")\n</code></pre>"},{"location":"api/core/#sumeh.core.get_schema_config","title":"<code>get_schema_config(source, **kwargs)</code>","text":"<p>Retrieve the schema configuration based on the provided data source.</p> <p>This function determines the appropriate method to extract schema information based on the format or type of the <code>source</code> string. It supports various data sources such as BigQuery, S3, CSV files, MySQL, PostgreSQL, AWS Glue, DuckDB, and Databricks.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>A string representing the data source. The format of the string determines the method used to retrieve the schema. Supported formats include: <code>bigquery://&lt;project&gt;.&lt;dataset&gt;.&lt;table&gt;</code>, <code>s3://&lt;bucket&gt;/&lt;path&gt;</code>,  <code>&lt;file&gt;.csv</code>, <code>mysql</code>, <code>postgresql</code>, <code>glue</code>, <code>duckdb://&lt;db_path&gt;.&lt;table&gt;</code>,  <code>databricks://&lt;catalog&gt;.&lt;schema&gt;.&lt;table&gt;</code></p> required <code>**kwargs</code> <p>Additional keyword arguments required by specific schema retrieval methods. For example: For DuckDB: <code>conn</code> (a database connection object). For other sources: Additional parameters specific to the source.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List[Dict[str, Any]]: A list of dictionaries representing the schema</p> <code>List[Dict[str, Any]]</code> <p>configuration. Each dictionary contains details about a column in the</p> <code>List[Dict[str, Any]]</code> <p>schema.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the <code>source</code> string does not match any supported format.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; get_schema_config(\"bigquery://my_project.my_dataset.my_table\")\n&gt;&gt;&gt; get_schema_config(\"s3://my_bucket/my_file.csv\")\n&gt;&gt;&gt; get_schema_config(\"mysql\", host=\"localhost\", user=\"root\", password=\"password\")\n</code></pre> Source code in <code>sumeh/core.py</code> <pre><code>def get_schema_config(source: str, **kwargs) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Retrieve the schema configuration based on the provided data source.\n\n    This function determines the appropriate method to extract schema information\n    based on the format or type of the `source` string. It supports various data\n    sources such as BigQuery, S3, CSV files, MySQL, PostgreSQL, AWS Glue, DuckDB,\n    and Databricks.\n\n    Args:\n        source (str): \n            A string representing the data source. The format of the\n            string determines the method used to retrieve the schema. Supported\n            formats include: `bigquery://&lt;project&gt;.&lt;dataset&gt;.&lt;table&gt;`, `s3://&lt;bucket&gt;/&lt;path&gt;`, \n            `&lt;file&gt;.csv`, `mysql`, `postgresql`, `glue`, `duckdb://&lt;db_path&gt;.&lt;table&gt;`, \n            `databricks://&lt;catalog&gt;.&lt;schema&gt;.&lt;table&gt;`\n        **kwargs: Additional keyword arguments required by specific schema\n            retrieval methods. For example:\n            For DuckDB: `conn` (a database connection object).\n            For other sources: Additional parameters specific to the source.\n\n    Returns:\n        List[Dict[str, Any]]: A list of dictionaries representing the schema\n        configuration. Each dictionary contains details about a column in the\n        schema.\n\n    Raises:\n        ValueError: If the `source` string does not match any supported format.\n\n    Examples:\n        &gt;&gt;&gt; get_schema_config(\"bigquery://my_project.my_dataset.my_table\")\n        &gt;&gt;&gt; get_schema_config(\"s3://my_bucket/my_file.csv\")\n        &gt;&gt;&gt; get_schema_config(\"mysql\", host=\"localhost\", user=\"root\", password=\"password\")\n    \"\"\"\n    match source:\n        case s if s.startswith(\"bigquery://\"):\n            _, path = s.split(\"://\", 1)\n            project, dataset, table = path.split(\".\")\n            return get_schema_from_bigquery(\n                project_id=project,\n                dataset_id=dataset,\n                table_id=table,\n                **kwargs,\n            )\n\n        case s if s.startswith(\"s3://\"):\n            return get_schema_from_s3(s, **kwargs)\n\n        case s if re.search(r\"\\.csv$\", s, re.IGNORECASE):\n            return get_schema_from_csv(s, **kwargs)\n\n        case \"mysql\":\n            return get_schema_from_mysql(**kwargs)\n\n        case \"postgresql\":\n            return get_schema_from_postgresql(**kwargs)\n\n        case \"glue\":\n            return get_schema_from_glue(**kwargs)\n\n        case s if s.startswith(\"duckdb://\"):\n            conn = kwargs.pop(\"conn\")\n            _, path = s.split(\"://\", 1)\n            db_path, table = path.rsplit(\".\", 1)\n            return get_schema_from_duckdb(conn=conn, table=table)\n\n        case s if s.startswith(\"databricks://\"):\n            parts = __parse_databricks_uri(s)\n            return get_schema_from_databricks(\n                catalog=parts[\"catalog\"],\n                schema=parts[\"schema\"],\n                table=parts[\"table\"],\n                **kwargs,\n            )\n\n        case _:\n            raise ValueError(f\"Unknow source: {source}\")\n</code></pre>"},{"location":"api/core/#sumeh.core.get_schema_from_bigquery","title":"<code>get_schema_from_bigquery(project_id, dataset_id, table_id, credentials_path=None)</code>","text":"Source code in <code>sumeh/services/config.py</code> <pre><code>def get_schema_from_bigquery(\n    project_id: str, dataset_id: str, table_id: str, credentials_path: str = None\n) -&gt; List[Dict[str, Any]]:\n    from google.cloud import bigquery\n\n    client = bigquery.Client(\n        project=project_id,\n        credentials=(\n            None\n            if credentials_path is None\n            else bigquery.Credentials.from_service_account_file(credentials_path)\n        ),\n    )\n    table = client.get_table(f\"{project_id}.{dataset_id}.{table_id}\")\n    return [\n        {\n            \"field\": schema_field.name,\n            \"data_type\": schema_field.field_type.lower(),\n            \"nullable\": schema_field.is_nullable,\n            \"max_length\": None,\n        }\n        for schema_field in table.schema\n    ]\n</code></pre>"},{"location":"api/core/#sumeh.core.get_schema_from_csv","title":"<code>get_schema_from_csv(file_path, delimiter=',', sample_size=1000)</code>","text":"Source code in <code>sumeh/services/config.py</code> <pre><code>def get_schema_from_csv(\n    file_path: str, delimiter: str = \",\", sample_size: int = 1_000\n) -&gt; List[Dict[str, Any]]:\n    import csv\n\n    cols: Dict[str, Dict[str, Any]] = {}\n    with open(file_path, newline=\"\", encoding=\"utf-8\") as f:\n        reader = csv.DictReader(f, delimiter=delimiter)\n        for idx, row in enumerate(reader):\n            if idx &gt;= sample_size:\n                break\n            for name, raw in row.items():\n                info = cols.setdefault(\n                    name,\n                    {\n                        \"field\": name,\n                        \"nullable\": False,\n                        \"max_length\": 0,\n                        \"type_hints\": set(),\n                    },\n                )\n                if raw == \"\" or raw is None:\n                    info[\"nullable\"] = True\n                    continue\n                info[\"max_length\"] = max(info[\"max_length\"], len(raw))\n                info[\"type_hints\"].add(infer_basic_type(raw))\n\n    out: List[Dict[str, Any]] = []\n    for info in cols.values():\n        hints = info[\"type_hints\"]\n        if hints == {\"integer\"}:\n            dtype = \"integer\"\n        elif hints &lt;= {\"integer\", \"float\"}:\n            dtype = \"float\"\n        elif hints == {\"date\"}:\n            dtype = \"date\"\n        else:\n            dtype = \"string\"\n        out.append(\n            {\n                \"field\": info[\"field\"],\n                \"data_type\": dtype,\n                \"nullable\": info[\"nullable\"],\n                \"max_length\": info[\"max_length\"] or None,\n            }\n        )\n    return out\n</code></pre>"},{"location":"api/core/#sumeh.core.get_schema_from_databricks","title":"<code>get_schema_from_databricks(catalog, schema, table, **kwargs)</code>","text":"Source code in <code>sumeh/services/config.py</code> <pre><code>def get_schema_from_databricks(\n    catalog: Optional[str], schema: Optional[str], table: str, **kwargs\n) -&gt; List[Dict[str, Any]]:\n    from pyspark.sql import SparkSession\n\n    spark = SparkSession.builder.getOrCreate()\n    if catalog and schema:\n        full = f\"{catalog}.{schema}.{table}\"\n    elif schema:\n        full = f\"{schema}.{table}\"\n    else:\n        full = table\n    schema = spark.table(full).schema\n    result = []\n    for f in schema.fields:\n        result.append(\n            {\n                \"field\": f.name,\n                \"data_type\": f.dataType.simpleString(),\n                \"nullable\": f.nullable,\n                \"max_length\": None,\n            }\n        )\n    return result\n</code></pre>"},{"location":"api/core/#sumeh.core.get_schema_from_duckdb","title":"<code>get_schema_from_duckdb(db_path, table, conn)</code>","text":"Source code in <code>sumeh/services/config.py</code> <pre><code>def get_schema_from_duckdb(db_path: str, table: str, conn) -&gt; List[Dict[str, Any]]:\n    df = conn.execute(f\"PRAGMA table_info('{table}')\").fetchdf()\n    return [\n        {\n            \"field\": row[\"name\"],\n            \"data_type\": row[\"type\"].lower(),\n            \"nullable\": not bool(row[\"notnull\"]),\n            \"max_length\": None,\n        }\n        for _, row in df.iterrows()\n    ]\n</code></pre>"},{"location":"api/core/#sumeh.core.get_schema_from_glue","title":"<code>get_schema_from_glue(glue_context, database_name, table_name)</code>","text":"Source code in <code>sumeh/services/config.py</code> <pre><code>def get_schema_from_glue(\n    glue_context, database_name: str, table_name: str\n) -&gt; List[Dict[str, Any]]:\n    from awsglue.context import GlueContext\n\n    if not isinstance(glue_context, GlueContext):\n        raise ValueError(\"Informe um GlueContext v\u00e1lido\")\n    df = glue_context.spark_session.read.table(f\"{database_name}.{table_name}\")\n    return [\n        {\n            \"field\": field.name,\n            \"data_type\": field.dataType.simpleString(),\n            \"nullable\": field.nullable,\n            \"max_length\": None,\n        }\n        for field in df.schema.fields\n    ]\n</code></pre>"},{"location":"api/core/#sumeh.core.get_schema_from_mysql","title":"<code>get_schema_from_mysql(host, user, password, database, table, port=3306)</code>","text":"Source code in <code>sumeh/services/config.py</code> <pre><code>def get_schema_from_mysql(\n    host: str, user: str, password: str, database: str, table: str, port: int = 3306\n) -&gt; List[Dict[str, Any]]:\n    import mysql.connector\n\n    conn = mysql.connector.connect(\n        host=host, user=user, password=password, database=database, port=port\n    )\n    cursor = conn.cursor(dictionary=True)\n    cursor.execute(\n        f\"\"\"\n        SELECT \n            column_name AS field,\n            data_type,\n            is_nullable = 'YES' AS nullable,\n            character_maximum_length AS max_length\n        FROM information_schema.columns\n        WHERE table_schema = %s AND table_name = %s\n    \"\"\",\n        (database, table),\n    )\n    schema = cursor.fetchall()\n    cursor.close()\n    conn.close()\n    return schema\n</code></pre>"},{"location":"api/core/#sumeh.core.get_schema_from_postgresql","title":"<code>get_schema_from_postgresql(host, user, password, database, table, port=5432)</code>","text":"Source code in <code>sumeh/services/config.py</code> <pre><code>def get_schema_from_postgresql(\n    host: str, user: str, password: str, database: str, table: str, port: int = 5432\n) -&gt; List[Dict[str, Any]]:\n    import psycopg2\n\n    conn = psycopg2.connect(\n        host=host, user=user, password=password, dbname=database, port=port\n    )\n    cursor = conn.cursor()\n    cursor.execute(\n        f\"\"\"\n        SELECT\n            column_name AS field,\n            data_type,\n            is_nullable = 'YES' AS nullable,\n            character_maximum_length AS max_length\n        FROM information_schema.columns\n        WHERE table_schema = 'public' AND table_name = %s\n    \"\"\",\n        (table,),\n    )\n    cols = cursor.fetchall()\n    cursor.close()\n    conn.close()\n    return [\n        {\"field\": f, \"data_type\": dt, \"nullable\": nl, \"max_length\": ml}\n        for f, dt, nl, ml in cols\n    ]\n</code></pre>"},{"location":"api/core/#sumeh.core.get_schema_from_s3","title":"<code>get_schema_from_s3(s3_path, **kwargs)</code>","text":"Source code in <code>sumeh/services/config.py</code> <pre><code>def get_schema_from_s3(s3_path: str, **kwargs) -&gt; List[Dict[str, Any]]:\n\n    content = __read_s3_file(s3_path)\n    with open(\"/tmp/temp.csv\", \"w\") as f:\n        f.write(content)\n    return get_schema_from_csv(\"/tmp/temp.csv\", **kwargs)\n</code></pre>"},{"location":"api/core/#sumeh.core.report","title":"<code>report(df, rules, name='Quality Check')</code>","text":"<p>Performs a quality check on the given DataFrame based on the provided rules.</p> <p>The function iterates over a list of rules and applies different checks to the specified fields of the DataFrame. The checks include validation of completeness, uniqueness, specific values, patterns, and other conditions. Each rule corresponds to a particular type of validation, such as 'is_complete', 'is_greater_than', 'has_mean', etc. After applying the checks, the function returns the result of the validation.</p> <p>Parameters: - df (DataFrame): The DataFrame to be validated. - rules (list of dict): A list of rules defining the checks to be performed.     Each rule is a dictionary with the following keys:     - \"check_type\": The type of check to apply.     - \"field\": The column of the DataFrame to check.     - \"value\" (optional): The value used for comparison in some checks (e.g., for 'is_greater_than').     - \"threshold\" (optional): A percentage threshold to be applied in some checks. - name (str): The name of the quality check (default is \"Quality Check\").</p> <p>Returns: - quality_check (CheckResult): The result of the quality validation.</p> <p>Warnings: - If an unknown rule name is encountered, a warning is generated.</p> Source code in <code>sumeh/core.py</code> <pre><code>def report(df, rules: list[dict], name: str = \"Quality Check\"):\n    \"\"\"\n    Performs a quality check on the given DataFrame based on the provided rules.\n\n    The function iterates over a list of rules and applies different checks to the\n    specified fields of the DataFrame. The checks include validation of completeness,\n    uniqueness, specific values, patterns, and other conditions. Each rule corresponds\n    to a particular type of validation, such as 'is_complete', 'is_greater_than',\n    'has_mean', etc. After applying the checks, the function returns the result of\n    the validation.\n\n    Parameters:\n    - df (DataFrame): The DataFrame to be validated.\n    - rules (list of dict): A list of rules defining the checks to be performed.\n        Each rule is a dictionary with the following keys:\n        - \"check_type\": The type of check to apply.\n        - \"field\": The column of the DataFrame to check.\n        - \"value\" (optional): The value used for comparison in some checks (e.g., for 'is_greater_than').\n        - \"threshold\" (optional): A percentage threshold to be applied in some checks.\n    - name (str): The name of the quality check (default is \"Quality Check\").\n\n    Returns:\n    - quality_check (CheckResult): The result of the quality validation.\n\n    Warnings:\n    - If an unknown rule name is encountered, a warning is generated.\n    \"\"\"\n\n    check = Check(CheckLevel.WARNING, name)\n    for rule in rules:\n        rule_name = rule[\"check_type\"]\n        field = rule[\"field\"]\n        threshold = rule.get(\"threshold\", 1.0)\n        threshold = 1.0 if threshold is None else threshold\n\n        match rule_name:\n\n            case \"is_complete\":\n                check = check.is_complete(field, pct=threshold)\n\n            case \"is_unique\":\n                check = check.is_unique(field, pct=threshold)\n\n            case \"is_primary_key\":\n                check = check.is_primary_key(field, pct=threshold)\n\n            case \"are_complete\":\n                check = check.are_complete(field, pct=threshold)\n\n            case \"are_unique\":\n                check = check.are_complete(field, pct=threshold)\n\n            case \"is_composite_key\":\n                check = check.are_complete(field, pct=threshold)\n\n            case \"is_greater_than\":\n                value = __convert_value(rule[\"value\"])\n                check = check.is_greater_than(field, value, pct=threshold)\n\n            case \"is_positive\":\n                check = check.is_positive(field, pct=threshold)\n\n            case \"is_negative\":\n                check = check.is_negative(field, pct=threshold)\n\n            case \"is_greater_or_equal_than\":\n                value = __convert_value(rule[\"value\"])\n                check = check.is_greater_or_equal_than(field, value, pct=threshold)\n\n            case \"is_less_than\":\n                value = __convert_value(rule[\"value\"])\n                check = check.is_less_than(field, value, pct=threshold)\n\n            case \"is_less_or_equal_than\":\n                value = __convert_value(rule[\"value\"])\n                check = check.is_less_or_equal_than(field, value, pct=threshold)\n\n            case \"is_equal_than\":\n                value = __convert_value(rule[\"value\"])\n                check = check.is_equal_than(field, value, pct=threshold)\n\n            case \"is_contained_in\" | \"is_in\":\n                values = rule[\"value\"]\n                values = values.replace(\"[\", \"\").replace(\"]\", \"\").split(\",\")\n                values = tuple([value.strip() for value in values])\n                check = check.is_contained_in(field, values, pct=threshold)\n\n            case \"not_contained_in\" | \"not_in\":\n                values = rule[\"value\"]\n                values = values.replace(\"[\", \"\").replace(\"]\", \"\").split(\",\")\n                values = tuple([value.strip() for value in values])\n                check = check.is_contained_in(field, values, pct=threshold)\n\n            case \"is_between\":\n                values = rule[\"value\"]\n                values = values.replace(\"[\", \"\").replace(\"]\", \"\").split(\",\")\n                values = tuple(__convert_value(value) for value in values)\n                check = check.is_between(field, values, pct=threshold)\n\n            case \"has_pattern\":\n                pattern = rule[\"value\"]\n                check = check.has_pattern(field, pattern, pct=threshold)\n\n            case \"is_legit\":\n                check = check.is_legit(field, pct=threshold)\n\n            case \"has_min\":\n                value = __convert_value(rule[\"value\"])\n                check = check.has_min(field, value)\n\n            case \"has_max\":\n                value = __convert_value(rule[\"value\"])\n                check = check.has_max(field, value)\n\n            case \"has_std\":\n                value = __convert_value(rule[\"value\"])\n                check = check.has_std(field, value)\n\n            case \"has_mean\":\n                value = __convert_value(rule[\"value\"])\n                check = check.has_mean(field, value)\n\n            case \"has_sum\":\n                value = __convert_value(rule[\"value\"])\n                check = check.has_sum(field, value)\n\n            case \"has_cardinality\":\n                value = __convert_value(rule[\"value\"])\n                check = check.has_cardinality(field, value)\n\n            case \"has_infogain\":\n                check = check.has_infogain(field, pct=threshold)\n\n            case \"has_entropy\":\n                value = __convert_value(rule[\"value\"])\n                check = check.has_entropy(field, value)\n\n            case \"is_in_millions\":\n                check = check.is_in_millions(field, pct=threshold)\n\n            case \"is_in_billions\":\n                check = check.is_in_millions(field, pct=threshold)\n\n            case \"is_t_minus_1\":\n                check = check.is_t_minus_1(field, pct=threshold)\n\n            case \"is_t_minus_2\":\n                check = check.is_t_minus_2(field, pct=threshold)\n\n            case \"is_t_minus_3\":\n                check = check.is_t_minus_3(field, pct=threshold)\n\n            case \"is_today\":\n                check = check.is_today(field, pct=threshold)\n\n            case \"is_yesterday\":\n                check = check.is_yesterday(field, pct=threshold)\n\n            case \"is_on_weekday\":\n                check = check.is_on_weekday(field, pct=threshold)\n\n            case \"is_on_weekend\":\n                check = check.is_on_weekend(field, pct=threshold)\n\n            case \"is_on_monday\":\n                check = check.is_on_monday(field, pct=threshold)\n\n            case \"is_on_tuesday\":\n                check = check.is_on_tuesday(field, pct=threshold)\n\n            case \"is_on_wednesday\":\n                check = check.is_on_wednesday(field, pct=threshold)\n\n            case \"is_on_thursday\":\n                check = check.is_on_thursday(field, pct=threshold)\n\n            case \"is_on_friday\":\n                check = check.is_on_friday(field, pct=threshold)\n\n            case \"is_on_saturday\":\n                check = check.is_on_saturday(field, pct=threshold)\n\n            case \"is_on_sunday\":\n                check = check.is_on_sunday(field, pct=threshold)\n\n            case \"satisfies\":\n                predicate = rule[\"value\"]\n                check = check.satisfies(field, predicate, pct=threshold)\n\n            case _:\n                warnings.warn(f\"Unknown rule name: {rule_name}, {field}\")\n\n    quality_check = check.validate(df)\n    return quality_check\n</code></pre>"},{"location":"api/core/#sumeh.core.summarize","title":"<code>summarize(df, rules, **context)</code>","text":"<p>Summarizes a DataFrame based on the provided rules and context.</p> <p>This function dynamically detects the appropriate engine to use for summarization based on the type of the input DataFrame. It delegates the summarization process to the corresponding engine module.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <p>The input DataFrame to be summarized. The type of the DataFrame determines the engine used for summarization.</p> required <code>rules</code> <code>list[dict]</code> <p>A list of dictionaries defining the summarization rules. Each dictionary specifies the operations or transformations to be applied.</p> required <code>**context</code> <p>Additional context parameters required by specific engines. Common parameters include: - conn: A database connection object (used by certain engines like DuckDB). - total_rows: The total number of rows in the DataFrame (optional).</p> <code>{}</code> <p>Returns:</p> Type Description <p>The summarized DataFrame as processed by the appropriate engine.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the type of the input DataFrame is unsupported.</p> Notes <ul> <li>The function uses the <code>__detect_engine</code> method to determine the engine name   based on the input DataFrame.</li> <li>Supported engines are dynamically imported from the <code>sumeh.engine</code> package.</li> <li>The \"duckdb_engine\" case requires a database connection (<code>conn</code>) to be passed   in the context.</li> </ul> Example <p>summarized_df = summarize(df, rules=[{\"operation\": \"sum\", \"column\": \"sales\"}], conn=my_conn)</p> Source code in <code>sumeh/core.py</code> <pre><code>def summarize(df, rules: list[dict], **context):\n    \"\"\"\n    Summarizes a DataFrame based on the provided rules and context.\n\n    This function dynamically detects the appropriate engine to use for summarization\n    based on the type of the input DataFrame. It delegates the summarization process\n    to the corresponding engine module.\n\n    Args:\n        df: The input DataFrame to be summarized. The type of the DataFrame determines\n            the engine used for summarization.\n        rules (list[dict]): A list of dictionaries defining the summarization rules.\n            Each dictionary specifies the operations or transformations to be applied.\n        **context: Additional context parameters required by specific engines. Common\n            parameters include:\n            - conn: A database connection object (used by certain engines like DuckDB).\n            - total_rows: The total number of rows in the DataFrame (optional).\n\n    Returns:\n        The summarized DataFrame as processed by the appropriate engine.\n\n    Raises:\n        TypeError: If the type of the input DataFrame is unsupported.\n\n    Notes:\n        - The function uses the `__detect_engine` method to determine the engine name\n          based on the input DataFrame.\n        - Supported engines are dynamically imported from the `sumeh.engine` package.\n        - The \"duckdb_engine\" case requires a database connection (`conn`) to be passed\n          in the context.\n\n    Example:\n        &gt;&gt;&gt; summarized_df = summarize(df, rules=[{\"operation\": \"sum\", \"column\": \"sales\"}], conn=my_conn)\n    \"\"\"\n    engine_name = __detect_engine(df)\n    engine = import_module(f\"sumeh.engine.{engine_name}\")\n    match engine_name:\n        case \"duckdb_engine\":\n            return engine.summarize(\n                df_rel=df,\n                rules=rules,\n                conn=context.get(\"conn\"),\n                total_rows=context.get(\"total_rows\"),\n            )\n        case _:\n            return engine.summarize(df, rules, total_rows=context.get(\"total_rows\"))\n\n    raise TypeError(f\"Unsupported DataFrame type: {type(df)}\")\n</code></pre>"},{"location":"api/core/#sumeh.core.validate","title":"<code>validate(df, rules, **context)</code>","text":"<p>Validates a DataFrame against a set of rules using the appropriate engine.</p> <p>This function dynamically detects the engine to use based on the input DataFrame and delegates the validation process to the corresponding engine's implementation.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to be validated.</p> required <code>rules</code> <code>list or dict</code> <p>The validation rules to be applied to the DataFrame.</p> required <code>**context</code> <p>Additional context parameters that may be required by the engine. - conn (optional): A database connection object, required for certain engines   like \"duckdb_engine\".</p> <code>{}</code> <p>Returns:</p> Type Description <p>bool or dict: The result of the validation process. The return type and structure</p> <p>depend on the specific engine's implementation.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If the required engine module cannot be imported.</p> <code>AttributeError</code> <p>If the detected engine does not have a <code>validate</code> method.</p> Notes <ul> <li>The engine is dynamically determined based on the DataFrame type or other   characteristics.</li> <li>For \"duckdb_engine\", a database connection object should be provided in the   context under the key \"conn\".</li> </ul> Source code in <code>sumeh/core.py</code> <pre><code>def validate(df, rules, **context):\n    \"\"\"\n    Validates a DataFrame against a set of rules using the appropriate engine.\n\n    This function dynamically detects the engine to use based on the input\n    DataFrame and delegates the validation process to the corresponding engine's\n    implementation.\n\n    Args:\n        df (DataFrame): The input DataFrame to be validated.\n        rules (list or dict): The validation rules to be applied to the DataFrame.\n        **context: Additional context parameters that may be required by the engine.\n            - conn (optional): A database connection object, required for certain engines\n              like \"duckdb_engine\".\n\n    Returns:\n        bool or dict: The result of the validation process. The return type and structure\n        depend on the specific engine's implementation.\n\n    Raises:\n        ImportError: If the required engine module cannot be imported.\n        AttributeError: If the detected engine does not have a `validate` method.\n\n    Notes:\n        - The engine is dynamically determined based on the DataFrame type or other\n          characteristics.\n        - For \"duckdb_engine\", a database connection object should be provided in the\n          context under the key \"conn\".\n    \"\"\"\n    engine_name = __detect_engine(df)\n    engine = import_module(f\"sumeh.engine.{engine_name}\")\n\n    match engine_name:\n        case \"duckdb_engine\":\n            return engine.validate(df, rules, context.get(\"conn\"))\n        case _:\n            return engine.validate(df, rules)\n</code></pre>"},{"location":"api/core/#sumeh.core.validate_schema","title":"<code>validate_schema(df_or_conn, expected, engine, **engine_kwargs)</code>","text":"<p>Validates the schema of a given data source or connection against an expected schema.</p> <p>Parameters:</p> Name Type Description Default <code>df_or_conn</code> <code>Any</code> <p>The data source or connection to validate. This can be a DataFrame,                database connection, or other supported data structure.</p> required <code>expected</code> <code>List[Dict[str, Any]]</code> <p>A list of dictionaries defining the expected schema.                               Each dictionary should describe a column or field,                               including its name, type, and other attributes.</p> required <code>engine</code> <code>str</code> <p>The name of the engine to use for validation. This determines the            specific validation logic to apply based on the data source type.</p> required <code>**engine_kwargs</code> <p>Additional keyword arguments to pass to the engine's validation logic.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tuple[bool, List[Tuple[str, str]]]</code> <p>Tuple[bool, List[Tuple[str, str]]]: A tuple where the first element is a boolean indicating                                  whether the schema is valid, and the second element is                                  a list of tuples containing error messages for any                                  validation failures. Each tuple consists of the field                                  name and the corresponding error message.</p> Source code in <code>sumeh/core.py</code> <pre><code>def validate_schema(df_or_conn: Any, expected: List[Dict[str, Any]], engine: str, **engine_kwargs) -&gt; Tuple[bool, List[Tuple[str, str]]]:\n    \"\"\"\n    Validates the schema of a given data source or connection against an expected schema.\n\n    Args:\n        df_or_conn (Any): The data source or connection to validate. This can be a DataFrame, \n                          database connection, or other supported data structure.\n        expected (List[Dict[str, Any]]): A list of dictionaries defining the expected schema. \n                                         Each dictionary should describe a column or field, \n                                         including its name, type, and other attributes.\n        engine (str): The name of the engine to use for validation. This determines the \n                      specific validation logic to apply based on the data source type.\n        **engine_kwargs: Additional keyword arguments to pass to the engine's validation logic.\n\n    Returns:\n        Tuple[bool, List[Tuple[str, str]]]: A tuple where the first element is a boolean indicating \n                                            whether the schema is valid, and the second element is \n                                            a list of tuples containing error messages for any \n                                            validation failures. Each tuple consists of the field \n                                            name and the corresponding error message.\n    \"\"\"\n    engine_name = __detect_engine(df_or_conn)\n    engine = import_module(f\"sumeh.engine.{engine_name}\")\n    return engine.validate_schema(df_or_conn, expected=expected, **engine_kwargs)\n</code></pre>"},{"location":"api/engine/engine-dask/","title":"Module <code>sumeh.engine.dask_engine</code>","text":"<p>This module provides a set of data quality validation functions for Dask DataFrames. It includes various checks such as completeness, uniqueness, value range, patterns, and schema validation. The module also provides utilities for summarizing validation results and schema comparison.</p> <p>Functions:</p> Name Description <code>is_positive</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>is_negative</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>is_complete</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>is_unique</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>are_complete</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>are_unique</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>is_greater_than</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>is_greater_or_equal_than</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>is_less_than</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>is_less_or_equal_than</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>is_equal</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>is_equal_than</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>is_contained_in</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>not_contained_in</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>is_between</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>has_pattern</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>is_legit</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>is_primary_key</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>is_composite_key</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>has_max</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>has_min</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>has_std</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>has_mean</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>has_sum</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>has_cardinality</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>has_infogain</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>has_entropy</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>satisfies</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>validate</code> <p>dd.DataFrame, rules: list[dict]) -&gt; tuple[dd.DataFrame, dd.DataFrame]:</p> <code>summarize</code> <p>dd.DataFrame, rules: list[dict], total_rows: int) -&gt; pd.DataFrame:</p> <code>validate_schema</code> <p>dd.DataFrame, expected: List[Dict[str, Any]]) -&gt; Tuple[bool, List[Tuple[str, str]]]:</p>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.__compare_schemas","title":"<code>__compare_schemas(actual, expected)</code>","text":"<p>Compare two lists of schema definitions and identify discrepancies.</p> <p>Parameters:</p> Name Type Description Default <code>actual</code> <code>List[SchemaDef]</code> <p>The list of actual schema definitions.</p> required <code>expected</code> <code>List[SchemaDef]</code> <p>The list of expected schema definitions.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>Tuple[bool, List[Tuple[str, str]]]: A tuple where the first element is a boolean indicating </p> <code>List[Tuple[str, str]]</code> <p>whether the schemas match (True if they match, False otherwise), and the second element </p> <code>Tuple[bool, List[Tuple[str, str]]]</code> <p>is a list of tuples describing the discrepancies. Each tuple contains: - The field name (str). - A description of the discrepancy (str), such as \"missing\", \"type mismatch\",    \"nullable but expected non-nullable\", or \"extra column\".</p> Notes <ul> <li>A field is considered \"missing\" if it exists in the expected schema but not in the actual schema.</li> <li>A \"type mismatch\" occurs if the data type of a field in the actual schema does not match    the expected data type.</li> <li>A field is considered \"nullable but expected non-nullable\" if it is nullable in the actual    schema but not nullable in the expected schema.</li> <li>An \"extra column\" is a field that exists in the actual schema but not in the expected schema.</li> </ul> Source code in <code>sumeh/services/utils.py</code> <pre><code>def __compare_schemas(actual: List[SchemaDef], expected: List[SchemaDef],) -&gt; Tuple[bool, List[Tuple[str, str]]]:\n    \"\"\"\n    Compare two lists of schema definitions and identify discrepancies.\n\n    Args:\n        actual (List[SchemaDef]): The list of actual schema definitions.\n        expected (List[SchemaDef]): The list of expected schema definitions.\n\n    Returns:\n        Tuple[bool, List[Tuple[str, str]]]: A tuple where the first element is a boolean indicating \n        whether the schemas match (True if they match, False otherwise), and the second element \n        is a list of tuples describing the discrepancies. Each tuple contains:\n            - The field name (str).\n            - A description of the discrepancy (str), such as \"missing\", \"type mismatch\", \n              \"nullable but expected non-nullable\", or \"extra column\".\n\n    Notes:\n        - A field is considered \"missing\" if it exists in the expected schema but not in the actual schema.\n        - A \"type mismatch\" occurs if the data type of a field in the actual schema does not match \n          the expected data type.\n        - A field is considered \"nullable but expected non-nullable\" if it is nullable in the actual \n          schema but not nullable in the expected schema.\n        - An \"extra column\" is a field that exists in the actual schema but not in the expected schema.\n    \"\"\"\n\n    exp_map = {c[\"field\"]: c for c in expected}\n    act_map = {c[\"field\"]: c for c in actual}\n\n    erros: List[Tuple[str, str]] = []\n\n    for fld, exp in exp_map.items():\n        if fld not in act_map:\n            erros.append((fld, \"missing\"))\n            continue\n        act = act_map[fld]\n        if act[\"data_type\"] != exp[\"data_type\"]:\n            erros.append(\n                (\n                    fld,\n                    f\"type mismatch (got {act['data_type']}, expected {exp['data_type']})\",\n                )\n            )\n\n        if act[\"nullable\"] and not exp[\"nullable\"]:\n            erros.append((fld, \"nullable but expected non-nullable\"))\n\n        if exp.get(\"max_length\") is not None:\n            pass\n\n    # 2. campos extras (se quiser)\n    extras = set(act_map) - set(exp_map)\n    for fld in extras:\n        erros.append((fld, \"extra column\"))\n\n    return len(erros) == 0, erros\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.__convert_value","title":"<code>__convert_value(value)</code>","text":"<p>Converts the provided value to the appropriate type (date, float, or int).</p> <p>Depending on the format of the input value, it will be converted to a datetime object, a floating-point number (float), or an integer (int).</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>The value to be converted, represented as a string.</p> required <p>Returns:</p> Type Description <p>Union[datetime, float, int]: The converted value, which can be a datetime object, float, or int.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the value does not match an expected format.</p> Source code in <code>sumeh/services/utils.py</code> <pre><code>def __convert_value(value):\n    \"\"\"\n    Converts the provided value to the appropriate type (date, float, or int).\n\n    Depending on the format of the input value, it will be converted to a datetime object,\n    a floating-point number (float), or an integer (int).\n\n    Args:\n        value (str): The value to be converted, represented as a string.\n\n    Returns:\n        Union[datetime, float, int]: The converted value, which can be a datetime object, float, or int.\n\n    Raises:\n        ValueError: If the value does not match an expected format.\n    \"\"\"\n    from datetime import datetime\n\n    value = value.strip()\n    try:\n        if \"-\" in value:\n            return datetime.strptime(value, \"%Y-%m-%d\")\n        else:\n            return datetime.strptime(value, \"%d/%m/%Y\")\n    except ValueError:\n        if \".\" in value:\n            return float(value)\n        return int(value)\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.__dask_schema_to_list","title":"<code>__dask_schema_to_list(df)</code>","text":"<p>Convert the schema of a Dask DataFrame into a list of dictionaries.</p> <p>Each dictionary in the resulting list represents a column in the DataFrame and contains metadata about the column, including its name, data type, nullability, and maximum length.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The Dask DataFrame whose schema is to be converted.</p> required <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List[Dict[str, Any]]: A list of dictionaries, where each dictionary</p> <code>List[Dict[str, Any]]</code> <p>contains the following keys: - \"field\" (str): The name of the column. - \"data_type\" (str): The data type of the column, converted to a lowercase string. - \"nullable\" (bool): Always set to True, indicating the column is nullable. - \"max_length\" (None): Always set to None, as maximum length is not determined.</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def __dask_schema_to_list(df: dd.DataFrame) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Convert the schema of a Dask DataFrame into a list of dictionaries.\n\n    Each dictionary in the resulting list represents a column in the DataFrame\n    and contains metadata about the column, including its name, data type,\n    nullability, and maximum length.\n\n    Args:\n        df (dd.DataFrame): The Dask DataFrame whose schema is to be converted.\n\n    Returns:\n        List[Dict[str, Any]]: A list of dictionaries, where each dictionary\n        contains the following keys:\n            - \"field\" (str): The name of the column.\n            - \"data_type\" (str): The data type of the column, converted to a lowercase string.\n            - \"nullable\" (bool): Always set to True, indicating the column is nullable.\n            - \"max_length\" (None): Always set to None, as maximum length is not determined.\n    \"\"\"\n    return [\n        {\n            \"field\": col,\n            \"data_type\": str(dtype).lower(),\n            \"nullable\": True,\n            \"max_length\": None,\n        }\n        for col, dtype in df.dtypes.items()\n    ]\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.__extract_params","title":"<code>__extract_params(rule)</code>","text":"Source code in <code>sumeh/services/utils.py</code> <pre><code>def __extract_params(rule: dict) -&gt; tuple:\n    rule_name = rule[\"check_type\"]\n    field = rule[\"field\"]\n    raw_value = rule.get(\"value\")\n    if isinstance(raw_value, str) and raw_value not in (None, \"\", \"NULL\"):\n        try:\n            value = __convert_value(raw_value)\n        except ValueError:\n            value = raw_value\n    else:\n        value = raw_value\n    value = value if value not in (None, \"\", \"NULL\") else \"\"\n    return field, rule_name, value\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine._rules_to_df","title":"<code>_rules_to_df(rules)</code>","text":"<p>Converts a list of rule dictionaries into a pandas DataFrame.</p> <p>Each rule dictionary is expected to have the following keys: - \"field\": The column(s) the rule applies to. Can be a string or a list of strings. - \"check_type\": The type of rule or check being applied. - \"threshold\" (optional): A numeric value representing the pass threshold. Defaults to 1.0 if not provided. - \"value\" (optional): Additional value associated with the rule. - \"execute\" (optional): A boolean indicating whether the rule should be executed. Defaults to True if not provided.</p> <p>Rules with \"execute\" set to False are skipped. The resulting DataFrame contains unique rows based on the combination of \"column\" and \"rule\".</p> <p>Parameters:</p> Name Type Description Default <code>rules</code> <code>list[dict]</code> <p>A list of dictionaries representing the rules.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame with the following columns: - \"column\": The column(s) the rule applies to, joined by a comma if multiple. - \"rule\": The type of rule or check being applied. - \"pass_threshold\": The numeric pass threshold for the rule. - \"value\": Additional value associated with the rule, if any.</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def _rules_to_df(rules: list[dict]) -&gt; pd.DataFrame:\n    \"\"\"\n    Converts a list of rule dictionaries into a pandas DataFrame.\n\n    Each rule dictionary is expected to have the following keys:\n    - \"field\": The column(s) the rule applies to. Can be a string or a list of strings.\n    - \"check_type\": The type of rule or check being applied.\n    - \"threshold\" (optional): A numeric value representing the pass threshold. Defaults to 1.0 if not provided.\n    - \"value\" (optional): Additional value associated with the rule.\n    - \"execute\" (optional): A boolean indicating whether the rule should be executed. Defaults to True if not provided.\n\n    Rules with \"execute\" set to False are skipped. The resulting DataFrame contains unique rows based on the combination\n    of \"column\" and \"rule\".\n\n    Args:\n        rules (list[dict]): A list of dictionaries representing the rules.\n\n    Returns:\n        pd.DataFrame: A DataFrame with the following columns:\n            - \"column\": The column(s) the rule applies to, joined by a comma if multiple.\n            - \"rule\": The type of rule or check being applied.\n            - \"pass_threshold\": The numeric pass threshold for the rule.\n            - \"value\": Additional value associated with the rule, if any.\n    \"\"\"\n    rows = []\n    for r in rules:\n        if not r.get(\"execute\", True):\n            continue\n        coln = \",\".join(r[\"field\"]) if isinstance(r[\"field\"], list) else r[\"field\"]\n        rows.append(\n            {\n                \"column\": coln.strip(),\n                \"rule\": r[\"check_type\"],\n                \"pass_threshold\": float(r.get(\"threshold\") or 1.0),\n                \"value\": r.get(\"value\") or None,\n            }\n        )\n    return pd.DataFrame(rows).drop_duplicates([\"column\", \"rule\"])\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.are_complete","title":"<code>are_complete(df, rule)</code>","text":"<p>Checks if the specified fields in a Dask DataFrame are complete (non-null)  based on the provided rule and returns a DataFrame of violations.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to check for completeness.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should  include the fields to check, the type of check, and the expected value.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A DataFrame containing rows that violate the completeness </p> <code>DataFrame</code> <p>rule, with an additional column <code>dq_status</code> indicating the rule details </p> <code>DataFrame</code> <p>in the format \"{fields}:{check}:{value}\".</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def are_complete(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Checks if the specified fields in a Dask DataFrame are complete (non-null) \n    based on the provided rule and returns a DataFrame of violations.\n\n    Args:\n        df (dd.DataFrame): The input Dask DataFrame to check for completeness.\n        rule (dict): A dictionary containing the rule parameters. It should \n            include the fields to check, the type of check, and the expected value.\n\n    Returns:\n        dd.DataFrame: A DataFrame containing rows that violate the completeness \n        rule, with an additional column `dq_status` indicating the rule details \n        in the format \"{fields}:{check}:{value}\".\n    \"\"\"\n    fields, check, value = __extract_params(rule)\n    mask = ~reduce(operator.and_, [df[f].notnull() for f in fields])\n    viol = df[mask]\n    return viol.assign(dq_status=f\"{str(fields)}:{check}:{value}\")\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.are_unique","title":"<code>are_unique(df, rule)</code>","text":"<p>Checks if the specified fields in a Dask DataFrame contain unique combinations of values.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to include: - 'fields': A list of column names to check for uniqueness. - 'check': A string describing the type of check being performed. - 'value': A value associated with the rule (used for status reporting).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A DataFrame containing rows that violate the uniqueness rule, </p> <code>DataFrame</code> <p>with an additional column <code>dq_status</code> indicating the rule that was violated.</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def are_unique(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Checks if the specified fields in a Dask DataFrame contain unique combinations of values.\n\n    Parameters:\n        df (dd.DataFrame): The input Dask DataFrame to be checked.\n        rule (dict): A dictionary containing the rule parameters. It is expected to include:\n            - 'fields': A list of column names to check for uniqueness.\n            - 'check': A string describing the type of check being performed.\n            - 'value': A value associated with the rule (used for status reporting).\n\n    Returns:\n        dd.DataFrame: A DataFrame containing rows that violate the uniqueness rule, \n        with an additional column `dq_status` indicating the rule that was violated.\n    \"\"\"\n    fields, check, value = __extract_params(rule)\n    combo = (\n        df[fields]\n        .astype(str)\n        .apply(lambda row: \"|\".join(row.values), axis=1, meta=(\"combo\", \"object\"))\n    )\n    counts = combo.value_counts().compute()\n    dupes = counts[counts &gt; 1].index.tolist()\n    viol = df[combo.isin(dupes)]\n    return viol.assign(dq_status=f\"{str(fields)}:{check}:{value}\")\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.has_cardinality","title":"<code>has_cardinality(df, rule)</code>","text":"<p>Checks if the cardinality (number of unique values) of a specified field in a Dask DataFrame exceeds a given threshold and returns a modified DataFrame based on the result.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to evaluate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The column name to check cardinality for. - 'check' (str): A descriptive label for the check (used in the output). - 'value' (int): The maximum allowed cardinality.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: If the cardinality of the specified field exceeds the given value, </p> <code>DataFrame</code> <p>returns the original DataFrame with an additional column <code>dq_status</code> indicating </p> <code>DataFrame</code> <p>the rule violation. Otherwise, returns an empty DataFrame with the same structure </p> <code>DataFrame</code> <p>as the input DataFrame.</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def has_cardinality(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Checks if the cardinality (number of unique values) of a specified field in a Dask DataFrame\n    exceeds a given threshold and returns a modified DataFrame based on the result.\n\n    Args:\n        df (dd.DataFrame): The input Dask DataFrame to evaluate.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field' (str): The column name to check cardinality for.\n            - 'check' (str): A descriptive label for the check (used in the output).\n            - 'value' (int): The maximum allowed cardinality.\n\n    Returns:\n        dd.DataFrame: If the cardinality of the specified field exceeds the given value, \n        returns the original DataFrame with an additional column `dq_status` indicating \n        the rule violation. Otherwise, returns an empty DataFrame with the same structure \n        as the input DataFrame.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    card = df[field].nunique().compute() or 0\n    if card &gt; value:\n        return df.assign(dq_status=f\"{field}:{check}:{value}\")\n    return df.head(0).pipe(dd.from_pandas, npartitions=1)\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.has_entropy","title":"<code>has_entropy(df, rule)</code>","text":"<p>Evaluates the entropy of a specified field in a Dask DataFrame and applies a rule to determine if the entropy exceeds a given threshold. If the threshold is exceeded, a new column <code>dq_status</code> is added to the DataFrame with information about the rule violation. Otherwise, an empty DataFrame is returned.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to evaluate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - <code>field</code> (str): The column name to evaluate. - <code>check</code> (str): The type of check being performed (used for status message). - <code>value</code> (float): The threshold value for the entropy.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A DataFrame with the <code>dq_status</code> column added if the entropy exceeds the threshold,</p> <code>DataFrame</code> <p>or an empty DataFrame if the threshold is not exceeded.</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def has_entropy(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Evaluates the entropy of a specified field in a Dask DataFrame and applies a rule to determine\n    if the entropy exceeds a given threshold. If the threshold is exceeded, a new column `dq_status`\n    is added to the DataFrame with information about the rule violation. Otherwise, an empty DataFrame\n    is returned.\n\n    Args:\n        df (dd.DataFrame): The input Dask DataFrame to evaluate.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - `field` (str): The column name to evaluate.\n            - `check` (str): The type of check being performed (used for status message).\n            - `value` (float): The threshold value for the entropy.\n\n    Returns:\n        dd.DataFrame: A DataFrame with the `dq_status` column added if the entropy exceeds the threshold,\n        or an empty DataFrame if the threshold is not exceeded.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    ent = df[field].nunique().compute() or 0.0\n    if ent &gt; value:\n        return df.assign(dq_status=f\"{field}:{check}:{value}\")\n    return df.head(0).pipe(dd.from_pandas, npartitions=1)\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.has_infogain","title":"<code>has_infogain(df, rule)</code>","text":"<p>Evaluates whether a given field in a Dask DataFrame satisfies an information gain condition based on the specified rule. If the condition is met, the DataFrame is updated with a  <code>dq_status</code> column indicating the rule applied. Otherwise, an empty DataFrame is returned.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to evaluate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The column name to evaluate. - 'check' (str): The type of check being performed (used for status annotation). - 'value' (float): The threshold value for the information gain.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: The original DataFrame with an added <code>dq_status</code> column if the condition </p> <code>DataFrame</code> <p>is met, or an empty DataFrame if the condition is not satisfied.</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def has_infogain(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Evaluates whether a given field in a Dask DataFrame satisfies an information gain condition\n    based on the specified rule. If the condition is met, the DataFrame is updated with a \n    `dq_status` column indicating the rule applied. Otherwise, an empty DataFrame is returned.\n\n    Args:\n        df (dd.DataFrame): The input Dask DataFrame to evaluate.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field' (str): The column name to evaluate.\n            - 'check' (str): The type of check being performed (used for status annotation).\n            - 'value' (float): The threshold value for the information gain.\n\n    Returns:\n        dd.DataFrame: The original DataFrame with an added `dq_status` column if the condition \n        is met, or an empty DataFrame if the condition is not satisfied.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    ig = df[field].nunique().compute() or 0.0\n    if ig &gt; value:\n        return df.assign(dq_status=f\"{field}:{check}:{value}\")\n    return df.head(0).pipe(dd.from_pandas, npartitions=1)\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.has_max","title":"<code>has_max(df, rule)</code>","text":"<p>Identifies rows in a Dask DataFrame where the value of a specified field exceeds a given maximum value.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The name of the column to check. - 'check': A string describing the check (e.g., 'max'). - 'value': The maximum allowable value for the specified field.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame containing only the rows that violate the rule.           An additional column <code>dq_status</code> is added to indicate the rule violation           in the format \"{field}:{check}:{value}\".</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def has_max(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Identifies rows in a Dask DataFrame where the value of a specified field exceeds a given maximum value.\n\n    Parameters:\n        df (dd.DataFrame): The input Dask DataFrame to be checked.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field': The name of the column to check.\n            - 'check': A string describing the check (e.g., 'max').\n            - 'value': The maximum allowable value for the specified field.\n\n    Returns:\n        dd.DataFrame: A new Dask DataFrame containing only the rows that violate the rule.\n                      An additional column `dq_status` is added to indicate the rule violation\n                      in the format \"{field}:{check}:{value}\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    viol = df[df[field] &gt; value]\n    return viol.assign(dq_status=f\"{field}:{check}:{value}\")\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.has_mean","title":"<code>has_mean(df, rule)</code>","text":"<p>Checks if the mean of a specified field in a Dask DataFrame satisfies a given condition.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to evaluate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule to apply. It should include: - 'field' (str): The column name to calculate the mean for. - 'check' (str): The type of check to perform (e.g., 'greater_than'). - 'value' (float): The threshold value to compare the mean against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame with an additional column <code>dq_status</code> if the mean</p> <code>DataFrame</code> <p>satisfies the condition. If the condition is not met, an empty Dask DataFrame is returned.</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def has_mean(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Checks if the mean of a specified field in a Dask DataFrame satisfies a given condition.\n\n    Parameters:\n        df (dd.DataFrame): The input Dask DataFrame to evaluate.\n        rule (dict): A dictionary containing the rule to apply. It should include:\n            - 'field' (str): The column name to calculate the mean for.\n            - 'check' (str): The type of check to perform (e.g., 'greater_than').\n            - 'value' (float): The threshold value to compare the mean against.\n\n    Returns:\n        dd.DataFrame: A new Dask DataFrame with an additional column `dq_status` if the mean\n        satisfies the condition. If the condition is not met, an empty Dask DataFrame is returned.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    mean_val = df[field].mean().compute() or 0.0\n    if mean_val &gt; value:\n        return df.assign(dq_status=f\"{field}:{check}:{value}\")\n    return df.head(0).pipe(dd.from_pandas, npartitions=1)\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.has_min","title":"<code>has_min(df, rule)</code>","text":"<p>Checks if the values in a specified field of a Dask DataFrame are greater than  or equal to a given minimum value. Returns a DataFrame containing rows that  violate this rule, with an additional column indicating the data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to validate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The column name to check. - 'check': The type of check being performed (e.g., 'min'). - 'value': The minimum value to compare against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A DataFrame containing rows that do not meet the minimum value </p> <code>DataFrame</code> <p>requirement, with an additional column <code>dq_status</code> indicating the rule </p> <code>DataFrame</code> <p>violation in the format \"field:check:value\".</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def has_min(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Checks if the values in a specified field of a Dask DataFrame are greater than \n    or equal to a given minimum value. Returns a DataFrame containing rows that \n    violate this rule, with an additional column indicating the data quality status.\n\n    Args:\n        df (dd.DataFrame): The input Dask DataFrame to validate.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field': The column name to check.\n            - 'check': The type of check being performed (e.g., 'min').\n            - 'value': The minimum value to compare against.\n\n    Returns:\n        dd.DataFrame: A DataFrame containing rows that do not meet the minimum value \n        requirement, with an additional column `dq_status` indicating the rule \n        violation in the format \"field:check:value\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    viol = df[df[field] &lt; value]\n    return viol.assign(dq_status=f\"{field}:{check}:{value}\")\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.has_pattern","title":"<code>has_pattern(df, rule)</code>","text":"<p>Identifies rows in a Dask DataFrame that do not match a specified pattern.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The column name in the DataFrame to apply the pattern check. - 'check': A descriptive label for the type of check being performed. - 'value': The regex pattern to match against the specified column.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A DataFrame containing rows that do not match the specified pattern.           An additional column <code>dq_status</code> is added, indicating the rule details           in the format \"{field}:{check}:{value}\".</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def has_pattern(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Identifies rows in a Dask DataFrame that do not match a specified pattern.\n\n    Args:\n        df (dd.DataFrame): The input Dask DataFrame to be checked.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field': The column name in the DataFrame to apply the pattern check.\n            - 'check': A descriptive label for the type of check being performed.\n            - 'value': The regex pattern to match against the specified column.\n\n    Returns:\n        dd.DataFrame: A DataFrame containing rows that do not match the specified pattern.\n                      An additional column `dq_status` is added, indicating the rule details\n                      in the format \"{field}:{check}:{value}\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    viol = df[~df[field].str.match(value, na=False)]\n    return viol.assign(dq_status=f\"{field}:{check}:{value}\")\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.has_std","title":"<code>has_std(df, rule)</code>","text":"<p>Checks if the standard deviation of a specified field in a Dask DataFrame exceeds a given value.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to evaluate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The name of the column to calculate the standard deviation for. - 'check' (str): A descriptive label for the check being performed. - 'value' (float): The threshold value for the standard deviation.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame:  - If the standard deviation of the specified field exceeds the given value,    returns the original DataFrame with an additional column <code>dq_status</code> indicating the rule details. - If the standard deviation does not exceed the value, returns an empty DataFrame with the same structure.</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def has_std(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Checks if the standard deviation of a specified field in a Dask DataFrame exceeds a given value.\n\n    Parameters:\n        df (dd.DataFrame): The input Dask DataFrame to evaluate.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field' (str): The name of the column to calculate the standard deviation for.\n            - 'check' (str): A descriptive label for the check being performed.\n            - 'value' (float): The threshold value for the standard deviation.\n\n    Returns:\n        dd.DataFrame: \n            - If the standard deviation of the specified field exceeds the given value, \n              returns the original DataFrame with an additional column `dq_status` indicating the rule details.\n            - If the standard deviation does not exceed the value, returns an empty DataFrame with the same structure.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    std_val = df[field].std().compute() or 0.0\n    if std_val &gt; value:\n        return df.assign(dq_status=f\"{field}:{check}:{value}\")\n    return df.head(0).pipe(dd.from_pandas, npartitions=1)\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.has_sum","title":"<code>has_sum(df, rule)</code>","text":"<p>Checks if the sum of a specified field in a Dask DataFrame exceeds a given value  and returns a modified DataFrame with a status column if the condition is met.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to evaluate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The column name to sum. - 'check' (str): A descriptive label for the check (used in the status message). - 'value' (float): The threshold value to compare the sum against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame. If the sum exceeds the threshold, the DataFrame </p> <code>DataFrame</code> <p>will include a <code>dq_status</code> column with a status message. Otherwise, an empty </p> <code>DataFrame</code> <p>DataFrame with the same structure as the input is returned.</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def has_sum(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Checks if the sum of a specified field in a Dask DataFrame exceeds a given value \n    and returns a modified DataFrame with a status column if the condition is met.\n\n    Args:\n        df (dd.DataFrame): The input Dask DataFrame to evaluate.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field' (str): The column name to sum.\n            - 'check' (str): A descriptive label for the check (used in the status message).\n            - 'value' (float): The threshold value to compare the sum against.\n\n    Returns:\n        dd.DataFrame: A new Dask DataFrame. If the sum exceeds the threshold, the DataFrame \n        will include a `dq_status` column with a status message. Otherwise, an empty \n        DataFrame with the same structure as the input is returned.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    sum_val = df[field].sum().compute() or 0.0\n    if sum_val &gt; value:\n        return df.assign(dq_status=f\"{field}:{check}:{value}\")\n    return df.head(0).pipe(dd.from_pandas, npartitions=1)\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.is_between","title":"<code>is_between(df, rule)</code>","text":"<p>Filters a Dask DataFrame to identify rows where a specified field's value  does not fall within a given range.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should  include: - 'field': The column name in the DataFrame to check. - 'check': The type of check being performed (e.g., \"between\"). - 'value': A string representing the range in the format \"[lo,hi]\".</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new DataFrame containing only the rows that violate </p> <code>DataFrame</code> <p>the specified range condition. An additional column <code>dq_status</code> is </p> <code>DataFrame</code> <p>added to indicate the field, check, and value that caused the violation.</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def is_between(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Filters a Dask DataFrame to identify rows where a specified field's value \n    does not fall within a given range.\n\n    Args:\n        df (dd.DataFrame): The input Dask DataFrame to be checked.\n        rule (dict): A dictionary containing the rule parameters. It should \n            include:\n            - 'field': The column name in the DataFrame to check.\n            - 'check': The type of check being performed (e.g., \"between\").\n            - 'value': A string representing the range in the format \"[lo,hi]\".\n\n    Returns:\n        dd.DataFrame: A new DataFrame containing only the rows that violate \n        the specified range condition. An additional column `dq_status` is \n        added to indicate the field, check, and value that caused the violation.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    lo, hi = value.strip(\"[]\").split(\",\")\n    viol = df[~df[field].between(__convert_value(lo), __convert_value(hi))]\n    return viol.assign(dq_status=f\"{field}:{check}:{value}\")\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.is_complete","title":"<code>is_complete(df, rule)</code>","text":"<p>Checks for completeness of a specified field in a Dask DataFrame based on a given rule.</p> <p>This function identifies rows where the specified field is null and marks them as violations. It then assigns a data quality status to these rows in the resulting DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The Dask DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The name of the field to check for completeness. - 'check': The type of check being performed (e.g., 'is_complete'). - 'value': Additional value associated with the rule (not used in this function).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A DataFrame containing rows where the specified field is null, </p> <code>DataFrame</code> <p>with an additional column <code>dq_status</code> indicating the data quality status in the format </p> <code>DataFrame</code> <p>\"{field}:{check}:{value}\".</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def is_complete(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Checks for completeness of a specified field in a Dask DataFrame based on a given rule.\n\n    This function identifies rows where the specified field is null and marks them as violations.\n    It then assigns a data quality status to these rows in the resulting DataFrame.\n\n    Args:\n        df (dd.DataFrame): The Dask DataFrame to be checked.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field': The name of the field to check for completeness.\n            - 'check': The type of check being performed (e.g., 'is_complete').\n            - 'value': Additional value associated with the rule (not used in this function).\n\n    Returns:\n        dd.DataFrame: A DataFrame containing rows where the specified field is null, \n        with an additional column `dq_status` indicating the data quality status in the format \n        \"{field}:{check}:{value}\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    viol = df[df[field].isnull()]\n    return viol.assign(dq_status=f\"{field}:{check}:{value}\")\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.is_composite_key","title":"<code>is_composite_key(df, rule)</code>","text":"<p>Determines if the given DataFrame satisfies the composite key condition based on the provided rule.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A Dask DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary defining the composite key rule.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A Dask DataFrame indicating whether the composite key condition is met.</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def is_composite_key(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Determines if the given DataFrame satisfies the composite key condition based on the provided rule.\n\n    Args:\n        df (dd.DataFrame): A Dask DataFrame to be checked.\n        rule (dict): A dictionary defining the composite key rule.\n\n    Returns:\n        dd.DataFrame: A Dask DataFrame indicating whether the composite key condition is met.\n    \"\"\"\n    return are_unique(df, rule)\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.is_contained_in","title":"<code>is_contained_in(df, rule)</code>","text":"<p>Filters a Dask DataFrame to identify rows where the values in a specified field are not contained within a given list of allowed values.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The column name in the DataFrame to check. - 'check': The type of check being performed (e.g., \"is_contained_in\"). - 'value': A string representation of a list of allowed values (e.g., \"[value1, value2]\").</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame containing only the rows that violate the rule.</p> <code>DataFrame</code> <p>An additional column <code>dq_status</code> is added to indicate the rule violation in the format:</p> <code>DataFrame</code> <p>\"{field}:{check}:{value}\".</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def is_contained_in(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Filters a Dask DataFrame to identify rows where the values in a specified field\n    are not contained within a given list of allowed values.\n\n    Args:\n        df (dd.DataFrame): The input Dask DataFrame to be checked.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field': The column name in the DataFrame to check.\n            - 'check': The type of check being performed (e.g., \"is_contained_in\").\n            - 'value': A string representation of a list of allowed values (e.g., \"[value1, value2]\").\n\n    Returns:\n        dd.DataFrame: A new Dask DataFrame containing only the rows that violate the rule.\n        An additional column `dq_status` is added to indicate the rule violation in the format:\n        \"{field}:{check}:{value}\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    lst = [v.strip() for v in value.strip(\"[]\").split(\",\")]\n    viol = df[~df[field].isin(lst)]\n    return viol.assign(dq_status=f\"{field}:{check}:{value}\")\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.is_equal","title":"<code>is_equal(df, rule)</code>","text":"<p>Filters a Dask DataFrame to identify rows where a specified field does not equal a given value.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The column name in the DataFrame to be checked. - 'check': The type of check to perform (expected to be 'equal' for this function). - 'value': The value to compare against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new DataFrame containing rows that violate the equality rule.           An additional column <code>dq_status</code> is added, indicating the rule details           in the format \"{field}:{check}:{value}\".</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def is_equal(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Filters a Dask DataFrame to identify rows where a specified field does not equal a given value.\n\n    Args:\n        df (dd.DataFrame): The input Dask DataFrame to be checked.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field': The column name in the DataFrame to be checked.\n            - 'check': The type of check to perform (expected to be 'equal' for this function).\n            - 'value': The value to compare against.\n\n    Returns:\n        dd.DataFrame: A new DataFrame containing rows that violate the equality rule.\n                      An additional column `dq_status` is added, indicating the rule details\n                      in the format \"{field}:{check}:{value}\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    viol = df[~df[field].eq(value)]\n    return viol.assign(dq_status=f\"{field}:{check}:{value}\")\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.is_equal_than","title":"<code>is_equal_than(df, rule)</code>","text":"<p>Filters a Dask DataFrame to identify rows where the specified field does not equal the given value.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The column name in the DataFrame to check. - 'check': The type of check being performed (expected to be 'equal' for this function). - 'value': The value to compare against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new DataFrame containing rows that violate the equality rule.            An additional column <code>dq_status</code> is added, indicating the rule details            in the format \"{field}:{check}:{value}\".</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def is_equal_than(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Filters a Dask DataFrame to identify rows where the specified field does not equal the given value.\n\n    Args:\n        df (dd.DataFrame): The input Dask DataFrame to be checked.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field': The column name in the DataFrame to check.\n            - 'check': The type of check being performed (expected to be 'equal' for this function).\n            - 'value': The value to compare against.\n\n    Returns:\n        dd.DataFrame: A new DataFrame containing rows that violate the equality rule. \n                      An additional column `dq_status` is added, indicating the rule details \n                      in the format \"{field}:{check}:{value}\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    viol = df[~df[field].eq(value)]\n    return viol.assign(dq_status=f\"{field}:{check}:{value}\")\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.is_greater_or_equal_than","title":"<code>is_greater_or_equal_than(df, rule)</code>","text":"<p>Filters a Dask DataFrame to identify rows where a specified field's value  is less than a given threshold, and annotates the resulting rows with a  data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should           include the following keys:          - 'field': The column name in the DataFrame to check.          - 'check': The type of check being performed (e.g., 'greater_or_equal').          - 'value': The threshold value to compare against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame containing only the rows that            violate the rule, with an additional column <code>dq_status</code>            indicating the field, check type, and threshold value.</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def is_greater_or_equal_than(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Filters a Dask DataFrame to identify rows where a specified field's value \n    is less than a given threshold, and annotates the resulting rows with a \n    data quality status.\n\n    Args:\n        df (dd.DataFrame): The input Dask DataFrame to be checked.\n        rule (dict): A dictionary containing the rule parameters. It should \n                     include the following keys:\n                     - 'field': The column name in the DataFrame to check.\n                     - 'check': The type of check being performed (e.g., 'greater_or_equal').\n                     - 'value': The threshold value to compare against.\n\n    Returns:\n        dd.DataFrame: A new Dask DataFrame containing only the rows that \n                      violate the rule, with an additional column `dq_status` \n                      indicating the field, check type, and threshold value.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    viol = df[df[field] &lt; value]\n    return viol.assign(dq_status=f\"{field}:{check}:{value}\")\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.is_greater_than","title":"<code>is_greater_than(df, rule)</code>","text":"<p>Filters a Dask DataFrame to identify rows where the value in a specified field is greater than a given threshold and annotates the result with a data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The column name to check. - 'check' (str): The type of check being performed (e.g., 'greater_than'). - 'value' (numeric): The threshold value to compare against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A filtered DataFrame containing rows that violate the rule,</p> <code>DataFrame</code> <p>with an additional column <code>dq_status</code> indicating the rule details in the format</p> <code>DataFrame</code> <p>\"{field}:{check}:{value}\".</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def is_greater_than(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Filters a Dask DataFrame to identify rows where the value in a specified field\n    is greater than a given threshold and annotates the result with a data quality status.\n\n    Args:\n        df (dd.DataFrame): The input Dask DataFrame to be checked.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field' (str): The column name to check.\n            - 'check' (str): The type of check being performed (e.g., 'greater_than').\n            - 'value' (numeric): The threshold value to compare against.\n\n    Returns:\n        dd.DataFrame: A filtered DataFrame containing rows that violate the rule,\n        with an additional column `dq_status` indicating the rule details in the format\n        \"{field}:{check}:{value}\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    viol = df[df[field] &gt; value]\n    return viol.assign(dq_status=f\"{field}:{check}:{value}\")\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.is_legit","title":"<code>is_legit(df, rule)</code>","text":"<p>Validates a Dask DataFrame against a specified rule and returns rows that violate the rule.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The Dask DataFrame to validate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the validation rule. It should include: - 'field': The column name in the DataFrame to validate. - 'check': The type of validation check (e.g., regex, condition). - 'value': The value or pattern to validate against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new DataFrame containing rows that violate the rule, with an additional</p> <code>DataFrame</code> <p>column <code>dq_status</code> indicating the field, check, and value of the failed validation.</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def is_legit(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Validates a Dask DataFrame against a specified rule and returns rows that violate the rule.\n\n    Args:\n        df (dd.DataFrame): The Dask DataFrame to validate.\n        rule (dict): A dictionary containing the validation rule. It should include:\n            - 'field': The column name in the DataFrame to validate.\n            - 'check': The type of validation check (e.g., regex, condition).\n            - 'value': The value or pattern to validate against.\n\n    Returns:\n        dd.DataFrame: A new DataFrame containing rows that violate the rule, with an additional\n        column `dq_status` indicating the field, check, and value of the failed validation.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    s = df[field].astype(\"string\")\n    mask = s.notnull() &amp; s.str.contains(r\"^\\S+$\", na=False)\n    viol = df[~mask]\n    return viol.assign(dq_status=f\"{field}:{check}:{value}\")\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.is_less_or_equal_than","title":"<code>is_less_or_equal_than(df, rule)</code>","text":"<p>Filters a Dask DataFrame to identify rows where the value in a specified field is greater than a given threshold, violating a \"less or equal than\" rule.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The column name in the DataFrame to be checked. - 'check': The type of check being performed (e.g., \"less_or_equal_than\"). - 'value': The threshold value to compare against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new DataFrame containing only the rows that violate the rule.</p> <code>DataFrame</code> <p>An additional column <code>dq_status</code> is added to indicate the rule violation</p> <code>DataFrame</code> <p>in the format \"{field}:{check}:{value}\".</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def is_less_or_equal_than(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Filters a Dask DataFrame to identify rows where the value in a specified field\n    is greater than a given threshold, violating a \"less or equal than\" rule.\n\n    Args:\n        df (dd.DataFrame): The input Dask DataFrame to be checked.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field': The column name in the DataFrame to be checked.\n            - 'check': The type of check being performed (e.g., \"less_or_equal_than\").\n            - 'value': The threshold value to compare against.\n\n    Returns:\n        dd.DataFrame: A new DataFrame containing only the rows that violate the rule.\n        An additional column `dq_status` is added to indicate the rule violation\n        in the format \"{field}:{check}:{value}\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    viol = df[df[field] &gt; value]\n    return viol.assign(dq_status=f\"{field}:{check}:{value}\")\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.is_less_than","title":"<code>is_less_than(df, rule)</code>","text":"<p>Filters a Dask DataFrame to identify rows where the value in a specified field is greater than or equal to a given threshold, and marks them with a data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The column name to check. - 'check' (str): The type of check being performed (e.g., \"less_than\"). - 'value' (numeric): The threshold value for the check.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame containing only the rows that violate the rule,</p> <code>DataFrame</code> <p>with an additional column <code>dq_status</code> indicating the rule that was violated in the</p> <code>DataFrame</code> <p>format \"{field}:{check}:{value}\".</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def is_less_than(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Filters a Dask DataFrame to identify rows where the value in a specified field\n    is greater than or equal to a given threshold, and marks them with a data quality status.\n\n    Args:\n        df (dd.DataFrame): The input Dask DataFrame to be checked.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field' (str): The column name to check.\n            - 'check' (str): The type of check being performed (e.g., \"less_than\").\n            - 'value' (numeric): The threshold value for the check.\n\n    Returns:\n        dd.DataFrame: A new Dask DataFrame containing only the rows that violate the rule,\n        with an additional column `dq_status` indicating the rule that was violated in the\n        format \"{field}:{check}:{value}\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    viol = df[df[field] &gt;= value]\n    return viol.assign(dq_status=f\"{field}:{check}:{value}\")\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.is_negative","title":"<code>is_negative(df, rule)</code>","text":"<p>Identifies rows in a Dask DataFrame where the specified field does not satisfy a \"negative\" check.</p> <p>This function filters the DataFrame to find rows where the value in the specified field is greater than or equal to 0 (i.e., not negative). It then assigns a new column <code>dq_status</code> to indicate the rule that was violated.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - <code>field</code> (str): The name of the column to check. - <code>check</code> (str): The type of check being performed (e.g., \"negative\"). - <code>value</code> (any): The expected value or condition for the check.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame containing only the rows that violate the rule,</p> <code>DataFrame</code> <p>with an additional column <code>dq_status</code> describing the violation in the format</p> <code>DataFrame</code> <p>\"{field}:{check}:{value}\".</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def is_negative(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Identifies rows in a Dask DataFrame where the specified field does not satisfy a \"negative\" check.\n\n    This function filters the DataFrame to find rows where the value in the specified field\n    is greater than or equal to 0 (i.e., not negative). It then assigns a new column `dq_status`\n    to indicate the rule that was violated.\n\n    Args:\n        df (dd.DataFrame): The input Dask DataFrame to be checked.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - `field` (str): The name of the column to check.\n            - `check` (str): The type of check being performed (e.g., \"negative\").\n            - `value` (any): The expected value or condition for the check.\n\n    Returns:\n        dd.DataFrame: A new Dask DataFrame containing only the rows that violate the rule,\n        with an additional column `dq_status` describing the violation in the format\n        \"{field}:{check}:{value}\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    viol = df[df[field] &gt;= 0]\n    return viol.assign(dq_status=f\"{field}:{check}:{value}\")\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.is_positive","title":"<code>is_positive(df, rule)</code>","text":"<p>Checks if the values in a specified field of a Dask DataFrame are positive.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to validate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The name of the field to check. - 'check': The type of check being performed (e.g., \"is_positive\"). - 'value': The expected value or condition (e.g., \"0\").</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A DataFrame containing rows where the specified field has </p> <code>DataFrame</code> <p>negative values, with an additional column <code>dq_status</code> indicating the </p> <code>DataFrame</code> <p>field, check, and value that failed.</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def is_positive(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Checks if the values in a specified field of a Dask DataFrame are positive.\n\n    Args:\n        df (dd.DataFrame): The input Dask DataFrame to validate.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field': The name of the field to check.\n            - 'check': The type of check being performed (e.g., \"is_positive\").\n            - 'value': The expected value or condition (e.g., \"0\").\n\n    Returns:\n        dd.DataFrame: A DataFrame containing rows where the specified field has \n        negative values, with an additional column `dq_status` indicating the \n        field, check, and value that failed.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    viol = df[df[field] &lt; 0]\n    return viol.assign(dq_status=f\"{field}:{check}:{value}\")\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.is_primary_key","title":"<code>is_primary_key(df, rule)</code>","text":"<p>Determines if the specified rule identifies a primary key in the given Dask DataFrame.</p> <p>This function checks whether the combination of columns specified in the rule results in unique values across the DataFrame, effectively identifying a primary key.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The Dask DataFrame to evaluate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary defining the rule to check for primary key uniqueness.          Typically, this includes the column(s) to be evaluated.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A Dask DataFrame indicating whether the rule satisfies the primary key condition.</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def is_primary_key(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Determines if the specified rule identifies a primary key in the given Dask DataFrame.\n\n    This function checks whether the combination of columns specified in the rule\n    results in unique values across the DataFrame, effectively identifying a primary key.\n\n    Args:\n        df (dd.DataFrame): The Dask DataFrame to evaluate.\n        rule (dict): A dictionary defining the rule to check for primary key uniqueness.\n                     Typically, this includes the column(s) to be evaluated.\n\n    Returns:\n        dd.DataFrame: A Dask DataFrame indicating whether the rule satisfies the primary key condition.\n    \"\"\"\n    return is_unique(df, rule)\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.is_unique","title":"<code>is_unique(df, rule)</code>","text":"<p>Checks for uniqueness of a specified field in a Dask DataFrame based on a given rule.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The Dask DataFrame to check for uniqueness.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to include: - 'field': The column name to check for uniqueness. - 'check': The type of check being performed (e.g., \"unique\"). - 'value': Additional value or metadata related to the check.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A DataFrame containing rows that violate the uniqueness rule, </p> <code>DataFrame</code> <p>with an additional column <code>dq_status</code> indicating the rule that was violated </p> <code>DataFrame</code> <p>in the format \"{field}:{check}:{value}\".</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def is_unique(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Checks for uniqueness of a specified field in a Dask DataFrame based on a given rule.\n\n    Parameters:\n        df (dd.DataFrame): The Dask DataFrame to check for uniqueness.\n        rule (dict): A dictionary containing the rule parameters. It is expected to include:\n            - 'field': The column name to check for uniqueness.\n            - 'check': The type of check being performed (e.g., \"unique\").\n            - 'value': Additional value or metadata related to the check.\n\n    Returns:\n        dd.DataFrame: A DataFrame containing rows that violate the uniqueness rule, \n        with an additional column `dq_status` indicating the rule that was violated \n        in the format \"{field}:{check}:{value}\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    counts = df[field].value_counts().compute()\n    dup_vals = counts[counts &gt; 1].index.tolist()\n    viol = df[df[field].isin(dup_vals)]\n    return viol.assign(dq_status=f\"{field}:{check}:{value}\")\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.not_contained_in","title":"<code>not_contained_in(df, rule)</code>","text":"<p>Filters a Dask DataFrame to identify rows where the specified field's value is  contained in a given list, and assigns a data quality status to the resulting rows.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The column name in the DataFrame to check. - 'check': The type of check being performed (e.g., \"not_contained_in\"). - 'value': A string representation of a list of values to check against,    formatted as \"[value1, value2, ...]\".</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new DataFrame containing only the rows where the specified </p> <code>DataFrame</code> <p>field's value is in the provided list, with an additional column <code>dq_status</code> </p> <code>DataFrame</code> <p>indicating the rule applied in the format \"field:check:value\".</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def not_contained_in(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Filters a Dask DataFrame to identify rows where the specified field's value is \n    contained in a given list, and assigns a data quality status to the resulting rows.\n\n    Args:\n        df (dd.DataFrame): The input Dask DataFrame to be filtered.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field': The column name in the DataFrame to check.\n            - 'check': The type of check being performed (e.g., \"not_contained_in\").\n            - 'value': A string representation of a list of values to check against, \n              formatted as \"[value1, value2, ...]\".\n\n    Returns:\n        dd.DataFrame: A new DataFrame containing only the rows where the specified \n        field's value is in the provided list, with an additional column `dq_status` \n        indicating the rule applied in the format \"field:check:value\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    lst = [v.strip() for v in value.strip(\"[]\").split(\",\")]\n    viol = df[df[field].isin(lst)]\n    return viol.assign(dq_status=f\"{field}:{check}:{value}\")\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.satisfies","title":"<code>satisfies(df, rule)</code>","text":"<p>Filters a Dask DataFrame based on a rule and returns rows that do not satisfy the rule.</p> <p>The function evaluates a rule on the given Dask DataFrame and identifies rows that violate the rule. The rule is specified as a dictionary containing a field, a check, and a value. The rule's logical expression is converted to Python syntax for evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary specifying the rule to evaluate. It should contain: - 'field': The column name in the DataFrame to evaluate. - 'check': The type of check or condition to apply. - 'value': The value or expression to evaluate against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame containing rows that do not satisfy the rule.</p> <code>DataFrame</code> <p>An additional column <code>dq_status</code> is added, which contains a string in the format</p> <code>DataFrame</code> <p>\"{field}:{check}:{value}\" to indicate the rule that was violated.</p> Example <p>import dask.dataframe as dd data = {'col1': [1, 2, 3], 'col2': [4, 5, 6]} df = dd.from_pandas(pd.DataFrame(data), npartitions=1) rule = {'field': 'col1', 'check': '&gt;', 'value': '2'} result = satisfies(df, rule) result.compute()</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def satisfies(df: dd.DataFrame, rule: dict) -&gt; dd.DataFrame:\n    \"\"\"\n    Filters a Dask DataFrame based on a rule and returns rows that do not satisfy the rule.\n\n    The function evaluates a rule on the given Dask DataFrame and identifies rows that\n    violate the rule. The rule is specified as a dictionary containing a field, a check,\n    and a value. The rule's logical expression is converted to Python syntax for evaluation.\n\n    Args:\n        df (dd.DataFrame): The input Dask DataFrame to be filtered.\n        rule (dict): A dictionary specifying the rule to evaluate. It should contain:\n            - 'field': The column name in the DataFrame to evaluate.\n            - 'check': The type of check or condition to apply.\n            - 'value': The value or expression to evaluate against.\n\n    Returns:\n        dd.DataFrame: A new Dask DataFrame containing rows that do not satisfy the rule.\n        An additional column `dq_status` is added, which contains a string in the format\n        \"{field}:{check}:{value}\" to indicate the rule that was violated.\n\n    Example:\n        &gt;&gt;&gt; import dask.dataframe as dd\n        &gt;&gt;&gt; data = {'col1': [1, 2, 3], 'col2': [4, 5, 6]}\n        &gt;&gt;&gt; df = dd.from_pandas(pd.DataFrame(data), npartitions=1)\n        &gt;&gt;&gt; rule = {'field': 'col1', 'check': '&gt;', 'value': '2'}\n        &gt;&gt;&gt; result = satisfies(df, rule)\n        &gt;&gt;&gt; result.compute()\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    py_expr = value\n    py_expr = re.sub(r\"(?&lt;![=!&lt;&gt;])=(?!=)\", \"==\", py_expr)\n    py_expr = re.sub(r\"\\bAND\\b\", \"&amp;\", py_expr, flags=re.IGNORECASE)\n    py_expr = re.sub(r\"\\bOR\\b\", \"|\", py_expr, flags=re.IGNORECASE)\n\n    def _filter_viol(pdf: pd.DataFrame) -&gt; pd.DataFrame:\n        mask = pdf.eval(py_expr)\n        return pdf.loc[~mask]\n\n    meta = df._meta\n    viol = df.map_partitions(_filter_viol, meta=meta)\n    return viol.assign(dq_status=f\"{field}:{check}:{value}\")\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.summarize","title":"<code>summarize(qc_ddf, rules, total_rows)</code>","text":"<p>Summarizes quality check results by evaluating rules against a Dask DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>qc_ddf</code> <code>DataFrame</code> <p>A Dask DataFrame containing quality check results.  The DataFrame must include a \"dq_status\" column with rule violations  in the format \"column:rule:value\".</p> required <code>rules</code> <code>list[dict]</code> <p>A list of dictionaries representing the rules to be  evaluated. Each dictionary should include keys such as \"column\",  \"rule\", \"value\", and \"pass_threshold\".</p> required <code>total_rows</code> <code>int</code> <p>The total number of rows in the original dataset.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A summarized Pandas DataFrame containing the following columns: - id: Unique identifier for each rule evaluation. - timestamp: Timestamp of the summary generation. - check: The type of check performed (e.g., \"Quality Check\"). - level: The severity level of the check (e.g., \"WARNING\"). - column: The column name associated with the rule. - rule: The rule being evaluated. - value: The value associated with the rule. - rows: The total number of rows in the dataset. - violations: The number of rows that violated the rule. - pass_rate: The proportion of rows that passed the rule. - pass_threshold: The threshold for passing the rule. - status: The status of the rule evaluation (\"PASS\" or \"FAIL\").</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def summarize(qc_ddf: dd.DataFrame, rules: list[dict], total_rows: int) -&gt; pd.DataFrame:\n    \"\"\"\n    Summarizes quality check results by evaluating rules against a Dask DataFrame.\n\n    Args:\n        qc_ddf (dd.DataFrame): A Dask DataFrame containing quality check results. \n            The DataFrame must include a \"dq_status\" column with rule violations \n            in the format \"column:rule:value\".\n        rules (list[dict]): A list of dictionaries representing the rules to be \n            evaluated. Each dictionary should include keys such as \"column\", \n            \"rule\", \"value\", and \"pass_threshold\".\n        total_rows (int): The total number of rows in the original dataset.\n\n    Returns:\n        pd.DataFrame: A summarized Pandas DataFrame containing the following columns:\n            - id: Unique identifier for each rule evaluation.\n            - timestamp: Timestamp of the summary generation.\n            - check: The type of check performed (e.g., \"Quality Check\").\n            - level: The severity level of the check (e.g., \"WARNING\").\n            - column: The column name associated with the rule.\n            - rule: The rule being evaluated.\n            - value: The value associated with the rule.\n            - rows: The total number of rows in the dataset.\n            - violations: The number of rows that violated the rule.\n            - pass_rate: The proportion of rows that passed the rule.\n            - pass_threshold: The threshold for passing the rule.\n            - status: The status of the rule evaluation (\"PASS\" or \"FAIL\").\n    \"\"\"\n    df = qc_ddf.compute()\n\n    df = df[df[\"dq_status\"].astype(bool)]\n    split = df[\"dq_status\"].str.split(\":\", expand=True)\n    split.columns = [\"column\", \"rule\", \"value\"]\n    viol_count = (\n        split.groupby([\"column\", \"rule\", \"value\"], dropna=False)\n        .size()\n        .reset_index(name=\"violations\")\n    )\n\n    rules_df = _rules_to_df(rules)\n\n    rules_df[\"value\"] = rules_df[\"value\"].fillna(\"\")\n    viol_count[\"value\"] = viol_count[\"value\"].fillna(\"\")\n\n    summary = (\n        rules_df.merge(viol_count, on=[\"column\", \"rule\", \"value\"], how=\"left\")\n        .assign(\n            violations=lambda df: df[\"violations\"].fillna(0).astype(int),\n            rows=total_rows,\n            pass_rate=lambda df: (total_rows - df[\"violations\"]) / total_rows,\n            status=lambda df: np.where(\n                df[\"pass_rate\"] &gt;= df[\"pass_threshold\"], \"PASS\", \"FAIL\"\n            ),\n            timestamp=datetime.now().replace(second=0, microsecond=0),\n            check=\"Quality Check\",\n            level=\"WARNING\",\n        )\n        .reset_index(drop=True)\n    )\n\n    summary.insert(0, \"id\", np.arange(1, len(summary) + 1))\n    summary = summary[\n        [\n            \"id\",\n            \"timestamp\",\n            \"check\",\n            \"level\",\n            \"column\",\n            \"rule\",\n            \"value\",\n            \"rows\",\n            \"violations\",\n            \"pass_rate\",\n            \"pass_threshold\",\n            \"status\",\n        ]\n    ]\n\n    return dd.from_pandas(summary, npartitions=1)\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.validate","title":"<code>validate(df, rules)</code>","text":"<p>Validate a Dask DataFrame against a set of rules and return the aggregated results  and raw violations.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to validate.</p> required <code>rules</code> <code>list[dict]</code> <p>A list of validation rules. Each rule is a dictionary  containing the following keys: - \"check_type\" (str): The name of the validation function to execute. - \"value\" (optional): The value to be used in the validation function. - \"execute\" (optional, bool): Whether to execute the rule. Defaults to True.</p> required <p>Returns:</p> Type Description <code>tuple[DataFrame, DataFrame]</code> <p>tuple[dd.DataFrame, dd.DataFrame]:  - The first DataFrame contains the aggregated validation results,    with a concatenated \"dq_status\" column indicating the validation status. - The second DataFrame contains the raw violations for each rule.</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def validate(df: dd.DataFrame, rules: list[dict]) -&gt; tuple[dd.DataFrame, dd.DataFrame]:\n    \"\"\"\n    Validate a Dask DataFrame against a set of rules and return the aggregated results \n    and raw violations.\n\n    Args:\n        df (dd.DataFrame): The input Dask DataFrame to validate.\n        rules (list[dict]): A list of validation rules. Each rule is a dictionary \n            containing the following keys:\n            - \"check_type\" (str): The name of the validation function to execute.\n            - \"value\" (optional): The value to be used in the validation function.\n            - \"execute\" (optional, bool): Whether to execute the rule. Defaults to True.\n\n    Returns:\n        tuple[dd.DataFrame, dd.DataFrame]: \n            - The first DataFrame contains the aggregated validation results, \n              with a concatenated \"dq_status\" column indicating the validation status.\n            - The second DataFrame contains the raw violations for each rule.\n    \"\"\"\n    empty = dd.from_pandas(\n        pd.DataFrame(columns=df.columns.tolist() + [\"dq_status\"]), npartitions=1\n    )\n    raw_df = empty\n\n    for rule in rules:\n        if not rule.get(\"execute\", True):\n            continue\n        rule_name = rule[\"check_type\"]\n        func = globals().get(rule_name)\n        if func is None:\n            warnings.warn(f\"Unknown rule: {rule_name}\")\n            continue\n\n        raw_val = rule.get(\"value\")\n        try:\n            value = (\n                __convert_value(raw_val)\n                if isinstance(raw_val, str) and raw_val not in (\"\", \"NULL\")\n                else raw_val\n            )\n        except ValueError:\n            value = raw_val\n\n        viol = func(df, rule)\n        raw_df = dd.concat([raw_df, viol], interleave_partitions=True)\n\n    group_cols = [c for c in df.columns if c != \"dq_status\"]\n\n    def _concat_status(series: pd.Series) -&gt; str:\n        return \";\".join([s for s in series.astype(str) if s])\n\n    agg_df = (\n        raw_df.groupby(group_cols)\n        .dq_status.apply(_concat_status, meta=(\"dq_status\", \"object\"))\n        .reset_index()\n    )\n\n    return agg_df, raw_df\n</code></pre>"},{"location":"api/engine/engine-dask/#sumeh.engine.dask_engine.validate_schema","title":"<code>validate_schema(df, expected)</code>","text":"<p>Validates the schema of a Dask DataFrame against an expected schema.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The Dask DataFrame whose schema is to be validated.</p> required <code>expected</code> <code>List[Dict[str, Any]]</code> <p>A list of dictionaries representing the expected schema. Each dictionary should define the expected column name and its properties.</p> required <p>Returns:</p> Type Description <code>Tuple[bool, List[Tuple[str, str]]]</code> <p>Tuple[bool, List[Tuple[str, str]]]: A tuple where the first element is a boolean indicating whether the schema matches the expected schema, and the second element is a list of tuples containing mismatched column names and their respective issues.</p> Source code in <code>sumeh/engine/dask_engine.py</code> <pre><code>def validate_schema(df: dd.DataFrame, expected: List[Dict[str, Any]]) -&gt; Tuple[bool, List[Tuple[str, str]]]:\n    \"\"\"\n    Validates the schema of a Dask DataFrame against an expected schema.\n\n    Args:\n        df (dd.DataFrame): The Dask DataFrame whose schema is to be validated.\n        expected (List[Dict[str, Any]]): A list of dictionaries representing the expected schema.\n            Each dictionary should define the expected column name and its properties.\n\n    Returns:\n        Tuple[bool, List[Tuple[str, str]]]: A tuple where the first element is a boolean indicating\n            whether the schema matches the expected schema, and the second element is a list of\n            tuples containing mismatched column names and their respective issues.\n    \"\"\"\n    actual = __dask_schema_to_list(df)\n    return __compare_schemas(actual, expected)\n</code></pre>"},{"location":"api/engine/engine-duckdb/","title":"Module <code>sumeh.engine.duckdb_engine</code>","text":"<p>This module provides utilities for generating and validating SQL expressions  and data quality rules using DuckDB. It includes functions for building SQL  expressions, validating dataframes against rules, summarizing rule violations,  and schema validation.</p> <p>Classes:</p> Name Description <code>__RuleCtx</code> <p>A dataclass representing the context required to generate SQL        expressions for data quality rules.</p> <p>Functions:</p> Name Description <code>__escape_single_quotes</code> <p>str) -&gt; str: Escapes single quotes in a string for SQL compatibility.</p> <code>_format_sequence</code> <p>Any) -&gt; str: Formats a sequence (list, tuple, or string) into a SQL-compatible  representation for IN/NOT IN clauses.</p> <code>_is_complete</code> <p>__RuleCtx) -&gt; str: Generates a SQL expression to check if a column is not NULL.</p> <code>_are_complete</code> <p>__RuleCtx) -&gt; str: Generates a SQL expression to check if all columns in a list are not NULL.</p> <code>_is_unique</code> <p>__RuleCtx) -&gt; str: Generates a SQL expression to check if a column has unique values.</p> <code>_are_unique</code> <p>__RuleCtx) -&gt; str: Generates a SQL expression to check if a combination of columns has unique values.</p> <code>_is_greater_than</code> <p>__RuleCtx) -&gt; str: Generates a SQL expression to check if a column's value is greater than a given value.</p> <code>_is_less_than</code> <p>__RuleCtx) -&gt; str: Generates a SQL expression to check if a column's value is less than a given value.</p> <code>_is_greater_or_equal_than</code> <p>__RuleCtx) -&gt; str: Generates a SQL expression to check if a column's value is greater than or equal to a given value.</p> <code>_is_less_or_equal_than</code> <p>__RuleCtx) -&gt; str: Generates a SQL expression to check if a column's value is less than or equal to a given value.</p> <code>_is_equal_than</code> <p>__RuleCtx) -&gt; str: Generates a SQL expression to check if a column's value is equal to a given value.</p> <code>_is_between</code> <p>__RuleCtx) -&gt; str: Generates a SQL expression to check if a column's value is between two values.</p> <code>_has_pattern</code> <p>__RuleCtx) -&gt; str: Generates a SQL expression to check if a column's value matches a regular expression pattern.</p> <code>_is_contained_in</code> <p>__RuleCtx) -&gt; str: Generates a SQL expression to check if a column's value is in a given sequence.</p> <code>_not_contained_in</code> <p>__RuleCtx) -&gt; str: Generates a SQL expression to check if a column's value is not in a given sequence.</p> <code>_satisfies</code> <p>__RuleCtx) -&gt; str: Generates a SQL expression based on a custom condition provided as a string.</p> <code>_build_union_sql</code> <p>List[Dict]) -&gt; str: Builds a SQL query that combines multiple rule-based conditions into a UNION ALL query.</p>"},{"location":"api/engine/engine-duckdb/#sumeh.engine.duckdb_engine.__RULE_DISPATCH","title":"<code>__RULE_DISPATCH = {'is_complete': _is_complete, 'are_complete': _are_complete, 'is_unique': _is_unique, 'are_unique': _are_unique, 'is_greater_than': _is_greater_than, 'is_less_than': _is_less_than, 'is_greater_or_equal_than': _is_greater_or_equal_than, 'is_less_or_equal_than': _is_less_or_equal_than, 'is_equal_than': _is_equal_than, 'is_between': _is_between, 'has_pattern': _has_pattern, 'is_contained_in': _is_contained_in, 'not_contained_in': _not_contained_in, 'satisfies': _satisfies}</code>  <code>module-attribute</code>","text":""},{"location":"api/engine/engine-duckdb/#sumeh.engine.duckdb_engine.__RuleCtx","title":"<code>__RuleCtx</code>  <code>dataclass</code>","text":"<p>__RuleCtx is a context class used to define rules for processing data.</p> <p>Attributes:</p> Name Type Description <code>column</code> <code>Any</code> <p>Represents the column(s) to which the rule applies.            It can be a string (for a single column) or a list of strings (for multiple columns).</p> <code>value</code> <code>Any</code> <p>The value associated with the rule. The type of this value depends on the specific rule implementation.</p> <code>name</code> <code>str</code> <p>The name of the rule, typically used to indicate the type of check being performed.</p> Source code in <code>sumeh/engine/duckdb_engine.py</code> <pre><code>@dataclass(slots=True)\nclass __RuleCtx:\n    \"\"\"\n    __RuleCtx is a context class used to define rules for processing data.\n\n    Attributes:\n        column (Any): Represents the column(s) to which the rule applies. \n                      It can be a string (for a single column) or a list of strings (for multiple columns).\n        value (Any): The value associated with the rule. The type of this value depends on the specific rule implementation.\n        name (str): The name of the rule, typically used to indicate the type of check being performed.\n    \"\"\"\n    column: Any  # str ou list[str]\n    value: Any\n    name: str  # check_type\n</code></pre>"},{"location":"api/engine/engine-duckdb/#sumeh.engine.duckdb_engine.__compare_schemas","title":"<code>__compare_schemas(actual, expected)</code>","text":"<p>Compare two lists of schema definitions and identify discrepancies.</p> <p>Parameters:</p> Name Type Description Default <code>actual</code> <code>List[SchemaDef]</code> <p>The list of actual schema definitions.</p> required <code>expected</code> <code>List[SchemaDef]</code> <p>The list of expected schema definitions.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>Tuple[bool, List[Tuple[str, str]]]: A tuple where the first element is a boolean indicating </p> <code>List[Tuple[str, str]]</code> <p>whether the schemas match (True if they match, False otherwise), and the second element </p> <code>Tuple[bool, List[Tuple[str, str]]]</code> <p>is a list of tuples describing the discrepancies. Each tuple contains: - The field name (str). - A description of the discrepancy (str), such as \"missing\", \"type mismatch\",    \"nullable but expected non-nullable\", or \"extra column\".</p> Notes <ul> <li>A field is considered \"missing\" if it exists in the expected schema but not in the actual schema.</li> <li>A \"type mismatch\" occurs if the data type of a field in the actual schema does not match    the expected data type.</li> <li>A field is considered \"nullable but expected non-nullable\" if it is nullable in the actual    schema but not nullable in the expected schema.</li> <li>An \"extra column\" is a field that exists in the actual schema but not in the expected schema.</li> </ul> Source code in <code>sumeh/services/utils.py</code> <pre><code>def __compare_schemas(actual: List[SchemaDef], expected: List[SchemaDef],) -&gt; Tuple[bool, List[Tuple[str, str]]]:\n    \"\"\"\n    Compare two lists of schema definitions and identify discrepancies.\n\n    Args:\n        actual (List[SchemaDef]): The list of actual schema definitions.\n        expected (List[SchemaDef]): The list of expected schema definitions.\n\n    Returns:\n        Tuple[bool, List[Tuple[str, str]]]: A tuple where the first element is a boolean indicating \n        whether the schemas match (True if they match, False otherwise), and the second element \n        is a list of tuples describing the discrepancies. Each tuple contains:\n            - The field name (str).\n            - A description of the discrepancy (str), such as \"missing\", \"type mismatch\", \n              \"nullable but expected non-nullable\", or \"extra column\".\n\n    Notes:\n        - A field is considered \"missing\" if it exists in the expected schema but not in the actual schema.\n        - A \"type mismatch\" occurs if the data type of a field in the actual schema does not match \n          the expected data type.\n        - A field is considered \"nullable but expected non-nullable\" if it is nullable in the actual \n          schema but not nullable in the expected schema.\n        - An \"extra column\" is a field that exists in the actual schema but not in the expected schema.\n    \"\"\"\n\n    exp_map = {c[\"field\"]: c for c in expected}\n    act_map = {c[\"field\"]: c for c in actual}\n\n    erros: List[Tuple[str, str]] = []\n\n    for fld, exp in exp_map.items():\n        if fld not in act_map:\n            erros.append((fld, \"missing\"))\n            continue\n        act = act_map[fld]\n        if act[\"data_type\"] != exp[\"data_type\"]:\n            erros.append(\n                (\n                    fld,\n                    f\"type mismatch (got {act['data_type']}, expected {exp['data_type']})\",\n                )\n            )\n\n        if act[\"nullable\"] and not exp[\"nullable\"]:\n            erros.append((fld, \"nullable but expected non-nullable\"))\n\n        if exp.get(\"max_length\") is not None:\n            pass\n\n    # 2. campos extras (se quiser)\n    extras = set(act_map) - set(exp_map)\n    for fld in extras:\n        erros.append((fld, \"extra column\"))\n\n    return len(erros) == 0, erros\n</code></pre>"},{"location":"api/engine/engine-duckdb/#sumeh.engine.duckdb_engine.__duckdb_schema_to_list","title":"<code>__duckdb_schema_to_list(conn, table)</code>","text":"<p>Retrieve the schema of a DuckDB table as a list of dictionaries. This function queries the schema of the specified table in a DuckDB database and returns a list of dictionaries where each dictionary represents a column in the table, including its name, data type, nullability, and maximum length. Args:     conn (dk.DuckDBPyConnection): The DuckDB connection object.     table (str): The name of the table whose schema is to be retrieved. Returns:     List[Dict[str, Any]]: A list of dictionaries, each containing the following keys:         - \"field\" (str): The name of the column.         - \"data_type\" (str): The data type of the column in lowercase.         - \"nullable\" (bool): Whether the column allows NULL values.         - \"max_length\" (None): Always None, as DuckDB does not provide maximum length information.</p> Source code in <code>sumeh/engine/duckdb_engine.py</code> <pre><code>def __duckdb_schema_to_list(conn: dk.DuckDBPyConnection, table: str) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Retrieve the schema of a DuckDB table as a list of dictionaries.\n    This function queries the schema of the specified table in a DuckDB database\n    and returns a list of dictionaries where each dictionary represents a column\n    in the table, including its name, data type, nullability, and maximum length.\n    Args:\n        conn (dk.DuckDBPyConnection): The DuckDB connection object.\n        table (str): The name of the table whose schema is to be retrieved.\n    Returns:\n        List[Dict[str, Any]]: A list of dictionaries, each containing the following keys:\n            - \"field\" (str): The name of the column.\n            - \"data_type\" (str): The data type of the column in lowercase.\n            - \"nullable\" (bool): Whether the column allows NULL values.\n            - \"max_length\" (None): Always None, as DuckDB does not provide maximum length information.\n    \"\"\"\n\n    df_info = conn.execute(f\"PRAGMA table_info('{table}')\").fetchdf()\n    return [\n        {\n            \"field\": row[\"name\"],\n            \"data_type\": row[\"type\"].lower(),\n            \"nullable\": not bool(row[\"notnull\"]),\n            \"max_length\": None,\n        }\n        for _, row in df_info.iterrows()\n    ]\n</code></pre>"},{"location":"api/engine/engine-duckdb/#sumeh.engine.duckdb_engine.__escape_single_quotes","title":"<code>__escape_single_quotes(txt)</code>","text":"<p>Escapes single quotes in a given string by replacing each single quote with two single quotes. This is commonly used to sanitize strings for use in SQL queries.</p> <p>Parameters:</p> Name Type Description Default <code>txt</code> <code>str</code> <p>The input string to process.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The processed string with single quotes escaped.</p> Source code in <code>sumeh/engine/duckdb_engine.py</code> <pre><code>def __escape_single_quotes(txt: str) -&gt; str:\n    \"\"\"\n    Escapes single quotes in a given string by replacing each single quote\n    with two single quotes. This is commonly used to sanitize strings for\n    use in SQL queries.\n\n    Args:\n        txt (str): The input string to process.\n\n    Returns:\n        str: The processed string with single quotes escaped.\n    \"\"\"\n    return txt.replace(\"'\", \"''\")\n</code></pre>"},{"location":"api/engine/engine-duckdb/#sumeh.engine.duckdb_engine.__rules_to_duckdb_df","title":"<code>__rules_to_duckdb_df(rules)</code>","text":"<p>Converts a list of rule dictionaries into a DuckDB-compatible SQL query string. Each rule in the input list is processed to generate a SQL <code>SELECT</code> statement with the following fields: - <code>col</code>: The column name(s) associated with the rule. - <code>rule</code>: The name of the rule (check type). - <code>pass_threshold</code>: A numeric threshold value for the rule, defaulting to 1.0 if not provided. - <code>value</code>: The value associated with the rule, which can be a string, list, tuple, or <code>NULL</code>. Rules with the <code>execute</code> field set to <code>False</code> are skipped. If the input list is empty or all rules are skipped, the function returns a SQL query that selects <code>NULL</code> values with a <code>LIMIT 0</code>. Args:     rules (List[Dict]): A list of dictionaries, where each dictionary represents a rule         with the following keys:         - <code>field</code> (str or list): The column(s) associated with the rule.         - <code>check_type</code> (str): The name of the rule.         - <code>value</code> (optional): The value associated with the rule.         - <code>threshold</code> (optional, float): The numeric threshold for the rule.         - <code>execute</code> (optional, bool): Whether the rule should be executed (default is <code>True</code>). Returns:     str: A DuckDB-compatible SQL query string representing the rules.</p> Source code in <code>sumeh/engine/duckdb_engine.py</code> <pre><code>def __rules_to_duckdb_df(rules: List[Dict]) -&gt; str:\n    \"\"\"\n    Converts a list of rule dictionaries into a DuckDB-compatible SQL query string.\n    Each rule in the input list is processed to generate a SQL `SELECT` statement\n    with the following fields:\n    - `col`: The column name(s) associated with the rule.\n    - `rule`: The name of the rule (check type).\n    - `pass_threshold`: A numeric threshold value for the rule, defaulting to 1.0 if not provided.\n    - `value`: The value associated with the rule, which can be a string, list, tuple, or `NULL`.\n    Rules with the `execute` field set to `False` are skipped.\n    If the input list is empty or all rules are skipped, the function returns a SQL query\n    that selects `NULL` values with a `LIMIT 0`.\n    Args:\n        rules (List[Dict]): A list of dictionaries, where each dictionary represents a rule\n            with the following keys:\n            - `field` (str or list): The column(s) associated with the rule.\n            - `check_type` (str): The name of the rule.\n            - `value` (optional): The value associated with the rule.\n            - `threshold` (optional, float): The numeric threshold for the rule.\n            - `execute` (optional, bool): Whether the rule should be executed (default is `True`).\n    Returns:\n        str: A DuckDB-compatible SQL query string representing the rules.\n    \"\"\"\n\n    parts: List[str] = []\n\n    for r in rules:\n        if not r.get(\"execute\", True):\n            continue\n\n        ctx = __RuleCtx(column=r[\"field\"], value=r.get(\"value\"), name=r[\"check_type\"])\n\n        # Formata\u00e7\u00e3o da coluna (string ou lista)\n        col = \", \".join(ctx.column) if isinstance(ctx.column, list) else ctx.column\n        col_sql = f\"'{__escape_single_quotes(col.strip())}'\"\n\n        # Formata\u00e7\u00e3o do nome da regra\n        rule_sql = f\"'{__escape_single_quotes(ctx.name)}'\"\n\n        # Threshold com fallback seguro\n        try:\n            thr = float(r.get(\"threshold\", 1.0))\n        except (TypeError, ValueError):\n            thr = 1.0\n\n        # Formata\u00e7\u00e3o do valor\n        if ctx.value is None:\n            val_sql = \"NULL\"\n        elif isinstance(ctx.value, str):\n            val_sql = f\"'{__escape_single_quotes(ctx.value)}'\"\n        elif isinstance(ctx.value, (list, tuple)):\n            try:\n                val_sql = _format_sequence(ctx.value)\n            except ValueError:\n                val_sql = \"NULL\"\n        else:\n            val_sql = str(ctx.value)\n\n        parts.append(\n            f\"SELECT {col_sql} AS col, \"\n            f\"{rule_sql} AS rule, \"\n            f\"{thr} AS pass_threshold, \"\n            f\"{val_sql} AS value\"\n        )\n\n    if not parts:\n        return \"SELECT NULL AS col, NULL AS rule, NULL AS pass_threshold, NULL AS value LIMIT 0\"\n\n    union_sql = \"\\nUNION ALL\\n\".join(parts)\n    return (\n        \"SELECT DISTINCT col, rule, pass_threshold, value\\n\"\n        \"FROM (\\n\"\n        f\"{union_sql}\\n\"\n        \") AS t\"\n    )\n</code></pre>"},{"location":"api/engine/engine-duckdb/#sumeh.engine.duckdb_engine._are_complete","title":"<code>_are_complete(r)</code>","text":"<p>Constructs a SQL condition string that checks if all specified columns in a rule context are not NULL.</p> <p>Parameters:</p> Name Type Description Default <code>r</code> <code>__RuleCtx</code> <p>The rule context containing the list of column names to check.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A SQL condition string in the format \"(column1 IS NOT NULL AND column2 IS NOT NULL ...)\".</p> Source code in <code>sumeh/engine/duckdb_engine.py</code> <pre><code>def _are_complete(r: __RuleCtx) -&gt; str:\n    \"\"\"\n    Constructs a SQL condition string that checks if all specified columns in a rule context are not NULL.\n\n    Args:\n        r (__RuleCtx): The rule context containing the list of column names to check.\n\n    Returns:\n        str: A SQL condition string in the format \"(column1 IS NOT NULL AND column2 IS NOT NULL ...)\".\n    \"\"\"\n    parts = \" AND \".join(f\"{c} IS NOT NULL\" for c in r.column)\n    return f\"({parts})\"\n</code></pre>"},{"location":"api/engine/engine-duckdb/#sumeh.engine.duckdb_engine._are_unique","title":"<code>_are_unique(r)</code>","text":"<p>Generates a SQL query string to check if the combination of specified columns in a table is unique for each row.</p> <p>Parameters:</p> Name Type Description Default <code>r</code> <code>__RuleCtx</code> <p>A context object containing the column names to be checked            for uniqueness.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A SQL query string that evaluates whether the combination of the  specified columns is unique for each row in the table.</p> Source code in <code>sumeh/engine/duckdb_engine.py</code> <pre><code>def _are_unique(r: __RuleCtx) -&gt; str:\n    \"\"\"\n    Generates a SQL query string to check if the combination of specified columns\n    in a table is unique for each row.\n\n    Args:\n        r (__RuleCtx): A context object containing the column names to be checked\n                       for uniqueness.\n\n    Returns:\n        str: A SQL query string that evaluates whether the combination of the\n             specified columns is unique for each row in the table.\n    \"\"\"\n    combo_outer = \" || '|' || \".join(f\"tbl.{c}\" for c in r.column)\n    combo_inner = \" || '|' || \".join(f\"d2.{c}\" for c in r.column)\n\n    return (\n        f\"(SELECT COUNT(*)                  \\n\"\n        f\"   FROM tbl AS d2                 \\n\"\n        f\"   WHERE ({combo_inner}) = ({combo_outer})\\n\"\n        f\") = 1\"\n    )\n</code></pre>"},{"location":"api/engine/engine-duckdb/#sumeh.engine.duckdb_engine._build_union_sql","title":"<code>_build_union_sql(rules)</code>","text":"<p>Constructs a SQL query that combines multiple rule-based checks into a single query using UNION ALL. Each rule specifies a condition to be checked on a table, and the resulting query flags rows that do not satisfy the condition.</p> <p>Parameters:</p> Name Type Description Default <code>rules</code> <code>List[Dict]</code> <p>A list of dictionaries where each dictionary represents a rule. Each rule should contain the following keys: - \"check_type\" (str): The type of check to perform. - \"field\" (str): The column name to apply the check on. - \"value\" (optional): The value to use in the check. - \"execute\" (optional, bool): Whether to execute the rule. Defaults to True.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A SQL query string that combines all active rules using UNION ALL. If no</p> <code>str</code> <p>rules are active, returns a query that produces an empty result set.</p> Notes <ul> <li>If a rule's \"check_type\" is not recognized, a warning is issued, and the rule   is skipped.</li> <li>The resulting query includes a \"dq_status\" column that indicates the rule   that flagged the row, formatted as \"column:check_type:value\".</li> <li>If no rules are active, the query returns an empty result set with the same   structure as the input table.</li> </ul> Source code in <code>sumeh/engine/duckdb_engine.py</code> <pre><code>def _build_union_sql(rules: List[Dict]) -&gt; str:\n    \"\"\"\n    Constructs a SQL query that combines multiple rule-based checks into a single query\n    using UNION ALL. Each rule specifies a condition to be checked on a table, and the\n    resulting query flags rows that do not satisfy the condition.\n\n    Args:\n        rules (List[Dict]): A list of dictionaries where each dictionary represents a rule.\n            Each rule should contain the following keys:\n            - \"check_type\" (str): The type of check to perform.\n            - \"field\" (str): The column name to apply the check on.\n            - \"value\" (optional): The value to use in the check.\n            - \"execute\" (optional, bool): Whether to execute the rule. Defaults to True.\n\n    Returns:\n        str: A SQL query string that combines all active rules using UNION ALL. If no\n        rules are active, returns a query that produces an empty result set.\n\n    Notes:\n        - If a rule's \"check_type\" is not recognized, a warning is issued, and the rule\n          is skipped.\n        - The resulting query includes a \"dq_status\" column that indicates the rule\n          that flagged the row, formatted as \"column:check_type:value\".\n        - If no rules are active, the query returns an empty result set with the same\n          structure as the input table.\n    \"\"\"\n    pieces: list[str] = []\n\n    for r in rules:\n        if not r.get(\"execute\", True):\n            continue\n\n        check = r[\"check_type\"]\n        builder = __RULE_DISPATCH.get(check)\n        if builder is None:\n            warnings.warn(f\"Regra desconhecida: {check}\")\n            continue\n\n        ctx = __RuleCtx(\n            column=r[\"field\"],\n            value=r.get(\"value\"),\n            name=check,\n        )\n\n        expr_ok = builder(ctx)  # condi\u00e7\u00e3o \u201cpassa\u201d\n        dq_tag = __escape_single_quotes(f\"{ctx.column}:{check}:{ctx.value}\")\n        pieces.append(\n            f\"SELECT *, '{dq_tag}' AS dq_status FROM tbl WHERE NOT ({expr_ok})\"\n        )\n\n    # se n\u00e3o h\u00e1 regras ativas: devolve DF vazio\n    if not pieces:\n        return \"SELECT *, '' AS dq_status FROM tbl WHERE 1=0\"\n\n    return \"\\nUNION ALL\\n\".join(pieces)\n</code></pre>"},{"location":"api/engine/engine-duckdb/#sumeh.engine.duckdb_engine._format_sequence","title":"<code>_format_sequence(value)</code>","text":"<p>Formats a sequence-like input into a string representation suitable for SQL queries.</p> Converts inputs into a tuple-like string format <ul> <li>'BR,US' -&gt; \"('BR','US')\"</li> <li>['BR', 'US'] -&gt; \"('BR','US')\"</li> <li>('BR', 'US') -&gt; \"('BR','US')\"</li> </ul> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>The input value to be formatted. Can be a string, list, or tuple.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A string representation of the input in the format \"('item1','item2',...)\".</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input value is None or cannot be interpreted as a sequence.</p> Notes <ul> <li>If the input is a string, it attempts to parse it as a Python literal.</li> <li>If parsing fails, it splits the string by commas and processes the resulting parts.</li> <li>Empty or invalid elements are ignored in the output.</li> </ul> Source code in <code>sumeh/engine/duckdb_engine.py</code> <pre><code>def _format_sequence(value: Any) -&gt; str:\n    \"\"\"\n    Formats a sequence-like input into a string representation suitable for SQL queries.\n\n    Converts inputs into a tuple-like string format:\n        - 'BR,US' -&gt; \"('BR','US')\"\n        - ['BR', 'US'] -&gt; \"('BR','US')\"\n        - ('BR', 'US') -&gt; \"('BR','US')\"\n\n    Args:\n        value (Any): The input value to be formatted. Can be a string, list, or tuple.\n\n    Returns:\n        str: A string representation of the input in the format \"('item1','item2',...)\".\n\n    Raises:\n        ValueError: If the input value is None or cannot be interpreted as a sequence.\n\n    Notes:\n        - If the input is a string, it attempts to parse it as a Python literal.\n        - If parsing fails, it splits the string by commas and processes the resulting parts.\n        - Empty or invalid elements are ignored in the output.\n    \"\"\"\n\n    if value is None:\n        raise ValueError(\"value cannot be None for IN/NOT IN\")\n\n    if isinstance(value, (list, tuple)):\n        seq = value\n    else:\n        try:  # tenta interpretar como literal Python\n            seq = ast.literal_eval(value)\n            if not isinstance(seq, (list, tuple)):\n                raise ValueError\n        except Exception:\n            seq = [v.strip(\" []()'\\\"\") for v in str(value).split(\",\")]\n\n    return \"(\" + \",\".join(repr(str(x).strip()) for x in seq if x != \"\") + \")\"\n</code></pre>"},{"location":"api/engine/engine-duckdb/#sumeh.engine.duckdb_engine._has_pattern","title":"<code>_has_pattern(r)</code>","text":"<p>Constructs a SQL expression to check if a column's value matches a given regular expression pattern.</p> <p>Parameters:</p> Name Type Description Default <code>r</code> <code>__RuleCtx</code> <p>An object containing the column name and the value to be used as the pattern.            The <code>value</code> attribute is expected to be a string or convertible to a string,            and the <code>column</code> attribute is the name of the column to be checked.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A SQL expression string that uses the REGEXP_MATCHES function to evaluate  whether the column matches the escaped pattern.</p> Source code in <code>sumeh/engine/duckdb_engine.py</code> <pre><code>def _has_pattern(r: __RuleCtx) -&gt; str:\n    \"\"\"\n    Constructs a SQL expression to check if a column's value matches a given regular expression pattern.\n\n    Args:\n        r (__RuleCtx): An object containing the column name and the value to be used as the pattern.\n                       The `value` attribute is expected to be a string or convertible to a string,\n                       and the `column` attribute is the name of the column to be checked.\n\n    Returns:\n        str: A SQL expression string that uses the REGEXP_MATCHES function to evaluate\n             whether the column matches the escaped pattern.\n    \"\"\"\n    pat = __escape_single_quotes(str(r.value))\n    return f\"REGEXP_MATCHES({r.column}, '{pat}')\"\n</code></pre>"},{"location":"api/engine/engine-duckdb/#sumeh.engine.duckdb_engine._is_between","title":"<code>_is_between(r)</code>","text":"<p>Constructs a SQL BETWEEN clause for a given rule context.</p> <p>Parameters:</p> Name Type Description Default <code>r</code> <code>__RuleCtx</code> <p>The rule context containing the column name and value(s).            The <code>value</code> attribute can be a list, tuple, or a string            representation of a range (e.g., \"lo, hi\").</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A SQL BETWEEN clause in the format \"column BETWEEN lo AND hi\".</p> Notes <ul> <li>If <code>r.value</code> is a list or tuple, it is expected to contain exactly two elements   representing the lower (lo) and upper (hi) bounds.</li> <li>If <code>r.value</code> is a string, it will be split by commas and stripped of any   surrounding brackets, parentheses, or quotes to extract the bounds.</li> </ul> Source code in <code>sumeh/engine/duckdb_engine.py</code> <pre><code>def _is_between(r: __RuleCtx) -&gt; str:\n    \"\"\"\n    Constructs a SQL BETWEEN clause for a given rule context.\n\n    Args:\n        r (__RuleCtx): The rule context containing the column name and value(s).\n                       The `value` attribute can be a list, tuple, or a string\n                       representation of a range (e.g., \"lo, hi\").\n\n    Returns:\n        str: A SQL BETWEEN clause in the format \"column BETWEEN lo AND hi\".\n\n    Notes:\n        - If `r.value` is a list or tuple, it is expected to contain exactly two elements\n          representing the lower (lo) and upper (hi) bounds.\n        - If `r.value` is a string, it will be split by commas and stripped of any\n          surrounding brackets, parentheses, or quotes to extract the bounds.\n    \"\"\"\n    val = r.value\n    if isinstance(val, (list, tuple)):\n        lo, hi = val\n    else:\n        lo, hi, *_ = [v.strip(\" []()'\\\"\") for v in str(val).split(\",\")]\n    return f\"{r.column} BETWEEN {lo} AND {hi}\"\n</code></pre>"},{"location":"api/engine/engine-duckdb/#sumeh.engine.duckdb_engine._is_complete","title":"<code>_is_complete(r)</code>","text":"<p>Constructs a SQL condition to check if a column is not NULL.</p> <p>Parameters:</p> Name Type Description Default <code>r</code> <code>__RuleCtx</code> <p>An object containing context information, including the column name.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A SQL condition string in the format \" IS NOT NULL\". Source code in <code>sumeh/engine/duckdb_engine.py</code> <pre><code>def _is_complete(r: __RuleCtx) -&gt; str:\n    \"\"\"\n    Constructs a SQL condition to check if a column is not NULL.\n\n    Args:\n        r (__RuleCtx): An object containing context information, including the column name.\n\n    Returns:\n        str: A SQL condition string in the format \"&lt;column&gt; IS NOT NULL\".\n    \"\"\"\n    return f\"{r.column} IS NOT NULL\"\n</code></pre>"},{"location":"api/engine/engine-duckdb/#sumeh.engine.duckdb_engine._is_contained_in","title":"<code>_is_contained_in(r)</code>","text":"<p>Generates a SQL fragment that checks if a column's value is contained within a sequence of values.</p> <p>Parameters:</p> Name Type Description Default <code>r</code> <code>__RuleCtx</code> <p>A context object containing the column name and the sequence of values.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A SQL fragment in the format \" IN (, , ...)\". Source code in <code>sumeh/engine/duckdb_engine.py</code> <pre><code>def _is_contained_in(r: __RuleCtx) -&gt; str:\n    \"\"\"\n    Generates a SQL fragment that checks if a column's value is contained within a sequence of values.\n\n    Args:\n        r (__RuleCtx): A context object containing the column name and the sequence of values.\n\n    Returns:\n        str: A SQL fragment in the format \"&lt;column&gt; IN (&lt;value1&gt;, &lt;value2&gt;, ...)\".\n    \"\"\"\n    return f\"{r.column} IN {_format_sequence(r.value)}\"\n</code></pre>"},{"location":"api/engine/engine-duckdb/#sumeh.engine.duckdb_engine._is_equal_than","title":"<code>_is_equal_than(r)</code>","text":"<p>Generates a SQL equality condition string for a given rule context.</p> <p>Parameters:</p> Name Type Description Default <code>r</code> <code>__RuleCtx</code> <p>The rule context containing the column and value to compare.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A string representing the SQL equality condition in the format \"column = value\".</p> Source code in <code>sumeh/engine/duckdb_engine.py</code> <pre><code>def _is_equal_than(r: __RuleCtx) -&gt; str:\n    \"\"\"\n    Generates a SQL equality condition string for a given rule context.\n\n    Args:\n        r (__RuleCtx): The rule context containing the column and value to compare.\n\n    Returns:\n        str: A string representing the SQL equality condition in the format \"column = value\".\n    \"\"\"\n    return f\"{r.column} = {r.value}\"\n</code></pre>"},{"location":"api/engine/engine-duckdb/#sumeh.engine.duckdb_engine._is_greater_or_equal_than","title":"<code>_is_greater_or_equal_than(r)</code>","text":"<p>Generates a SQL expression to check if a column's value is greater than or equal to a specified value.</p> <p>Parameters:</p> Name Type Description Default <code>r</code> <code>__RuleCtx</code> <p>A context object containing the column name and the value to compare.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A SQL expression in the format \" &gt;= \". Source code in <code>sumeh/engine/duckdb_engine.py</code> <pre><code>def _is_greater_or_equal_than(r: __RuleCtx) -&gt; str:\n    \"\"\"\n    Generates a SQL expression to check if a column's value is greater than or equal to a specified value.\n\n    Args:\n        r (__RuleCtx): A context object containing the column name and the value to compare.\n\n    Returns:\n        str: A SQL expression in the format \"&lt;column&gt; &gt;= &lt;value&gt;\".\n    \"\"\"\n    return f\"{r.column} &gt;= {r.value}\"\n</code></pre>"},{"location":"api/engine/engine-duckdb/#sumeh.engine.duckdb_engine._is_greater_than","title":"<code>_is_greater_than(r)</code>","text":"<p>Generates a SQL condition string to check if a column's value is greater than a specified value.</p> <p>Parameters:</p> Name Type Description Default <code>r</code> <code>__RuleCtx</code> <p>A context object containing the column name and the value to compare.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A SQL condition string in the format \" &gt; \". Source code in <code>sumeh/engine/duckdb_engine.py</code> <pre><code>def _is_greater_than(r: __RuleCtx) -&gt; str:\n    \"\"\"\n    Generates a SQL condition string to check if a column's value is greater than a specified value.\n\n    Args:\n        r (__RuleCtx): A context object containing the column name and the value to compare.\n\n    Returns:\n        str: A SQL condition string in the format \"&lt;column&gt; &gt; &lt;value&gt;\".\n    \"\"\"\n    return f\"{r.column} &gt; {r.value}\"\n</code></pre>"},{"location":"api/engine/engine-duckdb/#sumeh.engine.duckdb_engine._is_less_or_equal_than","title":"<code>_is_less_or_equal_than(r)</code>","text":"<p>Generates a SQL condition string that checks if a column's value is less than or equal to a specified value.</p> <p>Parameters:</p> Name Type Description Default <code>r</code> <code>__RuleCtx</code> <p>A context object containing the column name and the value to compare.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A SQL condition string in the format \" &lt;= \". Source code in <code>sumeh/engine/duckdb_engine.py</code> <pre><code>def _is_less_or_equal_than(r: __RuleCtx) -&gt; str:\n    \"\"\"\n    Generates a SQL condition string that checks if a column's value is less than or equal to a specified value.\n\n    Args:\n        r (__RuleCtx): A context object containing the column name and the value to compare.\n\n    Returns:\n        str: A SQL condition string in the format \"&lt;column&gt; &lt;= &lt;value&gt;\".\n    \"\"\"\n    return f\"{r.column} &lt;= {r.value}\"\n</code></pre>"},{"location":"api/engine/engine-duckdb/#sumeh.engine.duckdb_engine._is_less_than","title":"<code>_is_less_than(r)</code>","text":"<p>Generates a SQL condition string that checks if a column's value is less than a specified value.</p> <p>Parameters:</p> Name Type Description Default <code>r</code> <code>__RuleCtx</code> <p>A context object containing the column name and the value to compare against.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A SQL condition string in the format \" &lt; \". Source code in <code>sumeh/engine/duckdb_engine.py</code> <pre><code>def _is_less_than(r: __RuleCtx) -&gt; str:\n    \"\"\"\n    Generates a SQL condition string that checks if a column's value is less than a specified value.\n\n    Args:\n        r (__RuleCtx): A context object containing the column name and the value to compare against.\n\n    Returns:\n        str: A SQL condition string in the format \"&lt;column&gt; &lt; &lt;value&gt;\".\n    \"\"\"\n    return f\"{r.column} &lt; {r.value}\"\n</code></pre>"},{"location":"api/engine/engine-duckdb/#sumeh.engine.duckdb_engine._is_unique","title":"<code>_is_unique(r)</code>","text":"<p>Generates a SQL expression to check if a column value is unique within a table.</p> <p>Parameters:</p> Name Type Description Default <code>r</code> <code>__RuleCtx</code> <p>A context object containing metadata, including the column name to check.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A SQL string that evaluates to True if the column value is unique, otherwise False.</p> Source code in <code>sumeh/engine/duckdb_engine.py</code> <pre><code>def _is_unique(r: __RuleCtx) -&gt; str:\n    \"\"\"\n    Generates a SQL expression to check if a column value is unique within a table.\n\n    Args:\n        r (__RuleCtx): A context object containing metadata, including the column name to check.\n\n    Returns:\n        str: A SQL string that evaluates to True if the column value is unique, otherwise False.\n    \"\"\"\n    return (\n        f\"(SELECT COUNT(*)                            \\n\"\n        f\"   FROM tbl AS d2                           \\n\"\n        f\"   WHERE d2.{r.column} = tbl.{r.column}     \\n\"\n        f\") = 1\"\n    )\n</code></pre>"},{"location":"api/engine/engine-duckdb/#sumeh.engine.duckdb_engine._not_contained_in","title":"<code>_not_contained_in(r)</code>","text":"<p>Generates a SQL expression that checks if a column's value is not contained  within a specified sequence of values.</p> <p>Parameters:</p> Name Type Description Default <code>r</code> <code>__RuleCtx</code> <p>A context object containing the column name and the sequence             of values to check against.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A SQL string in the format \" NOT IN (, , ...)\". Source code in <code>sumeh/engine/duckdb_engine.py</code> <pre><code>def _not_contained_in(r: __RuleCtx) -&gt; str:\n    \"\"\"\n    Generates a SQL expression that checks if a column's value is not contained \n    within a specified sequence of values.\n\n    Args:\n        r (__RuleCtx): A context object containing the column name and the sequence \n                       of values to check against.\n\n    Returns:\n        str: A SQL string in the format \"&lt;column&gt; NOT IN (&lt;value1&gt;, &lt;value2&gt;, ...)\".\n    \"\"\"\n    return f\"{r.column} NOT IN {_format_sequence(r.value)}\"\n</code></pre>"},{"location":"api/engine/engine-duckdb/#sumeh.engine.duckdb_engine._satisfies","title":"<code>_satisfies(r)</code>","text":"<p>Constructs a string representation of the given rule context.</p> <p>Parameters:</p> Name Type Description Default <code>r</code> <code>__RuleCtx</code> <p>The rule context containing a value to be formatted.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A string in the format \"(value)\" where 'value' is the value of the rule context.</p> Source code in <code>sumeh/engine/duckdb_engine.py</code> <pre><code>def _satisfies(r: __RuleCtx) -&gt; str:\n    \"\"\"\n    Constructs a string representation of the given rule context.\n\n    Args:\n        r (__RuleCtx): The rule context containing a value to be formatted.\n\n    Returns:\n        str: A string in the format \"(value)\" where 'value' is the value of the rule context.\n    \"\"\"\n    return f\"({r.value})\"\n</code></pre>"},{"location":"api/engine/engine-duckdb/#sumeh.engine.duckdb_engine.summarize","title":"<code>summarize(df_rel, rules, conn, total_rows=None)</code>","text":"<p>Summarizes data quality checks for a given DuckDB relation based on specified rules.</p> <p>Parameters:</p> Name Type Description Default <code>df_rel</code> <code>DuckDBPyRelation</code> <p>The DuckDB relation containing the data to be analyzed.</p> required <code>rules</code> <code>List[Dict]</code> <p>A list of dictionaries defining the data quality rules to be applied.</p> required <code>conn</code> <code>DuckDBPyConnection</code> <p>The DuckDB connection used to execute SQL queries.</p> required <code>total_rows</code> <code>Optional[int]</code> <p>The total number of rows in the dataset. If not provided,                          it must be calculated externally.</p> <code>None</code> <p>Returns:</p> Type Description <code>DuckDBPyRelation</code> <p>dk.DuckDBPyRelation: A DuckDB relation containing the summary of data quality checks,                      including pass rates, violation counts, and statuses for each rule.</p> Notes <ul> <li>The function creates a temporary view named \"violations_raw\" from the input relation.</li> <li>It uses SQL to compute violations, pass rates, and statuses based on the provided rules.</li> <li>The output includes metadata such as timestamps, rule thresholds, and overall status      (PASS/FAIL) for each rule.</li> </ul> Source code in <code>sumeh/engine/duckdb_engine.py</code> <pre><code>def summarize(df_rel: dk.DuckDBPyRelation,rules: List[Dict],conn: dk.DuckDBPyConnection,total_rows: Optional[int] = None) -&gt; dk.DuckDBPyRelation:\n    \"\"\"\n    Summarizes data quality checks for a given DuckDB relation based on specified rules.\n\n    Args:\n        df_rel (dk.DuckDBPyRelation): The DuckDB relation containing the data to be analyzed.\n        rules (List[Dict]): A list of dictionaries defining the data quality rules to be applied.\n        conn (dk.DuckDBPyConnection): The DuckDB connection used to execute SQL queries.\n        total_rows (Optional[int]): The total number of rows in the dataset. If not provided, \n                                    it must be calculated externally.\n\n    Returns:\n        dk.DuckDBPyRelation: A DuckDB relation containing the summary of data quality checks, \n                                including pass rates, violation counts, and statuses for each rule.\n\n    Notes:\n        - The function creates a temporary view named \"violations_raw\" from the input relation.\n        - It uses SQL to compute violations, pass rates, and statuses based on the provided rules.\n        - The output includes metadata such as timestamps, rule thresholds, and overall status \n            (PASS/FAIL) for each rule.\n    \"\"\"\n    rules_sql = __rules_to_duckdb_df(rules)\n\n    df_rel.create_view(\"violations_raw\")\n\n    sql = f\"\"\"\n        WITH\n        rules      AS ({rules_sql}),\n        violations AS (\n            SELECT\n                split_part(dq_status, ':', 1) AS col,\n                split_part(dq_status, ':', 2) AS rule,\n                split_part(dq_status, ':', 2) AS value,\n                COUNT(*)               AS violations\n            FROM violations_raw\n            WHERE dq_status IS NOT NULL\n                AND dq_status &lt;&gt; ''\n            GROUP BY col, rule, value, value\n        ),\n        total_rows AS (\n            SELECT {total_rows} AS cnt\n        )\n        SELECT\n        ROW_NUMBER() OVER ()                            AS id,\n        date_trunc('minute', NOW())                     AS timestamp,\n        'Quality Check'                                 AS check,\n        'WARNING'                                       AS level,\n        r.col         AS col,\n        r.rule,\n        r.value,\n        tr.cnt                                         AS rows,\n        COALESCE(v.violations, 0)                      AS violations,\n        (tr.cnt - COALESCE(v.violations, 0))::DOUBLE / tr.cnt          AS pass_rate,\n        r.pass_threshold,\n        CASE\n            WHEN (tr.cnt - COALESCE(v.violations,0))::DOUBLE / tr.cnt \n             &gt;= r.pass_threshold THEN 'PASS'\n            ELSE 'FAIL'\n        END                                            AS status\n        FROM rules r\n        LEFT JOIN violations v ON r.col = v.col AND r.rule = v.rule,\n            total_rows tr\n    \"\"\"\n    return conn.sql(sql)\n</code></pre>"},{"location":"api/engine/engine-duckdb/#sumeh.engine.duckdb_engine.validate","title":"<code>validate(df_rel, rules, conn)</code>","text":"<p>Validates a DuckDB relation against a set of rules and returns the processed relation.</p> <p>Parameters:</p> Name Type Description Default <code>df_rel</code> <code>DuckDBPyRelation</code> <p>The input DuckDB relation to be validated.</p> required <code>rules</code> <code>List[Dict]</code> <p>A list of dictionaries representing validation rules.</p> required <code>conn</code> <code>DuckDBPyConnection</code> <p>The DuckDB connection object used for executing SQL queries.</p> required <p>Returns:</p> Type Description <code>DuckDBPyRelation</code> <p>dk.DuckDBPyRelation: A tuple containing: - The final DuckDB relation with aggregated validation statuses. - The raw DuckDB relation resulting from applying the validation rules.</p> Notes <ul> <li>The function creates a temporary view of the input relation named \"tbl\".</li> <li>Validation rules are combined into a union SQL query using <code>_build_union_sql</code>.</li> <li>The final relation includes all original columns and an aggregated <code>dq_status</code> column.</li> </ul> Source code in <code>sumeh/engine/duckdb_engine.py</code> <pre><code>def validate(df_rel: dk.DuckDBPyRelation, rules: List[Dict], conn: dk.DuckDBPyConnection) -&gt; dk.DuckDBPyRelation:\n    \"\"\"\n    Validates a DuckDB relation against a set of rules and returns the processed relation.\n\n    Args:\n        df_rel (dk.DuckDBPyRelation): The input DuckDB relation to be validated.\n        rules (List[Dict]): A list of dictionaries representing validation rules.\n        conn (dk.DuckDBPyConnection): The DuckDB connection object used for executing SQL queries.\n\n    Returns:\n        dk.DuckDBPyRelation: A tuple containing:\n            - The final DuckDB relation with aggregated validation statuses.\n            - The raw DuckDB relation resulting from applying the validation rules.\n\n    Notes:\n        - The function creates a temporary view of the input relation named \"tbl\".\n        - Validation rules are combined into a union SQL query using `_build_union_sql`.\n        - The final relation includes all original columns and an aggregated `dq_status` column.\n    \"\"\"\n    df_rel.create_view(\"tbl\")\n\n    union_sql = _build_union_sql(rules)\n\n    cols_df = conn.sql(\"PRAGMA table_info('tbl')\").df()\n    colnames = cols_df[\"name\"].tolist()\n    cols_sql = \", \".join(colnames)\n\n    raw_sql = f\"\"\"\n        {union_sql}\n    \"\"\"\n\n    raw = conn.sql(raw_sql)\n\n    final_sql = f\"\"\"\n    SELECT\n        {cols_sql},\n        STRING_AGG(dq_status, ';') AS dq_status\n    FROM raw\n    GROUP BY {cols_sql}\n    \"\"\"\n    final = conn.sql(final_sql)\n\n    return final, raw\n</code></pre>"},{"location":"api/engine/engine-duckdb/#sumeh.engine.duckdb_engine.validate_schema","title":"<code>validate_schema(conn, expected, table)</code>","text":"<p>Validates the schema of a DuckDB table against an expected schema.</p> <p>Parameters:</p> Name Type Description Default <code>conn</code> <code>DuckDBPyConnection</code> <p>The DuckDB connection object.</p> required <code>expected</code> <code>List[Dict[str, Any]]</code> <p>A list of dictionaries representing the expected schema. Each dictionary should define the expected attributes of the schema, such as column names and types.</p> required <code>table</code> <code>str</code> <p>The name of the table whose schema is to be validated.</p> required <p>Returns:</p> Type Description <code>Tuple[bool, List[Tuple[str, str]]]</code> <p>Tuple[bool, List[Tuple[str, str]]]: A tuple where the first element is a boolean indicating whether the actual schema matches the expected schema, and the second element is a list of tuples describing the mismatches (if any). Each tuple contains the column name and a description of the mismatch.</p> Source code in <code>sumeh/engine/duckdb_engine.py</code> <pre><code>def validate_schema(conn: dk.DuckDBPyConnection, expected: List[Dict[str, Any]], table: str) -&gt; Tuple[bool, List[Tuple[str, str]]]:\n    \"\"\"\n    Validates the schema of a DuckDB table against an expected schema.\n\n    Args:\n        conn (dk.DuckDBPyConnection): The DuckDB connection object.\n        expected (List[Dict[str, Any]]): A list of dictionaries representing the expected schema.\n            Each dictionary should define the expected attributes of the schema, such as column names and types.\n        table (str): The name of the table whose schema is to be validated.\n\n    Returns:\n        Tuple[bool, List[Tuple[str, str]]]: A tuple where the first element is a boolean indicating\n            whether the actual schema matches the expected schema, and the second element is a list\n            of tuples describing the mismatches (if any). Each tuple contains the column name and\n            a description of the mismatch.\n    \"\"\"\n    actual = __duckdb_schema_to_list(conn, table)\n    return __compare_schemas(actual, expected)\n</code></pre>"},{"location":"api/engine/engine-polars/","title":"Module <code>sumeh.engine.polars_engine</code>","text":"<p>This module provides a set of data quality validation functions using the Polars library.  It includes various checks for data validation, such as completeness, uniqueness, range checks,  pattern matching, and schema validation.</p> <p>Functions:</p> Name Description <code>is_positive</code> <p>pl.DataFrame, rule: dict) -&gt; pl.DataFrame:</p> <code>is_negative</code> <p>pl.DataFrame, rule: dict) -&gt; pl.DataFrame:</p> <code>is_complete</code> <p>pl.DataFrame, rule: dict) -&gt; pl.DataFrame:</p> <code>is_unique</code> <p>pl.DataFrame, rule: dict) -&gt; pl.DataFrame:</p> <code>are_complete</code> <p>pl.DataFrame, rule: dict) -&gt; pl.DataFrame:</p> <code>are_unique</code> <p>pl.DataFrame, rule: dict) -&gt; pl.DataFrame:</p> <code>is_greater_than</code> <p>pl.DataFrame, rule: dict) -&gt; pl.DataFrame:</p> <code>is_greater_or_equal_than</code> <p>pl.DataFrame, rule: dict) -&gt; pl.DataFrame:</p> <code>is_less_than</code> <p>pl.DataFrame, rule: dict) -&gt; pl.DataFrame:</p> <code>is_less_or_equal_than</code> <p>pl.DataFrame, rule: dict) -&gt; pl.DataFrame:</p> <code>is_equal</code> <p>pl.DataFrame, rule: dict) -&gt; pl.DataFrame:</p> <code>is_equal_than</code> <p>pl.DataFrame, rule: dict) -&gt; pl.DataFrame:</p> <code>is_contained_in</code> <p>pl.DataFrame, rule: dict) -&gt; pl.DataFrame:</p> <code>not_contained_in</code> <p>pl.DataFrame, rule: dict) -&gt; pl.DataFrame:</p> <code>is_between</code> <p>pl.DataFrame, rule: dict) -&gt; pl.DataFrame:</p> <code>has_pattern</code> <p>pl.DataFrame, rule: dict) -&gt; pl.DataFrame:</p> <code>is_legit</code> <p>pl.DataFrame, rule: dict) -&gt; pl.DataFrame:</p> <code>has_max</code> <p>pl.DataFrame, rule: dict) -&gt; pl.DataFrame:</p> <code>has_min</code> <p>pl.DataFrame, rule: dict) -&gt; pl.DataFrame:</p> <code>has_std</code> <p>pl.DataFrame, rule: dict) -&gt; pl.DataFrame:</p> <code>has_mean</code> <p>pl.DataFrame, rule: dict) -&gt; pl.DataFrame:</p> <code>has_sum</code> <p>pl.DataFrame, rule: dict) -&gt; pl.DataFrame:</p> <code>has_cardinality</code> <p>pl.DataFrame, rule: dict) -&gt; pl.DataFrame:</p> <code>has_infogain</code> <p>pl.DataFrame, rule: dict) -&gt; pl.DataFrame:</p> <code>has_entropy</code> <p>pl.DataFrame, rule: dict) -&gt; pl.DataFrame:</p> <code>satisfies</code> <p>pl.DataFrame, rule: dict) -&gt; pl.DataFrame:</p> <code>validate</code> <p>pl.DataFrame, rules: list[dict]) -&gt; Tuple[pl.DataFrame, pl.DataFrame]:</p> <code>__build_rules_df</code> <p>list[dict]) -&gt; pl.DataFrame:</p> <code>summarize</code> <p>pl.DataFrame, rules: list[dict], total_rows: int) -&gt; pl.DataFrame:</p> <code>__polars_schema_to_list</code> <p>pl.DataFrame) -&gt; List[Dict[str, Any]]:</p> <code>validate_schema</code>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.__build_rules_df","title":"<code>__build_rules_df(rules)</code>","text":"<p>Builds a Polars DataFrame from a list of rule dictionaries.</p> <p>This function processes a list of rule dictionaries, filters out rules that are not marked for execution, and constructs a DataFrame with the relevant rule information. It ensures uniqueness of rows based on  specific columns and casts the data to appropriate types.</p> <p>Parameters:</p> Name Type Description Default <code>rules</code> <code>list[dict]</code> <p>A list of dictionaries, where each dictionary  represents a rule. Each rule dictionary may contain the following keys: - \"field\" (str or list): The column(s) the rule applies to. - \"check_type\" (str): The type of rule or check. - \"threshold\" (float, optional): The pass threshold for the rule. Defaults to 1.0. - \"value\" (any, optional): Additional value associated with the rule. - \"execute\" (bool, optional): Whether the rule should be executed. Defaults to True.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A Polars DataFrame containing the processed rules with the following columns: - \"column\" (str): The column(s) the rule applies to, joined by commas if multiple. - \"rule\" (str): The type of rule or check. - \"pass_threshold\" (float): The pass threshold for the rule. - \"value\" (str): The value associated with the rule, or an empty string if not provided.</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def __build_rules_df(rules: list[dict]) -&gt; pl.DataFrame:\n    \"\"\"\n    Builds a Polars DataFrame from a list of rule dictionaries.\n\n    This function processes a list of rule dictionaries, filters out rules\n    that are not marked for execution, and constructs a DataFrame with the\n    relevant rule information. It ensures uniqueness of rows based on \n    specific columns and casts the data to appropriate types.\n\n    Args:\n        rules (list[dict]): A list of dictionaries, where each dictionary \n            represents a rule. Each rule dictionary may contain the following keys:\n            - \"field\" (str or list): The column(s) the rule applies to.\n            - \"check_type\" (str): The type of rule or check.\n            - \"threshold\" (float, optional): The pass threshold for the rule. Defaults to 1.0.\n            - \"value\" (any, optional): Additional value associated with the rule.\n            - \"execute\" (bool, optional): Whether the rule should be executed. Defaults to True.\n\n    Returns:\n        pl.DataFrame: A Polars DataFrame containing the processed rules with the following columns:\n            - \"column\" (str): The column(s) the rule applies to, joined by commas if multiple.\n            - \"rule\" (str): The type of rule or check.\n            - \"pass_threshold\" (float): The pass threshold for the rule.\n            - \"value\" (str): The value associated with the rule, or an empty string if not provided.\n    \"\"\"\n    rules_df = (\n        pl.DataFrame(\n            [\n                {\n                    \"column\": (\n                        \",\".join(r[\"field\"])\n                        if isinstance(r[\"field\"], list)\n                        else r[\"field\"]\n                    ),\n                    \"rule\": r[\"check_type\"],\n                    \"pass_threshold\": float(r.get(\"threshold\") or 1.0),\n                    \"value\": r.get(\"value\"),\n                }\n                for r in rules\n                if r.get(\"execute\", True)\n            ]\n        )\n        .unique(subset=[\"column\", \"rule\", \"value\"])\n        .with_columns(\n            [\n                pl.col(\"column\").cast(str),\n                pl.col(\"rule\").cast(str),\n                pl.col(\"value\").cast(str),\n            ]\n        )\n    ).with_columns(pl.col(\"value\").fill_null(\"\").alias(\"value\"))\n\n    return rules_df\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.__compare_schemas","title":"<code>__compare_schemas(actual, expected)</code>","text":"<p>Compare two lists of schema definitions and identify discrepancies.</p> <p>Parameters:</p> Name Type Description Default <code>actual</code> <code>List[SchemaDef]</code> <p>The list of actual schema definitions.</p> required <code>expected</code> <code>List[SchemaDef]</code> <p>The list of expected schema definitions.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>Tuple[bool, List[Tuple[str, str]]]: A tuple where the first element is a boolean indicating </p> <code>List[Tuple[str, str]]</code> <p>whether the schemas match (True if they match, False otherwise), and the second element </p> <code>Tuple[bool, List[Tuple[str, str]]]</code> <p>is a list of tuples describing the discrepancies. Each tuple contains: - The field name (str). - A description of the discrepancy (str), such as \"missing\", \"type mismatch\",    \"nullable but expected non-nullable\", or \"extra column\".</p> Notes <ul> <li>A field is considered \"missing\" if it exists in the expected schema but not in the actual schema.</li> <li>A \"type mismatch\" occurs if the data type of a field in the actual schema does not match    the expected data type.</li> <li>A field is considered \"nullable but expected non-nullable\" if it is nullable in the actual    schema but not nullable in the expected schema.</li> <li>An \"extra column\" is a field that exists in the actual schema but not in the expected schema.</li> </ul> Source code in <code>sumeh/services/utils.py</code> <pre><code>def __compare_schemas(actual: List[SchemaDef], expected: List[SchemaDef],) -&gt; Tuple[bool, List[Tuple[str, str]]]:\n    \"\"\"\n    Compare two lists of schema definitions and identify discrepancies.\n\n    Args:\n        actual (List[SchemaDef]): The list of actual schema definitions.\n        expected (List[SchemaDef]): The list of expected schema definitions.\n\n    Returns:\n        Tuple[bool, List[Tuple[str, str]]]: A tuple where the first element is a boolean indicating \n        whether the schemas match (True if they match, False otherwise), and the second element \n        is a list of tuples describing the discrepancies. Each tuple contains:\n            - The field name (str).\n            - A description of the discrepancy (str), such as \"missing\", \"type mismatch\", \n              \"nullable but expected non-nullable\", or \"extra column\".\n\n    Notes:\n        - A field is considered \"missing\" if it exists in the expected schema but not in the actual schema.\n        - A \"type mismatch\" occurs if the data type of a field in the actual schema does not match \n          the expected data type.\n        - A field is considered \"nullable but expected non-nullable\" if it is nullable in the actual \n          schema but not nullable in the expected schema.\n        - An \"extra column\" is a field that exists in the actual schema but not in the expected schema.\n    \"\"\"\n\n    exp_map = {c[\"field\"]: c for c in expected}\n    act_map = {c[\"field\"]: c for c in actual}\n\n    erros: List[Tuple[str, str]] = []\n\n    for fld, exp in exp_map.items():\n        if fld not in act_map:\n            erros.append((fld, \"missing\"))\n            continue\n        act = act_map[fld]\n        if act[\"data_type\"] != exp[\"data_type\"]:\n            erros.append(\n                (\n                    fld,\n                    f\"type mismatch (got {act['data_type']}, expected {exp['data_type']})\",\n                )\n            )\n\n        if act[\"nullable\"] and not exp[\"nullable\"]:\n            erros.append((fld, \"nullable but expected non-nullable\"))\n\n        if exp.get(\"max_length\") is not None:\n            pass\n\n    # 2. campos extras (se quiser)\n    extras = set(act_map) - set(exp_map)\n    for fld in extras:\n        erros.append((fld, \"extra column\"))\n\n    return len(erros) == 0, erros\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.__convert_value","title":"<code>__convert_value(value)</code>","text":"<p>Converts the provided value to the appropriate type (date, float, or int).</p> <p>Depending on the format of the input value, it will be converted to a datetime object, a floating-point number (float), or an integer (int).</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>The value to be converted, represented as a string.</p> required <p>Returns:</p> Type Description <p>Union[datetime, float, int]: The converted value, which can be a datetime object, float, or int.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the value does not match an expected format.</p> Source code in <code>sumeh/services/utils.py</code> <pre><code>def __convert_value(value):\n    \"\"\"\n    Converts the provided value to the appropriate type (date, float, or int).\n\n    Depending on the format of the input value, it will be converted to a datetime object,\n    a floating-point number (float), or an integer (int).\n\n    Args:\n        value (str): The value to be converted, represented as a string.\n\n    Returns:\n        Union[datetime, float, int]: The converted value, which can be a datetime object, float, or int.\n\n    Raises:\n        ValueError: If the value does not match an expected format.\n    \"\"\"\n    from datetime import datetime\n\n    value = value.strip()\n    try:\n        if \"-\" in value:\n            return datetime.strptime(value, \"%Y-%m-%d\")\n        else:\n            return datetime.strptime(value, \"%d/%m/%Y\")\n    except ValueError:\n        if \".\" in value:\n            return float(value)\n        return int(value)\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.__extract_params","title":"<code>__extract_params(rule)</code>","text":"Source code in <code>sumeh/services/utils.py</code> <pre><code>def __extract_params(rule: dict) -&gt; tuple:\n    rule_name = rule[\"check_type\"]\n    field = rule[\"field\"]\n    raw_value = rule.get(\"value\")\n    if isinstance(raw_value, str) and raw_value not in (None, \"\", \"NULL\"):\n        try:\n            value = __convert_value(raw_value)\n        except ValueError:\n            value = raw_value\n    else:\n        value = raw_value\n    value = value if value not in (None, \"\", \"NULL\") else \"\"\n    return field, rule_name, value\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.__polars_schema_to_list","title":"<code>__polars_schema_to_list(df)</code>","text":"<p>Converts the schema of a Polars DataFrame into a list of dictionaries,  where each dictionary represents a field in the schema.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The Polars DataFrame whose schema is to be converted.</p> required <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List[Dict[str, Any]]: A list of dictionaries, each containing the following keys: - \"field\" (str): The name of the field. - \"data_type\" (str): The data type of the field, converted to lowercase. - \"nullable\" (bool): Always set to True, as Polars does not expose nullability in the schema. - \"max_length\" (None): Always set to None, as max length is not applicable.</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def __polars_schema_to_list(df: pl.DataFrame) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Converts the schema of a Polars DataFrame into a list of dictionaries, \n    where each dictionary represents a field in the schema.\n\n    Args:\n        df (pl.DataFrame): The Polars DataFrame whose schema is to be converted.\n\n    Returns:\n        List[Dict[str, Any]]: A list of dictionaries, each containing the following keys:\n            - \"field\" (str): The name of the field.\n            - \"data_type\" (str): The data type of the field, converted to lowercase.\n            - \"nullable\" (bool): Always set to True, as Polars does not expose nullability in the schema.\n            - \"max_length\" (None): Always set to None, as max length is not applicable.\n    \"\"\"\n    return [\n        {\n            \"field\": name,\n            \"data_type\": str(dtype).lower(),\n            \"nullable\": True,  # Polars n\u00e3o exp\u00f5e nullability no schema\n            \"max_length\": None,\n        }\n        for name, dtype in df.schema.items()\n    ]\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.are_complete","title":"<code>are_complete(df, rule)</code>","text":"<p>Filters a Polars DataFrame to identify rows where specified fields contain null values and tags them with a data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'fields': A list of column names to check for null values. - 'check': A string representing the type of check (e.g., \"is_null\"). - 'value': A value associated with the check (not used in this function).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A filtered DataFrame containing only rows where at least one of the</p> <code>DataFrame</code> <p>specified fields is null, with an additional column \"dq_status\" indicating the</p> <code>DataFrame</code> <p>data quality status.</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def are_complete(df: pl.DataFrame, rule: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Filters a Polars DataFrame to identify rows where specified fields contain null values\n    and tags them with a data quality status.\n\n    Args:\n        df (pl.DataFrame): The input Polars DataFrame to be checked.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'fields': A list of column names to check for null values.\n            - 'check': A string representing the type of check (e.g., \"is_null\").\n            - 'value': A value associated with the check (not used in this function).\n\n    Returns:\n        pl.DataFrame: A filtered DataFrame containing only rows where at least one of the\n        specified fields is null, with an additional column \"dq_status\" indicating the\n        data quality status.\n    \"\"\"\n    fields, check, value = __extract_params(rule)\n    cond = reduce(operator.or_, [pl.col(f).is_null() for f in fields])\n\n    tag = f\"{fields}:{check}:{value}\"\n    return df.filter(cond).with_columns(pl.lit(tag).alias(\"dq_status\"))\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.are_unique","title":"<code>are_unique(df, rule)</code>","text":"<p>Checks for duplicate combinations of specified fields in a Polars DataFrame  and returns a DataFrame containing the rows with duplicates along with a  data quality status column.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to check for duplicates.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected           to include the following keys:          - 'fields': A list of column names to check for uniqueness.          - 'check': A string representing the type of check (e.g., \"unique\").          - 'value': A value associated with the check (e.g., \"True\").</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A DataFrame containing rows with duplicate combinations of            the specified fields. An additional column, \"dq_status\",            is added to indicate the data quality status in the format            \"{fields}:{check}:{value}\".</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def are_unique(df: pl.DataFrame, rule: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Checks for duplicate combinations of specified fields in a Polars DataFrame \n    and returns a DataFrame containing the rows with duplicates along with a \n    data quality status column.\n\n    Args:\n        df (pl.DataFrame): The input Polars DataFrame to check for duplicates.\n        rule (dict): A dictionary containing the rule parameters. It is expected \n                     to include the following keys:\n                     - 'fields': A list of column names to check for uniqueness.\n                     - 'check': A string representing the type of check (e.g., \"unique\").\n                     - 'value': A value associated with the check (e.g., \"True\").\n\n    Returns:\n        pl.DataFrame: A DataFrame containing rows with duplicate combinations of \n                      the specified fields. An additional column, \"dq_status\", \n                      is added to indicate the data quality status in the format \n                      \"{fields}:{check}:{value}\".\n    \"\"\"\n    fields, check, value = __extract_params(rule)\n    combo = df.with_columns(\n        pl.concat_str([pl.col(f).cast(str) for f in fields], separator=\"|\").alias(\n            \"_combo\"\n        )\n    )\n    dupes = (\n        combo.group_by(\"_combo\")\n        .agg(pl.len().alias(\"cnt\"))\n        .filter(pl.col(\"cnt\") &gt; 1)\n        .select(\"_combo\")\n        .to_series()\n        .to_list()\n    )\n    return (\n        combo.filter(pl.col(\"_combo\").is_in(dupes))\n        .drop(\"_combo\")\n        .with_columns(pl.lit(f\"{fields}:{check}:{value}\").alias(\"dq_status\"))\n    )\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.has_cardinality","title":"<code>has_cardinality(df, rule)</code>","text":"<p>Checks if the cardinality (number of unique values) of a specified field in the given DataFrame satisfies a condition defined in the rule. If the cardinality exceeds the specified value,  a new column \"dq_status\" is added to the DataFrame with a string indicating the rule violation.  Otherwise, an empty DataFrame is returned.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to evaluate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - \"field\" (str): The column name to check. - \"check\" (str): The type of check (e.g., \"greater_than\"). - \"value\" (int): The threshold value for the cardinality.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: The original DataFrame with an added \"dq_status\" column if the rule is violated,           or an empty DataFrame if the rule is not violated.</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def has_cardinality(df: pl.DataFrame, rule: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Checks if the cardinality (number of unique values) of a specified field in the given DataFrame\n    satisfies a condition defined in the rule. If the cardinality exceeds the specified value, \n    a new column \"dq_status\" is added to the DataFrame with a string indicating the rule violation. \n    Otherwise, an empty DataFrame is returned.\n\n    Args:\n        df (pl.DataFrame): The input DataFrame to evaluate.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - \"field\" (str): The column name to check.\n            - \"check\" (str): The type of check (e.g., \"greater_than\").\n            - \"value\" (int): The threshold value for the cardinality.\n\n    Returns:\n        pl.DataFrame: The original DataFrame with an added \"dq_status\" column if the rule is violated,\n                      or an empty DataFrame if the rule is not violated.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    card = df.select(pl.col(field).n_unique()).to_numpy()[0] or 0\n    if card &gt; value:\n        return df.with_columns(pl.lit(f\"{field}:{check}:{value}\").alias(\"dq_status\"))\n    return df.head(0)\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.has_entropy","title":"<code>has_entropy(df, rule)</code>","text":"<p>Evaluates the entropy of a specified field in a Polars DataFrame based on a given rule.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to evaluate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The column name in the DataFrame to evaluate. - 'check' (str): The type of check to perform (not used directly in this function). - 'value' (float): The threshold value for entropy comparison.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame:  - If the entropy of the specified field exceeds the given threshold (<code>value</code>),    returns the original DataFrame with an additional column <code>dq_status</code> indicating    the rule that was applied. - If the entropy does not exceed the threshold, returns an empty DataFrame with    the same schema as the input DataFrame.</p> Notes <ul> <li>The entropy is calculated as the number of unique values in the specified field.</li> <li>The <code>dq_status</code> column contains a string in the format \"{field}:{check}:{value}\".</li> </ul> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def has_entropy(df: pl.DataFrame, rule: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Evaluates the entropy of a specified field in a Polars DataFrame based on a given rule.\n\n    Parameters:\n        df (pl.DataFrame): The input Polars DataFrame to evaluate.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field' (str): The column name in the DataFrame to evaluate.\n            - 'check' (str): The type of check to perform (not used directly in this function).\n            - 'value' (float): The threshold value for entropy comparison.\n\n    Returns:\n        pl.DataFrame: \n            - If the entropy of the specified field exceeds the given threshold (`value`), \n              returns the original DataFrame with an additional column `dq_status` indicating \n              the rule that was applied.\n            - If the entropy does not exceed the threshold, returns an empty DataFrame with \n              the same schema as the input DataFrame.\n\n    Notes:\n        - The entropy is calculated as the number of unique values in the specified field.\n        - The `dq_status` column contains a string in the format \"{field}:{check}:{value}\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    ent = df.select(pl.col(field).n_unique()).to_numpy()[0] or 0.0\n    if ent &gt; value:\n        return df.with_columns(pl.lit(f\"{field}:{check}:{value}\").alias(\"dq_status\"))\n    return df.head(0)\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.has_infogain","title":"<code>has_infogain(df, rule)</code>","text":"<p>Evaluates whether a given DataFrame satisfies an information gain condition  based on a specified rule. If the condition is met, a new column indicating  the rule is added; otherwise, an empty DataFrame is returned.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to evaluate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should  include the following keys: - 'field': The column name to evaluate. - 'check': The type of check to perform (not used directly in this function). - 'value': The threshold value for the information gain.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: The original DataFrame with an additional column named </p> <code>DataFrame</code> <p>\"dq_status\" if the condition is met, or an empty DataFrame if the </p> <code>DataFrame</code> <p>condition is not met.</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def has_infogain(df: pl.DataFrame, rule: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Evaluates whether a given DataFrame satisfies an information gain condition \n    based on a specified rule. If the condition is met, a new column indicating \n    the rule is added; otherwise, an empty DataFrame is returned.\n\n    Args:\n        df (pl.DataFrame): The input DataFrame to evaluate.\n        rule (dict): A dictionary containing the rule parameters. It should \n            include the following keys:\n            - 'field': The column name to evaluate.\n            - 'check': The type of check to perform (not used directly in this function).\n            - 'value': The threshold value for the information gain.\n\n    Returns:\n        pl.DataFrame: The original DataFrame with an additional column named \n        \"dq_status\" if the condition is met, or an empty DataFrame if the \n        condition is not met.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    ig = df.select(pl.col(field).n_unique()).to_numpy()[0] or 0.0\n    if ig &gt; value:\n        return df.with_columns(pl.lit(f\"{field}:{check}:{value}\").alias(\"dq_status\"))\n    return df.head(0)\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.has_max","title":"<code>has_max(df, rule)</code>","text":"<p>Filters a Polars DataFrame to include only rows where the value in a specified  column exceeds a given threshold, and adds a new column indicating the rule applied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The column name to apply the filter on. - 'check' (str): The type of check being performed (e.g., \"max\"). - 'value' (numeric): The threshold value to compare against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new DataFrame containing only the rows that satisfy the condition, </p> <code>DataFrame</code> <p>with an additional column named \"dq_status\" that describes the applied rule.</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def has_max(df: pl.DataFrame, rule: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Filters a Polars DataFrame to include only rows where the value in a specified \n    column exceeds a given threshold, and adds a new column indicating the rule applied.\n\n    Args:\n        df (pl.DataFrame): The input Polars DataFrame to be filtered.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field' (str): The column name to apply the filter on.\n            - 'check' (str): The type of check being performed (e.g., \"max\").\n            - 'value' (numeric): The threshold value to compare against.\n\n    Returns:\n        pl.DataFrame: A new DataFrame containing only the rows that satisfy the condition, \n        with an additional column named \"dq_status\" that describes the applied rule.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    return df.filter(pl.col(field) &gt; value).with_columns(\n        pl.lit(f\"{field}:{check}:{value}\").alias(\"dq_status\")\n    )\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.has_mean","title":"<code>has_mean(df, rule)</code>","text":"<p>Checks if the mean value of a specified column in a Polars DataFrame satisfies a given condition.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The name of the column to calculate the mean for. - 'check' (str): The condition to check (e.g., 'greater than'). - 'value' (float): The threshold value to compare the mean against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame:  - If the mean value of the specified column is greater than the threshold value,    returns the original DataFrame with an additional column \"dq_status\" containing    a string in the format \"{field}:{check}:{value}\". - If the condition is not met, returns an empty DataFrame with the same schema as the input.</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def has_mean(df: pl.DataFrame, rule: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Checks if the mean value of a specified column in a Polars DataFrame satisfies a given condition.\n\n    Args:\n        df (pl.DataFrame): The input Polars DataFrame.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field' (str): The name of the column to calculate the mean for.\n            - 'check' (str): The condition to check (e.g., 'greater than').\n            - 'value' (float): The threshold value to compare the mean against.\n\n    Returns:\n        pl.DataFrame: \n            - If the mean value of the specified column is greater than the threshold value, \n              returns the original DataFrame with an additional column \"dq_status\" containing \n              a string in the format \"{field}:{check}:{value}\".\n            - If the condition is not met, returns an empty DataFrame with the same schema as the input.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    mean_val = df.select(pl.col(field).mean()).to_numpy()[0] or 0.0\n    if mean_val &gt; value:\n        return df.with_columns(pl.lit(f\"{field}:{check}:{value}\").alias(\"dq_status\"))\n    return df.head(0)\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.has_min","title":"<code>has_min(df, rule)</code>","text":"<p>Filters a Polars DataFrame to include only rows where the value of a specified  column is less than a given threshold and adds a new column indicating the  data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The name of the column to apply the filter on. - 'check': A string representing the type of check (e.g., 'min'). - 'value': The threshold value for the filter.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new Polars DataFrame containing only the rows that satisfy </p> <code>DataFrame</code> <p>the condition, with an additional column named \"dq_status\" indicating the </p> <code>DataFrame</code> <p>applied rule in the format \"field:check:value\".</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def has_min(df: pl.DataFrame, rule: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Filters a Polars DataFrame to include only rows where the value of a specified \n    column is less than a given threshold and adds a new column indicating the \n    data quality status.\n\n    Args:\n        df (pl.DataFrame): The input Polars DataFrame to be filtered.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field': The name of the column to apply the filter on.\n            - 'check': A string representing the type of check (e.g., 'min').\n            - 'value': The threshold value for the filter.\n\n    Returns:\n        pl.DataFrame: A new Polars DataFrame containing only the rows that satisfy \n        the condition, with an additional column named \"dq_status\" indicating the \n        applied rule in the format \"field:check:value\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    return df.filter(pl.col(field) &lt; value).with_columns(\n        pl.lit(f\"{field}:{check}:{value}\").alias(\"dq_status\")\n    )\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.has_pattern","title":"<code>has_pattern(df, rule)</code>","text":"<p>Filters a Polars DataFrame based on a pattern-matching rule and adds a data quality status column.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The column name in the DataFrame to apply the pattern check. - 'check': A descriptive label for the check being performed. - 'pattern': The regex pattern to match against the column values.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new DataFrame with rows not matching the pattern removed and an additional</p> <code>DataFrame</code> <p>column named \"dq_status\" indicating the rule applied in the format \"field:check:pattern\".</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def has_pattern(df: pl.DataFrame, rule: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Filters a Polars DataFrame based on a pattern-matching rule and adds a data quality status column.\n\n    Args:\n        df (pl.DataFrame): The input Polars DataFrame to be filtered.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field': The column name in the DataFrame to apply the pattern check.\n            - 'check': A descriptive label for the check being performed.\n            - 'pattern': The regex pattern to match against the column values.\n\n    Returns:\n        pl.DataFrame: A new DataFrame with rows not matching the pattern removed and an additional\n        column named \"dq_status\" indicating the rule applied in the format \"field:check:pattern\".\n    \"\"\"\n    field, check, pattern = __extract_params(rule)\n    return df.filter(~pl.col(field).str.contains(pattern, literal=False)).with_columns(\n        pl.lit(f\"{field}:{check}:{pattern}\").alias(\"dq_status\")\n    )\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.has_std","title":"<code>has_std(df, rule)</code>","text":"<p>Evaluates whether the standard deviation of a specified column in a Polars DataFrame exceeds a given threshold and returns a modified DataFrame accordingly.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to evaluate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The name of the column to calculate the standard deviation for. - 'check' (str): A descriptive label for the check being performed. - 'value' (float): The threshold value for the standard deviation.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A modified DataFrame. If the standard deviation of the specified column</p> <code>DataFrame</code> <p>exceeds the threshold, the DataFrame will include a new column <code>dq_status</code> with a</p> <code>DataFrame</code> <p>descriptive string. Otherwise, an empty DataFrame with the <code>dq_status</code> column is returned.</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def has_std(df: pl.DataFrame, rule: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Evaluates whether the standard deviation of a specified column in a Polars DataFrame\n    exceeds a given threshold and returns a modified DataFrame accordingly.\n\n    Args:\n        df (pl.DataFrame): The input Polars DataFrame to evaluate.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field' (str): The name of the column to calculate the standard deviation for.\n            - 'check' (str): A descriptive label for the check being performed.\n            - 'value' (float): The threshold value for the standard deviation.\n\n    Returns:\n        pl.DataFrame: A modified DataFrame. If the standard deviation of the specified column\n        exceeds the threshold, the DataFrame will include a new column `dq_status` with a\n        descriptive string. Otherwise, an empty DataFrame with the `dq_status` column is returned.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    std_val = df.select(pl.col(field).std()).to_numpy()[0] or 0.0\n    if std_val &gt; value:\n        return df.with_columns(pl.lit(f\"{field}:{check}:{value}\").alias(\"dq_status\"))\n    return df.head(0).with_columns(pl.lit(\"dq_status\").alias(\"dq_status\")).head(0)\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.has_sum","title":"<code>has_sum(df, rule)</code>","text":"<p>Checks if the sum of a specified column in a Polars DataFrame exceeds a given value.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The name of the column to sum. - 'check': A string representing the check type (not used in this function). - 'value': The threshold value to compare the sum against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: If the sum of the specified column exceeds the given value, </p> <code>DataFrame</code> <p>returns the original DataFrame with an additional column <code>dq_status</code> containing </p> <code>DataFrame</code> <p>a string in the format \"{field}:{check}:{value}\". Otherwise, returns an empty DataFrame.</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def has_sum(df: pl.DataFrame, rule: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Checks if the sum of a specified column in a Polars DataFrame exceeds a given value.\n\n    Args:\n        df (pl.DataFrame): The input Polars DataFrame.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field': The name of the column to sum.\n            - 'check': A string representing the check type (not used in this function).\n            - 'value': The threshold value to compare the sum against.\n\n    Returns:\n        pl.DataFrame: If the sum of the specified column exceeds the given value, \n        returns the original DataFrame with an additional column `dq_status` containing \n        a string in the format \"{field}:{check}:{value}\". Otherwise, returns an empty DataFrame.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    sum_val = df.select(pl.col(field).sum()).to_numpy()[0] or 0.0\n    if sum_val &gt; value:\n        return df.with_columns(pl.lit(f\"{field}:{check}:{value}\").alias(\"dq_status\"))\n    return df.head(0)\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.is_between","title":"<code>is_between(df, rule)</code>","text":"<p>Filters a Polars DataFrame to exclude rows where the specified field's value  falls within a given range, and adds a column indicating the data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The name of the column to check. - 'check': The type of check being performed (e.g., \"is_between\"). - 'value': A string representing the range in the format \"[lo,hi]\".</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new Polars DataFrame with rows outside the specified range </p> <code>DataFrame</code> <p>and an additional column named \"dq_status\" indicating the rule applied.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the 'value' parameter is not in the expected format \"[lo,hi]\".</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def is_between(df: pl.DataFrame, rule: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Filters a Polars DataFrame to exclude rows where the specified field's value \n    falls within a given range, and adds a column indicating the data quality status.\n\n    Args:\n        df (pl.DataFrame): The input Polars DataFrame to filter.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field': The name of the column to check.\n            - 'check': The type of check being performed (e.g., \"is_between\").\n            - 'value': A string representing the range in the format \"[lo,hi]\".\n\n    Returns:\n        pl.DataFrame: A new Polars DataFrame with rows outside the specified range \n        and an additional column named \"dq_status\" indicating the rule applied.\n\n    Raises:\n        ValueError: If the 'value' parameter is not in the expected format \"[lo,hi]\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    lo, hi = value.strip(\"[]\").split(\",\")\n    lo, hi = __convert_value(lo), __convert_value(hi)\n    return df.filter(~pl.col(field).is_between(lo, hi)).with_columns(\n        pl.lit(f\"{field}:{check}:{value}\").alias(\"dq_status\")\n    )\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.is_complete","title":"<code>is_complete(df, rule)</code>","text":"<p>Filters a Polars DataFrame to include only rows where the specified field is not null and appends a new column indicating the data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to be filtered and modified.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The name of the column to check for non-null values. - 'check' (str): A descriptive string for the type of check being performed. - 'value' (str): A value associated with the rule for status annotation.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new Polars DataFrame with rows filtered based on the rule and</p> <code>DataFrame</code> <p>an additional column named \"dq_status\" containing the data quality status.</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def is_complete(df: pl.DataFrame, rule: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Filters a Polars DataFrame to include only rows where the specified field is not null\n    and appends a new column indicating the data quality status.\n\n    Args:\n        df (pl.DataFrame): The input Polars DataFrame to be filtered and modified.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field' (str): The name of the column to check for non-null values.\n            - 'check' (str): A descriptive string for the type of check being performed.\n            - 'value' (str): A value associated with the rule for status annotation.\n\n    Returns:\n        pl.DataFrame: A new Polars DataFrame with rows filtered based on the rule and\n        an additional column named \"dq_status\" containing the data quality status.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    return df.filter(pl.col(field).is_not_null()).with_columns(\n        pl.lit(f\"{field}:{check}:{value}\").alias(\"dq_status\")\n    )\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.is_contained_in","title":"<code>is_contained_in(df, rule)</code>","text":"<p>Filters a Polars DataFrame to exclude rows where the specified field's value is  contained in a given list of values, and adds a new column indicating the rule applied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The column name to check. - 'check': The type of check being performed (e.g., \"is_contained_in\"). - 'value': A string representation of a list of values to check against,    e.g., \"[value1, value2, value3]\".</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new DataFrame with rows filtered based on the rule and an </p> <code>DataFrame</code> <p>additional column \"dq_status\" indicating the rule applied.</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def is_contained_in(df: pl.DataFrame, rule: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Filters a Polars DataFrame to exclude rows where the specified field's value is \n    contained in a given list of values, and adds a new column indicating the rule applied.\n\n    Args:\n        df (pl.DataFrame): The input Polars DataFrame to be filtered.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field': The column name to check.\n            - 'check': The type of check being performed (e.g., \"is_contained_in\").\n            - 'value': A string representation of a list of values to check against, \n              e.g., \"[value1, value2, value3]\".\n\n    Returns:\n        pl.DataFrame: A new DataFrame with rows filtered based on the rule and an \n        additional column \"dq_status\" indicating the rule applied.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    lst = [v.strip() for v in value.strip(\"[]\").split(\",\")]\n    return df.filter(~pl.col(field).is_in(lst)).with_columns(\n        pl.lit(f\"{field}:{check}:{value}\").alias(\"dq_status\")\n    )\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.is_equal","title":"<code>is_equal(df, rule)</code>","text":"<p>Filters rows in a Polars DataFrame that do not match a specified equality condition and adds a column indicating the data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The column name to apply the equality check on. - 'check': The type of check (expected to be 'eq' for equality). - 'value': The value to compare against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new DataFrame with rows filtered based on the rule and an</p> <code>DataFrame</code> <p>additional column named \"dq_status\" indicating the rule applied.</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def is_equal(df: pl.DataFrame, rule: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Filters rows in a Polars DataFrame that do not match a specified equality condition\n    and adds a column indicating the data quality status.\n\n    Args:\n        df (pl.DataFrame): The input Polars DataFrame to be filtered.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field': The column name to apply the equality check on.\n            - 'check': The type of check (expected to be 'eq' for equality).\n            - 'value': The value to compare against.\n\n    Returns:\n        pl.DataFrame: A new DataFrame with rows filtered based on the rule and an\n        additional column named \"dq_status\" indicating the rule applied.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    return df.filter(~pl.col(field).eq(value)).with_columns(\n        pl.lit(f\"{field}:{check}:{value}\").alias(\"dq_status\")\n    )\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.is_equal_than","title":"<code>is_equal_than(df, rule)</code>","text":"<p>Filters rows in a Polars DataFrame where the specified field is not equal to a given value and adds a new column indicating the data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The name of the column to check. - 'check': The type of check (expected to be 'equal' for this function). - 'value': The value to compare against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new Polars DataFrame with rows filtered based on the rule and an</p> <code>DataFrame</code> <p>additional column named \"dq_status\" indicating the applied rule.</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def is_equal_than(df: pl.DataFrame, rule: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Filters rows in a Polars DataFrame where the specified field is not equal to a given value\n    and adds a new column indicating the data quality status.\n\n    Args:\n        df (pl.DataFrame): The input Polars DataFrame to be filtered.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field': The name of the column to check.\n            - 'check': The type of check (expected to be 'equal' for this function).\n            - 'value': The value to compare against.\n\n    Returns:\n        pl.DataFrame: A new Polars DataFrame with rows filtered based on the rule and an\n        additional column named \"dq_status\" indicating the applied rule.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    return df.filter(~pl.col(field).eq(value)).with_columns(\n        pl.lit(f\"{field}:{check}:{value}\").alias(\"dq_status\")\n    )\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.is_greater_or_equal_than","title":"<code>is_greater_or_equal_than(df, rule)</code>","text":"<p>Filters a Polars DataFrame to include only rows where the specified field  is greater than or equal to a given value, and adds a new column indicating  the data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the filtering rule. It should  include the following keys: - 'field': The name of the column to be checked. - 'check': The type of check being performed (e.g., \"greater_or_equal\"). - 'value': The threshold value for the comparison.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new Polars DataFrame with rows filtered based on the </p> <code>DataFrame</code> <p>specified rule and an additional column named \"dq_status\" indicating </p> <code>DataFrame</code> <p>the data quality status in the format \"field:check:value\".</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def is_greater_or_equal_than(df: pl.DataFrame, rule: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Filters a Polars DataFrame to include only rows where the specified field \n    is greater than or equal to a given value, and adds a new column indicating \n    the data quality status.\n\n    Args:\n        df (pl.DataFrame): The input Polars DataFrame to be filtered.\n        rule (dict): A dictionary containing the filtering rule. It should \n            include the following keys:\n            - 'field': The name of the column to be checked.\n            - 'check': The type of check being performed (e.g., \"greater_or_equal\").\n            - 'value': The threshold value for the comparison.\n\n    Returns:\n        pl.DataFrame: A new Polars DataFrame with rows filtered based on the \n        specified rule and an additional column named \"dq_status\" indicating \n        the data quality status in the format \"field:check:value\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    return df.filter(pl.col(field) &lt; value).with_columns(\n        pl.lit(f\"{field}:{check}:{value}\").alias(\"dq_status\")\n    )\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.is_greater_than","title":"<code>is_greater_than(df, rule)</code>","text":"<p>Filters a Polars DataFrame to include only rows where the specified field's value  is less than or equal to a given value, and adds a new column indicating the  data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The Polars DataFrame to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the filtering rule. It should include: - 'field': The name of the column to apply the filter on. - 'check': A string describing the check (e.g., \"greater_than\"). - 'value': The value to compare against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new DataFrame with rows filtered based on the rule and an </p> <code>DataFrame</code> <p>additional column named \"dq_status\" indicating the applied rule.</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def is_greater_than(df: pl.DataFrame, rule: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Filters a Polars DataFrame to include only rows where the specified field's value \n    is less than or equal to a given value, and adds a new column indicating the \n    data quality status.\n\n    Args:\n        df (pl.DataFrame): The Polars DataFrame to filter.\n        rule (dict): A dictionary containing the filtering rule. It should include:\n            - 'field': The name of the column to apply the filter on.\n            - 'check': A string describing the check (e.g., \"greater_than\").\n            - 'value': The value to compare against.\n\n    Returns:\n        pl.DataFrame: A new DataFrame with rows filtered based on the rule and an \n        additional column named \"dq_status\" indicating the applied rule.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    return df.filter(pl.col(field) &lt;= value).with_columns(\n        pl.lit(f\"{field}:{check}:{value}\").alias(\"dq_status\")\n    )\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.is_legit","title":"<code>is_legit(df, rule)</code>","text":"<p>Filters a Polars DataFrame based on a validation rule and appends a data quality status column.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to validate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the validation rule. It should include: - 'field': The name of the column to validate. - 'check': The type of validation check (e.g., regex, condition). - 'value': The value or pattern to validate against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new DataFrame containing rows that failed the validation,</p> <code>DataFrame</code> <p>with an additional column 'dq_status' indicating the validation rule applied.</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def is_legit(df: pl.DataFrame, rule: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Filters a Polars DataFrame based on a validation rule and appends a data quality status column.\n\n    Args:\n        df (pl.DataFrame): The input Polars DataFrame to validate.\n        rule (dict): A dictionary containing the validation rule. It should include:\n            - 'field': The name of the column to validate.\n            - 'check': The type of validation check (e.g., regex, condition).\n            - 'value': The value or pattern to validate against.\n\n    Returns:\n        pl.DataFrame: A new DataFrame containing rows that failed the validation,\n        with an additional column 'dq_status' indicating the validation rule applied.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    mask = pl.col(field).is_not_null() &amp; pl.col(field).str.contains(r\"^\\S+$\")\n    return df.filter(~mask).with_columns(\n        pl.lit(f\"{field}:{check}:{value}\").alias(\"dq_status\")\n    )\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.is_less_or_equal_than","title":"<code>is_less_or_equal_than(df, rule)</code>","text":"<p>Filters a Polars DataFrame to include only rows where the specified field's value  is greater than the given value, and adds a new column indicating the rule applied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The name of the column to apply the filter on. - 'check': The type of check being performed (e.g., 'less_or_equal_than'). - 'value': The value to compare against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new DataFrame with rows filtered based on the rule and an </p> <code>DataFrame</code> <p>additional column named \"dq_status\" indicating the rule applied.</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def is_less_or_equal_than(df: pl.DataFrame, rule: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Filters a Polars DataFrame to include only rows where the specified field's value \n    is greater than the given value, and adds a new column indicating the rule applied.\n\n    Args:\n        df (pl.DataFrame): The input Polars DataFrame to be filtered.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field': The name of the column to apply the filter on.\n            - 'check': The type of check being performed (e.g., 'less_or_equal_than').\n            - 'value': The value to compare against.\n\n    Returns:\n        pl.DataFrame: A new DataFrame with rows filtered based on the rule and an \n        additional column named \"dq_status\" indicating the rule applied.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    return df.filter(pl.col(field) &gt; value).with_columns(\n        pl.lit(f\"{field}:{check}:{value}\").alias(\"dq_status\")\n    )\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.is_less_than","title":"<code>is_less_than(df, rule)</code>","text":"<p>Filters a Polars DataFrame to include only rows where the specified field is greater than or equal to a given value. Adds a new column indicating the data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the filtering rule. It should include the following keys: - 'field': The name of the column to apply the filter on. - 'check': A string representing the type of check (not used in logic). - 'value': The threshold value for the filter.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new Polars DataFrame with rows filtered based on the</p> <code>DataFrame</code> <p>condition and an additional column named \"dq_status\" containing the</p> <code>DataFrame</code> <p>rule description in the format \"field:check:value\".</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def is_less_than(df: pl.DataFrame, rule: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Filters a Polars DataFrame to include only rows where the specified field\n    is greater than or equal to a given value. Adds a new column indicating\n    the data quality status.\n\n    Args:\n        df (pl.DataFrame): The input Polars DataFrame to filter.\n        rule (dict): A dictionary containing the filtering rule. It should\n            include the following keys:\n            - 'field': The name of the column to apply the filter on.\n            - 'check': A string representing the type of check (not used in logic).\n            - 'value': The threshold value for the filter.\n\n    Returns:\n        pl.DataFrame: A new Polars DataFrame with rows filtered based on the\n        condition and an additional column named \"dq_status\" containing the\n        rule description in the format \"field:check:value\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    return df.filter(pl.col(field) &gt;= value).with_columns(\n        pl.lit(f\"{field}:{check}:{value}\").alias(\"dq_status\")\n    )\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.is_negative","title":"<code>is_negative(df, rule)</code>","text":"<p>Filters a Polars DataFrame to exclude rows where the specified field is negative and adds a new column indicating the data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The name of the column to check. - 'check': The type of check being performed (e.g., \"is_negative\"). - 'value': The value associated with the rule (not used in this function).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new DataFrame with rows where the specified field is non-negative</p> <code>DataFrame</code> <p>and an additional column named \"dq_status\" containing the rule details.</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def is_negative(df: pl.DataFrame, rule: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Filters a Polars DataFrame to exclude rows where the specified field is negative\n    and adds a new column indicating the data quality status.\n\n    Args:\n        df (pl.DataFrame): The input Polars DataFrame to be filtered.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field': The name of the column to check.\n            - 'check': The type of check being performed (e.g., \"is_negative\").\n            - 'value': The value associated with the rule (not used in this function).\n\n    Returns:\n        pl.DataFrame: A new DataFrame with rows where the specified field is non-negative\n        and an additional column named \"dq_status\" containing the rule details.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    return df.filter(pl.col(field) &gt;= 0).with_columns(\n        [pl.lit(f\"{field}:{check}:{value}\").alias(\"dq_status\")]\n    )\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.is_positive","title":"<code>is_positive(df, rule)</code>","text":"<p>Filters a Polars DataFrame to identify rows where the specified field  contains negative values and appends a new column indicating the data  quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is  expected to include the following keys: - 'field': The name of the column to check. - 'check': The type of check being performed (e.g., \"is_positive\"). - 'value': The reference value for the check.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new Polars DataFrame containing only the rows where </p> <code>DataFrame</code> <p>the specified field has negative values, with an additional column </p> <code>DataFrame</code> <p>named \"dq_status\" that describes the rule applied.</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def is_positive(df: pl.DataFrame, rule: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Filters a Polars DataFrame to identify rows where the specified field \n    contains negative values and appends a new column indicating the data \n    quality status.\n\n    Args:\n        df (pl.DataFrame): The input Polars DataFrame to be filtered.\n        rule (dict): A dictionary containing the rule parameters. It is \n            expected to include the following keys:\n            - 'field': The name of the column to check.\n            - 'check': The type of check being performed (e.g., \"is_positive\").\n            - 'value': The reference value for the check.\n\n    Returns:\n        pl.DataFrame: A new Polars DataFrame containing only the rows where \n        the specified field has negative values, with an additional column \n        named \"dq_status\" that describes the rule applied.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    return df.filter(pl.col(field) &lt; 0).with_columns(\n        [pl.lit(f\"{field}:{check}:{value}\").alias(\"dq_status\")]\n    )\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.is_unique","title":"<code>is_unique(df, rule)</code>","text":"<p>Checks for duplicate values in a specified field of a Polars DataFrame and  returns a filtered DataFrame containing only the rows with duplicate values.  Additionally, it adds a new column 'dq_status' with a formatted string  indicating the field, check type, and value.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to check for duplicates.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected           to have keys that allow extraction of the field to check,           the type of check, and a value.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A filtered DataFrame containing rows with duplicate values            in the specified field, along with an additional column            'dq_status' describing the rule applied.</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def is_unique(df: pl.DataFrame, rule: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Checks for duplicate values in a specified field of a Polars DataFrame and \n    returns a filtered DataFrame containing only the rows with duplicate values. \n    Additionally, it adds a new column 'dq_status' with a formatted string \n    indicating the field, check type, and value.\n\n    Args:\n        df (pl.DataFrame): The input Polars DataFrame to check for duplicates.\n        rule (dict): A dictionary containing the rule parameters. It is expected \n                     to have keys that allow extraction of the field to check, \n                     the type of check, and a value.\n\n    Returns:\n        pl.DataFrame: A filtered DataFrame containing rows with duplicate values \n                      in the specified field, along with an additional column \n                      'dq_status' describing the rule applied.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    dup_vals = (\n        df.group_by(field)\n        .agg(pl.len().alias(\"cnt\"))\n        .filter(pl.col(\"cnt\") &gt; 1)\n        .select(field)\n        .to_series()\n        .to_list()\n    )\n    return df.filter(pl.col(field).is_in(dup_vals)).with_columns(\n        pl.lit(f\"{field}:{check}:{value}\").alias(\"dq_status\")\n    )\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.not_contained_in","title":"<code>not_contained_in(df, rule)</code>","text":"<p>Filters a Polars DataFrame to include only rows where the specified field's value  is in a given list, and adds a new column indicating the data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the filtering rule. It should include: - 'field': The column name to apply the filter on. - 'check': A string representing the type of check (not used in logic). - 'value': A string representation of a list of values (e.g., \"[value1, value2]\").</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new Polars DataFrame with rows filtered based on the rule and </p> <code>DataFrame</code> <p>an additional column \"dq_status\" indicating the applied rule.</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def not_contained_in(df: pl.DataFrame, rule: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Filters a Polars DataFrame to include only rows where the specified field's value \n    is in a given list, and adds a new column indicating the data quality status.\n\n    Args:\n        df (pl.DataFrame): The input Polars DataFrame to filter.\n        rule (dict): A dictionary containing the filtering rule. It should include:\n            - 'field': The column name to apply the filter on.\n            - 'check': A string representing the type of check (not used in logic).\n            - 'value': A string representation of a list of values (e.g., \"[value1, value2]\").\n\n    Returns:\n        pl.DataFrame: A new Polars DataFrame with rows filtered based on the rule and \n        an additional column \"dq_status\" indicating the applied rule.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    lst = [v.strip() for v in value.strip(\"[]\").split(\",\")]\n    return df.filter(pl.col(field).is_in(lst)).with_columns(\n        pl.lit(f\"{field}:{check}:{value}\").alias(\"dq_status\")\n    )\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.satisfies","title":"<code>satisfies(df, rule)</code>","text":"<p>Evaluates a given rule against a Polars DataFrame and returns rows that do not satisfy the rule.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to be evaluated.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule to be applied. The rule should include          the following keys:          - 'field': The column name in the DataFrame to be checked.          - 'check': The type of check or condition to be applied.          - 'value': The value or expression to validate against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A DataFrame containing rows that do not satisfy the rule, with an additional           column <code>dq_status</code> indicating the rule that was violated in the format           \"field:check:value\".</p> Example <p>rule = {\"field\": \"age\", \"check\": \"&gt;\", \"value\": \"18\"} result = satisfies(df, rule)</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def satisfies(df: pl.DataFrame, rule: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Evaluates a given rule against a Polars DataFrame and returns rows that do not satisfy the rule.\n\n    Args:\n        df (pl.DataFrame): The input Polars DataFrame to be evaluated.\n        rule (dict): A dictionary containing the rule to be applied. The rule should include\n                     the following keys:\n                     - 'field': The column name in the DataFrame to be checked.\n                     - 'check': The type of check or condition to be applied.\n                     - 'value': The value or expression to validate against.\n\n    Returns:\n        pl.DataFrame: A DataFrame containing rows that do not satisfy the rule, with an additional\n                      column `dq_status` indicating the rule that was violated in the format\n                      \"field:check:value\".\n\n    Example:\n        rule = {\"field\": \"age\", \"check\": \"&gt;\", \"value\": \"18\"}\n        result = satisfies(df, rule)\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    ctx = pl.SQLContext(sumeh=df)\n    viol = ctx.execute(\n        f\"\"\"\n        SELECT *\n        FROM sumeh\n        WHERE NOT ({value})\n        \"\"\",\n        eager=True,\n    )\n    return viol.with_columns(pl.lit(f\"{field}:{check}:{value}\").alias(\"dq_status\"))\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.summarize","title":"<code>summarize(qc_df, rules, total_rows)</code>","text":"<p>Summarizes quality check results by processing a DataFrame containing  data quality statuses and comparing them against defined rules.</p> <p>Parameters:</p> Name Type Description Default <code>qc_df</code> <code>DataFrame</code> <p>A Polars DataFrame containing a column <code>dq_status</code>  with semicolon-separated strings representing data quality statuses  in the format \"column:rule:value\".</p> required <code>rules</code> <code>list[dict]</code> <p>A list of dictionaries where each dictionary defines  a rule with keys such as \"column\", \"rule\", \"value\", and \"pass_threshold\".</p> required <code>total_rows</code> <code>int</code> <p>The total number of rows in the original dataset, used  to calculate the pass rate.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A summarized DataFrame containing the following columns: - id: A unique identifier for each rule. - timestamp: The timestamp when the summary was generated. - check: A label indicating the type of check (e.g., \"Quality Check\"). - level: The severity level of the check (e.g., \"WARNING\"). - column: The column name associated with the rule. - rule: The rule being evaluated. - value: The specific value associated with the rule. - rows: The total number of rows in the dataset. - violations: The number of rows that violated the rule. - pass_rate: The proportion of rows that passed the rule. - pass_threshold: The threshold for passing the rule. - status: The status of the rule evaluation (\"PASS\" or \"FAIL\").</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def summarize(qc_df: pl.DataFrame, rules: list[dict], total_rows: int) -&gt; pl.DataFrame:\n    \"\"\"\n    Summarizes quality check results by processing a DataFrame containing \n    data quality statuses and comparing them against defined rules.\n\n    Args:\n        qc_df (pl.DataFrame): A Polars DataFrame containing a column `dq_status` \n            with semicolon-separated strings representing data quality statuses \n            in the format \"column:rule:value\".\n        rules (list[dict]): A list of dictionaries where each dictionary defines \n            a rule with keys such as \"column\", \"rule\", \"value\", and \"pass_threshold\".\n        total_rows (int): The total number of rows in the original dataset, used \n            to calculate the pass rate.\n\n    Returns:\n        pl.DataFrame: A summarized DataFrame containing the following columns:\n            - id: A unique identifier for each rule.\n            - timestamp: The timestamp when the summary was generated.\n            - check: A label indicating the type of check (e.g., \"Quality Check\").\n            - level: The severity level of the check (e.g., \"WARNING\").\n            - column: The column name associated with the rule.\n            - rule: The rule being evaluated.\n            - value: The specific value associated with the rule.\n            - rows: The total number of rows in the dataset.\n            - violations: The number of rows that violated the rule.\n            - pass_rate: The proportion of rows that passed the rule.\n            - pass_threshold: The threshold for passing the rule.\n            - status: The status of the rule evaluation (\"PASS\" or \"FAIL\").\n    \"\"\"\n    exploded = (\n        qc_df.select(\n            pl.col(\"dq_status\").str.split(\";\").list.explode().alias(\"dq_status\")\n        )\n        .filter(pl.col(\"dq_status\") != \"\")\n        .with_columns(\n            [\n                pl.col(\"dq_status\").str.split(\":\").list.get(0).alias(\"column\"),\n                pl.col(\"dq_status\").str.split(\":\").list.get(1).alias(\"rule\"),\n                pl.col(\"dq_status\").str.split(\":\").list.get(2).alias(\"value\"),\n            ]\n        )\n    ).drop(\"dq_status\")\n    viol_count = exploded.group_by([\"column\", \"rule\", \"value\"]).agg(\n        pl.len().alias(\"violations\")\n    )\n\n    rules_df = __build_rules_df(rules)\n\n    viol_count2 = viol_count.with_columns(pl.col(\"value\").fill_null(\"\").alias(\"value\"))\n\n    step1 = rules_df.join(\n        viol_count2,\n        on=[\"column\", \"rule\", \"value\"],\n        how=\"left\",\n    )\n\n    step2 = step1.with_columns([pl.col(\"violations\").fill_null(0).alias(\"violations\")])\n\n    step3 = step2.with_columns(\n        [\n            ((pl.lit(total_rows) - pl.col(\"violations\")) / pl.lit(total_rows)).alias(\n                \"pass_rate\"\n            )\n        ]\n    )\n\n    now = datetime.now().replace(second=0, microsecond=0)\n    step4 = step3.with_columns(\n        [\n            pl.lit(total_rows).alias(\"rows\"),\n            pl.when(pl.col(\"pass_rate\") &gt;= pl.col(\"pass_threshold\"))\n            .then(pl.lit(\"PASS\"))\n            .otherwise(pl.lit(\"FAIL\"))\n            .alias(\"status\"),\n            pl.lit(now).alias(\"timestamp\"),\n            pl.lit(\"Quality Check\").alias(\"check\"),\n            pl.lit(\"WARNING\").alias(\"level\"),\n        ]\n    )\n\n    summary = step4.with_columns(pl.arange(1, pl.len() + 1).alias(\"id\")).select(\n        [\n            \"id\",\n            \"timestamp\",\n            \"check\",\n            \"level\",\n            \"column\",\n            \"rule\",\n            \"value\",\n            \"rows\",\n            \"violations\",\n            \"pass_rate\",\n            \"pass_threshold\",\n            \"status\",\n        ]\n    )\n\n    return summary\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.validate","title":"<code>validate(df, rules)</code>","text":"<p>Validates a Polars DataFrame against a set of rules and returns the updated DataFrame  with validation statuses and a DataFrame containing the validation violations.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to validate.</p> required <code>rules</code> <code>list[dict]</code> <p>A list of dictionaries representing validation rules. Each rule  should contain the following keys: - \"check_type\" (str): The type of validation to perform (e.g., \"is_primary_key\",      \"is_composite_key\", \"has_pattern\", etc.). - \"value\" (optional): The value to validate against, depending on the rule type. - \"execute\" (bool, optional): Whether to execute the rule. Defaults to True.</p> required <p>Returns:</p> Type Description <code>Tuple[DataFrame, DataFrame]</code> <p>Tuple[pl.DataFrame, pl.DataFrame]: A tuple containing: - The original DataFrame with an additional \"dq_status\" column indicating the      validation status for each row. - A DataFrame containing rows that violated the validation rules, including      details of the violations.</p> Notes <ul> <li>The function dynamically resolves validation functions based on the \"check_type\"      specified in the rules.</li> <li>If a rule's \"check_type\" is unknown, a warning is issued, and the rule is skipped.</li> <li>The \"__id\" column is temporarily added to the DataFrame for internal processing      and is removed in the final output.</li> </ul> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def validate(df: pl.DataFrame, rules: list[dict]) -&gt; Tuple[pl.DataFrame,pl.DataFrame]:\n    \"\"\"\n    Validates a Polars DataFrame against a set of rules and returns the updated DataFrame \n    with validation statuses and a DataFrame containing the validation violations.\n\n    Args:\n        df (pl.DataFrame): The input Polars DataFrame to validate.\n        rules (list[dict]): A list of dictionaries representing validation rules. Each rule \n            should contain the following keys:\n            - \"check_type\" (str): The type of validation to perform (e.g., \"is_primary_key\", \n                \"is_composite_key\", \"has_pattern\", etc.).\n            - \"value\" (optional): The value to validate against, depending on the rule type.\n            - \"execute\" (bool, optional): Whether to execute the rule. Defaults to True.\n\n    Returns:\n        Tuple[pl.DataFrame, pl.DataFrame]: A tuple containing:\n            - The original DataFrame with an additional \"dq_status\" column indicating the \n                validation status for each row.\n            - A DataFrame containing rows that violated the validation rules, including \n                details of the violations.\n\n    Notes:\n        - The function dynamically resolves validation functions based on the \"check_type\" \n            specified in the rules.\n        - If a rule's \"check_type\" is unknown, a warning is issued, and the rule is skipped.\n        - The \"__id\" column is temporarily added to the DataFrame for internal processing \n            and is removed in the final output.\n    \"\"\"\n    df = df.with_columns(pl.arange(0, pl.len()).alias(\"__id\"))\n    df_with_dq = df.with_columns(pl.lit(\"\").alias(\"dq_status\"))\n    result = df_with_dq.head(0)\n    for rule in rules:\n        if not rule.get(\"execute\", True):\n            continue\n        rule_name = rule[\"check_type\"]\n        if rule_name == \"is_primary_key\":\n            rule_name = \"is_unique\"\n        elif rule_name == \"is_composite_key\":\n            rule_name = \"are_unique\"\n\n        func = globals().get(rule_name)\n        if func is None:\n            warnings.warn(f\"Unknown rule: {rule_name}\")\n            continue\n\n        raw_value = rule.get(\"value\")\n        if rule_name in (\"has_pattern\", \"satisfies\"):\n            value = raw_value\n        else:\n            try:\n                value = (\n                    __convert_value(raw_value)\n                    if isinstance(raw_value, str) and raw_value not in (\"\", \"NULL\")\n                    else raw_value\n                )\n            except ValueError:\n                value = raw_value\n\n        viol = func(df_with_dq, rule)\n        result = pl.concat([result, viol]) if not result.is_empty() else viol\n\n    summary = (\n        result.group_by(\"__id\", maintain_order=True)\n        .agg(\"dq_status\")\n        .with_columns(pl.col(\"dq_status\").list.join(\";\").alias(\"dq_status\"))\n    )\n    out = df.join(summary, on=\"__id\", how=\"left\").drop(\"__id\")\n\n    return out, result\n</code></pre>"},{"location":"api/engine/engine-polars/#sumeh.engine.polars_engine.validate_schema","title":"<code>validate_schema(df, expected)</code>","text":"<p>Validates the schema of a given DataFrame against an expected schema.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <p>The DataFrame whose schema needs to be validated.</p> required <code>expected</code> <p>The expected schema, represented as a list of tuples where each tuple       contains the column name and its data type.</p> required <p>Returns:</p> Type Description <code>Tuple[bool, List[Tuple[str, str]]]</code> <p>Tuple[bool, List[Tuple[str, str]]]: A tuple containing: - A boolean indicating whether the schema matches the expected schema. - A list of tuples representing the errors, where each tuple contains   the column name and a description of the mismatch.</p> Source code in <code>sumeh/engine/polars_engine.py</code> <pre><code>def validate_schema(df, expected) -&gt; Tuple[bool, List[Tuple[str, str]]]:\n    \"\"\"\n    Validates the schema of a given DataFrame against an expected schema.\n\n    Args:\n        df: The DataFrame whose schema needs to be validated.\n        expected: The expected schema, represented as a list of tuples where each tuple\n                  contains the column name and its data type.\n\n    Returns:\n        Tuple[bool, List[Tuple[str, str]]]: A tuple containing:\n            - A boolean indicating whether the schema matches the expected schema.\n            - A list of tuples representing the errors, where each tuple contains\n              the column name and a description of the mismatch.\n    \"\"\"\n    actual = __polars_schema_to_list(df)\n    result, errors = __compare_schemas(actual, expected)\n    return result, errors\n</code></pre>"},{"location":"api/engine/engine-pyspark/","title":"Module <code>sumeh.engine.pyspark_engine</code>","text":"<p>This module provides a set of functions for performing data quality checks on PySpark DataFrames. It includes various validation rules, schema validation, and summarization utilities.</p> <p>Functions:</p> Name Description <code>- is_positive</code> <p>DataFrame, rule: dict) -&gt; DataFrame: Filters rows where the specified field is negative and adds a data quality status column.</p> <code>- is_negative</code> <p>DataFrame, rule: dict) -&gt; DataFrame: Filters rows where the specified field is non-negative and adds a data quality status column.</p> <code>- is_complete</code> <p>DataFrame, rule: dict) -&gt; DataFrame: Filters rows where the specified field is null and adds a data quality status column.</p> <code>- is_unique</code> <p>DataFrame, rule: dict) -&gt; DataFrame: Identifies duplicate rows based on the specified field and adds a data quality status column.</p> <code>- are_complete</code> <p>DataFrame, rule: dict) -&gt; DataFrame: Filters rows where any of the specified fields are null and adds a data quality status column.</p> <code>- are_unique</code> <p>DataFrame, rule: dict) -&gt; DataFrame: Identifies duplicate rows based on a combination of specified fields and adds a data quality status column.</p> <code>- is_greater_than</code> <p>DataFrame, rule: dict) -&gt; DataFrame: Filters rows where the specified field is less than or equal to the given value.</p> <code>- is_greater_or_equal_than</code> <p>DataFrame, rule: dict) -&gt; DataFrame: Filters rows where the specified field is less than the given value.</p> <code>- is_less_than</code> <p>DataFrame, rule: dict) -&gt; DataFrame: Filters rows where the specified field is greater than or equal to the given value.</p> <code>- is_less_or_equal_than</code> <p>DataFrame, rule: dict) -&gt; DataFrame: Filters rows where the specified field is greater than the given value.</p> <code>- is_equal</code> <p>DataFrame, rule: dict) -&gt; DataFrame: Filters rows where the specified field is not equal to the given value.</p> <code>- is_equal_than</code> <p>DataFrame, rule: dict) -&gt; DataFrame: Alias for <code>is_equal</code>.</p> <code>- is_contained_in</code> <p>DataFrame, rule: dict) -&gt; DataFrame: Filters rows where the specified field is not in the given list of values.</p> <code>- not_contained_in</code> <p>DataFrame, rule: dict) -&gt; DataFrame: Filters rows where the specified field is in the given list of values.</p> <code>- is_between</code> <p>DataFrame, rule: dict) -&gt; DataFrame: Filters rows where the specified field is not within the given range.</p> <code>- has_pattern</code> <p>DataFrame, rule: dict) -&gt; DataFrame: Filters rows where the specified field does not match the given regex pattern.</p> <code>- is_legit</code> <p>DataFrame, rule: dict) -&gt; DataFrame: Filters rows where the specified field is null or does not match a non-whitespace pattern.</p> <code>- is_primary_key</code> <p>DataFrame, rule: dict): Checks if the specified field is unique (alias for <code>is_unique</code>).</p> <code>- is_composite_key</code> <p>DataFrame, rule: dict): Checks if the combination of specified fields is unique (alias for <code>are_unique</code>).</p> <code>- has_max</code> <p>DataFrame, rule: dict) -&gt; DataFrame: Filters rows where the specified field exceeds the given maximum value.</p> <code>- has_min</code> <p>DataFrame, rule: dict) -&gt; DataFrame: Filters rows where the specified field is below the given minimum value.</p> <code>- has_std</code> <p>DataFrame, rule: dict) -&gt; DataFrame: Checks if the standard deviation of the specified field exceeds the given value.</p> <code>- has_mean</code> <p>DataFrame, rule: dict) -&gt; DataFrame: Checks if the mean of the specified field exceeds the given value.</p> <code>- has_sum</code> <p>DataFrame, rule: dict) -&gt; DataFrame: Checks if the sum of the specified field exceeds the given value.</p> <code>- has_cardinality</code> <p>DataFrame, rule: dict) -&gt; DataFrame: Checks if the cardinality (distinct count) of the specified field exceeds the given value.</p> <code>- has_infogain</code> <p>DataFrame, rule: dict) -&gt; DataFrame: Checks if the information gain (distinct count) of the specified field exceeds the given value.</p> <code>- has_entropy</code> <p>DataFrame, rule: dict) -&gt; DataFrame: Checks if the entropy (distinct count) of the specified field exceeds the given value.</p> <code>- all_date_checks</code> <p>DataFrame, rule: dict) -&gt; DataFrame: Filters rows where the specified date field is earlier than the current date.</p> <code>- satisfies</code> <p>DataFrame, rule: dict) -&gt; DataFrame: Filters rows where the specified field matches the given regex pattern.</p> <code>- validate</code> <p>DataFrame, rules: list[dict]) -&gt; Tuple[DataFrame, DataFrame]: Applies a list of validation rules to the DataFrame and returns the results.</p> <code>- summarize</code> <p>DataFrame, rules: List[Dict], total_rows) -&gt; DataFrame: Summarizes the results of data quality checks, including pass rates and violations.</p> <code>- validate_schema</code> <p>DataFrame, expected) -&gt; Tuple[bool, List[Tuple[str, str]]]: Validates the schema of the DataFrame against the expected schema.</p> <code>- __rules_to_df</code> <p>List[Dict]) -&gt; DataFrame: Converts a list of rules into a DataFrame for further processing.</p> <code>- __pyspark_schema_to_list</code> <p>DataFrame) -&gt; List[Dict[str, Any]]: Converts the schema of a DataFrame into a list of dictionaries for comparison.</p>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.__compare_schemas","title":"<code>__compare_schemas(actual, expected)</code>","text":"<p>Compare two lists of schema definitions and identify discrepancies.</p> <p>Parameters:</p> Name Type Description Default <code>actual</code> <code>List[SchemaDef]</code> <p>The list of actual schema definitions.</p> required <code>expected</code> <code>List[SchemaDef]</code> <p>The list of expected schema definitions.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>Tuple[bool, List[Tuple[str, str]]]: A tuple where the first element is a boolean indicating </p> <code>List[Tuple[str, str]]</code> <p>whether the schemas match (True if they match, False otherwise), and the second element </p> <code>Tuple[bool, List[Tuple[str, str]]]</code> <p>is a list of tuples describing the discrepancies. Each tuple contains: - The field name (str). - A description of the discrepancy (str), such as \"missing\", \"type mismatch\",    \"nullable but expected non-nullable\", or \"extra column\".</p> Notes <ul> <li>A field is considered \"missing\" if it exists in the expected schema but not in the actual schema.</li> <li>A \"type mismatch\" occurs if the data type of a field in the actual schema does not match    the expected data type.</li> <li>A field is considered \"nullable but expected non-nullable\" if it is nullable in the actual    schema but not nullable in the expected schema.</li> <li>An \"extra column\" is a field that exists in the actual schema but not in the expected schema.</li> </ul> Source code in <code>sumeh/services/utils.py</code> <pre><code>def __compare_schemas(actual: List[SchemaDef], expected: List[SchemaDef],) -&gt; Tuple[bool, List[Tuple[str, str]]]:\n    \"\"\"\n    Compare two lists of schema definitions and identify discrepancies.\n\n    Args:\n        actual (List[SchemaDef]): The list of actual schema definitions.\n        expected (List[SchemaDef]): The list of expected schema definitions.\n\n    Returns:\n        Tuple[bool, List[Tuple[str, str]]]: A tuple where the first element is a boolean indicating \n        whether the schemas match (True if they match, False otherwise), and the second element \n        is a list of tuples describing the discrepancies. Each tuple contains:\n            - The field name (str).\n            - A description of the discrepancy (str), such as \"missing\", \"type mismatch\", \n              \"nullable but expected non-nullable\", or \"extra column\".\n\n    Notes:\n        - A field is considered \"missing\" if it exists in the expected schema but not in the actual schema.\n        - A \"type mismatch\" occurs if the data type of a field in the actual schema does not match \n          the expected data type.\n        - A field is considered \"nullable but expected non-nullable\" if it is nullable in the actual \n          schema but not nullable in the expected schema.\n        - An \"extra column\" is a field that exists in the actual schema but not in the expected schema.\n    \"\"\"\n\n    exp_map = {c[\"field\"]: c for c in expected}\n    act_map = {c[\"field\"]: c for c in actual}\n\n    erros: List[Tuple[str, str]] = []\n\n    for fld, exp in exp_map.items():\n        if fld not in act_map:\n            erros.append((fld, \"missing\"))\n            continue\n        act = act_map[fld]\n        if act[\"data_type\"] != exp[\"data_type\"]:\n            erros.append(\n                (\n                    fld,\n                    f\"type mismatch (got {act['data_type']}, expected {exp['data_type']})\",\n                )\n            )\n\n        if act[\"nullable\"] and not exp[\"nullable\"]:\n            erros.append((fld, \"nullable but expected non-nullable\"))\n\n        if exp.get(\"max_length\") is not None:\n            pass\n\n    # 2. campos extras (se quiser)\n    extras = set(act_map) - set(exp_map)\n    for fld in extras:\n        erros.append((fld, \"extra column\"))\n\n    return len(erros) == 0, erros\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.__convert_value","title":"<code>__convert_value(value)</code>","text":"<p>Converts the provided value to the appropriate type (date, float, or int).</p> <p>Depending on the format of the input value, it will be converted to a datetime object, a floating-point number (float), or an integer (int).</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>The value to be converted, represented as a string.</p> required <p>Returns:</p> Type Description <p>Union[datetime, float, int]: The converted value, which can be a datetime object, float, or int.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the value does not match an expected format.</p> Source code in <code>sumeh/services/utils.py</code> <pre><code>def __convert_value(value):\n    \"\"\"\n    Converts the provided value to the appropriate type (date, float, or int).\n\n    Depending on the format of the input value, it will be converted to a datetime object,\n    a floating-point number (float), or an integer (int).\n\n    Args:\n        value (str): The value to be converted, represented as a string.\n\n    Returns:\n        Union[datetime, float, int]: The converted value, which can be a datetime object, float, or int.\n\n    Raises:\n        ValueError: If the value does not match an expected format.\n    \"\"\"\n    from datetime import datetime\n\n    value = value.strip()\n    try:\n        if \"-\" in value:\n            return datetime.strptime(value, \"%Y-%m-%d\")\n        else:\n            return datetime.strptime(value, \"%d/%m/%Y\")\n    except ValueError:\n        if \".\" in value:\n            return float(value)\n        return int(value)\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.__extract_params","title":"<code>__extract_params(rule)</code>","text":"Source code in <code>sumeh/services/utils.py</code> <pre><code>def __extract_params(rule: dict) -&gt; tuple:\n    rule_name = rule[\"check_type\"]\n    field = rule[\"field\"]\n    raw_value = rule.get(\"value\")\n    if isinstance(raw_value, str) and raw_value not in (None, \"\", \"NULL\"):\n        try:\n            value = __convert_value(raw_value)\n        except ValueError:\n            value = raw_value\n    else:\n        value = raw_value\n    value = value if value not in (None, \"\", \"NULL\") else \"\"\n    return field, rule_name, value\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.__pyspark_schema_to_list","title":"<code>__pyspark_schema_to_list(df)</code>","text":"<p>Convert the schema of a PySpark DataFrame into a list of dictionaries.</p> <p>Each dictionary in the output list represents a field in the DataFrame schema and contains the following keys:     - \"field\": The name of the field.     - \"data_type\": The data type of the field as a lowercase string.     - \"nullable\": A boolean indicating whether the field allows null values.     - \"max_length\": Always set to None (reserved for future use).</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The PySpark DataFrame whose schema is to be converted.</p> required <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List[Dict[str, Any]]: A list of dictionaries representing the schema of the DataFrame.</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def __pyspark_schema_to_list(df: DataFrame) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Convert the schema of a PySpark DataFrame into a list of dictionaries.\n\n    Each dictionary in the output list represents a field in the DataFrame schema\n    and contains the following keys:\n        - \"field\": The name of the field.\n        - \"data_type\": The data type of the field as a lowercase string.\n        - \"nullable\": A boolean indicating whether the field allows null values.\n        - \"max_length\": Always set to None (reserved for future use).\n\n    Args:\n        df (DataFrame): The PySpark DataFrame whose schema is to be converted.\n\n    Returns:\n        List[Dict[str, Any]]: A list of dictionaries representing the schema of the DataFrame.\n    \"\"\"\n    out: List[Dict[str, Any]] = []\n    for f in df.schema.fields:\n        out.append(\n            {\n                \"field\": f.name,\n                \"data_type\": f.dataType.simpleString().lower(),\n                \"nullable\": f.nullable,\n                \"max_length\": None,\n            }\n        )\n    return out\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.__rules_to_df","title":"<code>__rules_to_df(rules)</code>","text":"<p>Converts a list of rule dictionaries into a PySpark DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>rules</code> <code>List[Dict]</code> <p>A list of dictionaries where each dictionary represents a rule. Each rule dictionary should contain the following keys:     - \"field\" (str or list): The name of the field or a list of field names.     - \"check_type\" (str): The type of rule or check to be applied.     - \"threshold\" (float, optional): The threshold value for the rule. Defaults to 1.0 if not provided.     - \"value\" (str, optional): The value associated with the rule. Defaults to \"N/A\" if not provided.     - \"execute\" (bool, optional): A flag indicating whether the rule should be executed. Defaults to True.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A PySpark DataFrame containing the following columns: - \"column\" (str): The name of the field. - \"rule\" (str): The type of rule or check. - \"pass_threshold\" (float): The threshold value for the rule. - \"value\" (str): The value associated with the rule.</p> Notes <ul> <li>Rows with \"execute\" set to False are skipped.</li> <li>Duplicate rows based on the \"column\" and \"rule\" columns are removed.</li> </ul> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def __rules_to_df(rules: List[Dict]) -&gt; DataFrame:\n    \"\"\"\n    Converts a list of rule dictionaries into a PySpark DataFrame.\n\n    Args:\n        rules (List[Dict]): A list of dictionaries where each dictionary represents a rule.\n            Each rule dictionary should contain the following keys:\n                - \"field\" (str or list): The name of the field or a list of field names.\n                - \"check_type\" (str): The type of rule or check to be applied.\n                - \"threshold\" (float, optional): The threshold value for the rule. Defaults to 1.0 if not provided.\n                - \"value\" (str, optional): The value associated with the rule. Defaults to \"N/A\" if not provided.\n                - \"execute\" (bool, optional): A flag indicating whether the rule should be executed. Defaults to True.\n\n    Returns:\n        DataFrame: A PySpark DataFrame containing the following columns:\n            - \"column\" (str): The name of the field.\n            - \"rule\" (str): The type of rule or check.\n            - \"pass_threshold\" (float): The threshold value for the rule.\n            - \"value\" (str): The value associated with the rule.\n\n    Notes:\n        - Rows with \"execute\" set to False are skipped.\n        - Duplicate rows based on the \"column\" and \"rule\" columns are removed.\n    \"\"\"\n    from pyspark.sql import SparkSession\n\n    spark = SparkSession.builder.getOrCreate()\n    rows = []\n    for r in rules:\n        if not r.get(\"execute\", True):\n            continue\n        col_name = str(r[\"field\"]) if isinstance(r[\"field\"], list) else r[\"field\"]\n        rows.append(\n            Row(\n                column=col_name.strip(),\n                rule=r[\"check_type\"],\n                pass_threshold=float(r.get(\"threshold\") or 1.0),\n                value=r.get(\"value\", \"N/A\") or \"N/A\",\n            )\n        )\n    return spark.createDataFrame(rows).dropDuplicates([\"column\", \"rule\"])\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.__transform_date_format_in_pattern","title":"<code>__transform_date_format_in_pattern(date_format)</code>","text":"Source code in <code>sumeh/services/utils.py</code> <pre><code>def __transform_date_format_in_pattern(date_format):\n    date_patterns = {\n        'DD': '(0[1-9]|[12][0-9]|3[01])',\n        'MM': '(0[1-9]|1[012])',\n        'YYYY': '(19|20)\\d\\d',\n        'YY': '\\d\\d',\n        ' ': '\\s',\n        '.': '\\.'\n    }\n\n    date_pattern = date_format\n    for single_format, pattern in date_patterns.items():\n        date_pattern = date_pattern.replace(single_format, pattern)\n\n    return date_pattern\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.all_date_checks","title":"<code>all_date_checks(df, rule)</code>","text":"<p>Filters the input DataFrame based on a date-related rule and adds a data quality status column.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The name of the column to apply the rule on. - 'check': The type of check to perform (e.g., comparison operator). - 'value': The value to be used in the check.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame filtered based on the rule, with an additional column </p> <code>DataFrame</code> <p>\"dq_status\" indicating the data quality status in the format \"field:check:value\".</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def all_date_checks(df: DataFrame, rule: dict) -&gt; DataFrame:\n    \"\"\"\n    Filters the input DataFrame based on a date-related rule and adds a data quality status column.\n\n    Args:\n        df (DataFrame): The input PySpark DataFrame to be filtered.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field': The name of the column to apply the rule on.\n            - 'check': The type of check to perform (e.g., comparison operator).\n            - 'value': The value to be used in the check.\n\n    Returns:\n        DataFrame: A new DataFrame filtered based on the rule, with an additional column \n        \"dq_status\" indicating the data quality status in the format \"field:check:value\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    return df.filter((col(field) &lt; current_date())).withColumn(\n        \"dq_status\", concat(lit(field), lit(\":\"), lit(check), lit(\":\"), lit(value))\n    )\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.are_complete","title":"<code>are_complete(df, rule)</code>","text":"<p>Filters rows in a DataFrame that do not meet the completeness rule and adds a data quality status column.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - \"fields\" (list): A list of column names to check for completeness (non-null values). - \"check\" (str): A descriptive label for the type of check being performed. - \"value\" (str): A descriptive value associated with the check.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame containing only the rows that fail the completeness check, </p> <code>DataFrame</code> <p>with an additional column \"dq_status\" describing the failed rule.</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def are_complete(df: DataFrame, rule: dict) -&gt; DataFrame:\n    \"\"\"\n    Filters rows in a DataFrame that do not meet the completeness rule and adds a data quality status column.\n\n    Args:\n        df (DataFrame): The input PySpark DataFrame to be checked.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - \"fields\" (list): A list of column names to check for completeness (non-null values).\n            - \"check\" (str): A descriptive label for the type of check being performed.\n            - \"value\" (str): A descriptive value associated with the check.\n\n    Returns:\n        DataFrame: A new DataFrame containing only the rows that fail the completeness check, \n        with an additional column \"dq_status\" describing the failed rule.\n    \"\"\"\n    fields, check, value = __extract_params(rule)\n    predicate = reduce(operator.and_, [col(field).isNotNull() for field in fields])\n    return df.filter(~predicate).withColumn(\n        \"dq_status\",\n        concat(lit(str(fields)), lit(\":\"), lit(check), lit(\":\"), lit(value)),\n    )\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.are_unique","title":"<code>are_unique(df, rule)</code>","text":"<p>Checks for uniqueness of specified fields in a PySpark DataFrame based on the provided rule.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'fields': A list of column names to check for uniqueness. - 'check': A string representing the type of check (e.g., \"unique\"). - 'value': A value associated with the rule for logging or identification.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A DataFrame containing rows that violate the uniqueness rule. </p> <code>DataFrame</code> <p>The resulting DataFrame includes an additional column <code>dq_status</code> that </p> <code>DataFrame</code> <p>describes the rule violation in the format: \"[fields]:[check]:[value]\".</p> Notes <ul> <li>The function concatenates the specified fields into a single column    and checks for duplicate values within that column.</li> <li>Rows that do not meet the uniqueness criteria are returned, while    rows that satisfy the criteria are excluded from the result.</li> </ul> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def are_unique(df: DataFrame, rule: dict) -&gt; DataFrame:\n    \"\"\"\n    Checks for uniqueness of specified fields in a PySpark DataFrame based on the provided rule.\n\n    Args:\n        df (DataFrame): The input PySpark DataFrame to be checked.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'fields': A list of column names to check for uniqueness.\n            - 'check': A string representing the type of check (e.g., \"unique\").\n            - 'value': A value associated with the rule for logging or identification.\n\n    Returns:\n        DataFrame: A DataFrame containing rows that violate the uniqueness rule. \n        The resulting DataFrame includes an additional column `dq_status` that \n        describes the rule violation in the format: \"[fields]:[check]:[value]\".\n\n    Notes:\n        - The function concatenates the specified fields into a single column \n          and checks for duplicate values within that column.\n        - Rows that do not meet the uniqueness criteria are returned, while \n          rows that satisfy the criteria are excluded from the result.\n    \"\"\"\n    fields, check, value = __extract_params(rule)\n    combined_col = concat_ws(\"|\", *[coalesce(col(f), lit(\"\")) for f in fields])\n    window = Window.partitionBy(combined_col)\n    result = (\n        df.withColumn(\"_count\", count(\"*\").over(window))\n        .filter(col(\"_count\") &gt; 1)\n        .drop(\"_count\")\n        .withColumn(\n            \"dq_status\",\n            concat(lit(str(fields)), lit(\":\"), lit(check), lit(\":\"), lit(value)),\n        )\n    )\n    return result\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.has_cardinality","title":"<code>has_cardinality(df, rule)</code>","text":"<p>Checks the cardinality of a specified field in a DataFrame against a given rule.</p> <p>This function evaluates whether the distinct count of values in a specified column (field) of the DataFrame exceeds a given threshold (value) as defined in the rule. If the cardinality exceeds the threshold, a new column <code>dq_status</code> is added to the DataFrame with information about the rule violation. Otherwise, an empty DataFrame is returned.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The name of the column to check. - 'check': The type of check being performed (e.g., \"cardinality\"). - 'value': The threshold value for the cardinality.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A DataFrame with the <code>dq_status</code> column added if the cardinality</p> <code>DataFrame</code> <p>exceeds the threshold, or an empty DataFrame if the condition is not met.</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def has_cardinality(df: DataFrame, rule: dict) -&gt; DataFrame:\n    \"\"\"\n    Checks the cardinality of a specified field in a DataFrame against a given rule.\n\n    This function evaluates whether the distinct count of values in a specified column\n    (field) of the DataFrame exceeds a given threshold (value) as defined in the rule.\n    If the cardinality exceeds the threshold, a new column `dq_status` is added to the\n    DataFrame with information about the rule violation. Otherwise, an empty DataFrame\n    is returned.\n\n    Args:\n        df (DataFrame): The input PySpark DataFrame to be checked.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field': The name of the column to check.\n            - 'check': The type of check being performed (e.g., \"cardinality\").\n            - 'value': The threshold value for the cardinality.\n\n    Returns:\n        DataFrame: A DataFrame with the `dq_status` column added if the cardinality\n        exceeds the threshold, or an empty DataFrame if the condition is not met.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    card_val = df.select(countDistinct(col(field))).first()[0] or 0\n    if card_val &gt; value:\n        return df.withColumn(\n            \"dq_status\", concat(lit(field), lit(\":\"), lit(check), lit(\":\"), lit(value))\n        )\n    return df.limit(0)\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.has_entropy","title":"<code>has_entropy(df, rule)</code>","text":"<p>Evaluates the entropy of a specified field in a DataFrame and applies a rule to determine whether the DataFrame should be processed further or filtered out.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to evaluate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The name of the column to evaluate. - 'check' (str): The type of check being performed (e.g., \"entropy\"). - 'value' (float): The threshold value for the entropy check.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>If the entropy of the specified field exceeds the given value, returns the</p> <code>DataFrame</code> <p>original DataFrame with an additional column \"dq_status\" indicating the rule applied.</p> <code>DataFrame</code> <p>Otherwise, returns an empty DataFrame with the same schema as the input.</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def has_entropy(df: DataFrame, rule: dict) -&gt; DataFrame:\n    \"\"\"\n    Evaluates the entropy of a specified field in a DataFrame and applies a rule to determine\n    whether the DataFrame should be processed further or filtered out.\n\n    Parameters:\n        df (DataFrame): The input PySpark DataFrame to evaluate.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field' (str): The name of the column to evaluate.\n            - 'check' (str): The type of check being performed (e.g., \"entropy\").\n            - 'value' (float): The threshold value for the entropy check.\n\n    Returns:\n        DataFrame: If the entropy of the specified field exceeds the given value, returns the\n        original DataFrame with an additional column \"dq_status\" indicating the rule applied.\n        Otherwise, returns an empty DataFrame with the same schema as the input.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    entropy_val = df.select(countDistinct(col(field))).first()[0] or 0.0\n    if entropy_val &gt; value:\n        return df.withColumn(\n            \"dq_status\", concat(lit(field), lit(\":\"), lit(check), lit(\":\"), lit(value))\n        )\n    return df.limit(0)\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.has_infogain","title":"<code>has_infogain(df, rule)</code>","text":"<p>Evaluates whether a given DataFrame satisfies an information gain condition  based on the provided rule. If the condition is met, it appends a column  indicating the status; otherwise, it returns an empty DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to evaluate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should           include the following keys:          - 'field': The column name to evaluate.          - 'check': The condition type (not used directly in the logic).          - 'value': The threshold value for information gain.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A DataFrame with an additional \"dq_status\" column if the         information gain condition is met, or an empty DataFrame         if the condition is not satisfied.</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def has_infogain(df: DataFrame, rule: dict) -&gt; DataFrame:\n    \"\"\"\n    Evaluates whether a given DataFrame satisfies an information gain condition \n    based on the provided rule. If the condition is met, it appends a column \n    indicating the status; otherwise, it returns an empty DataFrame.\n\n    Args:\n        df (DataFrame): The input PySpark DataFrame to evaluate.\n        rule (dict): A dictionary containing the rule parameters. It should \n                     include the following keys:\n                     - 'field': The column name to evaluate.\n                     - 'check': The condition type (not used directly in the logic).\n                     - 'value': The threshold value for information gain.\n\n    Returns:\n        DataFrame: A DataFrame with an additional \"dq_status\" column if the \n                   information gain condition is met, or an empty DataFrame \n                   if the condition is not satisfied.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    info_gain = df.select(countDistinct(col(field))).first()[0] or 0.0\n    if info_gain &gt; value:\n        return df.withColumn(\n            \"dq_status\", concat(lit(field), lit(\":\"), lit(check), lit(\":\"), lit(value))\n        )\n    return df.limit(0)\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.has_max","title":"<code>has_max(df, rule)</code>","text":"<p>Filters a PySpark DataFrame to include only rows where the value of a specified field is greater than a given threshold. Adds a new column 'dq_status' to indicate the rule applied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The name of the column to apply the rule on. - 'check' (str): The type of check being performed (e.g., 'max'). - 'value' (numeric): The threshold value to compare against.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame filtered based on the rule, with an additional column 'dq_status'</p> <code>DataFrame</code> <p>describing the rule applied in the format \"field:check:value\".</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def has_max(df: DataFrame, rule: dict) -&gt; DataFrame:\n    \"\"\"\n    Filters a PySpark DataFrame to include only rows where the value of a specified field\n    is greater than a given threshold. Adds a new column 'dq_status' to indicate the rule applied.\n\n    Args:\n        df (DataFrame): The input PySpark DataFrame to be filtered.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field' (str): The name of the column to apply the rule on.\n            - 'check' (str): The type of check being performed (e.g., 'max').\n            - 'value' (numeric): The threshold value to compare against.\n\n    Returns:\n        DataFrame: A new DataFrame filtered based on the rule, with an additional column 'dq_status'\n        describing the rule applied in the format \"field:check:value\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    return df.filter(col(field) &gt; value).withColumn(\n        \"dq_status\", concat(lit(field), lit(\":\"), lit(check), lit(\":\"), lit(value))\n    )\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.has_mean","title":"<code>has_mean(df, rule)</code>","text":"<p>Evaluates whether the mean value of a specified column in a DataFrame satisfies a given rule.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to evaluate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The name of the column to calculate the mean for. - 'check' (str): The type of check being performed (e.g., 'greater_than'). - 'value' (float): The threshold value to compare the mean against.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>If the mean value of the specified column exceeds the threshold, </p> <code>DataFrame</code> <p>returns the original DataFrame with an additional column <code>dq_status</code> indicating </p> <code>DataFrame</code> <p>the rule violation. If the mean value satisfies the rule, returns an empty DataFrame.</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def has_mean(df: DataFrame, rule: dict) -&gt; DataFrame:\n    \"\"\"\n    Evaluates whether the mean value of a specified column in a DataFrame satisfies a given rule.\n\n    Args:\n        df (DataFrame): The input PySpark DataFrame to evaluate.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field' (str): The name of the column to calculate the mean for.\n            - 'check' (str): The type of check being performed (e.g., 'greater_than').\n            - 'value' (float): The threshold value to compare the mean against.\n\n    Returns:\n        DataFrame: If the mean value of the specified column exceeds the threshold, \n        returns the original DataFrame with an additional column `dq_status` indicating \n        the rule violation. If the mean value satisfies the rule, returns an empty DataFrame.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    mean_val = (df.select(avg(col(field))).first()[0]) or 0.0\n    if mean_val &gt; value:  # regra falhou\n        return df.withColumn(\n            \"dq_status\", concat(lit(field), lit(\":\"), lit(check), lit(\":\"), lit(value))\n        )\n    else:  # passou\n        return df.limit(0)\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.has_min","title":"<code>has_min(df, rule)</code>","text":"<p>Filters rows in a DataFrame where the value of a specified field is less than a given threshold  and adds a new column indicating the data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The name of the column to check. - 'check' (str): The type of check being performed (e.g., \"min\"). - 'value' (numeric): The threshold value for the check.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame with rows filtered based on the rule and an additional </p> <code>DataFrame</code> <p>\"dq_status\" column containing a string representation of the rule applied.</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def has_min(df: DataFrame, rule: dict) -&gt; DataFrame:\n    \"\"\"\n    Filters rows in a DataFrame where the value of a specified field is less than a given threshold \n    and adds a new column indicating the data quality status.\n\n    Args:\n        df (DataFrame): The input PySpark DataFrame to be filtered.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field' (str): The name of the column to check.\n            - 'check' (str): The type of check being performed (e.g., \"min\").\n            - 'value' (numeric): The threshold value for the check.\n\n    Returns:\n        DataFrame: A new DataFrame with rows filtered based on the rule and an additional \n        \"dq_status\" column containing a string representation of the rule applied.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    return df.filter(col(field) &lt; value).withColumn(\n        \"dq_status\", concat(lit(field), lit(\":\"), lit(check), lit(\":\"), lit(value))\n    )\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.has_pattern","title":"<code>has_pattern(df, rule)</code>","text":"<p>Filters a PySpark DataFrame based on a pattern match and adds a data quality status column.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The name of the column to apply the pattern check. - 'check': A descriptive label for the type of check being performed. - 'value': The regex pattern to match against the column values.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame with rows that do not match the pattern filtered out.        Additionally, a \"dq_status\" column is added, containing a string        representation of the rule applied in the format \"field:check:value\".</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def has_pattern(df: DataFrame, rule: dict) -&gt; DataFrame:\n    \"\"\"\n    Filters a PySpark DataFrame based on a pattern match and adds a data quality status column.\n\n    Args:\n        df (DataFrame): The input PySpark DataFrame to be filtered.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field': The name of the column to apply the pattern check.\n            - 'check': A descriptive label for the type of check being performed.\n            - 'value': The regex pattern to match against the column values.\n\n    Returns:\n        DataFrame: A new DataFrame with rows that do not match the pattern filtered out.\n                   Additionally, a \"dq_status\" column is added, containing a string\n                   representation of the rule applied in the format \"field:check:value\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    return df.filter(~col(field).rlike(value)).withColumn(\n        \"dq_status\", concat(lit(field), lit(\":\"), lit(check), lit(\":\"), lit(value))\n    )\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.has_std","title":"<code>has_std(df, rule)</code>","text":"<p>Checks if the standard deviation of a specified field in a DataFrame exceeds a given value.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The name of the column to calculate the standard deviation for. - 'check' (str): A descriptive label for the check being performed. - 'value' (float): The threshold value for the standard deviation.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>If the standard deviation of the specified field exceeds the given value,</p> <code>DataFrame</code> <p>returns the original DataFrame with an additional column \"dq_status\" indicating the</p> <code>DataFrame</code> <p>field, check, and value. Otherwise, returns an empty DataFrame.</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def has_std(df: DataFrame, rule: dict) -&gt; DataFrame:\n    \"\"\"\n    Checks if the standard deviation of a specified field in a DataFrame exceeds a given value.\n\n    Args:\n        df (DataFrame): The input PySpark DataFrame.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field' (str): The name of the column to calculate the standard deviation for.\n            - 'check' (str): A descriptive label for the check being performed.\n            - 'value' (float): The threshold value for the standard deviation.\n\n    Returns:\n        DataFrame: If the standard deviation of the specified field exceeds the given value,\n        returns the original DataFrame with an additional column \"dq_status\" indicating the\n        field, check, and value. Otherwise, returns an empty DataFrame.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    std_val = df.select(stddev(col(field))).first()[0]\n    std_val = std_val or 0.0\n    if std_val &gt; value:\n        return df.withColumn(\n            \"dq_status\", concat(lit(field), lit(\":\"), lit(check), lit(\":\"), lit(value))\n        )\n    else:\n        return df.limit(0)\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.has_sum","title":"<code>has_sum(df, rule)</code>","text":"<p>Checks if the sum of values in a specified column of a DataFrame exceeds a given threshold.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to evaluate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The name of the column to sum. - 'check' (str): A descriptive label for the check being performed. - 'value' (float): The threshold value to compare the sum against.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>If the sum of the specified column exceeds the threshold, returns the original</p> <code>DataFrame</code> <p>DataFrame with an additional column <code>dq_status</code> indicating the rule details. If the sum</p> <code>DataFrame</code> <p>does not exceed the threshold, returns an empty DataFrame.</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def has_sum(df: DataFrame, rule: dict) -&gt; DataFrame:\n    \"\"\"\n    Checks if the sum of values in a specified column of a DataFrame exceeds a given threshold.\n\n    Args:\n        df (DataFrame): The input PySpark DataFrame to evaluate.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field' (str): The name of the column to sum.\n            - 'check' (str): A descriptive label for the check being performed.\n            - 'value' (float): The threshold value to compare the sum against.\n\n    Returns:\n        DataFrame: If the sum of the specified column exceeds the threshold, returns the original\n        DataFrame with an additional column `dq_status` indicating the rule details. If the sum\n        does not exceed the threshold, returns an empty DataFrame.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    sum_val = (df.select(sum(col(field))).first()[0]) or 0.0\n    if sum_val &gt; value:\n        return df.withColumn(\n            \"dq_status\", concat(lit(field), lit(\":\"), lit(check), lit(\":\"), lit(value))\n        )\n    return df.limit(0)\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.is_between","title":"<code>is_between(df, rule)</code>","text":"<p>Filters rows in a PySpark DataFrame where the value of a specified field is not within a given range. Adds a new column 'dq_status' to indicate the rule that was applied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The name of the column to check. - 'check': A string representing the type of check (e.g., \"between\"). - 'value': A string representing the range in the format \"[min_value,max_value]\".</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame with rows filtered based on the rule and an additional</p> <code>DataFrame</code> <p>'dq_status' column indicating the applied rule.</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def is_between(df: DataFrame, rule: dict) -&gt; DataFrame:\n    \"\"\"\n    Filters rows in a PySpark DataFrame where the value of a specified field is not within a given range.\n    Adds a new column 'dq_status' to indicate the rule that was applied.\n\n    Args:\n        df (DataFrame): The input PySpark DataFrame.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field': The name of the column to check.\n            - 'check': A string representing the type of check (e.g., \"between\").\n            - 'value': A string representing the range in the format \"[min_value,max_value]\".\n\n    Returns:\n        DataFrame: A new DataFrame with rows filtered based on the rule and an additional\n        'dq_status' column indicating the applied rule.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    min_value, max_value = value.strip(\"[]\").split(\",\")\n    return df.filter(~col(field).between(min_value, max_value)).withColumn(\n        \"dq_status\", concat(lit(field), lit(\":\"), lit(check), lit(\":\"), lit(value))\n    )\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.is_complete","title":"<code>is_complete(df, rule)</code>","text":"<p>Filters a DataFrame to identify rows where a specified field is null and adds a  \"dq_status\" column indicating the data quality rule applied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the data quality rule. It should include: - \"field\" (str): The name of the field to check for null values. - \"check\" (str): A description of the check being performed. - \"value\" (str): Additional information about the rule.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame filtered to include only rows where the specified </p> <code>DataFrame</code> <p>field is null, with an additional \"dq_status\" column describing the rule.</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def is_complete(df: DataFrame, rule: dict) -&gt; DataFrame:\n    \"\"\"\n    Filters a DataFrame to identify rows where a specified field is null and adds a \n    \"dq_status\" column indicating the data quality rule applied.\n\n    Args:\n        df (DataFrame): The input PySpark DataFrame to be checked.\n        rule (dict): A dictionary containing the data quality rule. It should include:\n            - \"field\" (str): The name of the field to check for null values.\n            - \"check\" (str): A description of the check being performed.\n            - \"value\" (str): Additional information about the rule.\n\n    Returns:\n        DataFrame: A new DataFrame filtered to include only rows where the specified \n        field is null, with an additional \"dq_status\" column describing the rule.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    return df.filter(col(field).isNull()).withColumn(\n        \"dq_status\", concat(lit(field), lit(\":\"), lit(check), lit(\":\"), lit(value))\n    )\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.is_composite_key","title":"<code>is_composite_key(df, rule)</code>","text":"<p>Determines if the given DataFrame satisfies the composite key condition based on the provided rule.</p> <p>A composite key is a combination of two or more columns in a DataFrame that uniquely identify a row.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The PySpark DataFrame to be evaluated.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rules or criteria to determine the composite key.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <p>True if the DataFrame satisfies the composite key condition, False otherwise.</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def is_composite_key(df: DataFrame, rule: dict):\n    \"\"\"\n    Determines if the given DataFrame satisfies the composite key condition based on the provided rule.\n\n    A composite key is a combination of two or more columns in a DataFrame that uniquely identify a row.\n\n    Args:\n        df (DataFrame): The PySpark DataFrame to be evaluated.\n        rule (dict): A dictionary containing the rules or criteria to determine the composite key.\n\n    Returns:\n        bool: True if the DataFrame satisfies the composite key condition, False otherwise.\n    \"\"\"\n    return are_unique(df, rule)\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.is_contained_in","title":"<code>is_contained_in(df, rule)</code>","text":"<p>Filters rows in a PySpark DataFrame based on whether a specified column's value  is not contained in a given list of values. Adds a new column 'dq_status' to  indicate the rule applied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The name of the column to check. - 'check': The type of check being performed (e.g., \"is_contained_in\"). - 'value': A string representation of a list of values (e.g., \"[value1,value2]\").</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new PySpark DataFrame with rows filtered based on the rule </p> <code>DataFrame</code> <p>and an additional column 'dq_status' describing the rule applied.</p> Example <p>rule = {\"field\": \"column_name\", \"check\": \"is_contained_in\", \"value\": \"[value1,value2]\"} result_df = is_contained_in(input_df, rule)</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def is_contained_in(df: DataFrame, rule: dict) -&gt; DataFrame:\n    \"\"\"\n    Filters rows in a PySpark DataFrame based on whether a specified column's value \n    is not contained in a given list of values. Adds a new column 'dq_status' to \n    indicate the rule applied.\n\n    Args:\n        df (DataFrame): The input PySpark DataFrame to filter.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field': The name of the column to check.\n            - 'check': The type of check being performed (e.g., \"is_contained_in\").\n            - 'value': A string representation of a list of values (e.g., \"[value1,value2]\").\n\n    Returns:\n        DataFrame: A new PySpark DataFrame with rows filtered based on the rule \n        and an additional column 'dq_status' describing the rule applied.\n\n    Example:\n        rule = {\"field\": \"column_name\", \"check\": \"is_contained_in\", \"value\": \"[value1,value2]\"}\n        result_df = is_contained_in(input_df, rule)\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    positive_list = value.strip(\"[]\").split(\",\")\n    return df.filter(~col(field).isin(positive_list)).withColumn(\n        \"dq_status\", concat(lit(field), lit(\":\"), lit(check), lit(\":\"), lit(value))\n    )\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.is_date_after","title":"<code>is_date_after(df, rule)</code>","text":"<p>Filters a DataFrame to identify rows where a specified field has a date lower than the date informed in the rule and adds a \"dq_status\" column indicating the data quality rule applied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the data quality rule. It should include: - \"field\" (str): The name of the field to check for null values. - \"check\" (str): A description of the check being performed. - \"value\" (str): Additional information about the rule.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame filtered to include only rows where the specified</p> <code>DataFrame</code> <p>field is null, with an additional \"dq_status\" column describing the rule.</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def is_date_after(df: DataFrame, rule: dict) -&gt; DataFrame:\n    \"\"\"\n    Filters a DataFrame to identify rows where a specified field has a date lower than the date informed in the rule\n    and adds a \"dq_status\" column indicating the data quality rule applied.\n\n    Args:\n        df (DataFrame): The input PySpark DataFrame to be checked.\n        rule (dict): A dictionary containing the data quality rule. It should include:\n            - \"field\" (str): The name of the field to check for null values.\n            - \"check\" (str): A description of the check being performed.\n            - \"value\" (str): Additional information about the rule.\n\n    Returns:\n        DataFrame: A new DataFrame filtered to include only rows where the specified\n        field is null, with an additional \"dq_status\" column describing the rule.\n    \"\"\"\n\n    field, check, value = __extract_params(rule)\n    return df.filter(col(field) &lt; value).withColumn(\n        \"dq_status\", concat(lit(field), lit(\":\"), lit(check), lit(\":\"), lit(value))\n    )\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.is_date_before","title":"<code>is_date_before(df, rule)</code>","text":"<p>Filters a DataFrame to identify rows where a specified field has a date greater than the date informed in the rule and adds a \"dq_status\" column indicating the data quality rule applied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the data quality rule. It should include: - \"field\" (str): The name of the field to check for null values. - \"check\" (str): A description of the check being performed. - \"value\" (str): Additional information about the rule.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame filtered to include only rows where the specified</p> <code>DataFrame</code> <p>field is null, with an additional \"dq_status\" column describing the rule.</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def is_date_before(df: DataFrame, rule: dict) -&gt; DataFrame:\n    \"\"\"\n    Filters a DataFrame to identify rows where a specified field has a date greater than the date informed in the rule\n    and adds a \"dq_status\" column indicating the data quality rule applied.\n\n    Args:\n        df (DataFrame): The input PySpark DataFrame to be checked.\n        rule (dict): A dictionary containing the data quality rule. It should include:\n            - \"field\" (str): The name of the field to check for null values.\n            - \"check\" (str): A description of the check being performed.\n            - \"value\" (str): Additional information about the rule.\n\n    Returns:\n        DataFrame: A new DataFrame filtered to include only rows where the specified\n        field is null, with an additional \"dq_status\" column describing the rule.\n    \"\"\"\n\n    field, check, value = __extract_params(rule)\n    return df.filter(col(field) &gt; value).withColumn(\n        \"dq_status\", concat(lit(field), lit(\":\"), lit(check), lit(\":\"), lit(value))\n    )\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.is_date_between","title":"<code>is_date_between(df, rule)</code>","text":"<p>Filters a DataFrame to identify rows where a specified field has a date between two dates passed in the rule using the format: \"[, ]\" and adds a \"dq_status\" column indicating the data quality rule applied. <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the data quality rule. It should include: - \"field\" (str): The name of the field to check for null values. - \"check\" (str): A description of the check being performed. - \"value\" (str): Additional information about the rule.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame filtered to include only rows where the specified</p> <code>DataFrame</code> <p>field is null, with an additional \"dq_status\" column describing the rule.</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def is_date_between(df: DataFrame, rule: dict) -&gt; DataFrame:\n    \"\"\"\n    Filters a DataFrame to identify rows where a specified field has a date between two dates passed in the rule using\n    the format: \"[&lt;initial_date&gt;, &lt;final_date&gt;]\" and adds a \"dq_status\" column indicating the data quality rule applied.\n\n    Args:\n        df (DataFrame): The input PySpark DataFrame to be checked.\n        rule (dict): A dictionary containing the data quality rule. It should include:\n            - \"field\" (str): The name of the field to check for null values.\n            - \"check\" (str): A description of the check being performed.\n            - \"value\" (str): Additional information about the rule.\n\n    Returns:\n        DataFrame: A new DataFrame filtered to include only rows where the specified\n        field is null, with an additional \"dq_status\" column describing the rule.\n    \"\"\"\n\n    field, check, value = __extract_params(rule)\n    start_date, end_date = value.strip(\"[]\").split(\",\")\n    return df.filter(~col(field).between(start_date, end_date)).withColumn(\n        \"dq_status\", concat(lit(field), lit(\":\"), lit(check), lit(\":\"), lit(value))\n    )\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.is_equal","title":"<code>is_equal(df, rule)</code>","text":"<p>Filters a PySpark DataFrame based on a rule that checks for equality between a specified field and a given value. Rows that do not satisfy the equality condition are retained, and a new  column \"dq_status\" is added to indicate the rule applied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - \"field\" (str): The name of the column to check. - \"check\" (str): The type of check (e.g., \"equal\"). This is used for logging purposes. - \"value\" (Any): The value to compare against.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame with rows that do not satisfy the equality condition and an </p> <code>DataFrame</code> <p>additional \"dq_status\" column describing the rule applied.</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def is_equal(df: DataFrame, rule: dict) -&gt; DataFrame:\n    \"\"\"\n    Filters a PySpark DataFrame based on a rule that checks for equality between a specified field\n    and a given value. Rows that do not satisfy the equality condition are retained, and a new \n    column \"dq_status\" is added to indicate the rule applied.\n\n    Args:\n        df (DataFrame): The input PySpark DataFrame to be filtered.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - \"field\" (str): The name of the column to check.\n            - \"check\" (str): The type of check (e.g., \"equal\"). This is used for logging purposes.\n            - \"value\" (Any): The value to compare against.\n\n    Returns:\n        DataFrame: A new DataFrame with rows that do not satisfy the equality condition and an \n        additional \"dq_status\" column describing the rule applied.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    return df.filter(~col(field).eqNullSafe(value)).withColumn(\n        \"dq_status\", concat(lit(field), lit(\":\"), lit(check), lit(\":\"), lit(value))\n    )\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.is_equal_than","title":"<code>is_equal_than(df, rule)</code>","text":"<p>Filters rows in a PySpark DataFrame that do not satisfy an equality condition  specified in the rule dictionary and adds a \"dq_status\" column with details  about the rule applied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - \"field\" (str): The name of the column to check. - \"check\" (str): The type of check being performed (e.g., \"equal\"). - \"value\" (Any): The value to compare against.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame with rows filtered based on the rule and an </p> <code>DataFrame</code> <p>additional \"dq_status\" column describing the rule applied.</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def is_equal_than(df: DataFrame, rule: dict) -&gt; DataFrame:\n    \"\"\"\n    Filters rows in a PySpark DataFrame that do not satisfy an equality condition \n    specified in the rule dictionary and adds a \"dq_status\" column with details \n    about the rule applied.\n\n    Args:\n        df (DataFrame): The input PySpark DataFrame to be filtered.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - \"field\" (str): The name of the column to check.\n            - \"check\" (str): The type of check being performed (e.g., \"equal\").\n            - \"value\" (Any): The value to compare against.\n\n    Returns:\n        DataFrame: A new DataFrame with rows filtered based on the rule and an \n        additional \"dq_status\" column describing the rule applied.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    return df.filter(~col(field).eqNullSafe(value)).withColumn(\n        \"dq_status\", concat(lit(field), lit(\":\"), lit(check), lit(\":\"), lit(value))\n    )\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.is_future_date","title":"<code>is_future_date(df, rule)</code>","text":"<p>Filters a DataFrame to identify rows where a specified field has a date greater than the current date and adds a \"dq_status\" column indicating the data quality rule applied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the data quality rule. It should include: - \"field\" (str): The name of the field to check for null values. - \"check\" (str): A description of the check being performed. - \"value\" (str): Additional information about the rule.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame filtered to include only rows where the specified</p> <code>DataFrame</code> <p>field is null, with an additional \"dq_status\" column describing the rule.</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def is_future_date(df: DataFrame, rule: dict) -&gt; DataFrame:\n    \"\"\"\n    Filters a DataFrame to identify rows where a specified field has a date greater than the current date and\n    adds a \"dq_status\" column indicating the data quality rule applied.\n\n    Args:\n        df (DataFrame): The input PySpark DataFrame to be checked.\n        rule (dict): A dictionary containing the data quality rule. It should include:\n            - \"field\" (str): The name of the field to check for null values.\n            - \"check\" (str): A description of the check being performed.\n            - \"value\" (str): Additional information about the rule.\n\n    Returns:\n        DataFrame: A new DataFrame filtered to include only rows where the specified\n        field is null, with an additional \"dq_status\" column describing the rule.\n    \"\"\"\n\n    field, check, value = __extract_params(rule)\n    return df.filter(col(field) &gt; current_date()).withColumn(\n        \"dq_status\", concat(lit(field), lit(\":\"), lit(check), lit(\":\"), lit(value))\n    )\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.is_greater_or_equal_than","title":"<code>is_greater_or_equal_than(df, rule)</code>","text":"<p>Filters rows in a DataFrame where the value of a specified field is less than a given value and adds a new column \"dq_status\" with a formatted string indicating the rule applied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - \"field\" (str): The name of the column to check. - \"check\" (str): A descriptive string for the check (e.g., \"greater_or_equal\"). - \"value\" (numeric): The threshold value for the comparison.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame with rows filtered based on the rule and an additional</p> <code>DataFrame</code> <p>\"dq_status\" column describing the rule applied.</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def is_greater_or_equal_than(df: DataFrame, rule: dict) -&gt; DataFrame:\n    \"\"\"\n    Filters rows in a DataFrame where the value of a specified field is less than a given value\n    and adds a new column \"dq_status\" with a formatted string indicating the rule applied.\n\n    Args:\n        df (DataFrame): The input PySpark DataFrame to filter.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - \"field\" (str): The name of the column to check.\n            - \"check\" (str): A descriptive string for the check (e.g., \"greater_or_equal\").\n            - \"value\" (numeric): The threshold value for the comparison.\n\n    Returns:\n        DataFrame: A new DataFrame with rows filtered based on the rule and an additional\n        \"dq_status\" column describing the rule applied.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    return df.filter(col(field) &lt; value).withColumn(\n        \"dq_status\", concat(lit(field), lit(\":\"), lit(check), lit(\":\"), lit(value))\n    )\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.is_greater_than","title":"<code>is_greater_than(df, rule)</code>","text":"<p>Filters rows in a DataFrame where the value of a specified field is less than  or equal to a given threshold and adds a new column indicating the rule applied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The name of the column to apply the rule on. - 'check' (str): A descriptive string for the rule (e.g., \"greater_than\"). - 'value' (int or float): The threshold value for the comparison.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame with rows filtered based on the rule and an </p> <code>DataFrame</code> <p>additional column \"dq_status\" describing the rule applied.</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def is_greater_than(df: DataFrame, rule: dict) -&gt; DataFrame:\n    \"\"\"\n    Filters rows in a DataFrame where the value of a specified field is less than \n    or equal to a given threshold and adds a new column indicating the rule applied.\n\n    Args:\n        df (DataFrame): The input PySpark DataFrame to filter.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field' (str): The name of the column to apply the rule on.\n            - 'check' (str): A descriptive string for the rule (e.g., \"greater_than\").\n            - 'value' (int or float): The threshold value for the comparison.\n\n    Returns:\n        DataFrame: A new DataFrame with rows filtered based on the rule and an \n        additional column \"dq_status\" describing the rule applied.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    return df.filter(col(field) &lt;= value).withColumn(\n        \"dq_status\", concat(lit(field), lit(\":\"), lit(check), lit(\":\"), lit(value))\n    )\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.is_legit","title":"<code>is_legit(df, rule)</code>","text":"<p>Filters a PySpark DataFrame to identify rows that do not meet a specified rule  and appends a column indicating the data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be validated.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The name of the column to validate. - 'check': The type of check being performed (e.g., \"is_legit\"). - 'value': The expected value or condition for the validation.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame containing only the rows that fail the validation </p> <code>DataFrame</code> <p>rule, with an additional column \"dq_status\" describing the validation status </p> <code>DataFrame</code> <p>in the format \"field:check:value\".</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def is_legit(df: DataFrame, rule: dict) -&gt; DataFrame:\n    \"\"\"\n    Filters a PySpark DataFrame to identify rows that do not meet a specified rule \n    and appends a column indicating the data quality status.\n\n    Args:\n        df (DataFrame): The input PySpark DataFrame to be validated.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field': The name of the column to validate.\n            - 'check': The type of check being performed (e.g., \"is_legit\").\n            - 'value': The expected value or condition for the validation.\n\n    Returns:\n        DataFrame: A new DataFrame containing only the rows that fail the validation \n        rule, with an additional column \"dq_status\" describing the validation status \n        in the format \"field:check:value\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    pattern_legit = \"\\S*\"\n    return df.filter(\n        ~(col(field).isNotNull() &amp; col(field).rlike(pattern_legit))\n    ).withColumn(\n        \"dq_status\", concat(lit(field), lit(\":\"), lit(check), lit(\":\"), lit(value))\n    )\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.is_less_or_equal_than","title":"<code>is_less_or_equal_than(df, rule)</code>","text":"<p>Filters rows in a PySpark DataFrame where the value of a specified field is greater than a given value and adds a new column \"dq_status\" with a formatted string indicating the rule applied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - \"field\" (str): The name of the column to evaluate. - \"check\" (str): A descriptive string for the check being performed. - \"value\" (numeric): The threshold value to compare against.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new PySpark DataFrame with rows filtered based on the rule and an additional</p> <code>DataFrame</code> <p>\"dq_status\" column describing the rule applied.</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def is_less_or_equal_than(df: DataFrame, rule: dict) -&gt; DataFrame:\n    \"\"\"\n    Filters rows in a PySpark DataFrame where the value of a specified field is greater than a given value\n    and adds a new column \"dq_status\" with a formatted string indicating the rule applied.\n\n    Args:\n        df (DataFrame): The input PySpark DataFrame to filter.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - \"field\" (str): The name of the column to evaluate.\n            - \"check\" (str): A descriptive string for the check being performed.\n            - \"value\" (numeric): The threshold value to compare against.\n\n    Returns:\n        DataFrame: A new PySpark DataFrame with rows filtered based on the rule and an additional\n        \"dq_status\" column describing the rule applied.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    return df.filter(col(field) &gt; value).withColumn(\n        \"dq_status\", concat(lit(field), lit(\":\"), lit(check), lit(\":\"), lit(value))\n    )\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.is_less_than","title":"<code>is_less_than(df, rule)</code>","text":"<p>Filters rows in a PySpark DataFrame where the specified field is greater than  or equal to a given value and adds a new column indicating the rule applied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The name of the column to apply the filter on. - 'check' (str): A descriptive string for the rule (e.g., \"less_than\"). - 'value' (int, float, or str): The value to compare the column against.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame with rows filtered based on the rule and an </p> <code>DataFrame</code> <p>additional column \"dq_status\" describing the rule applied.</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def is_less_than(df: DataFrame, rule: dict) -&gt; DataFrame:\n    \"\"\"\n    Filters rows in a PySpark DataFrame where the specified field is greater than \n    or equal to a given value and adds a new column indicating the rule applied.\n\n    Args:\n        df (DataFrame): The input PySpark DataFrame to filter.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field' (str): The name of the column to apply the filter on.\n            - 'check' (str): A descriptive string for the rule (e.g., \"less_than\").\n            - 'value' (int, float, or str): The value to compare the column against.\n\n    Returns:\n        DataFrame: A new DataFrame with rows filtered based on the rule and an \n        additional column \"dq_status\" describing the rule applied.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    return df.filter(col(field) &gt;= value).withColumn(\n        \"dq_status\", concat(lit(field), lit(\":\"), lit(check), lit(\":\"), lit(value))\n    )\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.is_negative","title":"<code>is_negative(df, rule)</code>","text":"<p>Filters rows in the given DataFrame where the specified field is non-negative and adds a new column \"dq_status\" containing a formatted string with rule details.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be filtered and modified.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The name of the column to check. - 'check' (str): A descriptive string for the check being performed. - 'value' (any): The value associated with the rule.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame with rows filtered based on the rule and an</p> <code>DataFrame</code> <p>additional \"dq_status\" column describing the rule applied.</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def is_negative(df: DataFrame, rule: dict) -&gt; DataFrame:\n    \"\"\"\n    Filters rows in the given DataFrame where the specified field is non-negative\n    and adds a new column \"dq_status\" containing a formatted string with rule details.\n\n    Args:\n        df (DataFrame): The input PySpark DataFrame to be filtered and modified.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field' (str): The name of the column to check.\n            - 'check' (str): A descriptive string for the check being performed.\n            - 'value' (any): The value associated with the rule.\n\n    Returns:\n        DataFrame: A new DataFrame with rows filtered based on the rule and an\n        additional \"dq_status\" column describing the rule applied.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    return df.filter(col(field) &gt;= 0).withColumn(\n        \"dq_status\", concat(lit(field), lit(\":\"), lit(check), lit(\":\"), lit(value))\n    )\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.is_past_date","title":"<code>is_past_date(df, rule)</code>","text":"<p>Filters a DataFrame to identify rows where a specified field has a date lower than the current date and adds a \"dq_status\" column indicating the data quality rule applied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the data quality rule. It should include: - \"field\" (str): The name of the field to check for null values. - \"check\" (str): A description of the check being performed. - \"value\" (str): Additional information about the rule.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame filtered to include only rows where the specified</p> <code>DataFrame</code> <p>field is null, with an additional \"dq_status\" column describing the rule.</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def is_past_date(df: DataFrame, rule: dict) -&gt; DataFrame:\n    \"\"\"\n    Filters a DataFrame to identify rows where a specified field has a date lower than the current date and\n    adds a \"dq_status\" column indicating the data quality rule applied.\n\n    Args:\n        df (DataFrame): The input PySpark DataFrame to be checked.\n        rule (dict): A dictionary containing the data quality rule. It should include:\n            - \"field\" (str): The name of the field to check for null values.\n            - \"check\" (str): A description of the check being performed.\n            - \"value\" (str): Additional information about the rule.\n\n    Returns:\n        DataFrame: A new DataFrame filtered to include only rows where the specified\n        field is null, with an additional \"dq_status\" column describing the rule.\n    \"\"\"\n\n    field, check, value = __extract_params(rule)\n    return df.filter(col(field) &lt; current_date()).withColumn(\n        \"dq_status\", concat(lit(field), lit(\":\"), lit(check), lit(\":\"), lit(value))\n    )\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.is_positive","title":"<code>is_positive(df, rule)</code>","text":"<p>Filters a DataFrame to identify rows where the specified field does not satisfy a positive check and adds a \"dq_status\" column with details of the rule applied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - \"field\" (str): The name of the column to check. - \"check\" (str): The type of check being performed (e.g., \"positive\"). - \"value\" (any): The value associated with the rule (not directly used in this function).</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame filtered to include only rows where the specified field is less than 0,</p> <code>DataFrame</code> <p>with an additional \"dq_status\" column describing the rule applied.</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def is_positive(df: DataFrame, rule: dict) -&gt; DataFrame:\n    \"\"\"\n    Filters a DataFrame to identify rows where the specified field does not satisfy a positive check\n    and adds a \"dq_status\" column with details of the rule applied.\n\n    Args:\n        df (DataFrame): The input PySpark DataFrame to be filtered.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - \"field\" (str): The name of the column to check.\n            - \"check\" (str): The type of check being performed (e.g., \"positive\").\n            - \"value\" (any): The value associated with the rule (not directly used in this function).\n\n    Returns:\n        DataFrame: A new DataFrame filtered to include only rows where the specified field is less than 0,\n        with an additional \"dq_status\" column describing the rule applied.\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    return df.filter(col(field) &lt; 0).withColumn(\n        \"dq_status\", concat(lit(field), lit(\":\"), lit(check), lit(\":\"), lit(value))\n    )\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.is_primary_key","title":"<code>is_primary_key(df, rule)</code>","text":"<p>Determines if a given DataFrame column or set of columns satisfies the primary key constraint.</p> <p>A primary key constraint requires that the specified column(s) in the DataFrame have unique values.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The PySpark DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rules or specifications for identifying the primary key.          Typically, this includes the column(s) to be checked for uniqueness.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <p>True if the specified column(s) in the DataFrame satisfy the primary key constraint, False otherwise.</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def is_primary_key(df: DataFrame, rule: dict):\n    \"\"\"\n    Determines if a given DataFrame column or set of columns satisfies the primary key constraint.\n\n    A primary key constraint requires that the specified column(s) in the DataFrame have unique values.\n\n    Args:\n        df (DataFrame): The PySpark DataFrame to be checked.\n        rule (dict): A dictionary containing the rules or specifications for identifying the primary key.\n                     Typically, this includes the column(s) to be checked for uniqueness.\n\n    Returns:\n        bool: True if the specified column(s) in the DataFrame satisfy the primary key constraint, False otherwise.\n    \"\"\"\n    return is_unique(df, rule)\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.is_unique","title":"<code>is_unique(df, rule)</code>","text":"<p>Checks for uniqueness of a specified field in a PySpark DataFrame based on the given rule.</p> <p>This function identifies rows where the specified field is not unique within the DataFrame. It adds a new column <code>dq_status</code> to the resulting DataFrame, which contains information about the field, the check type, and the value from the rule.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to check for uniqueness.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - <code>field</code> (str): The name of the field to check for uniqueness. - <code>check</code> (str): The type of check being performed (e.g., \"unique\"). - <code>value</code> (str): Additional value or metadata related to the check.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame containing rows where the specified field is not unique.</p> <code>DataFrame</code> <p>The resulting DataFrame includes a <code>dq_status</code> column with details about the rule violation.</p> Example <p>rule = {\"field\": \"column_name\", \"check\": \"unique\", \"value\": \"some_value\"} result_df = is_unique(input_df, rule)</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def is_unique(df: DataFrame, rule: dict) -&gt; DataFrame:\n    \"\"\"\n    Checks for uniqueness of a specified field in a PySpark DataFrame based on the given rule.\n\n    This function identifies rows where the specified field is not unique within the DataFrame.\n    It adds a new column `dq_status` to the resulting DataFrame, which contains information\n    about the field, the check type, and the value from the rule.\n\n    Args:\n        df (DataFrame): The input PySpark DataFrame to check for uniqueness.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - `field` (str): The name of the field to check for uniqueness.\n            - `check` (str): The type of check being performed (e.g., \"unique\").\n            - `value` (str): Additional value or metadata related to the check.\n\n    Returns:\n        DataFrame: A new DataFrame containing rows where the specified field is not unique.\n        The resulting DataFrame includes a `dq_status` column with details about the rule violation.\n\n    Example:\n        rule = {\"field\": \"column_name\", \"check\": \"unique\", \"value\": \"some_value\"}\n        result_df = is_unique(input_df, rule)\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    window = Window.partitionBy(col(field))\n    df_with_count = df.withColumn(\"count\", count(col(field)).over(window))\n    res = (\n        df_with_count.filter(col(\"count\") &gt; 1)\n        .withColumn(\n            \"dq_status\", concat(lit(field), lit(\":\"), lit(check), lit(\":\"), lit(value))\n        )\n        .drop(\"count\")\n    )\n    return res\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.not_contained_in","title":"<code>not_contained_in(df, rule)</code>","text":"<p>Filters rows in a DataFrame where the specified field's value is in a given list  and adds a column indicating the data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The name of the column to check. - 'check': A string representing the type of check (e.g., \"not_contained_in\"). - 'value': A string representation of a list (e.g., \"[value1,value2,...]\")    containing the values to check against.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame with rows filtered based on the rule and an </p> <code>DataFrame</code> <p>additional column \"dq_status\" indicating the data quality status in the </p> <code>DataFrame</code> <p>format \"field:check:value\".</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def not_contained_in(df: DataFrame, rule: dict) -&gt; DataFrame:\n    \"\"\"\n    Filters rows in a DataFrame where the specified field's value is in a given list \n    and adds a column indicating the data quality status.\n\n    Args:\n        df (DataFrame): The input PySpark DataFrame to filter.\n        rule (dict): A dictionary containing the rule parameters. It should include:\n            - 'field': The name of the column to check.\n            - 'check': A string representing the type of check (e.g., \"not_contained_in\").\n            - 'value': A string representation of a list (e.g., \"[value1,value2,...]\") \n              containing the values to check against.\n\n    Returns:\n        DataFrame: A new DataFrame with rows filtered based on the rule and an \n        additional column \"dq_status\" indicating the data quality status in the \n        format \"field:check:value\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    negative_list = value.strip(\"[]\").split(\",\")\n    return df.filter(col(field).isin(negative_list)).withColumn(\n        \"dq_status\", concat(lit(field), lit(\":\"), lit(check), lit(\":\"), lit(value))\n    )\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.satisfies","title":"<code>satisfies(df, rule)</code>","text":"<p>Filters a PySpark DataFrame based on a rule and adds a data quality status column.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the filtering rule. It should include: - 'field': The name of the column to apply the filter on. - 'check': The type of check to perform (currently unused in this implementation). - 'value': The expression in the pattern of pyspark.sql.functions.expr.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame filtered based on the rule, with an additional column</p> <code>DataFrame</code> <p>\"dq_status\" that describes the rule applied in the format \"field:check:value\".</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def satisfies(df: DataFrame, rule: dict) -&gt; DataFrame:\n    \"\"\"\n    Filters a PySpark DataFrame based on a rule and adds a data quality status column.\n\n    Args:\n        df (DataFrame): The input PySpark DataFrame to be filtered.\n        rule (dict): A dictionary containing the filtering rule. It should include:\n            - 'field': The name of the column to apply the filter on.\n            - 'check': The type of check to perform (currently unused in this implementation).\n            - 'value': The expression in the pattern of pyspark.sql.functions.expr.\n\n    Returns:\n        DataFrame: A new DataFrame filtered based on the rule, with an additional column\n        \"dq_status\" that describes the rule applied in the format \"field:check:value\".\n    \"\"\"\n    field, check, value = __extract_params(rule)\n    expression = expr(value)\n    return df.filter(~expression).withColumn(\n        \"dq_status\", concat(lit(field), lit(\":\"), lit(check), lit(\":\"), lit(value))\n    )\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.summarize","title":"<code>summarize(df, rules, total_rows)</code>","text":"<p>Summarizes data quality results based on provided rules and total rows.</p> <p>This function processes a DataFrame containing data quality statuses, applies rules to calculate violations, and generates a summary DataFrame with metrics such as pass rate, status, and other relevant information.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing a column <code>dq_status</code> with data quality statuses in the format \"column:rule:value\".</p> required <code>rules</code> <code>List[Dict]</code> <p>A list of dictionaries representing the data quality rules. Each dictionary should define the <code>column</code>, <code>rule</code>, and optional <code>value</code> and <code>pass_threshold</code>.</p> required <code>total_rows</code> <code>int</code> <p>The total number of rows in the input DataFrame.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A summary DataFrame containing the following columns: - id: A unique identifier for each row. - timestamp: The timestamp when the summary was generated. - check: The type of check performed (e.g., \"Quality Check\"). - level: The severity level of the check (e.g., \"WARNING\"). - column: The column name associated with the rule. - rule: The rule applied to the column. - value: The value associated with the rule. - rows: The total number of rows in the input DataFrame. - violations: The number of rows that violated the rule. - pass_rate: The percentage of rows that passed the rule. - pass_threshold: The threshold for passing the rule. - status: The overall status of the rule (e.g., \"PASS\" or \"FAIL\").</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def summarize(df: DataFrame, rules: List[Dict], total_rows) -&gt; DataFrame:\n    \"\"\"\n    Summarizes data quality results based on provided rules and total rows.\n\n    This function processes a DataFrame containing data quality statuses, applies\n    rules to calculate violations, and generates a summary DataFrame with metrics\n    such as pass rate, status, and other relevant information.\n\n    Args:\n        df (DataFrame): The input DataFrame containing a column `dq_status` with\n            data quality statuses in the format \"column:rule:value\".\n        rules (List[Dict]): A list of dictionaries representing the data quality\n            rules. Each dictionary should define the `column`, `rule`, and optional\n            `value` and `pass_threshold`.\n        total_rows (int): The total number of rows in the input DataFrame.\n\n    Returns:\n        DataFrame: A summary DataFrame containing the following columns:\n            - id: A unique identifier for each row.\n            - timestamp: The timestamp when the summary was generated.\n            - check: The type of check performed (e.g., \"Quality Check\").\n            - level: The severity level of the check (e.g., \"WARNING\").\n            - column: The column name associated with the rule.\n            - rule: The rule applied to the column.\n            - value: The value associated with the rule.\n            - rows: The total number of rows in the input DataFrame.\n            - violations: The number of rows that violated the rule.\n            - pass_rate: The percentage of rows that passed the rule.\n            - pass_threshold: The threshold for passing the rule.\n            - status: The overall status of the rule (e.g., \"PASS\" or \"FAIL\").\n    \"\"\"\n    now_ts = current_timestamp()\n\n    viol_df = (\n        df.filter(trim(col(\"dq_status\")) != lit(\"\"))\n        .withColumn(\"dq_status\", split(trim(col(\"dq_status\")), \":\"))\n        .withColumn(\"column\", col(\"dq_status\")[0])\n        .withColumn(\"rule\", col(\"dq_status\")[1])\n        .withColumn(\"value\", col(\"dq_status\")[2])\n        .groupBy(\"column\", \"rule\", \"value\")\n        .agg(count(\"*\").alias(\"violations\"))\n        .withColumn(\n            \"value\",\n            coalesce(\n                when(col(\"value\") == \"\", None).otherwise(col(\"value\")), lit(\"N/A\")\n            ),\n        )\n    )\n\n    rules_df = __rules_to_df(rules).withColumn(\n        \"value\", coalesce(col(\"value\"), lit(\"N/A\"))\n    )\n\n    base = rules_df.join(viol_df, [\"column\", \"rule\", \"value\"], how=\"left\").withColumn(\n        \"violations\", coalesce(col(\"violations\"), lit(0))\n    )\n\n    summary = (\n        base.withColumn(\"rows\", lit(total_rows))\n        .withColumn(\n            \"pass_rate\", (lit(total_rows) - col(\"violations\")) / lit(total_rows)\n        )\n        .withColumn(\n            \"status\",\n            when(col(\"pass_rate\") &gt;= col(\"pass_threshold\"), \"PASS\").otherwise(\"FAIL\"),\n        )\n        .withColumn(\"timestamp\", now_ts)\n        .withColumn(\"check\", lit(\"Quality Check\"))\n        .withColumn(\"level\", lit(\"WARNING\"))\n    )\n\n    summary = summary.withColumn(\"id\", monotonically_increasing_id() + 1)\n    summary = summary.select(\n        \"id\",\n        \"timestamp\",\n        \"check\",\n        \"level\",\n        \"column\",\n        \"rule\",\n        \"value\",\n        \"rows\",\n        \"violations\",\n        \"pass_rate\",\n        \"pass_threshold\",\n        \"status\",\n    )\n\n    return summary\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.validate","title":"<code>validate(df, rules)</code>","text":"<p>Validates a DataFrame against a set of rules and returns the validation results.</p> <p>This function applies a series of validation rules to the input DataFrame. Each rule is expected to be a dictionary containing the parameters required for validation. The function generates two DataFrames as output: 1. A summarized result DataFrame with aggregated validation statuses. 2. A raw result DataFrame containing detailed validation results.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to validate.</p> required <code>rules</code> <code>list[dict]</code> <p>A list of dictionaries, where each dictionary defines a validation rule. Each rule should include the following keys: - <code>field</code> (str): The column name to validate. - <code>rule_name</code> (str): The name of the validation function to apply. - <code>value</code> (any): The value or parameter required by the validation function.</p> required <p>Returns:</p> Type Description <code>Tuple[DataFrame, DataFrame]</code> <p>Tuple[DataFrame, DataFrame]: A tuple containing: - result (DataFrame): A DataFrame with aggregated validation statuses. - raw_result (DataFrame): A DataFrame with detailed validation results.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If a rule references a validation function that does not exist in the global scope.</p> Notes <ul> <li>The <code>dq_status</code> column is used to store validation statuses.</li> <li>The function assumes that the validation functions are defined in the global scope   and are accessible by their names.</li> <li>The <code>concat_ws</code> function is used to concatenate multiple validation statuses   into a single string for each record in the summarized result.</li> </ul> Example <p>from pyspark.sql import SparkSession spark = SparkSession.builder.getOrCreate() df = spark.createDataFrame([(1, \"Alice\"), (2, \"Bob\")], [\"id\", \"name\"]) rules = [{\"field\": \"id\", \"rule_name\": \"validate_positive\", \"value\": None}] result, raw_result = validate(df, rules)</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def validate(df: DataFrame, rules: list[dict]) -&gt; Tuple[DataFrame, DataFrame]:\n    \"\"\"\n    Validates a DataFrame against a set of rules and returns the validation results.\n\n    This function applies a series of validation rules to the input DataFrame. Each rule\n    is expected to be a dictionary containing the parameters required for validation.\n    The function generates two DataFrames as output:\n    1. A summarized result DataFrame with aggregated validation statuses.\n    2. A raw result DataFrame containing detailed validation results.\n\n    Args:\n        df (DataFrame): The input PySpark DataFrame to validate.\n        rules (list[dict]): A list of dictionaries, where each dictionary defines a validation rule.\n            Each rule should include the following keys:\n            - `field` (str): The column name to validate.\n            - `rule_name` (str): The name of the validation function to apply.\n            - `value` (any): The value or parameter required by the validation function.\n\n    Returns:\n        Tuple[DataFrame, DataFrame]: A tuple containing:\n            - result (DataFrame): A DataFrame with aggregated validation statuses.\n            - raw_result (DataFrame): A DataFrame with detailed validation results.\n\n    Raises:\n        KeyError: If a rule references a validation function that does not exist in the global scope.\n\n    Warnings:\n        If a rule references an unknown validation function, a warning is issued.\n\n    Notes:\n        - The `dq_status` column is used to store validation statuses.\n        - The function assumes that the validation functions are defined in the global scope\n          and are accessible by their names.\n        - The `concat_ws` function is used to concatenate multiple validation statuses\n          into a single string for each record in the summarized result.\n\n    Example:\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt; df = spark.createDataFrame([(1, \"Alice\"), (2, \"Bob\")], [\"id\", \"name\"])\n        &gt;&gt;&gt; rules = [{\"field\": \"id\", \"rule_name\": \"validate_positive\", \"value\": None}]\n        &gt;&gt;&gt; result, raw_result = validate(df, rules)\n    \"\"\"\n    df = df.withColumn(\"dq_status\", lit(\"\"))\n    raw_result = df.limit(0)\n    for rule in rules:\n        field, rule_name, value = __extract_params(rule)\n        try:\n            rule_func = globals()[rule_name]\n            raw_result = raw_result.unionByName(rule_func(df, rule))\n        except KeyError:\n            warnings.warn(f\"Unknown rule name: {rule_name}, {field}\")\n    group_columns = [c for c in df.columns if c != \"dq_status\"]\n    result = raw_result.groupBy(*group_columns).agg(\n        concat_ws(\";\", collect_list(\"dq_status\")).alias(\"dq_status\")\n    )\n    return result, raw_result\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.validate_date_format","title":"<code>validate_date_format(df, rule)</code>","text":"<p>Filters a DataFrame to identify rows where a specified field has wrong date format based in the format from the rule and adds a \"dq_status\" column indicating the data quality rule applied.</p> <p>YYYY = full year, ex: 2012; YY = only second part of the year, ex: 12; MM = Month number (1-12); DD = Day (1-31);</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the data quality rule. It should include: - \"field\" (str): The name of the field to check for null values. - \"check\" (str): A description of the check being performed. - \"value\" (str): Additional information about the rule.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame filtered to include only rows where the specified</p> <code>DataFrame</code> <p>field is null, with an additional \"dq_status\" column describing the rule.</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def validate_date_format(df: DataFrame, rule: dict) -&gt; DataFrame:\n    \"\"\"\n    Filters a DataFrame to identify rows where a specified field has wrong date format based in the format from the rule\n    and adds a \"dq_status\" column indicating the data quality rule applied.\n\n    YYYY = full year, ex: 2012;\n    YY = only second part of the year, ex: 12;\n    MM = Month number (1-12);\n    DD = Day (1-31);\n\n    Args:\n        df (DataFrame): The input PySpark DataFrame to be checked.\n        rule (dict): A dictionary containing the data quality rule. It should include:\n            - \"field\" (str): The name of the field to check for null values.\n            - \"check\" (str): A description of the check being performed.\n            - \"value\" (str): Additional information about the rule.\n\n    Returns:\n        DataFrame: A new DataFrame filtered to include only rows where the specified\n        field is null, with an additional \"dq_status\" column describing the rule.\n    \"\"\"\n\n    field, check, date_format = __extract_params(rule)\n\n    date_regex = __transform_date_format_in_pattern(date_format)\n\n    return df.filter(~col(field).rlike(date_regex) | col(field).isNull()).withColumn(\n        \"dq_status\", concat(lit(field), lit(\":\"), lit(check), lit(\":\"), lit(date_format))\n    )\n</code></pre>"},{"location":"api/engine/engine-pyspark/#sumeh.engine.pyspark_engine.validate_schema","title":"<code>validate_schema(df, expected)</code>","text":"<p>Validates the schema of a PySpark DataFrame against an expected schema.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The PySpark DataFrame whose schema is to be validated.</p> required <code>expected</code> <code>list</code> <p>The expected schema represented as a list of tuples,               where each tuple contains the column name and its data type              and a boolean, if the column is nullable or not.</p> required <p>Returns:</p> Type Description <code>Tuple[bool, List[Tuple[str, str]]]</code> <p>Tuple[bool, List[Tuple[str, str]]]: A tuple containing: - A boolean indicating whether the schema matches the expected schema. - A list of tuples representing the mismatched columns, where each tuple    contains the column name and the reason for the mismatch.</p> Source code in <code>sumeh/engine/pyspark_engine.py</code> <pre><code>def validate_schema(df: DataFrame, expected) -&gt; Tuple[bool, List[Tuple[str, str]]]:\n    \"\"\"\n    Validates the schema of a PySpark DataFrame against an expected schema.\n\n    Args:\n        df (DataFrame): The PySpark DataFrame whose schema is to be validated.\n        expected (list): The expected schema represented as a list of tuples, \n                         where each tuple contains the column name and its data type\n                         and a boolean, if the column is nullable or not.\n\n    Returns:\n        Tuple[bool, List[Tuple[str, str]]]: A tuple containing:\n            - A boolean indicating whether the schema matches the expected schema.\n            - A list of tuples representing the mismatched columns, where each tuple \n              contains the column name and the reason for the mismatch.\n    \"\"\"\n    actual = __pyspark_schema_to_list(df)\n    result, errors = __compare_schemas(actual, expected)\n    return result, errors\n</code></pre>"},{"location":"api/engine/engines/","title":"Module <code>sumeh.engine</code> - Overview","text":"<p>This package provides engine-specific implementations for data quality checks, schema validation, and result summarization across multiple backends.</p> <p>\ud83d\udce6 File Structure</p> <pre><code>sumeh/engine/\n\u251c\u2500\u2500 __init__.py         # \u21e8 engine registry and dynamic import logic\n\u251c\u2500\u2500 bigquery_engine.py  # \u21e8 BigQuery schema introspection &amp; validation\n\u251c\u2500\u2500 dask_engine.py      # \u21e8 Dask DataFrame rule execution, validation &amp; summarization\n\u251c\u2500\u2500 duckdb_engine.py    # \u21e8 DuckDB SQL builder for rule validation &amp; summarization\n\u251c\u2500\u2500 polars_engine.py    # \u21e8 Polars DataFrame rule execution, validation &amp; summarization\n\u2514\u2500\u2500 pyspark_engine.py   # \u21e8 PySpark DataFrame rule execution, validation &amp; summarization\n</code></pre> <ul> <li> <p><code>__init__.py</code>   Detects the active engine based on the DataFrame type and dynamically dispatches to the appropriate module.</p> </li> <li> <p><code>bigquery_engine.py</code>   Converts a BigQuery table schema into a unified format and compares it against an expected schema.</p> </li> <li> <p><code>dask_engine.py</code>   Implements all data quality checks (completeness, uniqueness, value ranges, patterns, etc.) on Dask DataFrames, plus functions to aggregate violations and produce a summary report.</p> </li> <li> <p><code>duckdb_engine.py</code>   Builds SQL expressions for each rule, executes them in DuckDB as UNION-ALL queries, and returns both raw violations and an aggregated result, along with schema validation via PRAGMA introspection.</p> </li> <li> <p><code>polars_engine.py</code>   Mirrors the full suite of quality checks for Polars DataFrames, annotating violations in a <code>dq_status</code> column and providing summarization and schema-comparison utilities.</p> </li> <li> <p><code>pyspark_engine.py</code>   Leverages PySpark SQL functions and window operations to apply the same rule set to Spark DataFrames, including logic for schema validation and summarization.</p> </li> </ul>"},{"location":"api/services/services-config/","title":"Module <code>sumeh.services.config</code>","text":"<p>This module provides a set of utility functions to retrieve and parse configuration data  from various data sources, including S3, MySQL, PostgreSQL, BigQuery, CSV files, AWS Glue  Data Catalog, DuckDB, and Databricks. Additionally, it includes functions to infer schema  information from these sources.</p> <p>Functions:</p> Name Description <code>get_config_from_s3</code> <p>str, delimiter: Optional[str] = \",\") -&gt; List[Dict[str, Any]]:</p> <code>get_config_from_mysql</code> <code>get_config_from_postgresql</code> <code>get_config_from_bigquery</code> <code>get_config_from_csv</code> <p>str, delimiter: Optional[str] = \",\") -&gt; List[Dict[str, str]]: Retrieves configuration data from a local CSV file.</p> <code>get_config_from_glue_data_catalog</code> <code>get_config_from_duckdb</code> <p>Retrieves configuration data from a DuckDB database.</p> <code>get_config_from_databricks</code> <p>Retrieves configuration data from a Databricks table.</p> <code>get_schema_from_csv</code> <p>str, delimiter: str = \",\", sample_size: int = 1_000) -&gt; List[Dict[str, Any]]: Infers the schema of a CSV file based on its content.</p> <code>get_schema_from_s3</code> <p>str, **kwargs) -&gt; List[Dict[str, Any]]: Infers the schema of a CSV file stored in S3.</p> <code>get_schema_from_mysql</code> <p>Retrieves schema information from a MySQL database table.</p> <code>get_schema_from_postgresql</code> <p>Retrieves schema information from a PostgreSQL database table.</p> <code>get_schema_from_bigquery</code> <p>Retrieves schema information from a Google BigQuery table.</p> <code>get_schema_from_glue</code> <p>Retrieves schema information from AWS Glue Data Catalog.</p> <code>get_schema_from_duckdb</code> <p>Retrieves schema information from a DuckDB database table.</p> <code>get_schema_from_databricks</code> <p>Retrieves schema information from a Databricks table.</p> <code>__read_s3_file</code> <p>str) -&gt; Optional[str]:</p> <code>__parse_s3_path</code> <p>str) -&gt; Tuple[str, str]:</p> <code>__read_local_file</code> <p>str) -&gt; str:</p> <code>__read_csv_file</code> <p>str, delimiter: Optional[str] = \",\") -&gt; List[Dict[str, str]]:</p> <code>__parse_data</code> <p>list[dict]) -&gt; list[dict]: Parses the configuration data into a structured format.</p> <code>__create_connection</code> <code>infer_basic_type</code> <p>str) -&gt; str: Infers the basic data type of a given value.</p>"},{"location":"api/services/services-config/#sumeh.services.config.__create_connection","title":"<code>__create_connection(connect_func, host, user, password, database, port)</code>","text":"<p>Helper function to create a database connection.</p> <p>Parameters:</p> Name Type Description Default <code>connect_func</code> <p>A connection function (e.g., <code>mysql.connector.connect</code> or <code>psycopg2.connect</code>).</p> required <code>host</code> <code>str</code> <p>The host of the database server.</p> required <code>user</code> <code>str</code> <p>The username for the database.</p> required <code>password</code> <code>str</code> <p>The password for the database.</p> required <code>database</code> <code>str</code> <p>The name of the database.</p> required <code>port</code> <code>int</code> <p>The port to connect to.</p> required <p>Returns:</p> Name Type Description <code>Connection</code> <code>Any</code> <p>A connection object for the database.</p> <p>Raises:</p> Type Description <code>ConnectionError</code> <p>If there is an error establishing the connection.</p> Source code in <code>sumeh/services/config.py</code> <pre><code>def __create_connection(connect_func, host, user, password, database, port) -&gt; Any:\n    \"\"\"\n    Helper function to create a database connection.\n\n    Args:\n        connect_func: A connection function (e.g., `mysql.connector.connect` or `psycopg2.connect`).\n        host (str): The host of the database server.\n        user (str): The username for the database.\n        password (str): The password for the database.\n        database (str): The name of the database.\n        port (int): The port to connect to.\n\n    Returns:\n        Connection: A connection object for the database.\n\n    Raises:\n        ConnectionError: If there is an error establishing the connection.\n    \"\"\"\n    try:\n        return connect_func(\n            host=host, user=user, password=password, database=database, port=port\n        )\n    except Exception as e:\n        raise ConnectionError(f\"Error creating connection: {e}\")\n</code></pre>"},{"location":"api/services/services-config/#sumeh.services.config.__parse_data","title":"<code>__parse_data(data)</code>","text":"<p>Parse the configuration data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>List[Dict[str, str]]</code> <p>The raw data to be parsed.</p> required <p>Returns:</p> Type Description <code>list[dict]</code> <p>List[Dict[str, str]]: A list of parsed configuration data.</p> Source code in <code>sumeh/services/config.py</code> <pre><code>def __parse_data(data: list[dict]) -&gt; list[dict]:\n    \"\"\"\n    Parse the configuration data.\n\n    Args:\n        data (List[Dict[str, str]]): The raw data to be parsed.\n\n    Returns:\n        List[Dict[str, str]]: A list of parsed configuration data.\n    \"\"\"\n    parsed_data = []\n\n    for row in data:\n        parsed_row = {\n            \"field\": (\n                row[\"field\"].strip(\"[]\").split(\",\")\n                if \"[\" in row[\"field\"]\n                else row[\"field\"]\n            ),\n            \"check_type\": row[\"check_type\"],\n            \"value\": None if row[\"value\"] == \"NULL\" else row[\"value\"],\n            \"threshold\": (\n                None if row[\"threshold\"] == \"NULL\" else float(row[\"threshold\"])\n            ),\n            \"execute\": (\n                row[\"execute\"].lower() == \"true\"\n                if isinstance(row[\"execute\"], str)\n                else row[\"execute\"] is True\n            ),\n            \"updated_at\": parser.parse(row[\"updated_at\"]),\n        }\n        parsed_data.append(parsed_row)\n\n    return parsed_data\n</code></pre>"},{"location":"api/services/services-config/#sumeh.services.config.__parse_s3_path","title":"<code>__parse_s3_path(s3_path)</code>","text":"<p>Parses an S3 path into its bucket and key components.</p> <p>Parameters:</p> Name Type Description Default <code>s3_path</code> <code>str</code> <p>The S3 path to parse. Must start with \"s3://\".</p> required <p>Returns:</p> Type Description <code>Tuple[str, str]</code> <p>Tuple[str, str]: A tuple containing the bucket name and the key.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the S3 path does not start with \"s3://\", or if the path         format is invalid and cannot be split into bucket and key.</p> Source code in <code>sumeh/services/config.py</code> <pre><code>def __parse_s3_path(s3_path: str) -&gt; Tuple[str, str]:\n    \"\"\"\n    Parses an S3 path into its bucket and key components.\n\n    Args:\n        s3_path (str): The S3 path to parse. Must start with \"s3://\".\n\n    Returns:\n        Tuple[str, str]: A tuple containing the bucket name and the key.\n\n    Raises:\n        ValueError: If the S3 path does not start with \"s3://\", or if the path\n                    format is invalid and cannot be split into bucket and key.\n    \"\"\"\n    try:\n        if not s3_path.startswith(\"s3://\"):\n            raise ValueError(\"S3 path must start with 's3://'\")\n\n        s3_path = s3_path[5:]\n        bucket, key = s3_path.split(\"/\", 1)\n        return bucket, key\n\n    except ValueError as e:\n        raise ValueError(\n            f\"Invalid S3 path format: '{s3_path}'. Expected format 's3://bucket/key'. Details: {e}\"\n        ) from e\n</code></pre>"},{"location":"api/services/services-config/#sumeh.services.config.__read_csv_file","title":"<code>__read_csv_file(file_content, delimiter=',')</code>","text":"<p>Parses the content of a CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>The content of the CSV file as a string.</p> required <code>delimiter</code> <code>str</code> <p>The delimiter used in the CSV file.</p> <code>','</code> <p>Returns:</p> Type Description <code>List[Dict[str, str]]</code> <p>List[Dict[str, str]]: A list of dictionaries representing the parsed CSV data.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If there is an error parsing the CSV content.</p> Source code in <code>sumeh/services/config.py</code> <pre><code>def __read_csv_file(\n    file_content: str, delimiter: Optional[str] = \",\"\n) -&gt; List[Dict[str, str]]:\n    \"\"\"\n    Parses the content of a CSV file.\n\n    Args:\n        content (str): The content of the CSV file as a string.\n        delimiter (str): The delimiter used in the CSV file.\n\n    Returns:\n        List[Dict[str, str]]: A list of dictionaries representing the parsed CSV data.\n\n    Raises:\n        ValueError: If there is an error parsing the CSV content.\n    \"\"\"\n    import csv\n\n    try:\n        reader = csv.DictReader(StringIO(file_content), delimiter=delimiter)\n        # next(reader, None)  # Skip the header row\n        return [dict(row) for row in reader]\n    except csv.Error as e:\n        raise ValueError(f\"Error: Could not parse CSV content. Details: {e}\") from e\n</code></pre>"},{"location":"api/services/services-config/#sumeh.services.config.__read_local_file","title":"<code>__read_local_file(file_path)</code>","text":"<p>Reads the content of a local file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The local file path to be read.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The content of the file.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the file is not found.</p> Source code in <code>sumeh/services/config.py</code> <pre><code>def __read_local_file(file_path: str) -&gt; str:\n    \"\"\"\n    Reads the content of a local file.\n\n    Args:\n        file_path (str): The local file path to be read.\n\n    Returns:\n        str: The content of the file.\n\n    Raises:\n        FileNotFoundError: If the file is not found.\n    \"\"\"\n    try:\n        with open(file_path, mode=\"r\", encoding=\"utf-8\") as file:\n            return file.read()\n    except FileNotFoundError as e:\n        raise FileNotFoundError(\n            f\"Error: The file at '{file_path}' was not found.\"\n        ) from e\n    except IOError as e:\n        raise IOError(f\"Error: Could not read file '{file_path}'. Details: {e}\") from e\n</code></pre>"},{"location":"api/services/services-config/#sumeh.services.config.__read_s3_file","title":"<code>__read_s3_file(s3_path)</code>","text":"<p>Reads the content of a file stored in S3.</p> <p>Parameters:</p> Name Type Description Default <code>s3_path</code> <code>str</code> <p>The S3 path of the file.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Optional[str]</code> <p>The content of the S3 file.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If there is an error retrieving the file from S3.</p> Source code in <code>sumeh/services/config.py</code> <pre><code>def __read_s3_file(s3_path: str) -&gt; Optional[str]:\n    \"\"\"\n    Reads the content of a file stored in S3.\n\n    Args:\n        s3_path (str): The S3 path of the file.\n\n    Returns:\n        str: The content of the S3 file.\n\n    Raises:\n        RuntimeError: If there is an error retrieving the file from S3.\n    \"\"\"\n    import boto3\n    from botocore.exceptions import BotoCoreError, ClientError\n\n    try:\n        s3 = boto3.client(\"s3\")\n        bucket, key = __parse_s3_path(s3_path)\n\n        response = s3.get_object(Bucket=bucket, Key=key)\n        return response[\"Body\"].read().decode(\"utf-8\")\n\n    except (BotoCoreError, ClientError) as e:\n        raise RuntimeError(\n            f\"Failed to read file from S3. Path: '{s3_path}'. Error: {e}\"\n        ) from e\n\n    except UnicodeDecodeError as e:\n        raise ValueError(\n            f\"Failed to decode file content from S3 path '{s3_path}' as UTF-8. Error: {e}\"\n        ) from e\n</code></pre>"},{"location":"api/services/services-config/#sumeh.services.config.get_config_from_bigquery","title":"<code>get_config_from_bigquery(project_id, dataset_id, table_id, credentials_path=None, query=None)</code>","text":"<p>Retrieves configuration data from a Google BigQuery table.</p> <p>Parameters:</p> Name Type Description Default <code>project_id</code> <code>str</code> <p>Google Cloud project ID.</p> required <code>dataset_id</code> <code>str</code> <p>BigQuery dataset ID.</p> required <code>table_id</code> <code>str</code> <p>BigQuery table ID.</p> required <code>credentials_path</code> <code>Optional[str]</code> <p>Path to service account credentials file (if not provided, defaults to default credentials).</p> <code>None</code> <code>query</code> <code>Optional[str]</code> <p>Custom SQL query to fetch data (if not provided, defaults to SELECT *).</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Dict[str, str]]</code> <p>List[Dict[str, str]]: A list of dictionaries representing the parsed configuration data.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If there is an error while querying BigQuery.</p> Source code in <code>sumeh/services/config.py</code> <pre><code>def get_config_from_bigquery(\n    project_id: str,\n    dataset_id: str,\n    table_id: str,\n    credentials_path: Optional[str] = None,\n    query: Optional[str] = None,\n) -&gt; List[Dict[str, str]]:\n    \"\"\"\n    Retrieves configuration data from a Google BigQuery table.\n\n    Args:\n        project_id (str): Google Cloud project ID.\n        dataset_id (str): BigQuery dataset ID.\n        table_id (str): BigQuery table ID.\n        credentials_path (Optional[str]): Path to service account credentials file (if not provided, defaults to default credentials).\n        query (Optional[str]): Custom SQL query to fetch data (if not provided, defaults to SELECT *).\n\n    Returns:\n        List[Dict[str, str]]: A list of dictionaries representing the parsed configuration data.\n\n    Raises:\n        RuntimeError: If there is an error while querying BigQuery.\n    \"\"\"\n    from google.cloud import bigquery\n    from google.auth.exceptions import DefaultCredentialsError\n\n    if query is None:\n        query = f\"SELECT * FROM `{project_id}.{dataset_id}.{table_id}`\"\n\n    try:\n        client = bigquery.Client(\n            project=project_id,\n            credentials=(\n                None\n                if credentials_path is None\n                else bigquery.Credentials.from_service_account_file(credentials_path)\n            ),\n        )\n\n        # Execute the query and convert the result to a pandas DataFrame\n        data = client.query(query).to_dataframe()\n\n        # Convert the DataFrame to a list of dictionaries\n        data_dict = data.to_dict(orient=\"records\")\n\n        # Parse the data and return the result\n        return __parse_data(data_dict)\n\n    except DefaultCredentialsError as e:\n        raise RuntimeError(f\"Credentials error: {e}\") from e\n\n    except Exception as e:\n        raise RuntimeError(f\"Error occurred while querying BigQuery: {e}\") from e\n</code></pre>"},{"location":"api/services/services-config/#sumeh.services.config.get_config_from_csv","title":"<code>get_config_from_csv(file_path, delimiter=',')</code>","text":"<p>Retrieves configuration data from a CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The local file path to the CSV file.</p> required <code>delimiter</code> <code>Optional[str]</code> <p>The delimiter used in the CSV file (default is \",\").</p> <code>','</code> <p>Returns:</p> Type Description <code>List[Dict[str, str]]</code> <p>List[Dict[str, str]]: A list of dictionaries representing the parsed configuration data.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If there is an error reading or processing the file.</p> Source code in <code>sumeh/services/config.py</code> <pre><code>def get_config_from_csv(\n    file_path: str, delimiter: Optional[str] = \",\"\n) -&gt; List[Dict[str, str]]:\n    \"\"\"\n    Retrieves configuration data from a CSV file.\n\n    Args:\n        file_path (str): The local file path to the CSV file.\n        delimiter (Optional[str]): The delimiter used in the CSV file (default is \",\").\n\n    Returns:\n        List[Dict[str, str]]: A list of dictionaries representing the parsed configuration data.\n\n    Raises:\n        RuntimeError: If there is an error reading or processing the file.\n    \"\"\"\n    try:\n        file_content = __read_local_file(file_path)\n        result = __read_csv_file(file_content, delimiter)\n\n        return __parse_data(result)\n\n    except FileNotFoundError as e:\n        raise RuntimeError(f\"File '{file_path}' not found. Error: {e}\") from e\n\n    except ValueError as e:\n        raise ValueError(\n            f\"Error while parsing CSV file '{file_path}'. Error: {e}\"\n        ) from e\n\n    except Exception as e:\n        # Catch any unexpected exceptions\n        raise RuntimeError(\n            f\"Unexpected error while processing CSV file '{file_path}'. Error: {e}\"\n        ) from e\n</code></pre>"},{"location":"api/services/services-config/#sumeh.services.config.get_config_from_databricks","title":"<code>get_config_from_databricks(catalog, schema, table, **kwargs)</code>","text":"<p>Retrieves configuration data from a Databricks table and returns it as a list of dictionaries.</p> <p>Parameters:</p> Name Type Description Default <code>catalog</code> <code>Optional[str]</code> <p>The catalog name in Databricks. If provided, it will be included in the table's full path.</p> required <code>schema</code> <code>Optional[str]</code> <p>The schema name in Databricks. If provided, it will be included in the table's full path.</p> required <code>table</code> <code>str</code> <p>The name of the table to retrieve data from.</p> required <code>**kwargs</code> <p>Additional keyword arguments (currently unused).</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List[Dict[str, Any]]: A list of dictionaries, where each dictionary represents a row of data from the table.</p> Source code in <code>sumeh/services/config.py</code> <pre><code>def get_config_from_databricks(catalog: Optional[str], schema: Optional[str], table: str, **kwargs) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Retrieves configuration data from a Databricks table and returns it as a list of dictionaries.\n\n    Args:\n        catalog (Optional[str]): The catalog name in Databricks. If provided, it will be included in the table's full path.\n        schema (Optional[str]): The schema name in Databricks. If provided, it will be included in the table's full path.\n        table (str): The name of the table to retrieve data from.\n        **kwargs: Additional keyword arguments (currently unused).\n\n    Returns:\n        List[Dict[str, Any]]: A list of dictionaries, where each dictionary represents a row of data from the table.\n    \"\"\"\n    from pyspark.sql import SparkSession\n\n    spark = SparkSession.builder.getOrCreate()\n    if catalog and schema:\n        full = f\"{catalog}.{schema}.{table}\"\n    elif schema:\n        full = f\"{schema}.{table}\"\n    else:\n        full = table\n    if 'query' in kwargs.keys():\n        df = spark.sql(f\"select * from {full} where {kwargs['query']}\")\n    else:\n        df = spark.table(full)\n    return [row.asDict() for row in df.collect()]\n</code></pre>"},{"location":"api/services/services-config/#sumeh.services.config.get_config_from_duckdb","title":"<code>get_config_from_duckdb(db_path, table=None, query=None, conn=None)</code>","text":"<p>Retrieve configuration data from a DuckDB database.</p> <p>This function fetches data from a DuckDB database either by executing a custom SQL query or by selecting all rows from a specified table. The data is then parsed into a list of dictionaries.</p> <p>Parameters:</p> Name Type Description Default <code>db_path</code> <code>str</code> <p>The path to the DuckDB database file.</p> required <code>table</code> <code>str</code> <p>The name of the table to fetch data from. Defaults to None.</p> <code>None</code> <code>query</code> <code>str</code> <p>A custom SQL query to execute. Defaults to None.</p> <code>None</code> <code>conn</code> <p>A valid DuckDB connection object.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List[Dict[str, Any]]: A list of dictionaries representing the fetched data.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If neither <code>table</code> nor <code>query</code> is provided, or if a valid <code>conn</code> is not supplied.</p> Example <p>import duckdb conn = duckdb.connect('my_db.duckdb') config = get_config_from_duckdb('my_db.duckdb', table='rules', conn=conn)</p> Source code in <code>sumeh/services/config.py</code> <pre><code>def get_config_from_duckdb(db_path: str, table: str = None, query: str = None, conn=None) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Retrieve configuration data from a DuckDB database.\n\n    This function fetches data from a DuckDB database either by executing a custom SQL query\n    or by selecting all rows from a specified table. The data is then parsed into a list of\n    dictionaries.\n\n    Args:\n        db_path (str): The path to the DuckDB database file.\n        table (str, optional): The name of the table to fetch data from. Defaults to None.\n        query (str, optional): A custom SQL query to execute. Defaults to None.\n        conn: A valid DuckDB connection object.\n\n    Returns:\n        List[Dict[str, Any]]: A list of dictionaries representing the fetched data.\n\n    Raises:\n        ValueError: If neither `table` nor `query` is provided, or if a valid `conn` is not supplied.\n\n    Example:\n        &gt;&gt;&gt; import duckdb\n        &gt;&gt;&gt; conn = duckdb.connect('my_db.duckdb')\n        &gt;&gt;&gt; config = get_config_from_duckdb('my_db.duckdb', table='rules', conn=conn)\n    \"\"\"\n\n    if query:\n        df = conn.execute(query).fetchdf()\n    elif table:\n        df = conn.execute(f\"SELECT * FROM {table}\").fetchdf()\n    else:\n        raise ValueError(\n            \"DuckDB configuration requires:\\n\"\n            \"1. Either a `table` name or custom `query`\\n\"\n            \"2. A valid database `conn` connection object\\n\"\n            \"Example: get_config('duckdb', table='rules', conn=duckdb.connect('my_db.duckdb'))\"\n        )\n\n    return __parse_data(df.to_dict(orient=\"records\"))\n</code></pre>"},{"location":"api/services/services-config/#sumeh.services.config.get_config_from_glue_data_catalog","title":"<code>get_config_from_glue_data_catalog(glue_context, database_name, table_name, query=None)</code>","text":"<p>Retrieves configuration data from AWS Glue Data Catalog.</p> <p>Parameters:</p> Name Type Description Default <code>glue_context</code> <p>An instance of <code>GlueContext</code>.</p> required <code>database_name</code> <code>str</code> <p>Glue database name.</p> required <code>table_name</code> <code>str</code> <p>Glue table name.</p> required <code>query</code> <code>Optional[str]</code> <p>Custom SQL query to fetch data (if provided).</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Dict[str, str]]</code> <p>List[Dict[str, str]]: A list of dictionaries representing the parsed configuration data.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If there is an error querying Glue Data Catalog.</p> Source code in <code>sumeh/services/config.py</code> <pre><code>def get_config_from_glue_data_catalog(\n    glue_context, database_name: str, table_name: str, query: Optional[str] = None\n) -&gt; List[Dict[str, str]]:\n    \"\"\"\n    Retrieves configuration data from AWS Glue Data Catalog.\n\n    Args:\n        glue_context: An instance of `GlueContext`.\n        database_name (str): Glue database name.\n        table_name (str): Glue table name.\n        query (Optional[str]): Custom SQL query to fetch data (if provided).\n\n    Returns:\n        List[Dict[str, str]]: A list of dictionaries representing the parsed configuration data.\n\n    Raises:\n        RuntimeError: If there is an error querying Glue Data Catalog.\n    \"\"\"\n    from awsglue.context import GlueContext\n\n    if not isinstance(glue_context, GlueContext):\n        raise ValueError(\"The provided context is not a valid GlueContext.\")\n\n    spark = glue_context.spark_session\n\n    try:\n        dynamic_frame = glue_context.create_dynamic_frame.from_catalog(\n            database=database_name, table_name=table_name\n        )\n\n        data_frame = dynamic_frame.toDF()\n\n        if query:\n            data_frame.createOrReplaceTempView(\"table_name\")\n            data_frame = spark.sql(query)\n\n        data_dict = [row.asDict() for row in data_frame.collect()]\n\n        return __parse_data(data_dict)\n\n    except Exception as e:\n        raise RuntimeError(\n            f\"Error occurred while querying Glue Data Catalog: {e}\"\n        ) from e\n</code></pre>"},{"location":"api/services/services-config/#sumeh.services.config.get_config_from_mysql","title":"<code>get_config_from_mysql(connection=None, host=None, user=None, password=None, database=None, port=3306, schema=None, table=None, query=None)</code>","text":"<p>Retrieves configuration data from a MySQL database.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Optional</code> <p>An existing MySQL connection object.</p> <code>None</code> <code>host</code> <code>Optional[str]</code> <p>Host of the MySQL server.</p> <code>None</code> <code>user</code> <code>Optional[str]</code> <p>Username to connect to MySQL.</p> <code>None</code> <code>password</code> <code>Optional[str]</code> <p>Password for the MySQL user.</p> <code>None</code> <code>database</code> <code>Optional[str]</code> <p>Database name to query.</p> <code>None</code> <code>port</code> <code>Optional[int]</code> <p>The port for the MySQL connection (default is 3306).</p> <code>3306</code> <code>schema</code> <code>Optional[str]</code> <p>Schema name if query is not provided.</p> <code>None</code> <code>table</code> <code>Optional[str]</code> <p>Table name if query is not provided.</p> <code>None</code> <code>query</code> <code>Optional[str]</code> <p>Custom SQL query to fetch data (if not provided, <code>schema</code> and <code>table</code> must be given).</p> <code>None</code> <p>Returns:</p> Type Description <p>List[Dict[str, Any]]: A list of dictionaries representing the parsed configuration data.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If neither <code>query</code> nor both <code>schema</code> and <code>table</code> are provided.</p> <code>ConnectionError</code> <p>If there is an error connecting to MySQL.</p> <code>RuntimeError</code> <p>If there is an error executing the query or processing the data.</p> Source code in <code>sumeh/services/config.py</code> <pre><code>def get_config_from_mysql(\n    connection: Optional = None,\n    host: Optional[str] = None,\n    user: Optional[str] = None,\n    password: Optional[str] = None,\n    database: Optional[str] = None,\n    port: Optional[int] = 3306,\n    schema: Optional[str] = None,\n    table: Optional[str] = None,\n    query: Optional[str] = None,\n):\n    \"\"\"\n    Retrieves configuration data from a MySQL database.\n\n    Args:\n        connection (Optional): An existing MySQL connection object.\n        host (Optional[str]): Host of the MySQL server.\n        user (Optional[str]): Username to connect to MySQL.\n        password (Optional[str]): Password for the MySQL user.\n        database (Optional[str]): Database name to query.\n        port (Optional[int]): The port for the MySQL connection (default is 3306).\n        schema (Optional[str]): Schema name if query is not provided.\n        table (Optional[str]): Table name if query is not provided.\n        query (Optional[str]): Custom SQL query to fetch data (if not provided, `schema` and `table` must be given).\n\n    Returns:\n        List[Dict[str, Any]]: A list of dictionaries representing the parsed configuration data.\n\n    Raises:\n        ValueError: If neither `query` nor both `schema` and `table` are provided.\n        ConnectionError: If there is an error connecting to MySQL.\n        RuntimeError: If there is an error executing the query or processing the data.\n    \"\"\"\n    import mysql.connector\n    import pandas as pd\n\n    if query is None and (schema is None or table is None):\n        raise ValueError(\n            \"You must provide either a 'query' or both 'schema' and 'table'.\"\n        )\n\n    if query is None:\n        query = f\"SELECT * FROM {schema}.{table}\"\n\n    try:\n        connection = connection or __create_connection(\n            mysql.connector.connect, host, user, password, database, port\n        )\n        data = pd.read_sql(query, connection)\n        data_dict = data.to_dict(orient=\"records\")\n        return __parse_data(data_dict)\n\n    except mysql.connector.Error as e:\n        raise ConnectionError(f\"Error connecting to MySQL database: {e}\")\n\n    except Exception as e:\n        raise RuntimeError(f\"Error executing the query or processing data: {e}\")\n\n    finally:\n        if connection and host is not None:\n            connection.close()\n</code></pre>"},{"location":"api/services/services-config/#sumeh.services.config.get_config_from_postgresql","title":"<code>get_config_from_postgresql(connection=None, host=None, user=None, password=None, database=None, port=5432, schema=None, table=None, query=None)</code>","text":"<p>Retrieves configuration data from a PostgreSQL database.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Optional</code> <p>An existing PostgreSQL connection object.</p> <code>None</code> <code>host</code> <code>Optional[str]</code> <p>Host of the PostgreSQL server.</p> <code>None</code> <code>user</code> <code>Optional[str]</code> <p>Username to connect to PostgreSQL.</p> <code>None</code> <code>password</code> <code>Optional[str]</code> <p>Password for the PostgreSQL user.</p> <code>None</code> <code>database</code> <code>Optional[str]</code> <p>Database name to query.</p> <code>None</code> <code>port</code> <code>Optional[int]</code> <p>The port for the PostgreSQL connection (default is 5432).</p> <code>5432</code> <code>schema</code> <code>Optional[str]</code> <p>Schema name if query is not provided.</p> <code>None</code> <code>table</code> <code>Optional[str]</code> <p>Table name if query is not provided.</p> <code>None</code> <code>query</code> <code>Optional[str]</code> <p>Custom SQL query to fetch data (if not provided, <code>schema</code> and <code>table</code> must be given).</p> <code>None</code> <p>Returns:</p> Type Description <code>list[dict]</code> <p>List[Dict[str, Any]]: A list of dictionaries representing the parsed configuration data.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If neither <code>query</code> nor both <code>schema</code> and <code>table</code> are provided.</p> <code>ConnectionError</code> <p>If there is an error connecting to PostgreSQL.</p> <code>RuntimeError</code> <p>If there is an error executing the query or processing the data.</p> Source code in <code>sumeh/services/config.py</code> <pre><code>def get_config_from_postgresql(\n    connection: Optional = None,\n    host: Optional[str] = None,\n    user: Optional[str] = None,\n    password: Optional[str] = None,\n    database: Optional[str] = None,\n    port: Optional[int] = 5432,\n    schema: Optional[str] = None,\n    table: Optional[str] = None,\n    query: Optional[str] = None,\n) -&gt; list[dict]:\n    \"\"\"\n    Retrieves configuration data from a PostgreSQL database.\n\n    Args:\n        connection (Optional): An existing PostgreSQL connection object.\n        host (Optional[str]): Host of the PostgreSQL server.\n        user (Optional[str]): Username to connect to PostgreSQL.\n        password (Optional[str]): Password for the PostgreSQL user.\n        database (Optional[str]): Database name to query.\n        port (Optional[int]): The port for the PostgreSQL connection (default is 5432).\n        schema (Optional[str]): Schema name if query is not provided.\n        table (Optional[str]): Table name if query is not provided.\n        query (Optional[str]): Custom SQL query to fetch data (if not provided, `schema` and `table` must be given).\n\n    Returns:\n        List[Dict[str, Any]]: A list of dictionaries representing the parsed configuration data.\n\n    Raises:\n        ValueError: If neither `query` nor both `schema` and `table` are provided.\n        ConnectionError: If there is an error connecting to PostgreSQL.\n        RuntimeError: If there is an error executing the query or processing the data.\n    \"\"\"\n    import psycopg2\n    import pandas as pd\n\n    if query is None and (schema is None or table is None):\n        raise ValueError(\n            \"You must provide either a 'query' or both 'schema' and 'table'.\"\n        )\n\n    if query is None:\n        query = f\"SELECT * FROM {schema}.{table}\"\n\n    try:\n        connection = connection or __create_connection(\n            psycopg2.connect, host, user, password, database, port\n        )\n\n        data = pd.read_sql(query, connection)\n\n        data_dict = data.to_dict(orient=\"records\")\n        return __parse_data(data_dict)\n\n    except psycopg2.Error as e:\n        raise ConnectionError(f\"Error connecting to PostgreSQL database: {e}\")\n\n    except Exception as e:\n        raise RuntimeError(f\"Error executing the query or processing data: {e}\")\n\n    finally:\n        if connection and host is not None:\n            connection.close()\n</code></pre>"},{"location":"api/services/services-config/#sumeh.services.config.get_config_from_s3","title":"<code>get_config_from_s3(s3_path, delimiter=',')</code>","text":"<p>Retrieves configuration data from a CSV file stored in an S3 bucket.</p> <p>Parameters:</p> Name Type Description Default <code>s3_path</code> <code>str</code> <p>The S3 path to the CSV file.</p> required <code>delimiter</code> <code>Optional[str]</code> <p>The delimiter used in the CSV file (default is \",\").</p> <code>','</code> <p>Returns:</p> Type Description <p>List[Dict[str, Any]]: A list of dictionaries representing the parsed configuration data.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If there is an error reading or processing the S3 file.</p> Source code in <code>sumeh/services/config.py</code> <pre><code>def get_config_from_s3(s3_path: str, delimiter: Optional[str] = \",\"):\n    \"\"\"\n    Retrieves configuration data from a CSV file stored in an S3 bucket.\n\n    Args:\n        s3_path (str): The S3 path to the CSV file.\n        delimiter (Optional[str]): The delimiter used in the CSV file (default is \",\").\n\n    Returns:\n        List[Dict[str, Any]]: A list of dictionaries representing the parsed configuration data.\n\n    Raises:\n        RuntimeError: If there is an error reading or processing the S3 file.\n    \"\"\"\n    try:\n        file_content = __read_s3_file(s3_path)\n        data = __read_csv_file(file_content, delimiter)\n        return __parse_data(data)\n\n    except Exception as e:\n        raise RuntimeError(f\"Error reading or processing the S3 file: {e}\")\n</code></pre>"},{"location":"api/services/services-config/#sumeh.services.config.get_schema_from_bigquery","title":"<code>get_schema_from_bigquery(project_id, dataset_id, table_id, credentials_path=None)</code>","text":"Source code in <code>sumeh/services/config.py</code> <pre><code>def get_schema_from_bigquery(\n    project_id: str, dataset_id: str, table_id: str, credentials_path: str = None\n) -&gt; List[Dict[str, Any]]:\n    from google.cloud import bigquery\n\n    client = bigquery.Client(\n        project=project_id,\n        credentials=(\n            None\n            if credentials_path is None\n            else bigquery.Credentials.from_service_account_file(credentials_path)\n        ),\n    )\n    table = client.get_table(f\"{project_id}.{dataset_id}.{table_id}\")\n    return [\n        {\n            \"field\": schema_field.name,\n            \"data_type\": schema_field.field_type.lower(),\n            \"nullable\": schema_field.is_nullable,\n            \"max_length\": None,\n        }\n        for schema_field in table.schema\n    ]\n</code></pre>"},{"location":"api/services/services-config/#sumeh.services.config.get_schema_from_csv","title":"<code>get_schema_from_csv(file_path, delimiter=',', sample_size=1000)</code>","text":"Source code in <code>sumeh/services/config.py</code> <pre><code>def get_schema_from_csv(\n    file_path: str, delimiter: str = \",\", sample_size: int = 1_000\n) -&gt; List[Dict[str, Any]]:\n    import csv\n\n    cols: Dict[str, Dict[str, Any]] = {}\n    with open(file_path, newline=\"\", encoding=\"utf-8\") as f:\n        reader = csv.DictReader(f, delimiter=delimiter)\n        for idx, row in enumerate(reader):\n            if idx &gt;= sample_size:\n                break\n            for name, raw in row.items():\n                info = cols.setdefault(\n                    name,\n                    {\n                        \"field\": name,\n                        \"nullable\": False,\n                        \"max_length\": 0,\n                        \"type_hints\": set(),\n                    },\n                )\n                if raw == \"\" or raw is None:\n                    info[\"nullable\"] = True\n                    continue\n                info[\"max_length\"] = max(info[\"max_length\"], len(raw))\n                info[\"type_hints\"].add(infer_basic_type(raw))\n\n    out: List[Dict[str, Any]] = []\n    for info in cols.values():\n        hints = info[\"type_hints\"]\n        if hints == {\"integer\"}:\n            dtype = \"integer\"\n        elif hints &lt;= {\"integer\", \"float\"}:\n            dtype = \"float\"\n        elif hints == {\"date\"}:\n            dtype = \"date\"\n        else:\n            dtype = \"string\"\n        out.append(\n            {\n                \"field\": info[\"field\"],\n                \"data_type\": dtype,\n                \"nullable\": info[\"nullable\"],\n                \"max_length\": info[\"max_length\"] or None,\n            }\n        )\n    return out\n</code></pre>"},{"location":"api/services/services-config/#sumeh.services.config.get_schema_from_databricks","title":"<code>get_schema_from_databricks(catalog, schema, table, **kwargs)</code>","text":"Source code in <code>sumeh/services/config.py</code> <pre><code>def get_schema_from_databricks(\n    catalog: Optional[str], schema: Optional[str], table: str, **kwargs\n) -&gt; List[Dict[str, Any]]:\n    from pyspark.sql import SparkSession\n\n    spark = SparkSession.builder.getOrCreate()\n    if catalog and schema:\n        full = f\"{catalog}.{schema}.{table}\"\n    elif schema:\n        full = f\"{schema}.{table}\"\n    else:\n        full = table\n    schema = spark.table(full).schema\n    result = []\n    for f in schema.fields:\n        result.append(\n            {\n                \"field\": f.name,\n                \"data_type\": f.dataType.simpleString(),\n                \"nullable\": f.nullable,\n                \"max_length\": None,\n            }\n        )\n    return result\n</code></pre>"},{"location":"api/services/services-config/#sumeh.services.config.get_schema_from_duckdb","title":"<code>get_schema_from_duckdb(db_path, table, conn)</code>","text":"Source code in <code>sumeh/services/config.py</code> <pre><code>def get_schema_from_duckdb(db_path: str, table: str, conn) -&gt; List[Dict[str, Any]]:\n    df = conn.execute(f\"PRAGMA table_info('{table}')\").fetchdf()\n    return [\n        {\n            \"field\": row[\"name\"],\n            \"data_type\": row[\"type\"].lower(),\n            \"nullable\": not bool(row[\"notnull\"]),\n            \"max_length\": None,\n        }\n        for _, row in df.iterrows()\n    ]\n</code></pre>"},{"location":"api/services/services-config/#sumeh.services.config.get_schema_from_glue","title":"<code>get_schema_from_glue(glue_context, database_name, table_name)</code>","text":"Source code in <code>sumeh/services/config.py</code> <pre><code>def get_schema_from_glue(\n    glue_context, database_name: str, table_name: str\n) -&gt; List[Dict[str, Any]]:\n    from awsglue.context import GlueContext\n\n    if not isinstance(glue_context, GlueContext):\n        raise ValueError(\"Informe um GlueContext v\u00e1lido\")\n    df = glue_context.spark_session.read.table(f\"{database_name}.{table_name}\")\n    return [\n        {\n            \"field\": field.name,\n            \"data_type\": field.dataType.simpleString(),\n            \"nullable\": field.nullable,\n            \"max_length\": None,\n        }\n        for field in df.schema.fields\n    ]\n</code></pre>"},{"location":"api/services/services-config/#sumeh.services.config.get_schema_from_mysql","title":"<code>get_schema_from_mysql(host, user, password, database, table, port=3306)</code>","text":"Source code in <code>sumeh/services/config.py</code> <pre><code>def get_schema_from_mysql(\n    host: str, user: str, password: str, database: str, table: str, port: int = 3306\n) -&gt; List[Dict[str, Any]]:\n    import mysql.connector\n\n    conn = mysql.connector.connect(\n        host=host, user=user, password=password, database=database, port=port\n    )\n    cursor = conn.cursor(dictionary=True)\n    cursor.execute(\n        f\"\"\"\n        SELECT \n            column_name AS field,\n            data_type,\n            is_nullable = 'YES' AS nullable,\n            character_maximum_length AS max_length\n        FROM information_schema.columns\n        WHERE table_schema = %s AND table_name = %s\n    \"\"\",\n        (database, table),\n    )\n    schema = cursor.fetchall()\n    cursor.close()\n    conn.close()\n    return schema\n</code></pre>"},{"location":"api/services/services-config/#sumeh.services.config.get_schema_from_postgresql","title":"<code>get_schema_from_postgresql(host, user, password, database, table, port=5432)</code>","text":"Source code in <code>sumeh/services/config.py</code> <pre><code>def get_schema_from_postgresql(\n    host: str, user: str, password: str, database: str, table: str, port: int = 5432\n) -&gt; List[Dict[str, Any]]:\n    import psycopg2\n\n    conn = psycopg2.connect(\n        host=host, user=user, password=password, dbname=database, port=port\n    )\n    cursor = conn.cursor()\n    cursor.execute(\n        f\"\"\"\n        SELECT\n            column_name AS field,\n            data_type,\n            is_nullable = 'YES' AS nullable,\n            character_maximum_length AS max_length\n        FROM information_schema.columns\n        WHERE table_schema = 'public' AND table_name = %s\n    \"\"\",\n        (table,),\n    )\n    cols = cursor.fetchall()\n    cursor.close()\n    conn.close()\n    return [\n        {\"field\": f, \"data_type\": dt, \"nullable\": nl, \"max_length\": ml}\n        for f, dt, nl, ml in cols\n    ]\n</code></pre>"},{"location":"api/services/services-config/#sumeh.services.config.get_schema_from_s3","title":"<code>get_schema_from_s3(s3_path, **kwargs)</code>","text":"Source code in <code>sumeh/services/config.py</code> <pre><code>def get_schema_from_s3(s3_path: str, **kwargs) -&gt; List[Dict[str, Any]]:\n\n    content = __read_s3_file(s3_path)\n    with open(\"/tmp/temp.csv\", \"w\") as f:\n        f.write(content)\n    return get_schema_from_csv(\"/tmp/temp.csv\", **kwargs)\n</code></pre>"},{"location":"api/services/services-config/#sumeh.services.config.infer_basic_type","title":"<code>infer_basic_type(val)</code>","text":"Source code in <code>sumeh/services/config.py</code> <pre><code>def infer_basic_type(val: str) -&gt; str:\n    try:\n        int(val)\n        return \"integer\"\n    except:\n        pass\n    try:\n        float(val)\n        return \"float\"\n    except:\n        pass\n    try:\n        _ = parser.parse(val)\n        return \"date\"\n    except:\n        pass\n    return \"string\"\n</code></pre>"},{"location":"api/services/services-utils/","title":"Module <code>sumeh.services.utils</code>","text":""},{"location":"api/services/services-utils/#sumeh.services.utils.SchemaDef","title":"<code>SchemaDef = Dict[str, Any]</code>  <code>module-attribute</code>","text":""},{"location":"api/services/services-utils/#sumeh.services.utils.__compare_schemas","title":"<code>__compare_schemas(actual, expected)</code>","text":"<p>Compare two lists of schema definitions and identify discrepancies.</p> <p>Parameters:</p> Name Type Description Default <code>actual</code> <code>List[SchemaDef]</code> <p>The list of actual schema definitions.</p> required <code>expected</code> <code>List[SchemaDef]</code> <p>The list of expected schema definitions.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>Tuple[bool, List[Tuple[str, str]]]: A tuple where the first element is a boolean indicating </p> <code>List[Tuple[str, str]]</code> <p>whether the schemas match (True if they match, False otherwise), and the second element </p> <code>Tuple[bool, List[Tuple[str, str]]]</code> <p>is a list of tuples describing the discrepancies. Each tuple contains: - The field name (str). - A description of the discrepancy (str), such as \"missing\", \"type mismatch\",    \"nullable but expected non-nullable\", or \"extra column\".</p> Notes <ul> <li>A field is considered \"missing\" if it exists in the expected schema but not in the actual schema.</li> <li>A \"type mismatch\" occurs if the data type of a field in the actual schema does not match    the expected data type.</li> <li>A field is considered \"nullable but expected non-nullable\" if it is nullable in the actual    schema but not nullable in the expected schema.</li> <li>An \"extra column\" is a field that exists in the actual schema but not in the expected schema.</li> </ul> Source code in <code>sumeh/services/utils.py</code> <pre><code>def __compare_schemas(actual: List[SchemaDef], expected: List[SchemaDef],) -&gt; Tuple[bool, List[Tuple[str, str]]]:\n    \"\"\"\n    Compare two lists of schema definitions and identify discrepancies.\n\n    Args:\n        actual (List[SchemaDef]): The list of actual schema definitions.\n        expected (List[SchemaDef]): The list of expected schema definitions.\n\n    Returns:\n        Tuple[bool, List[Tuple[str, str]]]: A tuple where the first element is a boolean indicating \n        whether the schemas match (True if they match, False otherwise), and the second element \n        is a list of tuples describing the discrepancies. Each tuple contains:\n            - The field name (str).\n            - A description of the discrepancy (str), such as \"missing\", \"type mismatch\", \n              \"nullable but expected non-nullable\", or \"extra column\".\n\n    Notes:\n        - A field is considered \"missing\" if it exists in the expected schema but not in the actual schema.\n        - A \"type mismatch\" occurs if the data type of a field in the actual schema does not match \n          the expected data type.\n        - A field is considered \"nullable but expected non-nullable\" if it is nullable in the actual \n          schema but not nullable in the expected schema.\n        - An \"extra column\" is a field that exists in the actual schema but not in the expected schema.\n    \"\"\"\n\n    exp_map = {c[\"field\"]: c for c in expected}\n    act_map = {c[\"field\"]: c for c in actual}\n\n    erros: List[Tuple[str, str]] = []\n\n    for fld, exp in exp_map.items():\n        if fld not in act_map:\n            erros.append((fld, \"missing\"))\n            continue\n        act = act_map[fld]\n        if act[\"data_type\"] != exp[\"data_type\"]:\n            erros.append(\n                (\n                    fld,\n                    f\"type mismatch (got {act['data_type']}, expected {exp['data_type']})\",\n                )\n            )\n\n        if act[\"nullable\"] and not exp[\"nullable\"]:\n            erros.append((fld, \"nullable but expected non-nullable\"))\n\n        if exp.get(\"max_length\") is not None:\n            pass\n\n    # 2. campos extras (se quiser)\n    extras = set(act_map) - set(exp_map)\n    for fld in extras:\n        erros.append((fld, \"extra column\"))\n\n    return len(erros) == 0, erros\n</code></pre>"},{"location":"api/services/services-utils/#sumeh.services.utils.__convert_value","title":"<code>__convert_value(value)</code>","text":"<p>Converts the provided value to the appropriate type (date, float, or int).</p> <p>Depending on the format of the input value, it will be converted to a datetime object, a floating-point number (float), or an integer (int).</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>The value to be converted, represented as a string.</p> required <p>Returns:</p> Type Description <p>Union[datetime, float, int]: The converted value, which can be a datetime object, float, or int.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the value does not match an expected format.</p> Source code in <code>sumeh/services/utils.py</code> <pre><code>def __convert_value(value):\n    \"\"\"\n    Converts the provided value to the appropriate type (date, float, or int).\n\n    Depending on the format of the input value, it will be converted to a datetime object,\n    a floating-point number (float), or an integer (int).\n\n    Args:\n        value (str): The value to be converted, represented as a string.\n\n    Returns:\n        Union[datetime, float, int]: The converted value, which can be a datetime object, float, or int.\n\n    Raises:\n        ValueError: If the value does not match an expected format.\n    \"\"\"\n    from datetime import datetime\n\n    value = value.strip()\n    try:\n        if \"-\" in value:\n            return datetime.strptime(value, \"%Y-%m-%d\")\n        else:\n            return datetime.strptime(value, \"%d/%m/%Y\")\n    except ValueError:\n        if \".\" in value:\n            return float(value)\n        return int(value)\n</code></pre>"},{"location":"api/services/services-utils/#sumeh.services.utils.__extract_params","title":"<code>__extract_params(rule)</code>","text":"Source code in <code>sumeh/services/utils.py</code> <pre><code>def __extract_params(rule: dict) -&gt; tuple:\n    rule_name = rule[\"check_type\"]\n    field = rule[\"field\"]\n    raw_value = rule.get(\"value\")\n    if isinstance(raw_value, str) and raw_value not in (None, \"\", \"NULL\"):\n        try:\n            value = __convert_value(raw_value)\n        except ValueError:\n            value = raw_value\n    else:\n        value = raw_value\n    value = value if value not in (None, \"\", \"NULL\") else \"\"\n    return field, rule_name, value\n</code></pre>"},{"location":"api/services/services-utils/#sumeh.services.utils.__parse_databricks_uri","title":"<code>__parse_databricks_uri(uri)</code>","text":"<p>Parses a Databricks URI into its catalog, schema, and table components.</p> <p>The URI is expected to follow the format <code>protocol://catalog.schema.table</code> or  <code>protocol://schema.table</code>. If the catalog is not provided, it will be set to <code>None</code>.  If the schema is not provided, the current database from the active Spark session  will be used.</p> <p>Parameters:</p> Name Type Description Default <code>uri</code> <code>str</code> <p>The Databricks URI to parse.</p> required <p>Returns:</p> Type Description <code>Dict[str, Optional[str]]</code> <p>Dict[str, Optional[str]]: A dictionary containing the parsed components: - \"catalog\" (Optional[str]): The catalog name, or <code>None</code> if not provided. - \"schema\" (Optional[str]): The schema name, or the current database if not provided. - \"table\" (Optional[str]): The table name.</p> Source code in <code>sumeh/services/utils.py</code> <pre><code>def __parse_databricks_uri(uri: str) -&gt; Dict[str, Optional[str]]:\n    \"\"\"\n    Parses a Databricks URI into its catalog, schema, and table components.\n\n    The URI is expected to follow the format `protocol://catalog.schema.table` or \n    `protocol://schema.table`. If the catalog is not provided, it will be set to `None`. \n    If the schema is not provided, the current database from the active Spark session \n    will be used.\n\n    Args:\n        uri (str): The Databricks URI to parse.\n\n    Returns:\n        Dict[str, Optional[str]]: A dictionary containing the parsed components:\n            - \"catalog\" (Optional[str]): The catalog name, or `None` if not provided.\n            - \"schema\" (Optional[str]): The schema name, or the current database if not provided.\n            - \"table\" (Optional[str]): The table name.\n    \"\"\"\n    _, path = uri.split(\"://\", 1)\n    parts = path.split(\".\")\n    if len(parts) == 3:\n        catalog, schema, table = parts\n    elif len(parts) == 2:\n        catalog, schema, table = None, parts[0], parts[1]\n    else:\n        from pyspark.sql import SparkSession\n\n        spark = SparkSession.builder.getOrCreate()\n        catalog = None\n        schema = spark.catalog.currentDatabase()\n        table = parts[0]\n    return {\"catalog\": catalog, \"schema\": schema, \"table\": table}\n</code></pre>"},{"location":"api/services/services-utils/#sumeh.services.utils.__transform_date_format_in_pattern","title":"<code>__transform_date_format_in_pattern(date_format)</code>","text":"Source code in <code>sumeh/services/utils.py</code> <pre><code>def __transform_date_format_in_pattern(date_format):\n    date_patterns = {\n        'DD': '(0[1-9]|[12][0-9]|3[01])',\n        'MM': '(0[1-9]|1[012])',\n        'YYYY': '(19|20)\\d\\d',\n        'YY': '\\d\\d',\n        ' ': '\\s',\n        '.': '\\.'\n    }\n\n    date_pattern = date_format\n    for single_format, pattern in date_patterns.items():\n        date_pattern = date_pattern.replace(single_format, pattern)\n\n    return date_pattern\n</code></pre>"},{"location":"api/services/services/","title":"Module <code>sumeh.services</code> - Overview","text":""},{"location":"api/services/services/#configuration-and-utilities-services-package-overview","title":"Configuration and Utilities Services Package \u2013 Overview","text":"<p>This package consolidates all the logic needed to load, parse, and infer both Data Quality rules (configurations) and schema metadata from multiple sources\u2014while also providing a lightweight web interface for initial interaction.</p> <p>\ud83d\udce6 File Structure</p> <pre><code>sumeh/services/\n\u251c\u2500\u2500 config.py       # \u21e8 loading and parsing of configurations and schemas\n\u251c\u2500\u2500 utils.py        # \u21e8 generic helper functions (conversion, comparison, extraction)\n\u2514\u2500\u2500 index.html      # \u21e8 static interface for configuration via browser\n</code></pre>"}]}