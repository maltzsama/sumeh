{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"home","text":""},{"location":"#sumeh-dq","title":"Sumeh DQ","text":"<p>Sumeh is a unified data quality validation framework supporting multiple backends (PySpark, Dask, Polars, DuckDB) with centralized rule configuration.</p>"},{"location":"#installation","title":"\ud83d\ude80 Installation","text":"<pre><code># Using pip\npip install sumeh\n\n# Or with conda-forge\nconda install -c conda-forge sumeh\n</code></pre> <p>Prerequisites: - Python 3.10+ - One or more of: <code>pyspark</code>, <code>dask[dataframe]</code>, <code>polars</code>, <code>duckdb</code>, <code>cuallee</code></p>"},{"location":"#core-api","title":"\ud83d\udd0d Core API","text":"<ul> <li><code>report(df, rules, name=\"Quality Check\")</code>   Apply your validation rules over any DataFrame (Pandas, Spark, Dask, Polars, or DuckDB).  </li> <li><code>validate(df, rules)</code> (per-engine)   Returns a DataFrame with a <code>dq_status</code> column listing violations.  </li> <li><code>summarize(qc_df, rules, total_rows)</code> (per-engine)   Consolidates violations into a summary report.</li> </ul>"},{"location":"#supported-engines","title":"\u2699\ufe0f Supported Engines","text":"<p>Each engine implements the <code>validate()</code> + <code>summarize()</code> pair:</p> Engine Module Status PySpark <code>sumeh.engines.pyspark_engine</code> \u2705 Fully implemented Dask <code>sumeh.engines.dask_engine</code> \u2705 Fully implemented Polars <code>sumeh.engines.polars_engine</code> \u2705 Fully implemented DuckDB <code>sumeh.engines.duckdb_engine</code> \u2705 Fully implemented Pandas <code>sumeh.engines.pandas_engine</code> \u2705 Fully implemented BigQuery (SQL) <code>sumeh.engines.bigquery_engine</code> \ud83d\udd27 Stub implementation"},{"location":"#configuration-sources","title":"\ud83c\udfd7 Configuration Sources","text":"<p>Load rules from CSV</p> <pre><code>from sumeh import get_rules_config\n\nrules = get_rules_config(\"rules.csv\", delimiter=\";\")\n</code></pre> <p>Load rules from S3 <pre><code>from sumeh import get_rules_config\nbucket_name = \"&lt;bucket&gt;\"\npath = \"&lt;path&gt;\"\nfile_name = \"&lt;file_name&gt;\"\n\nrules = get_rules_config(f\"s3://{bucket_name}/{path}/{file_name}\", delimiter=\";\")\n</code></pre></p> <p>Load rules from MySQL <pre><code>from sumeh import get_rules_config\n\nhost = \"&lt;host&gt;\"\nport = \"&lt;port&gt;\" #optional\nuser = \"&lt;username&gt;\"\npassword = \"&lt;passwd&gt;\"\ndatabase = \"&lt;database&gt;\"\ntable = \"&lt;rules_table&gt;\"\nquery = \"&lt;select * from rules&gt;\" # optional\n\nrules = get_rules_config(\n    source=\"mysql\", \n    host=host, \n    user=user, \n    password=password, \n    database=database, \n    table=table, \n    query=query\n)\n\n# or using Mysql Connector\nimport mysql.connector\nconn = mysql.connector.connect(\n    host=host,\n    port=port,\n    database=database,\n    user=user,\n    password=password\n)\n\nrules = get_rules_config(source=\"mysql\", connection=conn, query=query)\n</code></pre></p> <p>Load rules from Postgres <pre><code>from sumeh import get_rules_config\n\nhost = \"&lt;host&gt;\"\nport = \"&lt;port&gt;\" #optional\nuser = \"&lt;username&gt;\"\npassword = \"&lt;passwd&gt;\"\ndatabase = \"&lt;database&gt;\"\nschema = \"&lt;public&gt;\"\ntable = \"&lt;rules_table&gt;\"\nquery = \"&lt;select * from rules&gt;\" # optional\n\nrules_pgsql = get_rules_config(\n    source=\"postgresql\", \n    host=host, user=user, \n    password=password, \n    database=database, \n    schema=schema, \n    table=table, \n    query=query\n)\n\n# Or using the PostgreSQL Connector\nimport psycopg2\n\nconn = psycopg2.connect(\n            host=host,\n            database=database,\n            user=user,\n            password=password\n)\n\nrules_pgsql = get_rules_config(source=\"postgresql\", connection=conn, query=query)\n</code></pre></p> <p>Load rules from AWS Glue Data Catalog <pre><code>from pyspark.context import SparkContext\nfrom awsglue.utils import getResolvedOptions\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n\nfrom sumeh import get_rules_config\n\nargs = getResolvedOptions(sys.argv, ['JOB_NAME'])\n\nsc = SparkContext()\nglue_context = GlueContext(sc)\nspark = glue_context.spark_session\njob = Job(glue_context)\njob.init(args['JOB_NAME'], args)\n\n\ndatabase_name = \"&lt;database&gt;\"\ntable_name = \"&lt;table&gt;\"\n\nrules = get_rules_config(\n    source=\"glue\",\n    glue_context=glue_context,\n    database_name=database_name,\n    table_name=table_name\n)\n\njob.commit()\n</code></pre></p> <p>Load rules from Databricks Data Catalog <pre><code>from sumeh import get_rules_config\n\ncatalog = \"&lt;catalog&gt;\"\ndatabase_name = \"&lt;database&gt;\"\ntable_name = \"&lt;table&gt;\"\nquery = \"&lt;query&gt;\" # OPTIONAL\n\nrules = get_rules_config(\n    \"databricks\", \n    spark=spark, \n    catalog=\"sumeh_demo\", \n    schema=\"sample_data\", \n    table=\"rules\",\n    query=query\n)\n</code></pre></p>"},{"location":"#typical-workflow","title":"\ud83c\udfc3\u200d\u2642\ufe0f Typical Workflow","text":"<pre><code>from sumeh import report\nfrom sumeh.engines.polars_engine import validate, summarize\nimport polars as pl\n\n# 1) Load data\ndf = pl.read_csv(\"data.csv\")\n\n# 2) Run validation\nresult, result_raw = validate(df, rules)\n\n# 3) Generate summary\ntotal = df.height\nreport = summarize(result_raw, rules, total_rows=total)\nprint(report)\n</code></pre> <p>Or simply:</p> <pre><code>from sumeh import report\n\nreport = report(df, rules)\n</code></pre>"},{"location":"#rule-definition-example","title":"\ud83d\udccb Rule Definition Example","text":"<pre><code>{\n  \"field\": \"customer_id\",\n  \"check_type\": \"is_complete\",\n  \"threshold\": 0.99,\n  \"value\": null,\n  \"execute\": true\n}\n</code></pre>"},{"location":"#supported-validation-rules","title":"Supported Validation Rules","text":""},{"location":"#numeric-checks","title":"Numeric checks","text":"Test Description is_in_millions Retains rows where the column value is less than 1,000,000 (fails the \"in millions\" criteria). is_in_billions Retains rows where the column value is less than 1,000,000,000 (fails the \"in billions\" criteria)."},{"location":"#completeness-uniqueness","title":"Completeness &amp; Uniqueness","text":"Test Description is_complete Filters rows where the column value is null. are_complete Filters rows where any of the specified columns are null. is_unique Identifies rows with duplicate values in the specified column. are_unique Identifies rows with duplicate combinations of the specified columns. is_primary_key Alias for <code>is_unique</code> (checks uniqueness of a single column). is_composite_key Alias for <code>are_unique</code> (checks combined uniqueness of multiple columns)."},{"location":"#comparison-range","title":"Comparison &amp; Range","text":"Test Description is_equal Filters rows where the column is not equal to the provided value (null-safe). is_equal_than Alias for <code>is_equal</code>. is_between Filters rows where the column value is outside the numeric range <code>[min, max]</code>. is_greater_than Filters rows where the column value is \u2264 the threshold (fails \"greater than\"). is_greater_or_equal_than Filters rows where the column value is &lt; the threshold (fails \"greater or equal\"). is_less_than Filters rows where the column value is \u2265 the threshold (fails \"less than\"). is_less_or_equal_than Filters rows where the column value is &gt; the threshold (fails \"less or equal\"). is_positive Filters rows where the column value is &lt; 0 (fails \"positive\"). is_negative Filters rows where the column value is \u2265 0 (fails \"negative\")."},{"location":"#membership-pattern","title":"Membership &amp; Pattern","text":"Test Description is_contained_in Filters rows where the column value is not in the provided list. not_contained_in Filters rows where the column value is in the provided list. has_pattern Filters rows where the column value does not match the specified regex. is_legit Filters rows where the column value is null or contains whitespace (i.e., not <code>\\S+</code>)."},{"location":"#aggregate-checks","title":"Aggregate checks","text":"Test Description has_min Returns all rows if the column's minimum value causes failure (value &lt; threshold); otherwise returns empty. has_max Returns all rows if the column's maximum value causes failure (value &gt; threshold); otherwise returns empty. has_sum Returns all rows if the column's sum causes failure (sum &gt; threshold); otherwise returns empty. has_mean Returns all rows if the column's mean causes failure (mean &gt; threshold); otherwise returns empty. has_std Returns all rows if the column's standard deviation causes failure (std &gt; threshold); otherwise returns empty. has_cardinality Returns all rows if the number of distinct values causes failure (count &gt; threshold); otherwise returns empty. has_infogain Same logic as <code>has_cardinality</code> (proxy for information gain). has_entropy Same logic as <code>has_cardinality</code> (proxy for entropy)."},{"location":"#sql-schema","title":"SQL &amp; Schema","text":"Test Description satisfies Filters rows where the SQL expression (based on <code>rule[\"value\"]</code>) is not satisfied. validate_schema Compares the DataFrame's actual schema against the expected one and returns a match flag + error list. validate Executes a list of named rules and returns two DataFrames: one with aggregated status and one with raw violations."},{"location":"#date-related-checks","title":"Date-related checks","text":"Test Description is_t_minus_1 Retains rows where the date in the column is not equal to yesterday (T\u20131). is_t_minus_2 Retains rows where the date in the column is not equal to two days ago (T\u20132). is_t_minus_3 Retains rows where the date in the column is not equal to three days ago (T\u20133). is_today Retains rows where the date in the column is not equal to today. is_yesterday Retains rows where the date in the column is not equal to yesterday. is_on_weekday Retains rows where the date in the column NOT FALLS on a weekend (fails \"weekday\"). is_on_weekend Retains rows where the date in the column NOT FALLS on a weekday (fails \"weekend\"). is_on_monday Retains rows where the date in the column is not Monday. is_on_tuesday Retains rows where the date in the column is not Tuesday. is_on_wednesday Retains rows where the date in the column is not Wednesday. is_on_thursday Retains rows where the date in the column is not Thursday. is_on_friday Retains rows where the date in the column is not Friday. is_on_saturday Retains rows where the date in the column is not Saturday. is_on_sunday Retains rows where the date in the column is not Sunday. validate_date_format Filters rows where the date doesn't match the expected format or is null. is_future_date Filters rows where the date in the column is not after today. is_past_date Filters rows where the date in the column is not before today. is_date_after Filters rows where the date in the column is not before the date provided in the rule. is_date_before Filters rows where the date in the column is not after the date provided in the rule. is_date_between Filters rows where the date in the column is not outside the range <code>[start, end]</code>. all_date_checks Alias for <code>is_past_date</code> (same logic: date before today)."},{"location":"#schema-validation","title":"Schema Validation","text":"<p>Sumeh allows you to validate your DataFrame schemas against a schema registry stored in various data sources (BigQuery, MySQL, PostgreSQL, DuckDB, Databricks, Glue, CSV, S3).</p>"},{"location":"#step-1-store-your-schema-registry","title":"Step 1: Store Your Schema Registry","text":"<p>First, create a <code>schema_registry</code> table in your data source with the following structure:</p> Column Type Description id int Auto-increment ID environment string Environment (e.g., 'prod', 'staging', 'dev') source_type string Source type (e.g., 'bigquery', 'mysql') database_name string Database/project name catalog_name string Catalog name (for Databricks) schema_name string Schema name (for PostgreSQL) table_name string Table name field string Column name data_type string Data type nullable boolean Whether column can be null max_length int Maximum length for strings comment string Description/comment created_at datetime Creation timestamp updated_at datetime Last update timestamp"},{"location":"#step-2-get-schema-configuration","title":"Step 2: Get Schema Configuration","text":"<p>Use <code>get_schema_config()</code> to retrieve the expected schema from your registry:</p>"},{"location":"#bigquery","title":"BigQuery","text":"<pre><code>from sumeh.core import get_schema_config\n\nschema = get_schema_config(\n    source=\"bigquery\",\n    project_id=\"my-project\",\n    dataset_id=\"my-dataset\",\n    table_id=\"users\",\n    environment=\"prod\"  # optional, defaults to 'prod'\n)\n</code></pre>"},{"location":"#mysql","title":"MySQL","text":"<pre><code># Option 1: Create connection internally\nschema = get_schema_config(\n    source=\"mysql\",\n    host=\"localhost\",\n    user=\"root\",\n    password=\"secret\",\n    database=\"mydb\",\n    table=\"users\",\n    environment=\"prod\"\n)\n\n# Option 2: Reuse existing connection\nimport mysql.connector\nconn = mysql.connector.connect(host=\"localhost\", user=\"root\", password=\"secret\", database=\"mydb\")\n\nschema = get_schema_config(\n    source=\"mysql\",\n    conn=conn,\n    table=\"users\",\n    environment=\"prod\"\n)\n</code></pre>"},{"location":"#postgresql","title":"PostgreSQL","text":"<pre><code># Option 1: Create connection internally\nschema = get_schema_config(\n    source=\"postgresql\",\n    host=\"localhost\",\n    user=\"postgres\",\n    password=\"secret\",\n    database=\"mydb\",\n    schema=\"public\",\n    table=\"users\",\n    environment=\"prod\"\n)\n\n# Option 2: Reuse existing connection\nimport psycopg2\nconn = psycopg2.connect(host=\"localhost\", user=\"postgres\", password=\"secret\", dbname=\"mydb\")\n\nschema = get_schema_config(\n    source=\"postgresql\",\n    conn=conn,\n    schema=\"public\",\n    table=\"users\",\n    environment=\"prod\"\n)\n</code></pre>"},{"location":"#duckdb","title":"DuckDB","text":"<pre><code>import duckdb\n\nconn = duckdb.connect(\"my_database.db\")\n\nschema = get_schema_config(\n    source=\"duckdb\",\n    conn=conn,\n    table=\"users\",\n    environment=\"prod\"\n)\n</code></pre>"},{"location":"#databricks","title":"Databricks","text":"<pre><code>from pyspark.sql import SparkSession\n\nspark = SparkSession.builder.getOrCreate()\n\nschema = get_schema_config(\n    source=\"databricks\",\n    spark=spark,\n    catalog=\"main\",\n    schema=\"default\",\n    table=\"users\",\n    environment=\"prod\"\n)\n</code></pre>"},{"location":"#aws-glue","title":"AWS Glue","text":"<pre><code>from awsglue.context import GlueContext\nfrom pyspark.context import SparkContext\n\nglueContext = GlueContext(SparkContext.getOrCreate())\n\nschema = get_schema_config(\n    source=\"glue\",\n    glue_context=glueContext,\n    database_name=\"my_database\",\n    table_name=\"users\",\n    environment=\"prod\"\n)\n</code></pre>"},{"location":"#csv","title":"CSV","text":"<pre><code>schema = get_schema_config(\n    source=\"schema_registry.csv\",\n    table=\"users\",\n    environment=\"prod\"\n)\n</code></pre>"},{"location":"#s3","title":"S3","text":"<pre><code>schema = get_schema_config(\n    source=\"s3://my-bucket/path/schema_registry.csv\",\n    table=\"users\",\n    environment=\"prod\"\n)\n</code></pre>"},{"location":"#step-3-validate-dataframe-schema","title":"Step 3: Validate DataFrame Schema","text":"<p>Once you have the expected schema, validate your DataFrame against it:</p> <pre><code>from sumeh.core import validate_schema\n\n# Load your DataFrame (example with pandas)\nimport pandas as pd\ndf = pd.read_csv(\"users.csv\")\n\n# Validate\nis_valid, errors = validate_schema(\n    df_or_conn=df,\n    expected=schema\n)\n\nif is_valid:\n    print(\"\u2705 Schema is valid!\")\nelse:\n    print(\"\u274c Schema validation failed:\")\n    for field, error in errors:\n        print(f\"  - {field}: {error}\")\n</code></pre>"},{"location":"#example-output","title":"Example Output","text":"<pre><code>\u274c Schema validation failed:\n  - email: missing\n  - age: type mismatch (got 'object', expected 'int64')\n  - created_at: nullable but expected non-nullable\n  - extra_field: extra column\n</code></pre>"},{"location":"#advanced-custom-filters","title":"Advanced: Custom Filters","text":"<p>You can add custom WHERE clauses to filter the schema registry:</p> <pre><code>schema = get_schema_config(\n    source=\"bigquery\",\n    project_id=\"my-project\",\n    dataset_id=\"my-dataset\",\n    table_id=\"users\",\n    environment=\"prod\",\n    query=\"source_type = 'bigquery' AND catalog_name IS NOT NULL\"\n)\n</code></pre> <p>Note: The <code>query</code> parameter adds additional filters to the base filter (<code>table_name</code> and <code>environment</code>).</p>"},{"location":"#supported-engines_1","title":"Supported Engines","text":"<p>Schema validation works with all supported DataFrame engines: - Dask - DuckDB - Pandas - Polars - PySpark</p> <p>Important: Make sure the <code>data_type</code> values in your <code>schema_registry</code> match the exact format returned by your DataFrame engine (e.g., <code>int64</code> for pandas, <code>string</code> for PySpark). Comparisons are case-insensitive.</p>"},{"location":"#project-layout","title":"\ud83d\udcc2 Project Layout","text":"<pre><code>sumeh/\n\u251c\u2500\u2500 poetry.lock\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 README.md\n\u2514\u2500\u2500 sumeh\n    \u251c\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 cli.py\n    \u251c\u2500\u2500 core.py\n    \u251c\u2500\u2500 engine\n    \u2502   \u251c\u2500\u2500 __init__.py\n    \u2502   \u251c\u2500\u2500 bigquery_engine.py\n    \u2502   \u251c\u2500\u2500 dask_engine.py\n    \u2502   \u251c\u2500\u2500 duckdb_engine.py\n    \u2502   \u251c\u2500\u2500 pandas_engine.py\n    \u2502   \u251c\u2500\u2500 polars_engine.py\n    \u2502   \u2514\u2500\u2500 pyspark_engine.py\n    \u2514\u2500\u2500 services\n        \u251c\u2500\u2500 __init__.py\n        \u251c\u2500\u2500 config.py\n        \u251c\u2500\u2500 index.html\n        \u2514\u2500\u2500 utils.py\n</code></pre>"},{"location":"#roadmap","title":"\ud83d\udcc8 Roadmap","text":"<ul> <li>[ ] Complete BigQuery engine implementation</li> <li>\u2705 Complete Pandas engine implementation</li> <li>\u2705 Enhanced documentation</li> <li>\u2705 More validation rule types</li> <li>[ ] Performance optimizations</li> </ul>"},{"location":"#contributing","title":"\ud83e\udd1d Contributing","text":"<ol> <li>Fork &amp; create a feature branch  </li> <li>Implement new checks or engines, following existing signatures  </li> <li>Add tests under <code>tests/</code> </li> <li>Open a PR and ensure CI passes</li> </ol>"},{"location":"#license","title":"\ud83d\udcdc License","text":"<p>Licensed under the Apache License 2.0.</p>"},{"location":"quickstart/","title":"Quickstart \ud83d\ude80","text":"<p>A concise guide to get started with Sumeh\u2019s unified data quality framework.</p>"},{"location":"quickstart/#1-installation","title":"1. Installation \ud83d\udcbb","text":"<p>Install Sumeh via pip (recommended) or conda:</p> <pre><code>pip install sumeh\n# or\nconda install -c conda-forge sumeh\n</code></pre>"},{"location":"quickstart/#2-loading-rules-and-schema-configuration","title":"2. Loading Rules and Schema Configuration \u2699\ufe0f","text":"<p>Use <code>get_rules_config</code> and <code>get_schema_config</code> to fetch your validation rules and expected schema from various sources.</p> <pre><code>from sumeh import get_rules_config, get_schema_config\n\n# Load rules from CSV\nrules = get_rules_config(\"path/to/rules.csv\", delimiter=';')\n\n# Load expected schema from Glue Data Catalog\nschema = get_schema_config(\n    \"glue\",\n    catalog_name=\"my_catalog\",\n    database_name=\"my_db\",\n    table_name=\"my_table\"\n)\n</code></pre> <p>Supported rule/schema sources include:</p> <ul> <li><code>bigquery://project.dataset.table</code> \ud83c\udf10</li> <li><code>s3://bucket/path</code> \u2601\ufe0f</li> <li>Local CSV (<code>*.csv</code>) \ud83d\udcc4</li> <li>Relational (\"mysql\", \"postgresql\") via kwargs \ud83d\uddc4\ufe0f</li> <li>AWS Glue (<code>\"glue\"</code>) \ud83d\udd25</li> <li>DuckDB (<code>duckdb://db_path.table</code>) \ud83e\udd86</li> <li>Databricks (<code>databricks://catalog.schema.table</code>) \ud83d\udc8e</li> </ul>"},{"location":"quickstart/#3-schema-validation","title":"3. Schema Validation \ud83d\udcd0","text":"<p>Before validating data, ensure your DataFrame or connection matches the expected schema:</p> <pre><code>from sumeh import validate_schema\n\n# For a Spark DataFrame:\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\ndf = spark.read.parquet(\"data.parquet\")\n\nis_valid, errors = validate_schema(\n    df,\n    expected=schema,\n    engine=\"pyspark_engine\"\n)\nif not is_valid:\n    print(\"Schema mismatches:\", errors)\n</code></pre>"},{"location":"quickstart/#4-data-validation","title":"4. Data Validation \ud83d\udd0d","text":"<p>Apply your loaded rules to any supported DataFrame using <code>validate</code>:</p> <pre><code>from sumeh import validate\n\n# Example with Pandas:\nimport pandas as pd\ndf = pd.read_csv(\"data.csv\")\n\n# Validate (detects engine automatically)\nresult = validate(df, rules)\n# `result` structure depends on engine (e.g., CheckResult for cuallee engines)\n</code></pre>"},{"location":"quickstart/#5-summarization","title":"5. Summarization \ud83d\udcca","text":"<p>Generate a tabular summary of violations and pass rates with <code>summarize</code>:</p> <pre><code>from sumeh import summarize\n\n# For DataFrames requiring manual total_rows (e.g., Pandas):\ntotal = len(df)\nsummary_df = summarize(\n    df=result,       # could be validation output or raw DataFrame\n    rules=rules,\n    total_rows=total\n)\nprint(summary_df)\n</code></pre>"},{"location":"quickstart/#6-one-step-reporting","title":"6. One-Step Reporting \ud83d\udcdd","text":"<p>Use <code>report</code> for an end-to-end quality check and summary in one call:</p> <pre><code>from sumeh import report\n\nreport_df = report(\n    df,              # your DataFrame or connection\n    rules,\n    name=\"My Quality Check\"\n)\nprint(report_df)\n</code></pre> <p>For deeper customization and engine-specific options, explore the full API and examples in the Sumeh repository.</p>"},{"location":"api/services/services-config/","title":"Module <code>sumeh.core</code>","text":""},{"location":"api/services/services-config/#sumeh.core","title":"sumeh.core","text":"<p>This module provides a set of functions and utilities for data validation, schema retrieval, and summarization. It supports multiple data sources and engines, including BigQuery, S3, CSV files, MySQL, PostgreSQL, AWS Glue, DuckDB, and Databricks.</p> <p>Functions:</p> Name Description <code>get_rules_config</code> <p>str, **kwargs) -&gt; List[Dict[str, Any]]: Retrieves configuration rules based on the specified source.</p> <code>get_schema_config</code> <p>str, **kwargs) -&gt; List[Dict[str, Any]]: Retrieves the schema configuration based on the provided data source.</p> <code>validate</code> <code>summarize</code> <p>list[dict], **context):</p> <code>report</code> <p>list[dict], name: str = \"Quality Check\"):</p> Imports <p>cuallee: Provides the <code>Check</code> and <code>CheckLevel</code> classes for data validation. warnings: Used to issue warnings for unknown rule names. importlib: Dynamically imports modules based on engine detection. typing: Provides type hints for function arguments and return values. re: Used for regular expression matching in source string parsing. sumeh.core: Contains functions for retrieving configurations and schemas   from various data sources. sumeh.core.utils: Provides utility functions for value conversion and URI parsing.</p> <p>The module uses Python's structural pattern matching (<code>match-case</code>) to handle different data source types and validation rules. The <code>report</code> function supports a wide range of validation checks, including completeness, uniqueness, value comparisons, patterns, and date-related checks. The <code>validate</code> and <code>summarize</code> functions dynamically detect the appropriate engine based on the input DataFrame type and delegate the processing to the corresponding engine module.</p>"},{"location":"api/services/services-config/#sumeh.core.get_rules_config","title":"get_rules_config","text":"<pre><code>get_rules_config(source: str, **kwargs) -&gt; List[Dict[str, Any]]\n</code></pre> <p>Retrieve configuration rules based on the specified source.</p> <p>Dispatches to the appropriate loader according to the format of <code>source</code>, returning a list of parsed rule dictionaries.</p> Supported sources <ul> <li><code>\"bigquery\" &lt;project&gt;.&lt;dataset&gt;.&lt;table&gt;</code></li> <li><code>s3://&lt;bucket&gt;/&lt;path&gt;</code></li> <li><code>&lt;file&gt;.csv</code></li> <li><code>\"mysql\"</code> or <code>\"postgresql\"</code> (requires host/user/etc. in kwargs)</li> <li><code>\"glue\"</code> (AWS Glue Data Catalog)</li> <li><code>duckdb://&lt;db_path&gt;.&lt;table&gt;</code></li> <li><code>databricks://&lt;catalog&gt;.&lt;schema&gt;.&lt;table&gt;</code></li> </ul> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>Identifier of the rules configuration location. Determines which handler is invoked.</p> required <code>**kwargs</code> <p>Loader-specific parameters (e.g. <code>host</code>, <code>user</code>, <code>password</code>, <code>connection</code>, <code>query</code>, <code>delimiter</code>).</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List[Dict[str, Any]]: A list of dictionaries, each representing a validation rule with keys like <code>\"field\"</code>, <code>\"check_type\"</code>, <code>\"value\"</code>, <code>\"threshold\"</code>, and <code>\"execute\"</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>source</code> does not match any supported format.</p>"},{"location":"api/services/services-config/#sumeh.core.get_schema_config","title":"get_schema_config","text":"<pre><code>get_schema_config(source: str, **kwargs) -&gt; List[Dict[str, Any]]\n</code></pre> <p>Retrieve the schema configuration based on the provided data source.</p> <p>This function reads from a schema_registry table/file to get the expected schema for a given table. Supports various data sources such as BigQuery, S3, CSV files, MySQL, PostgreSQL, AWS Glue, DuckDB, and Databricks.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>A string representing the data source. Supported formats: - <code>bigquery</code>: BigQuery source - <code>s3://&lt;bucket&gt;/&lt;path&gt;</code>: S3 CSV file - <code>&lt;file&gt;.csv</code>: Local CSV file - <code>mysql</code>: MySQL database - <code>postgresql</code>: PostgreSQL database - <code>glue</code>: AWS Glue Data Catalog - <code>duckdb</code>: DuckDB database - <code>databricks</code>: Databricks Unity Catalog</p> required <code>**kwargs</code> <p>Source-specific parameters. Common ones: - table (str): Table name to look up (REQUIRED for all sources) - environment (str): Environment filter (default: 'prod') - query (str): Additional WHERE filters (optional)</p> <p>For BigQuery: project_id, dataset_id, table_id For MySQL/PostgreSQL: host, user, password, database OR conn For Glue: glue_context, database_name, table_name For DuckDB: conn, table For Databricks: spark, catalog, schema, table For CSV/S3: file_path/s3_path, table</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List[Dict[str, Any]]: Schema configuration from schema_registry</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If source format is invalid or required params are missing</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; get_schema_config(\"bigquery\", project_id=\"proj\", dataset_id=\"ds\", table_id=\"users\")\n&gt;&gt;&gt; get_schema_config(\"mysql\", conn=my_conn, table=\"users\")\n&gt;&gt;&gt; get_schema_config(\"s3://bucket/registry.csv\", table=\"users\", environment=\"prod\")\n</code></pre>"},{"location":"api/services/services-config/#sumeh.core.validate","title":"validate","text":"<pre><code>validate(df, rules, **context)\n</code></pre> <p>Validates a DataFrame against a set of rules using the appropriate engine.</p> <p>This function dynamically detects the engine to use based on the input DataFrame and delegates the validation process to the corresponding engine's implementation.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to be validated.</p> required <code>rules</code> <code>list or dict</code> <p>The validation rules to be applied to the DataFrame.</p> required <code>**context</code> <p>Additional context parameters that may be required by the engine. - conn (optional): A database connection object, required for certain engines   like \"duckdb_engine\".</p> <code>{}</code> <p>Returns:</p> Type Description <p>bool or dict: The result of the validation process. The return type and structure</p> <p>depend on the specific engine's implementation.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If the required engine module cannot be imported.</p> <code>AttributeError</code> <p>If the detected engine does not have a <code>validate</code> method.</p> Notes <ul> <li>The engine is dynamically determined based on the DataFrame type or other   characteristics.</li> <li>For \"duckdb_engine\", a database connection object should be provided in the   context under the key \"conn\".</li> </ul>"},{"location":"sumeh/cli/","title":"cli","text":""},{"location":"sumeh/cli/#sumeh.cli","title":"sumeh.cli","text":"<p>Sumeh Command-Line Interface (CLI)</p> <p>This module provides the command-line tools for the Sumeh Data Quality framework.</p> <p>It exposes multiple entry points for different purposes: - config \u2014 launches a lightweight web interface (via Streamlit) for setup and exploration. - sql \u2014 generates SQL DDL definitions for Sumeh metadata tables in multiple dialects. - validate \u2014 validates datasets against quality rules and produces reports or dashboards.</p> <p>Each command is designed to be modular and extensible, supporting multiple engines and formats.</p> Example usage <pre><code>sumeh config\nsumeh sql --table rules --dialect postgres\nsumeh validate data.csv rules.csv --dashboard --engine polars\n</code></pre> <p>Environment:     The CLI can run in any environment with Python 3.9+.     For dashboard rendering, Streamlit and Plotly are required.</p>"},{"location":"sumeh/cli/#sumeh.cli.generate_sql","title":"generate_sql","text":"<pre><code>generate_sql()\n</code></pre> <p>Generate SQL DDL statements for Sumeh system tables.</p> <p>Allows exporting table definitions (e.g., <code>rules</code>, <code>schema_registry</code>) in multiple SQL dialects, including PostgreSQL, BigQuery, DuckDB, Snowflake, etc.</p> <p>The output can be printed to stdout or saved to a file via <code>--output</code>.</p> Usage examples <pre><code>sumeh sql --table rules --dialect postgres\nsumeh sql --table schema_registry --dialect bigquery --schema mydataset\nsumeh sql --list-dialects\nsumeh sql --list-tables\n</code></pre> <p>Supported dialects:     postgres, mysql, bigquery, duckdb, athena, sqlite, snowflake, redshift</p> Supported tables <p>rules, schema_registry, all</p> <p>Raises:</p> Type Description <code>SystemExit</code> <p>If invalid arguments or generation errors occur.</p>"},{"location":"sumeh/cli/#sumeh.cli.main","title":"main","text":"<pre><code>main()\n</code></pre> <p>Sumeh CLI entry point.</p> Dispatches subcommands to their respective handlers <p>config   \u2192 serve_index() sql      \u2192 generate_sql() validate \u2192 run_validation()</p> Example <p>sumeh validate data.csv rules.csv --dashboard</p> <p>The command is also registered as the default entry point when Sumeh is installed via pip or Poetry.</p> See <p><code>sumeh --help</code> for global options. <code>sumeh &lt;command&gt; --help</code> for command-specific usage.</p>"},{"location":"sumeh/cli/#sumeh.cli.run_validation","title":"run_validation","text":"<pre><code>run_validation()\n</code></pre> <p>Validate datasets against Sumeh quality rules.</p> Executes a complete validation pipeline <ol> <li>Loads dataset and rules.</li> <li>Runs validation using the selected DataFrame engine.</li> <li>Summarizes results and generates reports or dashboards.</li> </ol> <p>Supports multiple output formats and engines.</p> Supported Engines <ul> <li>pandas</li> <li>polars</li> <li>dask</li> </ul> <p>Examples:</p> <pre><code>sumeh validate data.csv rules.csv\nsumeh validate data.parquet rules.csv --output results.json\nsumeh validate data.csv rules.csv --dashboard\nsumeh validate data.csv rules.csv --engine polars --verbose\n</code></pre>"},{"location":"sumeh/cli/#sumeh.cli.serve_index","title":"serve_index","text":"<pre><code>serve_index()\n</code></pre> <p>Serve the local Sumeh configuration UI.</p> <p>Starts a simple HTTP server on http://localhost:8000 and serves the static configuration interface (index.html) located in <code>sumeh/dash/</code>. Automatically opens the page in the default browser.</p> <p>This command is primarily used for initial setup or demo purposes.</p> Usage <p>sumeh config</p> Notes <ul> <li>Press Ctrl+C to stop the local server.</li> <li>Runs only on localhost; not intended for production deployment.</li> </ul>"},{"location":"sumeh/core/config/","title":"config","text":""},{"location":"sumeh/core/config/#sumeh.core","title":"sumeh.core","text":"<p>This module provides a set of functions and utilities for data validation, schema retrieval, and summarization. It supports multiple data sources and engines, including BigQuery, S3, CSV files, MySQL, PostgreSQL, AWS Glue, DuckDB, and Databricks.</p> <p>Functions:</p> Name Description <code>get_rules_config</code> <p>str, **kwargs) -&gt; List[Dict[str, Any]]: Retrieves configuration rules based on the specified source.</p> <code>get_schema_config</code> <p>str, **kwargs) -&gt; List[Dict[str, Any]]: Retrieves the schema configuration based on the provided data source.</p> <code>validate</code> <code>summarize</code> <p>list[dict], **context):</p> <code>report</code> <p>list[dict], name: str = \"Quality Check\"):</p> Imports <p>cuallee: Provides the <code>Check</code> and <code>CheckLevel</code> classes for data validation. warnings: Used to issue warnings for unknown rule names. importlib: Dynamically imports modules based on engine detection. typing: Provides type hints for function arguments and return values. re: Used for regular expression matching in source string parsing. sumeh.core: Contains functions for retrieving configurations and schemas   from various data sources. sumeh.core.utils: Provides utility functions for value conversion and URI parsing.</p> <p>The module uses Python's structural pattern matching (<code>match-case</code>) to handle different data source types and validation rules. The <code>report</code> function supports a wide range of validation checks, including completeness, uniqueness, value comparisons, patterns, and date-related checks. The <code>validate</code> and <code>summarize</code> functions dynamically detect the appropriate engine based on the input DataFrame type and delegate the processing to the corresponding engine module.</p>"},{"location":"sumeh/core/config/#sumeh.core.get_rules_config","title":"get_rules_config","text":"<pre><code>get_rules_config(source: str, **kwargs) -&gt; List[Dict[str, Any]]\n</code></pre> <p>Retrieve configuration rules based on the specified source.</p> <p>Dispatches to the appropriate loader according to the format of <code>source</code>, returning a list of parsed rule dictionaries.</p> Supported sources <ul> <li><code>\"bigquery\" &lt;project&gt;.&lt;dataset&gt;.&lt;table&gt;</code></li> <li><code>s3://&lt;bucket&gt;/&lt;path&gt;</code></li> <li><code>&lt;file&gt;.csv</code></li> <li><code>\"mysql\"</code> or <code>\"postgresql\"</code> (requires host/user/etc. in kwargs)</li> <li><code>\"glue\"</code> (AWS Glue Data Catalog)</li> <li><code>duckdb://&lt;db_path&gt;.&lt;table&gt;</code></li> <li><code>databricks://&lt;catalog&gt;.&lt;schema&gt;.&lt;table&gt;</code></li> </ul> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>Identifier of the rules configuration location. Determines which handler is invoked.</p> required <code>**kwargs</code> <p>Loader-specific parameters (e.g. <code>host</code>, <code>user</code>, <code>password</code>, <code>connection</code>, <code>query</code>, <code>delimiter</code>).</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List[Dict[str, Any]]: A list of dictionaries, each representing a validation rule with keys like <code>\"field\"</code>, <code>\"check_type\"</code>, <code>\"value\"</code>, <code>\"threshold\"</code>, and <code>\"execute\"</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>source</code> does not match any supported format.</p>"},{"location":"sumeh/core/config/#sumeh.core.get_schema_config","title":"get_schema_config","text":"<pre><code>get_schema_config(source: str, **kwargs) -&gt; List[Dict[str, Any]]\n</code></pre> <p>Retrieve the schema configuration based on the provided data source.</p> <p>This function reads from a schema_registry table/file to get the expected schema for a given table. Supports various data sources such as BigQuery, S3, CSV files, MySQL, PostgreSQL, AWS Glue, DuckDB, and Databricks.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>A string representing the data source. Supported formats: - <code>bigquery</code>: BigQuery source - <code>s3://&lt;bucket&gt;/&lt;path&gt;</code>: S3 CSV file - <code>&lt;file&gt;.csv</code>: Local CSV file - <code>mysql</code>: MySQL database - <code>postgresql</code>: PostgreSQL database - <code>glue</code>: AWS Glue Data Catalog - <code>duckdb</code>: DuckDB database - <code>databricks</code>: Databricks Unity Catalog</p> required <code>**kwargs</code> <p>Source-specific parameters. Common ones: - table (str): Table name to look up (REQUIRED for all sources) - environment (str): Environment filter (default: 'prod') - query (str): Additional WHERE filters (optional)</p> <p>For BigQuery: project_id, dataset_id, table_id For MySQL/PostgreSQL: host, user, password, database OR conn For Glue: glue_context, database_name, table_name For DuckDB: conn, table For Databricks: spark, catalog, schema, table For CSV/S3: file_path/s3_path, table</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List[Dict[str, Any]]: Schema configuration from schema_registry</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If source format is invalid or required params are missing</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; get_schema_config(\"bigquery\", project_id=\"proj\", dataset_id=\"ds\", table_id=\"users\")\n&gt;&gt;&gt; get_schema_config(\"mysql\", conn=my_conn, table=\"users\")\n&gt;&gt;&gt; get_schema_config(\"s3://bucket/registry.csv\", table=\"users\", environment=\"prod\")\n</code></pre>"},{"location":"sumeh/core/config/#sumeh.core.validate","title":"validate","text":"<pre><code>validate(df, rules, **context)\n</code></pre> <p>Validates a DataFrame against a set of rules using the appropriate engine.</p> <p>This function dynamically detects the engine to use based on the input DataFrame and delegates the validation process to the corresponding engine's implementation.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to be validated.</p> required <code>rules</code> <code>list or dict</code> <p>The validation rules to be applied to the DataFrame.</p> required <code>**context</code> <p>Additional context parameters that may be required by the engine. - conn (optional): A database connection object, required for certain engines   like \"duckdb_engine\".</p> <code>{}</code> <p>Returns:</p> Type Description <p>bool or dict: The result of the validation process. The return type and structure</p> <p>depend on the specific engine's implementation.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If the required engine module cannot be imported.</p> <code>AttributeError</code> <p>If the detected engine does not have a <code>validate</code> method.</p> Notes <ul> <li>The engine is dynamically determined based on the DataFrame type or other   characteristics.</li> <li>For \"duckdb_engine\", a database connection object should be provided in the   context under the key \"conn\".</li> </ul>"},{"location":"sumeh/core/utils/","title":"utils","text":""},{"location":"sumeh/core/utils/#sumeh.core.utils","title":"sumeh.core.utils","text":""},{"location":"sumeh/core/utils/#sumeh.core.utils.__compare_schemas","title":"__compare_schemas","text":"<pre><code>__compare_schemas(actual: List[SchemaDef], expected: List[SchemaDef]) -&gt; Tuple[bool, List[Dict[str, Any]]]\n</code></pre> <p>Compare two lists of schema definitions and identify discrepancies.</p> <p>Parameters:</p> Name Type Description Default <code>actual</code> <code>List[SchemaDef]</code> <p>The list of actual schema definitions.</p> required <code>expected</code> <code>List[SchemaDef]</code> <p>The list of expected schema definitions.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>Tuple[bool, List[Tuple[str, str]]]: A tuple where the first element is a boolean indicating</p> <code>List[Dict[str, Any]]</code> <p>whether the schemas match (True if they match, False otherwise), and the second element</p> <code>Tuple[bool, List[Dict[str, Any]]]</code> <p>is a list of tuples describing the discrepancies. Each tuple contains: - The field name (str). - A description of the discrepancy (str), such as \"missing\", \"type mismatch\",   \"nullable but expected non-nullable\", or \"extra column\".</p> Notes <ul> <li>A field is considered \"missing\" if it exists in the expected schema but not in the actual schema.</li> <li>A \"type mismatch\" occurs if the data type of field in the actual schema does not match   the expected data type.</li> <li>A field is considered \"nullable but expected non-nullable\" if it is nullable in the actual   schema but not nullable in the expected schema.</li> <li>An \"extra column\" is a field that exists in the actual schema but not in the expected schema.</li> </ul>"},{"location":"sumeh/core/utils/#sumeh.core.utils.__convert_value","title":"__convert_value","text":"<pre><code>__convert_value(value)\n</code></pre> <p>Converts the provided value to the appropriate type (date, float, or int).</p> <p>Depending on the format of the input value, it will be converted to a datetime object, a floating-point number (float), or an integer (int).</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>The value to be converted, represented as a string.</p> required <p>Returns:</p> Type Description <p>Union[datetime, float, int]: The converted value, which can be a datetime object, float, or int.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the value does not match an expected format.</p>"},{"location":"sumeh/core/utils/#sumeh.core.utils.__detect_engine","title":"__detect_engine","text":"<pre><code>__detect_engine(df, **context)\n</code></pre> <p>Detects the engine type of the given DataFrame based on its module.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <p>The DataFrame object whose engine type is to be detected.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>A string representing the detected engine type. Possible values are: - \"pyspark_engine\" for PySpark DataFrames - \"dask_engine\" for Dask DataFrames - \"polars_engine\" for Polars DataFrames - \"pandas_engine\" for Pandas DataFrames - \"duckdb_engine\" for DuckDB or BigQuery DataFrames</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the DataFrame type is unsupported.</p>"},{"location":"sumeh/core/utils/#sumeh.core.utils.__parse_databricks_uri","title":"__parse_databricks_uri","text":"<pre><code>__parse_databricks_uri(uri: str) -&gt; Dict[str, Optional[str]]\n</code></pre> <p>Parses a Databricks URI into its catalog, schema, and table components.</p> <p>The URI is expected to follow the format <code>protocol://catalog.schema.table</code> or <code>protocol://schema.table</code>. If the catalog is not provided, it will be set to <code>None</code>. If the schema is not provided, the current database from the active Spark session will be used.</p> <p>Parameters:</p> Name Type Description Default <code>uri</code> <code>str</code> <p>The Databricks URI to parse.</p> required <p>Returns:</p> Type Description <code>Dict[str, Optional[str]]</code> <p>Dict[str, Optional[str]]: A dictionary containing the parsed components: - \"catalog\" (Optional[str]): The catalog name, or <code>None</code> if not provided. - \"schema\" (Optional[str]): The schema name, or the current database if not provided. - \"table\" (Optional[str]): The table name.</p>"},{"location":"sumeh/dash/","title":"dash","text":""},{"location":"sumeh/dash/#sumeh.dash.app","title":"sumeh.dash.app","text":"<p>Sumeh Data Quality Dashboard - The Ultimate Validation Experience</p>"},{"location":"sumeh/dash/#sumeh.dash.app.launch_dashboard","title":"launch_dashboard","text":"<pre><code>launch_dashboard(validation_results=None, summary=None, metadata=None) -&gt; None\n</code></pre> <p>Launch the ultimate Sumeh dashboard!</p> <p>Parameters:</p> Name Type Description Default <code>validation_results</code> <p>Results from validate() function</p> <code>None</code> <code>rules_config</code> <p>Rules configuration</p> required <code>summary</code> <p>Validation summary</p> <code>None</code> <code>metadata</code> <p>Additional metadata</p> <code>None</code>"},{"location":"sumeh/engines/bigquery/","title":"bigquery","text":""},{"location":"sumeh/engines/bigquery/#sumeh.engines.bigquery_engine","title":"sumeh.engines.bigquery_engine","text":"<p>BigQuery data quality validation engine for Sumeh.</p> <p>This module provides validation functions for data quality rules in BigQuery using SQLGlot for SQL generation. It supports various validation types including completeness, uniqueness, pattern matching, date validations, and numeric comparisons.</p>"},{"location":"sumeh/engines/bigquery/#sumeh.engines.bigquery_engine.__RuleCtx","title":"__RuleCtx  <code>dataclass</code>","text":"<pre><code>__RuleCtx(column: Any, value: Any, name: str)\n</code></pre> <p>Context for validation rule execution.</p> <p>Attributes:</p> Name Type Description <code>column</code> <code>Any</code> <p>Column name(s) to validate (str or list of str)</p> <code>value</code> <code>Any</code> <p>Threshold or comparison value for the rule</p> <code>name</code> <code>str</code> <p>Check type identifier</p>"},{"location":"sumeh/engines/bigquery/#sumeh.engines.bigquery_engine.__rules_to_bq_sql","title":"__rules_to_bq_sql","text":"<pre><code>__rules_to_bq_sql(rules: List[Dict]) -&gt; str\n</code></pre> <p>Converts rule definitions into SQL representation using SQLGlot.</p> <p>Generates SQL query that represents each rule as a row with columns: col, rule, pass_threshold, value</p> <p>Parameters:</p> Name Type Description Default <code>rules</code> <code>List[Dict]</code> <p>List of validation rule dictionaries</p> required <p>Returns:</p> Type Description <code>str</code> <p>SQL query string with DISTINCT rule definitions</p>"},{"location":"sumeh/engines/bigquery/#sumeh.engines.bigquery_engine.count_rows","title":"count_rows","text":"<pre><code>count_rows(client: Client, table_ref: str) -&gt; int\n</code></pre> <p>Counts total number of rows in a BigQuery table using SQLGlot.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Client</code> <p>Authenticated BigQuery client instance</p> required <code>table_ref</code> <code>str</code> <p>Fully qualified table reference (project.dataset.table)</p> required <p>Returns:</p> Type Description <code>int</code> <p>Total row count as integer</p>"},{"location":"sumeh/engines/bigquery/#sumeh.engines.bigquery_engine.extract_schema","title":"extract_schema","text":"<pre><code>extract_schema(table: Table) -&gt; List[Dict[str, Any]]\n</code></pre> <p>Extracts schema definition from BigQuery table object.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>Table</code> <p>BigQuery Table object with schema information</p> required <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List of schema field dictionaries, each containing: - field: Field name - data_type: BigQuery data type - nullable: Whether field allows NULL values - max_length: Always None (reserved for future use)</p>"},{"location":"sumeh/engines/bigquery/#sumeh.engines.bigquery_engine.summarize","title":"summarize","text":"<pre><code>summarize(df: RowIterator, rules: List[Dict], total_rows: int, client: Client) -&gt; List[Dict[str, Any]]\n</code></pre> <p>Generates validation summary report with pass/fail status for each rule.</p> <p>Analyzes violation results and compares against rule thresholds to determine pass/fail status for each validation rule.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>RowIterator</code> <p>Row iterator containing validation violations from validate()</p> required <code>rules</code> <code>List[Dict]</code> <p>List of validation rules that were executed</p> required <code>total_rows</code> <code>int</code> <p>Total number of rows in validated table</p> required <code>client</code> <code>Client</code> <p>BigQuery client instance (for compatibility, not actively used)</p> required <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List of summary dictionaries, each containing: - id: Unique identifier for the summary record - timestamp: Validation execution timestamp - check: Check category (always \"Quality Check\") - level: Severity level (always \"WARNING\") - column: Column name(s) validated - rule: Rule type applied - value: Rule threshold/comparison value - rows: Total rows evaluated - violations: Number of violating rows - pass_rate: Percentage of passing rows (0.0-1.0) - pass_threshold: Required pass rate from rule - status: \"PASS\" or \"FAIL\" based on pass_rate vs pass_threshold</p>"},{"location":"sumeh/engines/bigquery/#sumeh.engines.bigquery_engine.validate","title":"validate","text":"<pre><code>validate(client: Client, table_ref: str, rules: List[Dict]) -&gt; Tuple[bigquery.table.RowIterator, bigquery.table.RowIterator]\n</code></pre> <p>Validates BigQuery table data against specified quality rules.</p> <p>Executes two queries: 1. Raw violations - all violating rows with individual dq_status 2. Aggregated violations - rows grouped with concatenated dq_status</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Client</code> <p>Authenticated BigQuery client instance</p> required <code>table_ref</code> <code>str</code> <p>Fully qualified table reference (project.dataset.table)</p> required <code>rules</code> <code>List[Dict]</code> <p>List of validation rule dictionaries</p> required <p>Returns:</p> Type Description <code>Tuple[RowIterator, RowIterator]</code> <p>Tuple containing: - Aggregated results with grouped violations - Raw results with individual violations</p>"},{"location":"sumeh/engines/bigquery/#sumeh.engines.bigquery_engine.validate_schema","title":"validate_schema","text":"<pre><code>validate_schema(client: Client, expected: List[Dict[str, Any]], table_ref: str) -&gt; tuple[bool, list[dict[str, Any]]]\n</code></pre> <p>Validates BigQuery table schema against expected schema definition.</p> <p>Compares actual table schema with expected schema and identifies mismatches in field names, data types, and nullability constraints.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Client</code> <p>Authenticated BigQuery client instance</p> required <code>expected</code> <code>List[Dict[str, Any]]</code> <p>List of expected schema field dictionaries</p> required <code>table_ref</code> <code>str</code> <p>Fully qualified table reference (project.dataset.table)</p> required <p>Returns:</p> Type Description <code>tuple[bool, list[dict[str, Any]]]</code> <p>Tuple containing: - Boolean indicating if schemas match exactly - List of error dictionaries describing any mismatches</p>"},{"location":"sumeh/engines/dask/","title":"dask","text":""},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine","title":"sumeh.engines.dask_engine","text":"<p>This module provides a set of data quality validation functions for Dask DataFrames. It includes various checks such as completeness, uniqueness, value range, patterns, and schema validation. The module also provides utilities for summarizing validation results and schema comparison.</p> <p>Functions:</p> Name Description <code>is_positive</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>is_negative</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>is_in_millions</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>is_in_billions</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>is_t_minus_1</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>is_t_minus_2</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>is_t_minus_3</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>is_today</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>is_yesterday</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>is_on_weekday</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>is_on_weekend</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>is_on_monday</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>is_on_tuesday</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>is_on_wednesday</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>is_on_thursday</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>is_on_friday</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>is_on_saturday</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>is_on_sunday</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>is_complete</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>is_unique</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>are_complete</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>are_unique</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>is_greater_than</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>is_greater_or_equal_than</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>is_less_than</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>is_less_or_equal_than</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>is_equal</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>is_equal_than</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>is_contained_in</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>not_contained_in</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>is_between</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>has_pattern</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>is_legit</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>is_primary_key</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>is_composite_key</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>has_max</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>has_min</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>has_std</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>has_mean</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>has_sum</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>has_cardinality</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>has_infogain</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>has_entropy</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>satisfies</code> <p>dd.DataFrame, rule: dict) -&gt; dd.DataFrame:</p> <code>validate</code> <p>dd.DataFrame, rules: list[dict]) -&gt; tuple[dd.DataFrame, dd.DataFrame]:</p> <code>summarize</code> <p>dd.DataFrame, rules: list[dict], total_rows: int) -&gt; pd.DataFrame:</p> <code>validate_schema</code> <p>dd.DataFrame, expected: List[Dict[str, Any]]) -&gt; Tuple[bool, List[Tuple[str, str]]]:</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.all_date_checks","title":"all_date_checks","text":"<pre><code>all_date_checks(df: DataFrame, rule: dict) -&gt; dd.DataFrame\n</code></pre> <p>Applies date validation checks on a Dask DataFrame based on the provided rule.</p> <p>This function serves as an alias for the <code>is_past_date</code> function, which performs checks to determine if dates in the DataFrame meet the specified criteria.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The Dask DataFrame containing the data to be validated.</p> required <code>rule</code> <code>dict</code> <p>A dictionary specifying the validation rules to be applied.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A Dask DataFrame with the results of the date validation checks.</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.are_complete","title":"are_complete","text":"<pre><code>are_complete(df: DataFrame, rule: dict) -&gt; dd.DataFrame\n</code></pre> <p>Checks if the specified fields in a Dask DataFrame are complete (non-null) based on the provided rule and returns a DataFrame of violations.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to check for completeness.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include the fields to check, the type of check, and the expected value.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A DataFrame containing rows that violate the completeness</p> <code>DataFrame</code> <p>rule, with an additional column <code>dq_status</code> indicating the rule details</p> <code>DataFrame</code> <p>in the format \"{fields}:{check}:{value}\".</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.are_unique","title":"are_unique","text":"<pre><code>are_unique(df: DataFrame, rule: dict) -&gt; dd.DataFrame\n</code></pre> <p>Checks if the specified fields in a Dask DataFrame contain unique combinations of values.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to include: - 'fields': A list of column names to check for uniqueness. - 'check': A string describing the type of check being performed. - 'value': A value associated with the rule (used for status reporting).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A DataFrame containing rows that violate the uniqueness rule,</p> <code>DataFrame</code> <p>with an additional column <code>dq_status</code> indicating the rule that was violated.</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.extract_schema","title":"extract_schema","text":"<pre><code>extract_schema(df: DataFrame) -&gt; List[Dict[str, Any]]\n</code></pre> <p>Convert the schema of a Dask DataFrame into a list of dictionaries.</p> <p>Each dictionary in the resulting list represents a column in the DataFrame and contains metadata about the column, including its name, data type, nullability, and maximum length.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The Dask DataFrame whose schema is to be converted.</p> required <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List[Dict[str, Any]]: A list of dictionaries, where each dictionary</p> <code>List[Dict[str, Any]]</code> <p>contains the following keys: - \"field\" (str): The name of the column. - \"data_type\" (str): The data type of the column, converted to a lowercase string. - \"nullable\" (bool): Always set to True, indicating the column is nullable. - \"max_length\" (None): Always set to None, as maximum length is not determined.</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.has_cardinality","title":"has_cardinality","text":"<pre><code>has_cardinality(df: DataFrame, rule: dict) -&gt; dd.DataFrame\n</code></pre> <p>Checks if the cardinality (number of unique values) of a specified field in a Dask DataFrame exceeds a given threshold and returns a modified DataFrame based on the result.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to evaluate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The column name to check cardinality for. - 'check' (str): A descriptive label for the check (used in the output). - 'value' (int): The maximum allowed cardinality.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: If the cardinality of the specified field exceeds the given value,</p> <code>DataFrame</code> <p>returns the original DataFrame with an additional column <code>dq_status</code> indicating</p> <code>DataFrame</code> <p>the rule violation. Otherwise, returns an empty DataFrame with the same structure</p> <code>DataFrame</code> <p>as the input DataFrame.</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.has_entropy","title":"has_entropy","text":"<pre><code>has_entropy(df: DataFrame, rule: dict) -&gt; dd.DataFrame\n</code></pre> <p>Evaluates the entropy of a specified field in a Dask DataFrame and applies a rule to determine if the entropy exceeds a given threshold. If the threshold is exceeded, a new column <code>dq_status</code> is added to the DataFrame with information about the rule violation. Otherwise, an empty DataFrame is returned.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to evaluate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - <code>field</code> (str): The column name to evaluate. - <code>check</code> (str): The type of check being performed (used for status message). - <code>value</code> (float): The threshold value for the entropy.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A DataFrame with the <code>dq_status</code> column added if the entropy exceeds the threshold,</p> <code>DataFrame</code> <p>or an empty DataFrame if the threshold is not exceeded.</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.has_infogain","title":"has_infogain","text":"<pre><code>has_infogain(df: DataFrame, rule: dict) -&gt; dd.DataFrame\n</code></pre> <p>Evaluates whether a given field in a Dask DataFrame satisfies an information gain condition based on the specified rule. If the condition is met, the DataFrame is updated with a <code>dq_status</code> column indicating the rule applied. Otherwise, an empty DataFrame is returned.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to evaluate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The column name to evaluate. - 'check' (str): The type of check being performed (used for status annotation). - 'value' (float): The threshold value for the information gain.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: The original DataFrame with an added <code>dq_status</code> column if the condition</p> <code>DataFrame</code> <p>is met, or an empty DataFrame if the condition is not satisfied.</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.has_max","title":"has_max","text":"<pre><code>has_max(df: DataFrame, rule: dict) -&gt; dd.DataFrame\n</code></pre> <p>Identifies rows in a Dask DataFrame where the value of a specified field exceeds a given maximum value.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The name of the column to check. - 'check': A string describing the check (e.g., 'max'). - 'value': The maximum allowable value for the specified field.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame containing only the rows that violate the rule.           An additional column <code>dq_status</code> is added to indicate the rule violation           in the format \"{field}:{check}:{value}\".</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.has_mean","title":"has_mean","text":"<pre><code>has_mean(df: DataFrame, rule: dict) -&gt; dd.DataFrame\n</code></pre> <p>Checks if the mean of a specified field in a Dask DataFrame satisfies a given condition.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to evaluate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule to apply. It should include: - 'field' (str): The column name to calculate the mean for. - 'check' (str): The type of check to perform (e.g., 'greater_than'). - 'value' (float): The threshold value to compare the mean against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame with an additional column <code>dq_status</code> if the mean</p> <code>DataFrame</code> <p>satisfies the condition. If the condition is not met, an empty Dask DataFrame is returned.</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.has_min","title":"has_min","text":"<pre><code>has_min(df: DataFrame, rule: dict) -&gt; dd.DataFrame\n</code></pre> <p>Checks if the values in a specified field of a Dask DataFrame are greater than or equal to a given minimum value. Returns a DataFrame containing rows that violate this rule, with an additional column indicating the data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to validate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The column name to check. - 'check': The type of check being performed (e.g., 'min'). - 'value': The minimum value to compare against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A DataFrame containing rows that do not meet the minimum value</p> <code>DataFrame</code> <p>requirement, with an additional column <code>dq_status</code> indicating the rule</p> <code>DataFrame</code> <p>violation in the format \"field:check:value\".</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.has_pattern","title":"has_pattern","text":"<pre><code>has_pattern(df: DataFrame, rule: dict) -&gt; dd.DataFrame\n</code></pre> <p>Identifies rows in a Dask DataFrame that do not match a specified pattern.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The column name in the DataFrame to apply the pattern check. - 'check': A descriptive label for the type of check being performed. - 'value': The regex pattern to match against the specified column.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A DataFrame containing rows that do not match the specified pattern.           An additional column <code>dq_status</code> is added, indicating the rule details           in the format \"{field}:{check}:{value}\".</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.has_std","title":"has_std","text":"<pre><code>has_std(df: DataFrame, rule: dict) -&gt; dd.DataFrame\n</code></pre> <p>Checks if the standard deviation of a specified field in a Dask DataFrame exceeds a given value.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to evaluate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The name of the column to calculate the standard deviation for. - 'check' (str): A descriptive label for the check being performed. - 'value' (float): The threshold value for the standard deviation.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: - If the standard deviation of the specified field exceeds the given value,   returns the original DataFrame with an additional column <code>dq_status</code> indicating the rule details. - If the standard deviation does not exceed the value, returns an empty DataFrame with the same structure.</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.has_sum","title":"has_sum","text":"<pre><code>has_sum(df: DataFrame, rule: dict) -&gt; dd.DataFrame\n</code></pre> <p>Checks if the sum of a specified field in a Dask DataFrame exceeds a given value and returns a modified DataFrame with a status column if the condition is met.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to evaluate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The column name to sum. - 'check' (str): A descriptive label for the check (used in the status message). - 'value' (float): The threshold value to compare the sum against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame. If the sum exceeds the threshold, the DataFrame</p> <code>DataFrame</code> <p>will include a <code>dq_status</code> column with a status message. Otherwise, an empty</p> <code>DataFrame</code> <p>DataFrame with the same structure as the input is returned.</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.is_between","title":"is_between","text":"<pre><code>is_between(df: DataFrame, rule: dict) -&gt; dd.DataFrame\n</code></pre> <p>Filters a Dask DataFrame to identify rows where a specified field's value does not fall within a given range.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The column name in the DataFrame to check. - 'check': The type of check being performed (e.g., \"between\"). - 'value': A string representing the range in the format \"[lo,hi]\".</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new DataFrame containing only the rows that violate</p> <code>DataFrame</code> <p>the specified range condition. An additional column <code>dq_status</code> is</p> <code>DataFrame</code> <p>added to indicate the field, check, and value that caused the violation.</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.is_complete","title":"is_complete","text":"<pre><code>is_complete(df: DataFrame, rule: dict) -&gt; dd.DataFrame\n</code></pre> <p>Checks for completeness of a specified field in a Dask DataFrame based on a given rule.</p> <p>This function identifies rows where the specified field is null and marks them as violations. It then assigns a data quality status to these rows in the resulting DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The Dask DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The name of the field to check for completeness. - 'check': The type of check being performed (e.g., 'is_complete'). - 'value': Additional value associated with the rule (not used in this function).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A DataFrame containing rows where the specified field is null,</p> <code>DataFrame</code> <p>with an additional column <code>dq_status</code> indicating the data quality status in the format</p> <code>DataFrame</code> <p>\"{field}:{check}:{value}\".</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.is_composite_key","title":"is_composite_key","text":"<pre><code>is_composite_key(df: DataFrame, rule: dict) -&gt; dd.DataFrame\n</code></pre> <p>Determines if the given DataFrame satisfies the composite key condition based on the provided rule.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A Dask DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary defining the composite key rule.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A Dask DataFrame indicating whether the composite key condition is met.</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.is_contained_in","title":"is_contained_in","text":"<pre><code>is_contained_in(df: DataFrame, rule: dict) -&gt; dd.DataFrame\n</code></pre> <p>Filters a Dask DataFrame to identify rows where the values in a specified field are not contained within a given list of allowed values.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The column name in the DataFrame to check. - 'check': The type of check being performed (e.g., \"is_contained_in\"). - 'value': A string representation of a list of allowed values (e.g., \"[value1, value2]\").</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame containing only the rows that violate the rule.</p> <code>DataFrame</code> <p>An additional column <code>dq_status</code> is added to indicate the rule violation in the format:</p> <code>DataFrame</code> <p>\"{field}:{check}:{value}\".</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.is_date_after","title":"is_date_after","text":"<pre><code>is_date_after(df: DataFrame, rule: dict) -&gt; dd.DataFrame\n</code></pre> <p>Filters a Dask DataFrame to identify rows where a specified date field is earlier than a given reference date.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - field (str): The name of the column to check. - check (str): A descriptive label for the check (used in the   output status). - date_str (str): The reference date as a string in a format   compatible with <code>pd.Timestamp</code>.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame containing only the rows where the</p> <code>DataFrame</code> <p>specified date field is earlier than the reference date. An additional</p> <code>DataFrame</code> <p>column <code>dq_status</code> is added, which contains a string describing the</p> <code>DataFrame</code> <p>rule violation in the format <code>field:check:date_str</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the <code>rule</code> dictionary does not contain the required keys.</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.is_date_before","title":"is_date_before","text":"<pre><code>is_date_before(df: DataFrame, rule: dict) -&gt; dd.DataFrame\n</code></pre> <p>Checks if the values in a specified date column of a Dask DataFrame are before a given reference date.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame containing the data to be validated.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The name of the column to check. - 'check': A descriptive string for the check (e.g., \"is_date_before\"). - 'date_str': The reference date as a string in a format parsable by pandas.Timestamp.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame containing only the rows where the date in the specified column</p> <code>DataFrame</code> <p>is after the reference date. An additional column 'dq_status' is added to indicate the validation</p> <code>DataFrame</code> <p>status in the format \"{field}:{check}:{date_str}\".</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.is_date_between","title":"is_date_between","text":"<pre><code>is_date_between(df: DataFrame, rule: dict) -&gt; dd.DataFrame\n</code></pre> <p>Filters a Dask DataFrame to identify rows where a date field does not fall within a specified range.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame containing the data to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The name of the column in the DataFrame to check. - 'check': A string representing the type of check (used for status annotation). - 'val': A string representing the date range in the format \"[start_date, end_date]\".</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A DataFrame containing rows where the date field does not fall within the specified range.           An additional column 'dq_status' is added to indicate the rule violation in the format           \"{field}:{check}:{val}\".</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.is_equal","title":"is_equal","text":"<pre><code>is_equal(df: DataFrame, rule: dict) -&gt; dd.DataFrame\n</code></pre> <p>Filters a Dask DataFrame to identify rows where a specified field does not equal a given value.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The column name in the DataFrame to be checked. - 'check': The type of check to perform (expected to be 'equal' for this function). - 'value': The value to compare against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new DataFrame containing rows that violate the equality rule.           An additional column <code>dq_status</code> is added, indicating the rule details           in the format \"{field}:{check}:{value}\".</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.is_equal_than","title":"is_equal_than","text":"<pre><code>is_equal_than(df: DataFrame, rule: dict) -&gt; dd.DataFrame\n</code></pre> <p>Filters a Dask DataFrame to identify rows where the specified field does not equal the given value.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The column name in the DataFrame to check. - 'check': The type of check being performed (expected to be 'equal' for this function). - 'value': The value to compare against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new DataFrame containing rows that violate the equality rule.           An additional column <code>dq_status</code> is added, indicating the rule details           in the format \"{field}:{check}:{value}\".</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.is_future_date","title":"is_future_date","text":"<pre><code>is_future_date(df: DataFrame, rule: dict) -&gt; dd.DataFrame\n</code></pre> <p>Checks for rows in a Dask DataFrame where the specified date field contains a future date.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to validate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to include: - field: The name of the column to check. - check: A descriptive label for the check (used in the output). - _: Additional parameters (ignored in this function).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame containing only the rows where the date in the specified</p> <code>DataFrame</code> <p>field is in the future. An additional column <code>dq_status</code> is added to indicate the status</p> <code>DataFrame</code> <p>of the validation in the format: \"::\". Notes <ul> <li>The function coerces the specified column to datetime format, and invalid parsing results   in NaT (Not a Time).</li> <li>Rows with NaT in the specified column are excluded from the output.</li> <li>The current date is determined using the system's local date.</li> </ul>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.is_greater_or_equal_than","title":"is_greater_or_equal_than","text":"<pre><code>is_greater_or_equal_than(df: DataFrame, rule: dict) -&gt; dd.DataFrame\n</code></pre> <p>Filters a Dask DataFrame to identify rows where a specified field's value is less than a given threshold, and annotates the resulting rows with a data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should          include the following keys:          - 'field': The column name in the DataFrame to check.          - 'check': The type of check being performed (e.g., 'greater_or_equal').          - 'value': The threshold value to compare against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame containing only the rows that           violate the rule, with an additional column <code>dq_status</code>           indicating the field, check type, and threshold value.</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.is_greater_than","title":"is_greater_than","text":"<pre><code>is_greater_than(df: DataFrame, rule: dict) -&gt; dd.DataFrame\n</code></pre> <p>Filters a Dask DataFrame to identify rows where the value in a specified field is greater than a given threshold and annotates the result with a data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The column name to check. - 'check' (str): The type of check being performed (e.g., 'greater_than'). - 'value' (numeric): The threshold value to compare against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A filtered DataFrame containing rows that violate the rule,</p> <code>DataFrame</code> <p>with an additional column <code>dq_status</code> indicating the rule details in the format</p> <code>DataFrame</code> <p>\"{field}:{check}:{value}\".</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.is_in","title":"is_in","text":"<pre><code>is_in(df: DataFrame, rule: dict) -&gt; dd.DataFrame\n</code></pre> <p>Checks if the specified rule is contained within the given Dask DataFrame.</p> <p>This function acts as a wrapper for the <code>is_contained_in</code> function, passing the provided DataFrame and rule to it.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The Dask DataFrame to evaluate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary representing the rule to check against the DataFrame.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A Dask DataFrame resulting from the evaluation of the rule.</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.is_in_billions","title":"is_in_billions","text":"<pre><code>is_in_billions(df: DataFrame, rule: dict) -&gt; dd.DataFrame\n</code></pre> <p>Identifies rows in a Dask DataFrame where the value in a specified field is greater than or equal to one billion and marks them with a data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected          to include the field name, check type, and value.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A Dask DataFrame containing only the rows where the specified           field's value is greater than or equal to one billion. An           additional column <code>dq_status</code> is added, indicating the field,           check type, and value that triggered the rule.</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.is_in_millions","title":"is_in_millions","text":"<pre><code>is_in_millions(df: DataFrame, rule: dict) -&gt; dd.DataFrame\n</code></pre> <p>Checks if the values in a specified field of a Dask DataFrame are in the millions (greater than or equal to 1,000,000) and returns a DataFrame of violations.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to check.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to          include the field name, check type, and value.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A DataFrame containing rows where the specified field's value           is greater than or equal to 1,000,000. An additional column           <code>dq_status</code> is added to indicate the field, check, and value           that triggered the violation.</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.is_legit","title":"is_legit","text":"<pre><code>is_legit(df: DataFrame, rule: dict) -&gt; dd.DataFrame\n</code></pre> <p>Validates a Dask DataFrame against a specified rule and returns rows that violate the rule.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The Dask DataFrame to validate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the validation rule. It should include: - 'field': The column name in the DataFrame to validate. - 'check': The type of validation check (e.g., regex, condition). - 'value': The value or pattern to validate against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new DataFrame containing rows that violate the rule, with an additional</p> <code>DataFrame</code> <p>column <code>dq_status</code> indicating the field, check, and value of the failed validation.</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.is_less_or_equal_than","title":"is_less_or_equal_than","text":"<pre><code>is_less_or_equal_than(df: DataFrame, rule: dict) -&gt; dd.DataFrame\n</code></pre> <p>Filters a Dask DataFrame to identify rows where the value in a specified field is greater than a given threshold, violating a \"less or equal than\" rule.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The column name in the DataFrame to be checked. - 'check': The type of check being performed (e.g., \"less_or_equal_than\"). - 'value': The threshold value to compare against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new DataFrame containing only the rows that violate the rule.</p> <code>DataFrame</code> <p>An additional column <code>dq_status</code> is added to indicate the rule violation</p> <code>DataFrame</code> <p>in the format \"{field}:{check}:{value}\".</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.is_less_than","title":"is_less_than","text":"<pre><code>is_less_than(df: DataFrame, rule: dict) -&gt; dd.DataFrame\n</code></pre> <p>Filters a Dask DataFrame to identify rows where the value in a specified field is greater than or equal to a given threshold, and marks them with a data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The column name to check. - 'check' (str): The type of check being performed (e.g., \"less_than\"). - 'value' (numeric): The threshold value for the check.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame containing only the rows that violate the rule,</p> <code>DataFrame</code> <p>with an additional column <code>dq_status</code> indicating the rule that was violated in the</p> <code>DataFrame</code> <p>format \"{field}:{check}:{value}\".</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.is_negative","title":"is_negative","text":"<pre><code>is_negative(df: DataFrame, rule: dict) -&gt; dd.DataFrame\n</code></pre> <p>Identifies rows in a Dask DataFrame where the specified field does not satisfy a \"negative\" check.</p> <p>This function filters the DataFrame to find rows where the value in the specified field is greater than or equal to 0 (i.e., not negative). It then assigns a new column <code>dq_status</code> to indicate the rule that was violated.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - <code>field</code> (str): The name of the column to check. - <code>check</code> (str): The type of check being performed (e.g., \"negative\"). - <code>value</code> (any): The expected value or condition for the check.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame containing only the rows that violate the rule,</p> <code>DataFrame</code> <p>with an additional column <code>dq_status</code> describing the violation in the format</p> <code>DataFrame</code> <p>\"{field}:{check}:{value}\".</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.is_on_friday","title":"is_on_friday","text":"<pre><code>is_on_friday(df: DataFrame, rule: dict) -&gt; dd.DataFrame\n</code></pre> <p>Filters a Dask DataFrame to identify rows where a specified date field falls on a Friday.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame containing the data to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to have the following keys: - field (str): The name of the column in the DataFrame to check. - check (str): A descriptive string for the check being performed. - value (str): A value associated with the rule, used for status annotation.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame containing only the rows where the specified</p> <code>DataFrame</code> <p>date field falls on a Friday. An additional column <code>dq_status</code> is added to the</p> <code>DataFrame</code> <p>DataFrame, containing a string in the format \"{field}:{check}:{value}\" to indicate</p> <code>DataFrame</code> <p>the rule applied.</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.is_on_monday","title":"is_on_monday","text":"<pre><code>is_on_monday(df: DataFrame, rule: dict) -&gt; dd.DataFrame\n</code></pre> <p>Filters a Dask DataFrame to identify rows where the date in a specified column falls on a Monday.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame containing the data to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to include: - 'field': The name of the column in the DataFrame to check. - 'check': A string representing the type of check (used for status assignment). - 'value': A value associated with the rule (used for status assignment).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame containing only the rows where the date in the specified</p> <code>DataFrame</code> <p>column falls on a Monday. An additional column <code>dq_status</code> is added to indicate the rule</p> <code>DataFrame</code> <p>applied in the format \"{field}:{check}:{value}\".</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.is_on_saturday","title":"is_on_saturday","text":"<pre><code>is_on_saturday(df: DataFrame, rule: dict) -&gt; dd.DataFrame\n</code></pre> <p>Filters a Dask DataFrame to identify rows where the date in a specified column falls on a Saturday.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame containing the data to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to include: - 'field': The name of the column in the DataFrame to check. - 'check': A string representing the type of check (used for status assignment). - 'value': A value associated with the rule (used for status assignment).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame containing only the rows where the date in the specified</p> <code>DataFrame</code> <p>column falls on a Saturday. An additional column <code>dq_status</code> is added to indicate the rule</p> <code>DataFrame</code> <p>applied in the format \"{field}:{check}:{value}\".</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.is_on_sunday","title":"is_on_sunday","text":"<pre><code>is_on_sunday(df: DataFrame, rule: dict) -&gt; dd.DataFrame\n</code></pre> <p>Filters a Dask DataFrame to identify rows where a specified date field falls on a Sunday.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame containing the data to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to include: - field (str): The name of the column in the DataFrame to check. - check (str): A descriptive string for the check being performed. - value (str): A value associated with the rule, used for status annotation.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame containing only the rows where the specified</p> <code>DataFrame</code> <p>date field falls on a Sunday. An additional column <code>dq_status</code> is added to indicate</p> <code>DataFrame</code> <p>the rule applied in the format \"{field}:{check}:{value}\".</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.is_on_thursday","title":"is_on_thursday","text":"<pre><code>is_on_thursday(df: DataFrame, rule: dict) -&gt; dd.DataFrame\n</code></pre> <p>Filters a Dask DataFrame to identify rows where the specified date field falls on a Thursday.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame containing the data to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to include: - field (str): The name of the column in the DataFrame to check. - check (str): A descriptive string for the type of check being performed. - value (str): A value associated with the rule (not used in the logic but included in the output).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame containing only the rows where the specified date field</p> <code>DataFrame</code> <p>falls on a Thursday. An additional column <code>dq_status</code> is added to indicate the rule applied</p> <code>DataFrame</code> <p>in the format \"{field}:{check}:{value}\".</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.is_on_tuesday","title":"is_on_tuesday","text":"<pre><code>is_on_tuesday(df: DataFrame, rule: dict) -&gt; dd.DataFrame\n</code></pre> <p>Filters a Dask DataFrame to identify rows where the specified date field falls on a Tuesday.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame containing the data to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to include: - 'field': The name of the column in the DataFrame to check. - 'check': A string representing the type of check (used for status annotation). - 'value': A value associated with the rule (used for status annotation).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame containing only the rows where the specified date field</p> <code>DataFrame</code> <p>falls on a Tuesday. An additional column <code>dq_status</code> is added to indicate the rule applied</p> <code>DataFrame</code> <p>in the format \"{field}:{check}:{value}\".</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.is_on_wednesday","title":"is_on_wednesday","text":"<pre><code>is_on_wednesday(df: DataFrame, rule: dict) -&gt; dd.DataFrame\n</code></pre> <p>Filters a Dask DataFrame to identify rows where the date in a specified column falls on a Wednesday.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame containing the data to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to include: - <code>field</code> (str): The name of the column in the DataFrame to check. - <code>check</code> (str): A descriptive string for the check being performed. - <code>value</code> (str): A value associated with the rule (not directly used in the function).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame containing only the rows where the date in the specified column</p> <code>DataFrame</code> <p>falls on a Wednesday. An additional column <code>dq_status</code> is added to indicate the rule applied in the</p> <code>DataFrame</code> <p>format <code>{field}:{check}:{value}</code>.</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.is_on_weekday","title":"is_on_weekday","text":"<pre><code>is_on_weekday(df: DataFrame, rule: dict) -&gt; dd.DataFrame\n</code></pre> <p>Filters a Dask DataFrame to include only rows where the date in the specified field falls on a weekday (Monday to Friday) and assigns a data quality status column.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame containing the data to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to have the following keys: - field (str): The name of the column in the DataFrame containing date values. - check (str): A descriptive string for the check being performed. - value (str): A value associated with the rule, used for constructing the <code>dq_status</code> column.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame containing only the rows where the date in the specified field</p> <code>DataFrame</code> <p>falls on a weekday. An additional column <code>dq_status</code> is added to indicate the rule applied.</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.is_on_weekend","title":"is_on_weekend","text":"<pre><code>is_on_weekend(df: DataFrame, rule: dict) -&gt; dd.DataFrame\n</code></pre> <p>Identifies rows in a Dask DataFrame where the date in a specified column falls on a weekend.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame containing the data to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to have          the following keys:          - 'field': The name of the column in the DataFrame to check.          - 'check': A string representing the type of check (used for status annotation).          - 'value': A value associated with the rule (used for status annotation).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame containing only the rows where the date in the specified           column falls on a weekend (Saturday or Sunday). An additional column <code>dq_status</code>           is added to indicate the rule applied in the format \"{field}:{check}:{value}\".</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.is_past_date","title":"is_past_date","text":"<pre><code>is_past_date(df: DataFrame, rule: dict) -&gt; dd.DataFrame\n</code></pre> <p>Checks if the values in a specified date column of a Dask DataFrame are in the past.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame containing the data to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to include          the field name to check, the check type, and additional parameters.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A Dask DataFrame containing rows where the date in the specified column           is in the past. An additional column <code>dq_status</code> is added to indicate           the field, check type, and the date of the check in the format           \"field:check:YYYY-MM-DD\".</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.is_positive","title":"is_positive","text":"<pre><code>is_positive(df: DataFrame, rule: dict) -&gt; dd.DataFrame\n</code></pre> <p>Checks if the values in a specified field of a Dask DataFrame are positive.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to validate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The name of the field to check. - 'check': The type of check being performed (e.g., \"is_positive\"). - 'value': The expected value or condition (e.g., \"0\").</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A DataFrame containing rows where the specified field has</p> <code>DataFrame</code> <p>negative values, with an additional column <code>dq_status</code> indicating the</p> <code>DataFrame</code> <p>field, check, and value that failed.</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.is_primary_key","title":"is_primary_key","text":"<pre><code>is_primary_key(df: DataFrame, rule: dict) -&gt; dd.DataFrame\n</code></pre> <p>Determines if the specified rule identifies a primary key in the given Dask DataFrame.</p> <p>This function checks whether the combination of columns specified in the rule results in unique values across the DataFrame, effectively identifying a primary key.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The Dask DataFrame to evaluate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary defining the rule to check for primary key uniqueness.          Typically, this includes the column(s) to be evaluated.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A Dask DataFrame indicating whether the rule satisfies the primary key condition.</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.is_t_minus_1","title":"is_t_minus_1","text":"<pre><code>is_t_minus_1(df: DataFrame, rule: dict) -&gt; dd.DataFrame\n</code></pre> <p>Filters a Dask DataFrame to identify rows where a specified datetime column matches the date of \"T-1\" (yesterday) and assigns a data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame containing the data to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to include the following keys: - 'field': The name of the column to check. - 'check': A string describing the check being performed. - 'value': Additional value or metadata related to the check.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame containing only the rows where the</p> <code>DataFrame</code> <p>specified column matches \"T-1\". An additional column <code>dq_status</code> is added</p> <code>DataFrame</code> <p>to indicate the data quality status in the format \"{field}:{check}:{value}\".</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.is_t_minus_2","title":"is_t_minus_2","text":"<pre><code>is_t_minus_2(df: DataFrame, rule: dict) -&gt; dd.DataFrame\n</code></pre> <p>Filters a Dask DataFrame to identify rows where a specified datetime column matches the date two days prior to the current date.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame containing the data to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to include the following keys: - 'field': The name of the column in the DataFrame to check. - 'check': A string representing the type of check (used for metadata). - 'value': A value associated with the rule (used for metadata).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame containing only the rows where the specified</p> <code>DataFrame</code> <p>column matches the target date (two days prior to the current date). An additional</p> <code>DataFrame</code> <p>column <code>dq_status</code> is added to indicate the rule applied in the format</p> <code>DataFrame</code> <p>\"{field}:{check}:{value}\".</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.is_t_minus_3","title":"is_t_minus_3","text":"<pre><code>is_t_minus_3(df: DataFrame, rule: dict) -&gt; dd.DataFrame\n</code></pre> <p>Filters a Dask DataFrame to identify rows where the specified date field matches exactly three days prior to the current date.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame containing the data to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing rule parameters. It is expected to include          the field name to check, the type of check, and the value (unused in this function).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A filtered Dask DataFrame containing only the rows where the specified           date field matches three days prior to the current date. An additional           column <code>dq_status</code> is added to indicate the rule applied in the format           \"{field}:{check}:{value}\".</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.is_today","title":"is_today","text":"<pre><code>is_today(df: DataFrame, rule: dict) -&gt; dd.DataFrame\n</code></pre> <p>Filters a Dask DataFrame to identify rows where the specified field matches today's date.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to have the following keys: - field (str): The name of the column in the DataFrame to check. - check (str): A descriptive label for the type of check being performed. - value (str): A descriptive label for the expected value.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame containing only the rows where the specified</p> <code>DataFrame</code> <p>field matches today's date. An additional column <code>dq_status</code> is added to indicate</p> <code>DataFrame</code> <p>the rule applied in the format \"{field}:{check}:{value}\".</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.is_unique","title":"is_unique","text":"<pre><code>is_unique(df: DataFrame, rule: dict) -&gt; dd.DataFrame\n</code></pre> <p>Checks for uniqueness of a specified field in a Dask DataFrame based on a given rule.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The Dask DataFrame to check for uniqueness.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to include: - 'field': The column name to check for uniqueness. - 'check': The type of check being performed (e.g., \"unique\"). - 'value': Additional value or metadata related to the check.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A DataFrame containing rows that violate the uniqueness rule,</p> <code>DataFrame</code> <p>with an additional column <code>dq_status</code> indicating the rule that was violated</p> <code>DataFrame</code> <p>in the format \"{field}:{check}:{value}\".</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.is_yesterday","title":"is_yesterday","text":"<pre><code>is_yesterday(df: DataFrame, rule: dict) -&gt; dd.DataFrame\n</code></pre> <p>Determines if the rows in a Dask DataFrame correspond to \"yesterday\" based on a given rule.</p> <p>This function acts as a wrapper for the <code>is_t_minus_1</code> function, applying the same logic to check if the data corresponds to the previous day.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to evaluate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule or criteria          to determine \"yesterday\".</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A Dask DataFrame with the evaluation results.</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.not_contained_in","title":"not_contained_in","text":"<pre><code>not_contained_in(df: DataFrame, rule: dict) -&gt; dd.DataFrame\n</code></pre> <p>Filters a Dask DataFrame to identify rows where the specified field's value is contained in a given list, and assigns a data quality status to the resulting rows.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The column name in the DataFrame to check. - 'check': The type of check being performed (e.g., \"not_contained_in\"). - 'value': A string representation of a list of values to check against,   formatted as \"[value1, value2, ...]\".</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new DataFrame containing only the rows where the specified</p> <code>DataFrame</code> <p>field's value is in the provided list, with an additional column <code>dq_status</code></p> <code>DataFrame</code> <p>indicating the rule applied in the format \"field:check:value\".</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.not_in","title":"not_in","text":"<pre><code>not_in(df: DataFrame, rule: dict) -&gt; dd.DataFrame\n</code></pre> <p>Filters a Dask DataFrame by excluding rows where the specified rule is satisfied.</p> <p>This function delegates the filtering logic to the <code>not_contained_in</code> function.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary defining the filtering rule. The structure and          interpretation of this rule depend on the implementation of          <code>not_contained_in</code>.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame with rows excluded based on the rule.</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.satisfies","title":"satisfies","text":"<pre><code>satisfies(df: DataFrame, rule: dict) -&gt; dd.DataFrame\n</code></pre> <p>Filters a Dask DataFrame based on a rule and returns rows that do not satisfy the rule.</p> <p>The function evaluates a rule on the given Dask DataFrame and identifies rows that violate the rule. The rule is specified as a dictionary containing a field, a check, and a value. The rule's logical expression is converted to Python syntax for evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary specifying the rule to evaluate. It should contain: - 'field': The column name in the DataFrame to evaluate. - 'check': The type of check or condition to apply. - 'value': The value or expression to evaluate against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame containing rows that do not satisfy the rule.</p> <code>DataFrame</code> <p>An additional column <code>dq_status</code> is added, which contains a string in the format</p> <code>DataFrame</code> <p>\"{field}:{check}:{value}\" to indicate the rule that was violated.</p> Example <p>import dask.dataframe as dd data = {'col1': [1, 2, 3], 'col2': [4, 5, 6]} df = dd.from_pandas(pd.DataFrame(data), npartitions=1) rule = {'field': 'col1', 'check': '&gt;', 'value': '2'} result = satisfies(df, rule) result.compute()</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.summarize","title":"summarize","text":"<pre><code>summarize(qc_ddf: DataFrame, rules: list[dict], total_rows: int) -&gt; pd.DataFrame\n</code></pre> <p>Summarizes quality check results by evaluating rules against a Dask DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>qc_ddf</code> <code>DataFrame</code> <p>A Dask DataFrame containing quality check results. The DataFrame must include a \"dq_status\" column with rule violations in the format \"column:rule:value\".</p> required <code>rules</code> <code>list[dict]</code> <p>A list of dictionaries representing the rules to be evaluated. Each dictionary should include keys such as \"column\", \"rule\", \"value\", and \"pass_threshold\".</p> required <code>total_rows</code> <code>int</code> <p>The total number of rows in the original dataset.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A summarized Pandas DataFrame containing the following columns: - id: Unique identifier for each rule evaluation. - timestamp: Timestamp of the summary generation. - check: The type of check performed (e.g., \"Quality Check\"). - level: The severity level of the check (e.g., \"WARNING\"). - column: The column name associated with the rule. - rule: The rule being evaluated. - value: The value associated with the rule. - rows: The total number of rows in the dataset. - violations: The number of rows that violated the rule. - pass_rate: The proportion of rows that passed the rule. - pass_threshold: The threshold for passing the rule. - status: The status of the rule evaluation (\"PASS\" or \"FAIL\").</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.validate","title":"validate","text":"<pre><code>validate(df: DataFrame, rules: list[dict]) -&gt; tuple[dd.DataFrame, dd.DataFrame]\n</code></pre> <p>Validate a Dask DataFrame against a set of rules and return the aggregated results and raw violations.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to validate.</p> required <code>rules</code> <code>list[dict]</code> <p>A list of validation rules. Each rule is a dictionary containing the following keys: - \"check_type\" (str): The name of the validation function to execute. - \"value\" (optional): The value to be used in the validation function. - \"execute\" (optional, bool): Whether to execute the rule. Defaults to True.</p> required <p>Returns:</p> Type Description <code>tuple[DataFrame, DataFrame]</code> <p>tuple[dd.DataFrame, dd.DataFrame]: - The first DataFrame contains the aggregated validation results,   with a concatenated \"dq_status\" column indicating the validation status. - The second DataFrame contains the raw violations for each rule.</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.validate_date_format","title":"validate_date_format","text":"<pre><code>validate_date_format(df: DataFrame, rule: dict) -&gt; dd.DataFrame\n</code></pre> <p>Validates the date format of a specified column in a Dask DataFrame.</p> <p>This function checks whether the values in a specified column of the DataFrame conform to a given date format. Rows with invalid date formats are returned with an additional column indicating the validation status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The Dask DataFrame to validate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the validation rule. It should          include the following keys:          - 'field': The name of the column to validate.          - 'check': A string describing the validation check.          - 'fmt': The expected date format (e.g., '%Y-%m-%d').</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A DataFrame containing rows where the date format           validation failed. An additional column <code>dq_status</code>           is added, which contains a string describing the           validation status in the format \"{field}:{check}:{fmt}\".</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.validate_schema","title":"validate_schema","text":"<pre><code>validate_schema(df: DataFrame, expected: List[Dict[str, Any]]) -&gt; tuple[bool, list[dict[str, Any]]]\n</code></pre> <p>Validates the schema of a Dask DataFrame against an expected schema.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The Dask DataFrame whose schema is to be validated.</p> required <code>expected</code> <code>List[Dict[str, Any]]</code> <p>A list of dictionaries representing the expected schema. Each dictionary should define the expected column name and its properties.</p> required <p>Returns:</p> Type Description <code>tuple[bool, list[dict[str, Any]]]</code> <p>Tuple[bool, List[Tuple[str, str]]]: A tuple where the first element is a boolean indicating whether the schema matches the expected schema, and the second element is a list of tuples containing mismatched column names and their respective issues.</p>"},{"location":"sumeh/engines/duckdb/","title":"duckdb","text":""},{"location":"sumeh/engines/duckdb/#sumeh.engines.duckdb_engine","title":"sumeh.engines.duckdb_engine","text":"<p>This module provides utilities for generating and validating SQL expressions and data quality rules using DuckDB. It includes functions for building SQL expressions, validating dataframes against rules, summarizing rule violations, and schema validation.</p> <p>Classes:</p> Name Description <code>__RuleCtx</code> <p>A dataclass representing the context required to generate SQL       expressions for data quality rules.</p> <p>Functions:</p> Name Description <code>__escape_single_quotes</code> <p>Escapes single quotes in a string for SQL compatibility.</p> <code>__format_sequence</code> <p>Formats a sequence (list, tuple, or string) into a SQL-compatible representation for IN/NOT IN clauses.</p> <code>_is_complete</code> <p>Generates a SQL expression to check if a column is not NULL.</p> <code>_are_complete</code> <p>Generates a SQL expression to check if all columns in a list are not NULL.</p> <code>_is_unique</code> <p>Generates a SQL expression to check if a column has unique values.</p> <code>_are_unique</code> <p>Generates a SQL expression to check if a combination of columns has unique values.</p> <code>_is_greater_than</code> <p>Generates a SQL expression to check if a column's value is greater than a given value.</p> <code>_is_less_than</code> <p>Generates a SQL expression to check if a column's value is less than a given value.</p> <code>_is_greater_or_equal_than</code> <p>Generates a SQL expression to check if a column's value is greater than or equal to a given value.</p> <code>_is_less_or_equal_than</code> <p>Generates a SQL expression to check if a column's value is less than or equal to a given value.</p> <code>_is_equal_than</code> <p>Generates a SQL expression to check if a column's value is equal to a given value.</p> <code>_is_between</code> <p>Generates a SQL expression to check if a column's value is between two values.</p> <code>_has_pattern</code> <p>Generates a SQL expression to check if a column's value matches a regular expression pattern.</p> <code>_is_contained_in</code> <p>Generates a SQL expression to check if a column's value is in a given sequence.</p> <code>_not_contained_in</code> <p>Generates a SQL expression to check if a column's value is not in a given sequence.</p> <code>_satisfies</code> <p>Generates a SQL expression based on a custom condition provided as a string.</p> <code>_build_union_sql</code> <p>Builds a SQL query that combines multiple rule-based conditions into a UNION ALL query.</p> <code>validate</code> <p>Validates a DuckDB dataframe against a set of rules and returns the results.</p> <code>summarize</code> <p>Summarizes rule violations and calculates pass rates for each rule.</p> <code>validate_schema</code> <p>Validates the schema of a DuckDB table against an expected schema.</p>"},{"location":"sumeh/engines/duckdb/#sumeh.engines.duckdb_engine.__RuleCtx","title":"__RuleCtx  <code>dataclass</code>","text":"<pre><code>__RuleCtx(column: Any, value: Any, name: str)\n</code></pre> <p>__RuleCtx is a context class used to define rules for processing data.</p> <p>Attributes:</p> Name Type Description <code>column</code> <code>Any</code> <p>Represents the column(s) to which the rule applies.           It can be a string (for a single column) or a list of strings (for multiple columns).</p> <code>value</code> <code>Any</code> <p>The value associated with the rule. The type of this value depends on the specific rule implementation.</p> <code>name</code> <code>str</code> <p>The name of the rule, typically used to indicate the type of check being performed.</p>"},{"location":"sumeh/engines/duckdb/#sumeh.engines.duckdb_engine.__escape_single_quotes","title":"__escape_single_quotes","text":"<pre><code>__escape_single_quotes(txt: str) -&gt; str\n</code></pre> <p>Escapes single quotes in a given string by replacing each single quote with two single quotes. This is commonly used to sanitize strings for use in SQL queries.</p> <p>Parameters:</p> Name Type Description Default <code>txt</code> <code>str</code> <p>The input string to process.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The processed string with single quotes escaped.</p>"},{"location":"sumeh/engines/duckdb/#sumeh.engines.duckdb_engine.__format_sequence","title":"__format_sequence","text":"<pre><code>__format_sequence(value: Any) -&gt; str\n</code></pre> <p>Formats a sequence-like input into a string representation suitable for SQL queries.</p> Converts inputs into a tuple-like string format <ul> <li>'BR,US' -&gt; \"('BR','US')\"</li> <li>['BR', 'US'] -&gt; \"('BR','US')\"</li> <li>('BR', 'US') -&gt; \"('BR','US')\"</li> </ul> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>The input value to be formatted. Can be a string, list, or tuple.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A string representation of the input in the format \"('item1','item2',...)\".</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input value is None or cannot be interpreted as a sequence.</p> Notes <ul> <li>If the input is a string, it attempts to parse it as a Python literal.</li> <li>If parsing fails, it splits the string by commas and processes the resulting parts.</li> <li>Empty or invalid elements are ignored in the output.</li> </ul>"},{"location":"sumeh/engines/duckdb/#sumeh.engines.duckdb_engine.__rules_to_duckdb_df","title":"__rules_to_duckdb_df","text":"<pre><code>__rules_to_duckdb_df(rules: List[Dict]) -&gt; str\n</code></pre> <p>Converts a list of rule dictionaries into a DuckDB-compatible SQL query string. Each rule in the input list is processed to generate a SQL <code>SELECT</code> statement with the following fields: - <code>col</code>: The column name(s) associated with the rule. - <code>rule</code>: The name of the rule (check type). - <code>pass_threshold</code>: A numeric threshold value for the rule, defaulting to 1.0 if not provided. - <code>value</code>: The value associated with the rule, which can be a string, list, tuple, or <code>NULL</code>. Rules with the <code>execute</code> field set to <code>False</code> are skipped. If the input list is empty or all rules are skipped, the function returns a SQL query that selects <code>NULL</code> values with a <code>LIMIT 0</code>. Args:     rules (List[Dict]): A list of dictionaries, where each dictionary represents a rule         with the following keys:         - <code>field</code> (str or list): The column(s) associated with the rule.         - <code>check_type</code> (str): The name of the rule.         - <code>value</code> (optional): The value associated with the rule.         - <code>threshold</code> (optional, float): The numeric threshold for the rule.         - <code>execute</code> (optional, bool): Whether the rule should be executed (default is <code>True</code>). Returns:     str: A DuckDB-compatible SQL query string representing the rules.</p>"},{"location":"sumeh/engines/duckdb/#sumeh.engines.duckdb_engine.extract_schema","title":"extract_schema","text":"<pre><code>extract_schema(conn: DuckDBPyConnection, table: str) -&gt; List[Dict[str, Any]]\n</code></pre> <p>Retrieve the schema of a DuckDB table as a list of dictionaries. This function queries the schema of the specified table in a DuckDB database and returns a list of dictionaries where each dictionary represents a column in the table, including its name, data type, nullability, and maximum length. Args:     conn (dk.DuckDBPyConnection): The DuckDB connection object.     table (str): The name of the table whose schema is to be retrieved. Returns:     List[Dict[str, Any]]: A list of dictionaries, each containing the following keys:         - \"field\" (str): The name of the column.         - \"data_type\" (str): The data type of the column in lowercase.         - \"nullable\" (bool): Whether the column allows NULL values.         - \"max_length\" (None): Always None, as DuckDB does not provide maximum length information.</p>"},{"location":"sumeh/engines/duckdb/#sumeh.engines.duckdb_engine.summarize","title":"summarize","text":"<pre><code>summarize(df_rel: DuckDBPyRelation, rules: List[Dict], conn: DuckDBPyConnection, total_rows: Optional[int] = None) -&gt; dk.DuckDBPyRelation\n</code></pre> <p>Summarizes data quality checks for a given DuckDB relation based on specified rules.</p> <p>Parameters:</p> Name Type Description Default <code>df_rel</code> <code>DuckDBPyRelation</code> <p>The DuckDB relation containing the data to be analyzed.</p> required <code>rules</code> <code>List[Dict]</code> <p>A list of dictionaries defining the data quality rules to be applied.</p> required <code>conn</code> <code>DuckDBPyConnection</code> <p>The DuckDB connection used to execute SQL queries.</p> required <code>total_rows</code> <code>Optional[int]</code> <p>The total number of rows in the dataset. If not provided,                         it must be calculated externally.</p> <code>None</code> <p>Returns:</p> Type Description <code>DuckDBPyRelation</code> <p>dk.DuckDBPyRelation: A DuckDB relation containing the summary of data quality checks,                     including pass rates, violation counts, and statuses for each rule.</p> Notes <ul> <li>The function creates a temporary view named \"violations_raw\" from the input relation.</li> <li>It uses SQL to compute violations, pass rates, and statuses based on the provided rules.</li> <li>The output includes metadata such as timestamps, rule thresholds, and overall status     (PASS/FAIL) for each rule.</li> </ul>"},{"location":"sumeh/engines/duckdb/#sumeh.engines.duckdb_engine.validate","title":"validate","text":"<pre><code>validate(df_rel: DuckDBPyRelation, rules: List[Dict], conn: DuckDBPyConnection) -&gt; tuple[DuckDBPyRelation, DuckDBPyRelation]\n</code></pre> <p>Validates a DuckDB relation against a set of rules and returns the processed relation.</p> <p>Parameters:</p> Name Type Description Default <code>df_rel</code> <code>DuckDBPyRelation</code> <p>The input DuckDB relation to be validated.</p> required <code>rules</code> <code>List[Dict]</code> <p>A list of dictionaries representing validation rules.</p> required <code>conn</code> <code>DuckDBPyConnection</code> <p>The DuckDB connection object used for executing SQL queries.</p> required <p>Returns:</p> Type Description <code>tuple[DuckDBPyRelation, DuckDBPyRelation]</code> <p>dk.DuckDBPyRelation: A tuple containing: - The final DuckDB relation with aggregated validation statuses. - The raw DuckDB relation resulting from applying the validation rules.</p> Notes <ul> <li>The function creates a temporary view of the input relation named \"tbl\".</li> <li>Validation rules are combined into a union SQL query using <code>_build_union_sql</code>.</li> <li>The final relation includes all original columns and an aggregated <code>dq_status</code> column.</li> </ul>"},{"location":"sumeh/engines/duckdb/#sumeh.engines.duckdb_engine.validate_schema","title":"validate_schema","text":"<pre><code>validate_schema(conn: DuckDBPyConnection, expected: List[Dict[str, Any]], table: str) -&gt; tuple[bool, list[dict[str, Any]]]\n</code></pre> <p>Validates the schema of a DuckDB table against an expected schema.</p> <p>Parameters:</p> Name Type Description Default <code>conn</code> <code>DuckDBPyConnection</code> <p>The DuckDB connection object.</p> required <code>expected</code> <code>List[Dict[str, Any]]</code> <p>A list of dictionaries representing the expected schema. Each dictionary should define the expected attributes of the schema, such as column names and types.</p> required <code>table</code> <code>str</code> <p>The name of the table whose schema is to be validated.</p> required <p>Returns:</p> Type Description <code>tuple[bool, list[dict[str, Any]]]</code> <p>Tuple[bool, List[Tuple[str, str]]]: A tuple where the first element is a boolean indicating whether the actual schema matches the expected schema, and the second element is a list of tuples describing the mismatches (if any). Each tuple contains the column name and a description of the mismatch.</p>"},{"location":"sumeh/engines/engines/","title":"Engines Overview","text":"<p>Sumeh supports multiple DataFrame processing engines to validate data at any scale, from local CSVs to distributed big data workloads.</p>"},{"location":"sumeh/engines/engines/#what-are-engines","title":"What are Engines?","text":"<p>Engines are adapters that allow Sumeh to work with different DataFrame implementations. Each engine provides the same validation API but optimized for different use cases and scales.</p> <pre><code>from sumeh import validate, summarize\nfrom sumeh.core.config import get_rules_config\n\n# Works with ANY engine!\nrules = get_rules_config(source=\"rules.csv\")\ninvalid_raw, invalid_agg = validate(df, rules)\nsummary = summarize(invalid_raw, rules, total_rows=len(df))\n</code></pre> <p>Sumeh automatically detects which engine your DataFrame is using and applies the appropriate validation logic.</p>"},{"location":"sumeh/engines/engines/#supported-engines","title":"Supported Engines","text":"Engine Best For Scale Speed Memory Pandas General purpose, prototyping Small-Medium Moderate High Polars Fast analytics, large datasets Medium-Large Very Fast Low Dask Distributed computing, clusters Large-Huge Parallel Scalable PySpark Big data, Spark clusters Huge Parallel Distributed DuckDB Analytical queries, OLAP Medium-Large Very Fast Low BigQuery Cloud data warehouse Huge Serverless Cloud"},{"location":"sumeh/engines/engines/#engine-selection-guide","title":"Engine Selection Guide","text":""},{"location":"sumeh/engines/engines/#pandas","title":"\ud83d\udc3c Pandas","text":"<p>Use when: - Dataset &lt; 10GB - Local development - Quick prototyping - Standard data science workflow</p> <pre><code>import pandas as pd\nfrom sumeh import validate\n\ndf = pd.read_csv(\"data.csv\")\ninvalid_raw, invalid_agg = validate(df, rules)\n</code></pre> <p>Pros: Simple, widely-used, great ecosystem Cons: Single-threaded, memory-intensive</p>"},{"location":"sumeh/engines/engines/#polars","title":"\ud83d\udc3b\u200d\u2744\ufe0f Polars","text":"<p>Use when: - Dataset 1GB - 100GB - Need better performance than Pandas - Memory efficiency is important - Modern Python syntax</p> <pre><code>import polars as pl\nfrom sumeh import validate\n\ndf = pl.read_csv(\"data.csv\")\ninvalid_raw, invalid_agg = validate(df, rules)\n</code></pre> <p>Pros: 10-100x faster than Pandas, lazy evaluation, low memory Cons: Newer ecosystem, fewer integrations</p>"},{"location":"sumeh/engines/engines/#dask","title":"\u26a1 Dask","text":"<p>Use when: - Dataset &gt; 100GB - Multi-core processing needed - Out-of-memory computation - Scaling to multiple machines</p> <pre><code>import dask.dataframe as dd\nfrom sumeh import validate\n\ndf = dd.read_csv(\"data/*.csv\")\ninvalid_raw, invalid_agg = validate(df, rules)\n</code></pre> <p>Pros: Scales to clusters, familiar Pandas API, parallel Cons: Overhead for small data, complex debugging</p>"},{"location":"sumeh/engines/engines/#pyspark","title":"\ud83d\udd25 PySpark","text":"<p>Use when: - Dataset &gt; 1TB - Already using Spark infrastructure - Complex distributed processing - Databricks, EMR, or Dataproc</p> <pre><code>from pyspark.sql import SparkSession\nfrom sumeh import validate\n\nspark = SparkSession.builder.getOrCreate()\ndf = spark.read.csv(\"data.csv\", header=True)\ninvalid_raw, invalid_agg = validate(df, rules)\n</code></pre> <p>Pros: Industry standard for big data, fault-tolerant, mature Cons: JVM overhead, complex setup, verbose API</p>"},{"location":"sumeh/engines/engines/#duckdb","title":"\ud83e\udd86 DuckDB","text":"<p>Use when: - Analytical queries on large CSVs/Parquet - Need SQL-like performance - In-process OLAP - Embedded analytics</p> <pre><code>import duckdb\nfrom sumeh import validate\n\nconn = duckdb.connect()\ndf = conn.execute(\"SELECT * FROM 'data.parquet'\").df()\ninvalid_raw, invalid_agg = validate(df, rules, conn=conn)\n</code></pre> <p>Pros: Extremely fast for analytics, SQL support, columnar Cons: Less mature than Pandas, limited ML ecosystem</p>"},{"location":"sumeh/engines/engines/#bigquery","title":"\u2601\ufe0f BigQuery","text":"<p>Use when: - Data already in BigQuery - Serverless compute needed - Petabyte-scale datasets - Google Cloud Platform</p> <pre><code>from google.cloud import bigquery\nfrom sumeh import validate\n\nclient = bigquery.Client()\nquery = \"SELECT * FROM `project.dataset.table` LIMIT 1000000\"\ndf = client.query(query).to_dataframe()\ninvalid_raw, invalid_agg = validate(df, rules)\n</code></pre> <p>Pros: Serverless, auto-scaling, SQL interface Cons: Network latency, costs per query</p>"},{"location":"sumeh/engines/engines/#engine-auto-detection","title":"Engine Auto-Detection","text":"<p>Sumeh automatically detects your DataFrame engine:</p> <pre><code>from sumeh.core import __detect_engine\n\n# Returns engine name based on DataFrame type\nengine = __detect_engine(df)\n# \u2192 \"pandas_engine\" | \"polars_engine\" | \"pyspark_engine\" | ...\n</code></pre> <p>Detection logic:</p> <pre><code>def __detect_engine(df):\n    mod = df.__class__.__module__\n\n    if mod.startswith(\"pyspark\"):\n        return \"pyspark_engine\"\n    elif mod.startswith(\"dask\"):\n        return \"dask_engine\"\n    elif mod.startswith(\"polars\"):\n        return \"polars_engine\"\n    elif mod.startswith(\"pandas\"):\n        return \"pandas_engine\"\n    elif mod.startswith(\"duckdb\"):\n        return \"duckdb_engine\"\n    else:\n        raise TypeError(f\"Unsupported DataFrame type: {type(df)}\")\n</code></pre>"},{"location":"sumeh/engines/engines/#cli-engine-selection","title":"CLI Engine Selection","text":"<p>Use the <code>--engine</code> flag to specify which engine to use for loading data:</p> <pre><code># Use Pandas (default)\nsumeh validate data.csv rules.csv\n\n# Use Polars for better performance\nsumeh validate data.csv rules.csv --engine polars\n\n# Use Dask for large files\nsumeh validate data.csv rules.csv --engine dask\n</code></pre> <p>Available engines: - <code>pandas</code> (default) - <code>polars</code> - <code>dask</code></p>"},{"location":"sumeh/engines/engines/#engine-specific-features","title":"Engine-Specific Features","text":""},{"location":"sumeh/engines/engines/#context-parameters","title":"Context Parameters","text":"<p>Some engines require additional context:</p> <pre><code># DuckDB requires connection\nimport duckdb\nconn = duckdb.connect()\ninvalid_raw, invalid_agg = validate(df, rules, conn=conn)\n\n# PySpark can use custom executors\ninvalid_raw, invalid_agg = validate(\n    df, \n    rules, \n    num_partitions=100\n)\n</code></pre>"},{"location":"sumeh/engines/engines/#lazy-evaluation","title":"Lazy Evaluation","text":"<p>Polars and Dask support lazy evaluation:</p> <pre><code>import polars as pl\n\n# Build computation graph (lazy)\ndf = pl.scan_csv(\"data.csv\")\ndf_filtered = df.filter(pl.col(\"age\") &gt; 18)\n\n# Execute validation (triggers computation)\ninvalid_raw, invalid_agg = validate(df_filtered, rules)\n</code></pre>"},{"location":"sumeh/engines/engines/#custom-engine-implementation","title":"Custom Engine Implementation","text":"<p>Want to add support for a new DataFrame library? Implement these functions:</p> <pre><code># sumeh/engines/myengine_engine.py\n\ndef validate(df, rules):\n    \"\"\"Run validation checks.\"\"\"\n    # Implementation here\n    return invalid_raw, invalid_agg\n\ndef validate_schema(df, expected, **kwargs):\n    \"\"\"Validate schema matches expected.\"\"\"\n    # Implementation here\n    return is_valid, errors\n\ndef summarize(df, rules, total_rows=None):\n    \"\"\"Summarize validation results.\"\"\"\n    # Implementation here\n    return summary_df\n</code></pre> <p>See Custom Engine Guide for details.</p>"},{"location":"sumeh/engines/engines/#best-practices","title":"Best Practices","text":""},{"location":"sumeh/engines/engines/#1-match-engine-to-data-size","title":"1. Match Engine to Data Size","text":"<pre><code># &lt; 1GB \u2192 Pandas\nif data_size &lt; 1_000_000_000:\n    engine = \"pandas\"\n\n# 1-100GB \u2192 Polars or DuckDB\nelif data_size &lt; 100_000_000_000:\n    engine = \"polars\"  # or \"duckdb\"\n\n# &gt; 100GB \u2192 Dask or Spark\nelse:\n    engine = \"dask\"  # or \"pyspark\"\n</code></pre>"},{"location":"sumeh/engines/engines/#2-use-lazy-evaluation","title":"2. Use Lazy Evaluation","text":"<pre><code># Polars\ndf = pl.scan_csv(\"*.csv\")  # Lazy\ndf = df.filter(...)  # Still lazy\nresult = validate(df, rules)  # Triggers execution\n\n# Dask\ndf = dd.read_csv(\"*.csv\")  # Lazy\ndf = df[df.age &gt; 18]  # Still lazy\nresult = validate(df, rules)  # Triggers execution\n</code></pre>"},{"location":"sumeh/engines/engines/#3-optimize-for-your-environment","title":"3. Optimize for Your Environment","text":"<pre><code># Local laptop \u2192 Pandas/Polars\n# Corporate server \u2192 DuckDB/Dask\n# Cloud \u2192 BigQuery/Spark\n# Edge/IoT \u2192 Polars (smallest footprint)\n</code></pre>"},{"location":"sumeh/engines/engines/#4-consider-data-format","title":"4. Consider Data Format","text":"Format Best Engine CSV DuckDB, Polars Parquet DuckDB, Polars, Spark JSON Pandas, Polars Database Native connectors Cloud Storage BigQuery, Spark"},{"location":"sumeh/engines/engines/#troubleshooting","title":"Troubleshooting","text":""},{"location":"sumeh/engines/engines/#unsupported-dataframe-type","title":"\"Unsupported DataFrame type\"","text":"<p>Problem: Engine not detected</p> <p>Solution: Check if the DataFrame is from a supported library</p> <pre><code>print(type(df))\nprint(df.__class__.__module__)\n\n# Should be one of:\n# - pandas.core.frame.DataFrame\n# - polars.dataframe.frame.DataFrame\n# - dask.dataframe.core.DataFrame\n# - pyspark.sql.dataframe.DataFrame\n</code></pre>"},{"location":"sumeh/engines/engines/#memory-errors","title":"Memory Errors","text":"<p>Problem: Pandas runs out of memory</p> <p>Solutions: 1. Switch to Polars (lower memory footprint) 2. Use Dask (out-of-core processing) 3. Sample the data first 4. Use columnar format (Parquet)</p> <pre><code># Option 1: Polars\nimport polars as pl\ndf = pl.read_csv(\"large.csv\")\n\n# Option 2: Dask\nimport dask.dataframe as dd\ndf = dd.read_csv(\"large.csv\")\n\n# Option 3: Sample\ndf = pd.read_csv(\"large.csv\", nrows=100000)\n</code></pre>"},{"location":"sumeh/engines/engines/#slow-performance","title":"Slow Performance","text":"<p>Problem: Validation takes too long</p> <p>Solutions: 1. Use faster engine (Polars, DuckDB) 2. Parallelize with Dask/Spark 3. Reduce data size (filter first) 4. Optimize rules (remove duplicates)</p> <pre><code># Use faster engine\nimport polars as pl\ndf = pl.read_csv(\"data.csv\")\n\n# Or parallelize\nimport dask.dataframe as dd\ndf = dd.read_csv(\"data.csv\")\n</code></pre>"},{"location":"sumeh/engines/engines/#migration-guide","title":"Migration Guide","text":""},{"location":"sumeh/engines/engines/#pandas-polars","title":"Pandas \u2192 Polars","text":"<pre><code># Before (Pandas)\nimport pandas as pd\ndf = pd.read_csv(\"data.csv\")\ndf_clean = df[df.age &gt; 18]\n\n# After (Polars)\nimport polars as pl\ndf = pl.read_csv(\"data.csv\")\ndf_clean = df.filter(pl.col(\"age\") &gt; 18)\n\n# Validation works the same!\ninvalid_raw, invalid_agg = validate(df_clean, rules)\n</code></pre>"},{"location":"sumeh/engines/engines/#pandas-dask","title":"Pandas \u2192 Dask","text":"<pre><code># Before (Pandas)\nimport pandas as pd\ndf = pd.read_csv(\"data.csv\")\n\n# After (Dask)\nimport dask.dataframe as dd\ndf = dd.read_csv(\"data.csv\")\n\n# Validation works the same!\ninvalid_raw, invalid_agg = validate(df, rules)\n</code></pre>"},{"location":"sumeh/engines/pandas/","title":"pandas","text":""},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine","title":"sumeh.engines.pandas_engine","text":"<p>This module provides a set of data quality validation functions using the Pandas library. It includes various checks for data validation, such as completeness, uniqueness, range checks, pattern matching, date validations, SQL-style custom expressions, and schema validation.</p> <p>Functions:</p> Name Description <code>is_positive</code> <p>Filters rows where the specified field is less than zero.</p> <code>is_negative</code> <p>Filters rows where the specified field is greater than or equal to zero.</p> <code>is_in_millions</code> <p>Retains rows where the field value is at least 1,000,000 and flags them with dq_status.</p> <code>is_in_billions</code> <p>Retains rows where the field value is at least 1,000,000,000 and flags them with dq_status.</p> <code>is_t_minus_1</code> <p>Retains rows where the date field equals yesterday (T-1) and flags them with dq_status.</p> <code>is_t_minus_2</code> <p>Retains rows where the date field equals two days ago (T-2) and flags them with dq_status.</p> <code>is_t_minus_3</code> <p>Retains rows where the date field equals three days ago (T-3) and flags them with dq_status.</p> <code>is_today</code> <p>Retains rows where the date field equals today and flags them with dq_status.</p> <code>is_yesterday</code> <p>Retains rows where the date field equals yesterday and flags them with dq_status.</p> <code>is_on_weekday</code> <p>Retains rows where the date field falls on a weekday (Mon-Fri) and flags them with dq_status.</p> <code>is_on_weekend</code> <p>Retains rows where the date field is on a weekend (Sat-Sun) and flags them with dq_status.</p> <code>is_on_monday</code> <p>Retains rows where the date field is on Monday and flags them with dq_status.</p> <code>is_on_tuesday</code> <p>Retains rows where the date field is on Tuesday and flags them with dq_status.</p> <code>is_on_wednesday</code> <p>Retains rows where the date field is on Wednesday and flags them with dq_status.</p> <code>is_on_thursday</code> <p>Retains rows where the date field is on Thursday and flags them with dq_status.</p> <code>is_on_friday</code> <p>Retains rows where the date field is on Friday and flags them with dq_status.</p> <code>is_on_saturday</code> <p>Retains rows where the date field is on Saturday and flags them with dq_status.</p> <code>is_on_sunday</code> <p>Retains rows where the date field is on Sunday and flags them with dq_status.</p> <code>is_complete</code> <p>Filters rows where the specified field is null.</p> <code>is_unique</code> <p>Filters rows with duplicate values in the specified field.</p> <code>are_complete</code> <p>Filters rows where any of the specified fields are null.</p> <code>are_unique</code> <p>Filters rows with duplicate combinations of the specified fields.</p> <code>is_greater_than</code> <p>Filters rows where the specified field is less than or equal to the given value.</p> <code>is_greater_or_equal_than</code> <p>Filters rows where the specified field is less than the given value.</p> <code>is_less_than</code> <p>Filters rows where the specified field is greater than or equal to the given value.</p> <code>is_less_or_equal_than</code> <p>Filters rows where the specified field is greater than the given value.</p> <code>is_equal</code> <p>Filters rows where the specified field is not equal to the given value.</p> <code>is_equal_than</code> <p>Alias for <code>is_equal</code>.</p> <code>is_contained_in</code> <p>Filters rows where the specified field is not in the given list of values.</p> <code>not_contained_in</code> <p>Filters rows where the specified field is in the given list of values.</p> <code>is_between</code> <p>Filters rows where the specified field is not within the given range.</p> <code>has_pattern</code> <p>Filters rows where the specified field does not match the given regex pattern.</p> <code>is_legit</code> <p>Filters rows where the specified field is null or contains whitespace.</p> <code>has_max</code> <p>Filters rows where the specified field exceeds the given maximum value.</p> <code>has_min</code> <p>Filters rows where the specified field is below the given minimum value.</p> <code>has_std</code> <p>Checks if the standard deviation of the specified field exceeds the given value.</p> <code>has_mean</code> <p>Checks if the mean of the specified field exceeds the given value.</p> <code>has_sum</code> <p>Checks if the sum of the specified field exceeds the given value.</p> <code>has_cardinality</code> <p>Checks if the cardinality (number of unique values) of the specified field exceeds the given value.</p> <code>has_infogain</code> <p>Placeholder for information gain validation (currently uses cardinality).</p> <code>has_entropy</code> <p>Placeholder for entropy validation (currently uses cardinality).</p> <code>satisfies</code> <p>Filters rows that do not satisfy the given custom expression.</p> <code>validate_date_format</code> <p>Filters rows where the specified field does not match the expected date format or is null.</p> <code>is_future_date</code> <p>Filters rows where the specified date field is after today\u2019s date.</p> <code>is_past_date</code> <p>Filters rows where the specified date field is before today\u2019s date.</p> <code>is_date_between</code> <p>Filters rows where the specified date field is not within the given [start,end] range.</p> <code>is_date_after</code> <p>Filters rows where the specified date field is before the given date.</p> <code>is_date_before</code> <p>Filters rows where the specified date field is after the given date.</p> <code>all_date_checks</code> <p>Alias for <code>is_past_date</code> (checks date against today).</p> <code>validate</code> <p>Validates a DataFrame against a list of rules and returns the original DataFrame with data quality status and a DataFrame of violations.</p> <code>__build_rules_df</code> <p>Converts a list of rules into a Pandas DataFrame for summarization.</p> <code>summarize</code> <p>Summarizes the results of data quality checks, including pass rates and statuses.</p> <code>validate_schema</code> <p>Validates the schema of a DataFrame against an expected schema and returns a boolean result and a list of errors.</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.__build_rules_df","title":"__build_rules_df","text":"<pre><code>__build_rules_df(rules: List[dict]) -&gt; pd.DataFrame\n</code></pre> <p>Builds a pandas DataFrame from a list of rule dictionaries.</p> <p>Parameters:</p> Name Type Description Default <code>rules</code> <code>List[dict]</code> <p>A list of dictionaries where each dictionary represents a rule. Each rule dictionary may contain the following keys:     - \"field\" (str or list): The column(s) the rule applies to.     - \"check_type\" (str): The type of check or rule to apply.     - \"value\" (optional): The value associated with the rule.     - \"threshold\" (optional): A numeric threshold for the rule. Defaults to 1.0 if not provided or invalid.     - \"execute\" (optional): A boolean indicating whether the rule should be executed. Defaults to True.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing the processed rules with the following columns: - \"column\": The column(s) the rule applies to, as a comma-separated string if multiple. - \"rule\": The type of check or rule. - \"value\": The value associated with the rule, or an empty string if not provided. - \"pass_threshold\": The numeric threshold for the rule.</p> Notes <ul> <li>Rules with \"execute\" set to False are skipped.</li> <li>Duplicate rows based on \"column\", \"rule\", and \"value\" are removed from the resulting DataFrame.</li> </ul>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.all_date_checks","title":"all_date_checks","text":"<pre><code>all_date_checks(df: DataFrame, rule: dict) -&gt; pd.DataFrame\n</code></pre> <p>Applies all date-related validation checks on the given DataFrame based on the specified rule.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing the data to be validated.</p> required <code>rule</code> <code>dict</code> <p>A dictionary specifying the validation rules to be applied.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame with the results of the date validation checks.</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.are_complete","title":"are_complete","text":"<pre><code>are_complete(df: DataFrame, rule: dict) -&gt; pd.DataFrame\n</code></pre> <p>Checks for completeness of specified fields in a DataFrame based on a given rule.</p> <p>This function identifies rows in the DataFrame where any of the specified fields contain missing values (NaN). It returns a DataFrame containing only the rows that violate the completeness rule, along with an additional column <code>dq_status</code> that describes the rule violation.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to check for completeness.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to include the following keys: - fields: A list of column names to check for completeness. - check: A string describing the type of check (e.g., \"completeness\"). - value: A value associated with the rule (e.g., a threshold or description).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing rows that violate the completeness rule.</p> <code>DataFrame</code> <p>The returned DataFrame includes all original columns and an additional column</p> <code>DataFrame</code> <p><code>dq_status</code> that describes the rule violation in the format \"fields:check:value\".</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.are_unique","title":"are_unique","text":"<pre><code>are_unique(df: DataFrame, rule: dict) -&gt; pd.DataFrame\n</code></pre> <p>Checks for duplicate rows in the specified fields of a DataFrame based on a given rule.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to check for uniqueness.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - fields: A list of column names to check for uniqueness. - check: A string representing the type of check (e.g., \"unique\"). - value: A value associated with the rule (e.g., a description or identifier).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing the rows that violate the uniqueness rule.           An additional column 'dq_status' is added to indicate the rule           that was violated in the format \"{fields}:{check}:{value}\".</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.has_cardinality","title":"has_cardinality","text":"<pre><code>has_cardinality(df: DataFrame, rule: dict) -&gt; pd.DataFrame\n</code></pre> <p>Checks if the cardinality (number of unique values) of a specified field in the DataFrame exceeds a given value and returns a modified DataFrame if the condition is met.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to check.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The column name in the DataFrame to check. - 'check': The type of check being performed (e.g., 'cardinality'). - 'value': The threshold value for the cardinality.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: - If the cardinality of the specified field exceeds the given value,   a copy of the DataFrame is returned with an additional column 'dq_status'   indicating the field, check, and value. - If the cardinality does not exceed the value, an empty DataFrame is returned.</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.has_entropy","title":"has_entropy","text":"<pre><code>has_entropy(df: DataFrame, rule: dict) -&gt; pd.DataFrame\n</code></pre> <p>Checks if the given DataFrame satisfies a specific rule related to entropy.</p> <p>This function is a wrapper around the <code>has_cardinality</code> function, delegating the rule-checking logic to it.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to be evaluated.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule to be applied.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The resulting DataFrame after applying the rule.</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.has_infogain","title":"has_infogain","text":"<pre><code>has_infogain(df: DataFrame, rule: dict) -&gt; pd.DataFrame\n</code></pre> <p>Checks if the given DataFrame satisfies the information gain criteria defined by the provided rule. This function internally delegates the operation to the <code>has_cardinality</code> function.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to be evaluated.</p> required <code>rule</code> <code>dict</code> <p>A dictionary defining the rule for information gain.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The resulting DataFrame after applying the rule.</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.has_max","title":"has_max","text":"<pre><code>has_max(df: DataFrame, rule: dict) -&gt; pd.DataFrame\n</code></pre> <p>Identifies rows in a DataFrame where the value in a specified field exceeds a given maximum value.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The column name to check. - 'check' (str): The type of check being performed (e.g., 'max'). - 'value' (numeric): The maximum allowable value for the specified field.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing rows that violate the rule, with an additional column</p> <code>DataFrame</code> <p>'dq_status' indicating the rule violation in the format \"field:check:value\".</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.has_mean","title":"has_mean","text":"<pre><code>has_mean(df: DataFrame, rule: dict) -&gt; pd.DataFrame\n</code></pre> <p>Checks if the mean of a specified column in a DataFrame satisfies a given condition.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to evaluate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The column name to calculate the mean for. - 'check' (str): The condition to check (e.g., 'greater_than'). - 'value' (float): The threshold value to compare the mean against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A copy of the input DataFrame with an additional column 'dq_status'</p> <code>DataFrame</code> <p>if the condition is met. The 'dq_status' column contains a string in the format</p> <code>DataFrame</code> <p>\"{field}:{check}:{value}\". If the condition is not met, an empty DataFrame is returned.</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.has_min","title":"has_min","text":"<pre><code>has_min(df: DataFrame, rule: dict) -&gt; pd.DataFrame\n</code></pre> <p>Filters a DataFrame to identify rows where a specified field's value is less than a given threshold.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The column name in the DataFrame to be checked. - 'check': The type of check being performed (e.g., 'min'). - 'value': The threshold value for the check.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A new DataFrame containing rows that violate the rule, with an additional</p> <code>DataFrame</code> <p>column 'dq_status' indicating the field, check type, and threshold value.</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.has_pattern","title":"has_pattern","text":"<pre><code>has_pattern(df: DataFrame, rule: dict) -&gt; pd.DataFrame\n</code></pre> <p>Checks if the values in a specified column of a DataFrame match a given pattern.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to check.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The column name in the DataFrame to check. - 'check': A descriptive label for the check being performed. - 'pattern': The regex pattern to match against the column values.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing rows that do not match the pattern.           An additional column 'dq_status' is added to indicate the           field, check, and pattern that caused the violation.</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.has_std","title":"has_std","text":"<pre><code>has_std(df: DataFrame, rule: dict) -&gt; pd.DataFrame\n</code></pre> <p>Checks if the standard deviation of a specified field in the DataFrame exceeds a given value.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to evaluate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The column name in the DataFrame to calculate the standard deviation for. - 'check': A string representing the type of check (not used in the logic but included in the output). - 'value': A numeric threshold to compare the standard deviation against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: - If the standard deviation of the specified field exceeds the given value,   returns a copy of the DataFrame with an additional column 'dq_status' indicating the rule details. - If the standard deviation does not exceed the value, returns an empty DataFrame with the same structure as the input.</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.has_sum","title":"has_sum","text":"<pre><code>has_sum(df: DataFrame, rule: dict) -&gt; pd.DataFrame\n</code></pre> <p>Checks if the sum of values in a specified column of a DataFrame exceeds a given threshold.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing the data to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The column name to calculate the sum for. - 'check' (str): A descriptive label for the check (used in the output). - 'value' (float): The threshold value to compare the sum against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: - If the sum of the specified column exceeds the threshold, returns a copy of the input DataFrame   with an additional column 'dq_status' indicating the rule that was applied. - If the sum does not exceed the threshold, returns an empty DataFrame with the same structure as the input.</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.is_between","title":"is_between","text":"<pre><code>is_between(df: DataFrame, rule: dict) -&gt; pd.DataFrame\n</code></pre> <p>Filters a DataFrame to identify rows where a specified field's values are not within a given range.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The column name in the DataFrame to check. - 'check': A descriptive label for the check being performed. - 'value': A string representation of the range in the format '[lo, hi]'.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing rows that violate the range condition.           An additional column 'dq_status' is added to indicate the rule violation in the format 'field:check:value'.</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.is_complete","title":"is_complete","text":"<pre><code>is_complete(df: DataFrame, rule: dict) -&gt; pd.DataFrame\n</code></pre> <p>Checks for missing values in a specified field of a DataFrame based on a given rule.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to check for completeness.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The name of the field/column to check for missing values. - 'check': The type of check being performed (not used in this function). - 'value': Additional value associated with the rule (not used in this function).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing rows where the specified field has missing values.           An additional column 'dq_status' is added to indicate the rule that was violated.</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.is_contained_in","title":"is_contained_in","text":"<pre><code>is_contained_in(df: DataFrame, rule: dict) -&gt; pd.DataFrame\n</code></pre> <p>Filters a DataFrame to identify rows where the values in a specified field are not contained within a given set of values.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected          to include the following keys:          - 'field': The column name in the DataFrame to check.          - 'check': A descriptive string for the check being performed.          - 'value': A list or string representation of the allowed values.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing rows from the input DataFrame that           do not meet the rule criteria. An additional column           'dq_status' is added to indicate the rule violation in           the format \"field:check:value\".</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.is_date_after","title":"is_date_after","text":"<pre><code>is_date_after(df: DataFrame, rule: dict) -&gt; pd.DataFrame\n</code></pre> <p>Filters a DataFrame to return rows where a specified date field is earlier than a given target date.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing the data to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - field (str): The name of the column in the DataFrame to check. - check (str): A descriptive label for the check being performed. - date_str (str): The target date as a string in a format parsable by <code>pd.to_datetime</code>.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing rows where the date in the specified field is earlier</p> <code>DataFrame</code> <p>than the target date. An additional column <code>dq_status</code> is added to indicate the rule that</p> <code>DataFrame</code> <p>was violated in the format \"{field}:{check}:{date_str}\".</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.is_date_before","title":"is_date_before","text":"<pre><code>is_date_before(df: DataFrame, rule: dict) -&gt; pd.DataFrame\n</code></pre> <p>Filters a DataFrame to identify rows where a date field is after a specified target date.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing the data to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - field (str): The name of the column in the DataFrame containing date values. - check (str): A descriptive label for the check being performed. - date_str (str): The target date as a string in a format parsable by <code>pd.to_datetime</code>.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing rows where the date in the specified field is after</p> <code>DataFrame</code> <p>the target date. An additional column <code>dq_status</code> is added to indicate the rule that was</p> <code>DataFrame</code> <p>violated in the format \"{field}:{check}:{date_str}\".</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.is_date_between","title":"is_date_between","text":"<pre><code>is_date_between(df: DataFrame, rule: dict) -&gt; pd.DataFrame\n</code></pre> <p>Filters rows in a DataFrame where the values in a specified date column are not within a given date range.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing the data to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected          to include the following:          - field: The name of the column to check.          - check: A string representing the type of check (used for                   status annotation).          - raw: A string representing the date range in the format                 '[start_date, end_date]'.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing the rows where the date values in           the specified column are outside the given range. An           additional column 'dq_status' is added to indicate the           rule that was violated.</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.is_equal","title":"is_equal","text":"<pre><code>is_equal(df: DataFrame, rule: dict) -&gt; pd.DataFrame\n</code></pre> <p>Filters a DataFrame to identify rows where the value in a specified field does not match a given value, and annotates these rows with a data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The column name in the DataFrame to check. - 'check': A string describing the check being performed (e.g., \"is_equal\"). - 'value': The value to compare against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing rows that do not satisfy the equality check.</p> <code>DataFrame</code> <p>An additional column 'dq_status' is added to indicate the data quality status</p> <code>DataFrame</code> <p>in the format \"{field}:{check}:{value}\".</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.is_equal_than","title":"is_equal_than","text":"<pre><code>is_equal_than(df: DataFrame, rule: dict) -&gt; pd.DataFrame\n</code></pre> <p>Compares the values in a DataFrame against a specified rule and returns the result.</p> <p>This function acts as a wrapper for the <code>is_equal</code> function, passing the given DataFrame and rule to it.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The DataFrame to be evaluated.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the comparison rule.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame indicating the result of the comparison.</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.is_future_date","title":"is_future_date","text":"<pre><code>is_future_date(df: DataFrame, rule: dict) -&gt; pd.DataFrame\n</code></pre> <p>Identifies rows in a DataFrame where the date in a specified field is in the future.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing the data to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to include          the field name to check and the check type.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing only the rows where the date in the specified           field is in the future. An additional column 'dq_status' is added to           indicate the field, check type, and the current date in ISO format.</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.is_greater_or_equal_than","title":"is_greater_or_equal_than","text":"<pre><code>is_greater_or_equal_than(df: DataFrame, rule: dict) -&gt; pd.DataFrame\n</code></pre> <p>Filters a DataFrame to include only rows where the value in a specified field is greater than or equal to a given threshold. Adds a 'dq_status' column to indicate the rule applied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The column name to apply the rule on. - 'check' (str): The type of check being performed (e.g., 'greater_or_equal'). - 'value' (numeric): The threshold value for the comparison.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A new DataFrame containing only the rows that satisfy the rule,</p> <code>DataFrame</code> <p>with an additional 'dq_status' column describing the rule applied.</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.is_greater_than","title":"is_greater_than","text":"<pre><code>is_greater_than(df: DataFrame, rule: dict) -&gt; pd.DataFrame\n</code></pre> <p>Filters a DataFrame to return rows where a specified field's value is greater than a given threshold.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The column name in the DataFrame to be checked. - 'check' (str): The type of check being performed (e.g., 'greater_than'). - 'value' (numeric): The threshold value to compare against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A new DataFrame containing rows where the specified field's value is greater than the given threshold.           An additional column 'dq_status' is added to indicate the rule applied in the format \"field:check:value\".</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.is_in","title":"is_in","text":"<pre><code>is_in(df: DataFrame, rule: dict) -&gt; pd.DataFrame\n</code></pre> <p>Checks if the values in a DataFrame satisfy a given rule by delegating the operation to the <code>is_contained_in</code> function.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to be evaluated.</p> required <code>rule</code> <code>dict</code> <p>A dictionary defining the rule to check against the DataFrame.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame indicating whether each element satisfies the rule.</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.is_in_billions","title":"is_in_billions","text":"<pre><code>is_in_billions(df: DataFrame, rule: dict) -&gt; pd.DataFrame\n</code></pre> <p>Filters a DataFrame to include only rows where the specified field's value is greater than or equal to one billion, and adds a data quality status column.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - field (str): The column name to check. - check (str): The type of check being performed (used for status annotation). - value (any): The value associated with the rule (used for status annotation).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A new DataFrame containing rows where the specified field's</p> <code>DataFrame</code> <p>value is greater than or equal to one billion. Includes an additional</p> <code>DataFrame</code> <p>column <code>dq_status</code> with the format \"{field}:{check}:{value}\".</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.is_in_millions","title":"is_in_millions","text":"<pre><code>is_in_millions(df: DataFrame, rule: dict) -&gt; pd.DataFrame\n</code></pre> <p>Filters rows in the DataFrame where the specified field's value is greater than or equal to one million and adds a \"dq_status\" column with a formatted string indicating the rule applied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to include: - field (str): The column name to check. - check (str): The type of check being performed (e.g., \"greater_than\"). - value (any): The value associated with the rule (not used in this function).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A new DataFrame containing rows where the specified field's value is &gt;= 1,000,000.           Includes an additional \"dq_status\" column with the rule details.</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.is_legit","title":"is_legit","text":"<pre><code>is_legit(df: DataFrame, rule: dict) -&gt; pd.DataFrame\n</code></pre> <p>Validates a DataFrame against a specified rule and identifies rows that violate the rule.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to validate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the validation rule. It is expected to have          keys that define the field to check, the type of check, and the value          to validate against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing rows that violate the rule. An additional           column 'dq_status' is added to indicate the field, check, and value           that caused the violation in the format \"{field}:{check}:{value}\".</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.is_less_or_equal_than","title":"is_less_or_equal_than","text":"<pre><code>is_less_or_equal_than(df: DataFrame, rule: dict) -&gt; pd.DataFrame\n</code></pre> <p>Filters rows in a DataFrame where the value in a specified field is less than or equal to a given value.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The column name in the DataFrame to apply the rule on. - 'check' (str): A descriptive label for the check being performed. - 'value' (numeric): The threshold value to compare against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A new DataFrame containing only the rows that satisfy the condition.           An additional column 'dq_status' is added to indicate the rule applied           in the format \"{field}:{check}:{value}\".</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.is_less_than","title":"is_less_than","text":"<pre><code>is_less_than(df: DataFrame, rule: dict) -&gt; pd.DataFrame\n</code></pre> <p>Filters a DataFrame to return rows where a specified field's value is less than a given threshold.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The column name in the DataFrame to be checked. - 'check' (str): A descriptive string for the check (e.g., \"less_than\"). - 'value' (numeric): The threshold value to compare against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A new DataFrame containing only the rows where the specified field's value</p> <code>DataFrame</code> <p>is less than the given threshold. An additional column 'dq_status' is added to indicate</p> <code>DataFrame</code> <p>the rule applied in the format \"field:check:value\".</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.is_negative","title":"is_negative","text":"<pre><code>is_negative(df: DataFrame, rule: dict) -&gt; pd.DataFrame\n</code></pre> <p>Filters a DataFrame to identify rows where a specified field does not satisfy a \"negative\" condition.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to include: - 'field': The column name in the DataFrame to check. - 'check': The type of check being performed (e.g., \"negative\"). - 'value': Additional value associated with the rule (not used in this function).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A new DataFrame containing rows where the specified field is non-negative (&gt;= 0).           An additional column 'dq_status' is added to indicate the rule violation in the format           \"{field}:{check}:{value}\".</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.is_on_friday","title":"is_on_friday","text":"<pre><code>is_on_friday(df: DataFrame, rule: dict) -&gt; pd.DataFrame\n</code></pre> <p>Filters the rows of a DataFrame based on whether a specific date column corresponds to a Friday.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing the data to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rules or parameters for filtering.          It should specify the column to check for the day of the week.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A filtered DataFrame containing only the rows where the specified date column corresponds to a Friday.</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.is_on_monday","title":"is_on_monday","text":"<pre><code>is_on_monday(df: DataFrame, rule: dict) -&gt; pd.DataFrame\n</code></pre> <p>Filters the rows of a DataFrame based on whether a specific date column corresponds to a Monday.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing the data to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the filtering rules, including the column to check.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A filtered DataFrame containing only the rows where the specified date column corresponds to a Monday.</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.is_on_saturday","title":"is_on_saturday","text":"<pre><code>is_on_saturday(df: DataFrame, rule: dict) -&gt; pd.DataFrame\n</code></pre> <p>Filters a DataFrame to include only rows where the date corresponds to a Saturday.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing date information.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing rules or parameters for filtering.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A filtered DataFrame containing only rows where the date is a Saturday.</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.is_on_sunday","title":"is_on_sunday","text":"<pre><code>is_on_sunday(df: DataFrame, rule: dict) -&gt; pd.DataFrame\n</code></pre> <p>Determines whether the dates in a given DataFrame fall on a Sunday.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing date-related data.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing rules or parameters for the operation.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame indicating whether each date falls on a Sunday.</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.is_on_thursday","title":"is_on_thursday","text":"<pre><code>is_on_thursday(df: DataFrame, rule: dict) -&gt; pd.DataFrame\n</code></pre> <p>Filters the rows of a DataFrame based on whether a date column corresponds to a Thursday.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing the data to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the filtering rules, including the column to check.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A filtered DataFrame containing only the rows where the specified date column           corresponds to a Thursday.</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.is_on_tuesday","title":"is_on_tuesday","text":"<pre><code>is_on_tuesday(df: DataFrame, rule: dict) -&gt; pd.DataFrame\n</code></pre> <p>Filters the rows of a DataFrame based on whether a specific date column corresponds to a Tuesday.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing the data to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the filtering rules, including the column to check.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A filtered DataFrame containing only the rows where the specified date column corresponds to a Tuesday.</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.is_on_wednesday","title":"is_on_wednesday","text":"<pre><code>is_on_wednesday(df: DataFrame, rule: dict) -&gt; pd.DataFrame\n</code></pre> <p>Filters the rows of a DataFrame based on whether a date column corresponds to Wednesday.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing the data to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule configuration.          It is expected to specify the column to evaluate.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A filtered DataFrame containing only the rows where the specified date column           corresponds to Wednesday.</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.is_on_weekday","title":"is_on_weekday","text":"<pre><code>is_on_weekday(df: DataFrame, rule: dict) -&gt; pd.DataFrame\n</code></pre> <p>Filters a DataFrame to include only rows where the specified date field falls on a weekday (Monday to Friday) and adds a \"dq_status\" column indicating the rule applied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing the data to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - field (str): The name of the date column to check. - check (str): A descriptive string for the check being performed. - value (str): A value associated with the rule for documentation purposes.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A filtered DataFrame containing only rows where the specified date field</p> <code>DataFrame</code> <p>falls on a weekday, with an additional \"dq_status\" column describing the rule applied.</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.is_on_weekend","title":"is_on_weekend","text":"<pre><code>is_on_weekend(df: DataFrame, rule: dict) -&gt; pd.DataFrame\n</code></pre> <p>Filters a DataFrame to include only rows where the specified date field falls on a weekend (Saturday or Sunday) and adds a \"dq_status\" column indicating the rule applied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing the data to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to include: - field (str): The name of the date column to check. - check (str): A descriptive string for the type of check being performed. - value (str): A value associated with the rule for documentation purposes.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A new DataFrame containing only the rows where the specified date field</p> <code>DataFrame</code> <p>falls on a weekend. Includes an additional \"dq_status\" column with the rule details.</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.is_past_date","title":"is_past_date","text":"<pre><code>is_past_date(df: DataFrame, rule: dict) -&gt; pd.DataFrame\n</code></pre> <p>Identifies rows in a DataFrame where the date in a specified column is in the past.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing the data to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to include          the field name to check and the check type.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing the rows where the date in the specified column           is earlier than the current date. An additional column 'dq_status' is           added to indicate the field, check type, and the current date.</p> Notes <ul> <li>The function uses <code>pd.to_datetime</code> to convert the specified column to datetime format.   Any invalid date entries will be coerced to NaT (Not a Time).</li> <li>Rows with invalid or missing dates are excluded from the result.</li> </ul>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.is_positive","title":"is_positive","text":"<pre><code>is_positive(df: DataFrame, rule: dict) -&gt; pd.DataFrame\n</code></pre> <p>Identifies rows in a DataFrame where the specified field contains negative values.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to include: - 'field': The column name in the DataFrame to check. - 'check': A descriptive label for the type of check being performed. - 'value': A value associated with the rule (not directly used in this function).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing only the rows where the specified field has negative values.           An additional column 'dq_status' is added to indicate the rule violation in the format           \"{field}:{check}:{value}\".</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.is_t_minus_2","title":"is_t_minus_2","text":"<pre><code>is_t_minus_2(df: DataFrame, rule: dict) -&gt; pd.DataFrame\n</code></pre> <p>Filters a DataFrame to include only rows where the specified date field matches the date two days prior to the current date.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing the data to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to include the field name, check type, and value.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A filtered DataFrame containing only the rows where the</p> <code>DataFrame</code> <p>specified date field matches the target date (two days prior). An</p> <code>DataFrame</code> <p>additional column \"dq_status\" is added to indicate the rule applied.</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.is_t_minus_3","title":"is_t_minus_3","text":"<pre><code>is_t_minus_3(df: DataFrame, rule: dict) -&gt; pd.DataFrame\n</code></pre> <p>Filters a DataFrame to include only rows where the specified date field matches the date three days prior to the current date.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing the data to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. The rule should include the field to check, the type of check, and the value.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A filtered DataFrame containing only the rows where the</p> <code>DataFrame</code> <p>specified date field matches the target date (three days prior). An</p> <code>DataFrame</code> <p>additional column \"dq_status\" is added to indicate the rule applied.</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.is_today","title":"is_today","text":"<pre><code>is_today(df: DataFrame, rule: dict) -&gt; pd.DataFrame\n</code></pre> <p>Filters a DataFrame to include only rows where the specified date field matches today's date.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing the data to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to include          the field name, a check operation, and a value.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A new DataFrame containing only the rows where the specified date field           matches today's date. An additional column \"dq_status\" is added to indicate           the rule applied in the format \"{field}:{check}:{value}\".</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.is_unique","title":"is_unique","text":"<pre><code>is_unique(df: DataFrame, rule: dict) -&gt; pd.DataFrame\n</code></pre> <p>Checks for duplicate values in a specified field of a DataFrame based on a rule.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to check for duplicates.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to          include the field to check, the type of check, and a value.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing the rows with duplicate values in the           specified field. An additional column 'dq_status' is added           to indicate the field, check type, and value associated with           the rule.</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.is_yesterday","title":"is_yesterday","text":"<pre><code>is_yesterday(df: DataFrame, rule: dict) -&gt; pd.DataFrame\n</code></pre> <p>Filters a DataFrame to include only rows where the specified date field matches yesterday's date.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing the data to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to have          keys that allow <code>__extract_params(rule)</code> to return the field name,          check type, and value.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A filtered DataFrame containing only rows where the specified date field           matches yesterday's date. An additional column <code>dq_status</code> is added to           indicate the data quality status in the format \"{field}:{check}:{value}\".</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.not_contained_in","title":"not_contained_in","text":"<pre><code>not_contained_in(df: DataFrame, rule: dict) -&gt; pd.DataFrame\n</code></pre> <p>Filters a DataFrame to return rows where the specified field contains values that are not allowed according to the provided rule.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The column name in the DataFrame to check. - 'check': The type of check being performed (used for status annotation). - 'value': A list or string representation of values that are not allowed.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing rows that violate the rule. An additional</p> <code>DataFrame</code> <p>column 'dq_status' is added to indicate the rule violation in the format</p> <code>DataFrame</code> <p>\"{field}:{check}:{value}\".</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.not_in","title":"not_in","text":"<pre><code>not_in(df: DataFrame, rule: dict) -&gt; pd.DataFrame\n</code></pre> <p>Filters a DataFrame by excluding rows that match the specified rule.</p> <p>This function is a wrapper around the <code>not_contained_in</code> function, which performs the actual filtering logic.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary specifying the filtering criteria.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A new DataFrame with rows that do not match the rule.</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.satisfies","title":"satisfies","text":"<pre><code>satisfies(df: DataFrame, rule: dict) -&gt; pd.DataFrame\n</code></pre> <p>Filters a DataFrame based on a rule and returns rows that do not satisfy the rule.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to be evaluated.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule to be applied. It is expected to contain parameters that can be extracted using the <code>__extract_params</code> function.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing rows that do not satisfy the rule. An additional</p> <code>DataFrame</code> <p>column <code>dq_status</code> is added to indicate the field, check, and expression that failed.</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.summarize","title":"summarize","text":"<pre><code>summarize(qc_df: DataFrame, rules: list[dict], total_rows: int) -&gt; pd.DataFrame\n</code></pre> <p>Summarizes quality check results for a given DataFrame based on specified rules.</p> <p>Parameters:</p> Name Type Description Default <code>qc_df</code> <code>DataFrame</code> <p>The input DataFrame containing a 'dq_status' column with quality check results in the format 'column:rule:value', separated by semicolons.</p> required <code>rules</code> <code>list[dict]</code> <p>A list of dictionaries representing the quality check rules. Each dictionary should define the 'column', 'rule', 'value', and 'pass_threshold'.</p> required <code>total_rows</code> <code>int</code> <p>The total number of rows in the original dataset.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame summarizing the quality check results with the following columns: - 'id': A unique identifier for each rule. - 'timestamp': The timestamp of the summary generation. - 'check': The type of check performed (e.g., 'Quality Check'). - 'level': The severity level of the check (e.g., 'WARNING'). - 'column': The column name associated with the rule. - 'rule': The rule being checked. - 'value': The value associated with the rule. - 'rows': The total number of rows in the dataset. - 'violations': The number of rows that violated the rule. - 'pass_rate': The proportion of rows that passed the rule. - 'pass_threshold': The threshold for passing the rule. - 'status': The status of the rule ('PASS' or 'FAIL') based on the pass rate.</p> Notes <ul> <li>The function calculates the number of violations for each rule and merges it with the   provided rules to compute the pass rate and status.</li> <li>The 'timestamp' column is set to the current time with seconds and microseconds set to zero.</li> </ul>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.validate","title":"validate","text":"<pre><code>validate(df: DataFrame, rules: list[dict]) -&gt; Tuple[pd.DataFrame, pd.DataFrame]\n</code></pre> <p>Validates a pandas DataFrame against a set of rules and returns the processed DataFrame along with a DataFrame containing validation violations.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to validate.</p> required <code>rules</code> <code>list[dict]</code> <p>A list of dictionaries, where each dictionary represents a validation rule. Each rule should contain the following keys: - 'check_type' (str): The type of validation to perform. This should correspond to a   function name available in the global scope. Special cases include 'is_primary_key'   and 'is_composite_key', which map to 'is_unique' and 'are_unique', respectively. - 'execute' (bool, optional): Whether to execute the rule. Defaults to True.</p> required <p>Returns:</p> Type Description <code>Tuple[DataFrame, DataFrame]</code> <p>Tuple[pd.DataFrame, pd.DataFrame]: A tuple containing: - The processed DataFrame with validation statuses merged. - A DataFrame containing rows that violated the validation rules.</p> Notes <ul> <li>The input DataFrame is copied and reset to ensure the original data is not modified.</li> <li>An '_id' column is temporarily added to track row indices during validation.</li> <li>If a rule's 'check_type' does not correspond to a known function, a warning is issued.</li> <li>The 'dq_status' column in the violations DataFrame summarizes validation issues for   each row.</li> </ul>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.validate_date_format","title":"validate_date_format","text":"<pre><code>validate_date_format(df: DataFrame, rule: dict) -&gt; pd.DataFrame\n</code></pre> <p>Validates the date format of a specified field in a DataFrame against a given format.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing the data to validate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the validation rule. It should include: - 'field': The name of the column to validate. - 'check': A description or identifier for the validation check. - 'fmt': The expected date format to validate against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing rows that violate the date format rule.           An additional column 'dq_status' is added to indicate the           validation status in the format \"{field}:{check}:{fmt}\".</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.validate_schema","title":"validate_schema","text":"<pre><code>validate_schema(df, expected) -&gt; tuple[bool, list[dict[str, Any]]]\n</code></pre> <p>Validates the schema of a given DataFrame against an expected schema.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <p>The DataFrame whose schema needs to be validated.</p> required <code>expected</code> <p>The expected schema, represented as a list of tuples where each tuple       contains the column name and its data type.</p> required <p>Returns:</p> Type Description <code>tuple[bool, list[dict[str, Any]]]</code> <p>Tuple[bool, List[Tuple[str, str]]]: A tuple containing: - A boolean indicating whether the schema matches the expected schema. - A list of tuples representing the errors, where each tuple contains   the column name and a description of the mismatch.</p>"},{"location":"sumeh/engines/polars/","title":"polars","text":""},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine","title":"sumeh.engines.polars_engine","text":"<p>This module provides a set of data quality validation functions using the Polars library. It includes various checks for data validation, such as completeness, uniqueness, range checks, pattern matching, and schema validation.</p> <p>Functions:</p> Name Description <code>is_positive</code> <p>Filters rows where the specified field is less than zero.</p> <code>is_negative</code> <p>Filters rows where the specified field is greater than or equal to zero.</p> <code>is_complete</code> <p>Filters rows where the specified field is null.</p> <code>is_unique</code> <p>Filters rows with duplicate values in the specified field.</p> <code>are_complete</code> <p>Filters rows where any of the specified fields are null.</p> <code>are_unique</code> <p>Filters rows with duplicate combinations of the specified fields.</p> <code>is_greater_than</code> <p>Filters rows where the specified field is less than or equal to the given value.</p> <code>is_greater_or_equal_than</code> <p>Filters rows where the specified field is less than the given value.</p> <code>is_less_than</code> <p>Filters rows where the specified field is greater than or equal to the given value.</p> <code>is_less_or_equal_than</code> <p>Filters rows where the specified field is greater than the given value.</p> <code>is_equal</code> <p>Filters rows where the specified field is not equal to the given value.</p> <code>is_equal_than</code> <p>Alias for <code>is_equal</code>.</p> <code>is_in_millions</code> <p>Retains rows where the field value is less than 1,000,000 and flags them with dq_status.</p> <code>is_in_billions</code> <p>Retains rows where the field value is less than 1,000,000,000 and flags them with dq_status.</p> <code>is_t_minus_1</code> <p>Retains rows where the date field not equals yesterday (T-1) and flags them with dq_status.</p> <code>is_t_minus_2</code> <p>Retains rows where the date field not equals two days ago (T-2) and flags them with dq_status.</p> <code>is_t_minus_3</code> <p>Retains rows where the date field not equals three days ago (T-3) and flags them with dq_status.</p> <code>is_today</code> <p>Retains rows where the date field not equals today and flags them with dq_status.</p> <code>is_yesterday</code> <p>Retains rows where the date field not equals yesterday and flags them with dq_status.</p> <code>is_on_weekday</code> <p>Retains rows where the date field not falls on a weekday (Mon-Fri) and flags them with dq_status.</p> <code>is_on_weekend</code> <p>Retains rows where the date field is not on a weekend (Sat-Sun) and flags them with dq_status.</p> <code>is_on_monday</code> <p>Retains rows where the date field is not on Monday and flags them with dq_status.</p> <code>is_on_tuesday</code> <p>Retains rows where the date field is not on Tuesday and flags them with dq_status.</p> <code>is_on_wednesday</code> <p>Retains rows where the date field is not on Wednesday and flags them with dq_status.</p> <code>is_on_thursday</code> <p>Retains rows where the date field is not on Thursday and flags them with dq_status.</p> <code>is_on_friday</code> <p>Retains rows where the date field is not on Friday and flags them with dq_status.</p> <code>is_on_saturday</code> <p>Retains rows where the date field is not on Saturday and flags them with dq_status.</p> <code>is_on_sunday</code> <p>Retains rows where the date field is not on Sunday and flags them with dq_status.</p> <code>is_contained_in</code> <p>Filters rows where the specified field is not in the given list of values.</p> <code>not_contained_in</code> <p>Filters rows where the specified field is in the given list of values.</p> <code>is_between</code> <p>Filters rows where the specified field is not within the given range.</p> <code>has_pattern</code> <p>Filters rows where the specified field does not match the given regex pattern.</p> <code>is_legit</code> <p>Filters rows where the specified field is null or contains whitespace.</p> <code>has_max</code> <p>Filters rows where the specified field exceeds the given maximum value.</p> <code>has_min</code> <p>Filters rows where the specified field is below the given minimum value.</p> <code>has_std</code> <p>Checks if the standard deviation of the specified field exceeds the given value.</p> <code>has_mean</code> <p>Checks if the mean of the specified field exceeds the given value.</p> <code>has_sum</code> <p>Checks if the sum of the specified field exceeds the given value.</p> <code>has_cardinality</code> <p>Checks if the cardinality (number of unique values) of the specified field exceeds the given value.</p> <code>has_infogain</code> <p>Placeholder for information gain validation (currently uses cardinality).</p> <code>has_entropy</code> <p>Placeholder for entropy validation (currently uses cardinality).</p> <code>satisfies</code> <p>Filters rows that do not satisfy the given SQL condition.</p> <code>validate_date_format</code> <p>Filters rows where the specified field does not match the expected date format or is null.</p> <code>is_future_date</code> <p>Filters rows where the specified date field is after today.</p> <code>is_past_date</code> <p>Filters rows where the specified date field is before today.</p> <code>is_date_between</code> <p>Filters rows where the specified date field is not within the given [start,end] range.</p> <code>is_date_after</code> <p>Filters rows where the specified date field is before the given date.</p> <code>is_date_before</code> <p>Filters rows where the specified date field is after the given date.</p> <code>all_date_checks</code> <p>Alias for <code>is_past_date</code> (checks date against today).</p> <code>validate</code> <p>Validates a DataFrame against a list of rules and returns the original DataFrame with data quality status and a DataFrame of violations.</p> <code>__build_rules_df</code> <p>Converts a list of rules into a Polars DataFrame for summarization.</p> <code>summarize</code> <p>Summarizes the results of data quality checks, including pass rates and statuses.</p> <code>__polars_schema_to_list</code> <p>Converts a Polars DataFrame schema into a list of dictionaries.</p> <code>validate_schema</code> <p>Validates the schema of a DataFrame against an expected schema and returns a boolean result and a list of errors.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.__build_rules_df","title":"__build_rules_df","text":"<pre><code>__build_rules_df(rules: list[dict]) -&gt; pl.DataFrame\n</code></pre> <p>Builds a Polars DataFrame from a list of rule dictionaries.</p> <p>This function processes a list of rule dictionaries, filters out rules that are not marked for execution, and constructs a DataFrame with the relevant rule information. It ensures uniqueness of rows based on specific columns and casts the data to appropriate types.</p> <p>Parameters:</p> Name Type Description Default <code>rules</code> <code>list[dict]</code> <p>A list of dictionaries, where each dictionary represents a rule. Each rule dictionary may contain the following keys: - \"field\" (str or list): The column(s) the rule applies to. - \"check_type\" (str): The type of rule or check. - \"threshold\" (float, optional): The pass threshold for the rule. Defaults to 1.0. - \"value\" (any, optional): Additional value associated with the rule. - \"execute\" (bool, optional): Whether the rule should be executed. Defaults to True.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A Polars DataFrame containing the processed rules with the following columns: - \"column\" (str): The column(s) the rule applies to, joined by commas if multiple. - \"rule\" (str): The type of rule or check. - \"pass_threshold\" (float): The pass threshold for the rule. - \"value\" (str): The value associated with the rule, or an empty string if not provided.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.all_date_checks","title":"all_date_checks","text":"<pre><code>all_date_checks(df: DataFrame, rule: dict) -&gt; pl.DataFrame\n</code></pre> <p>Applies all date-related validation checks on the given DataFrame based on the specified rule.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to validate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the validation rules to apply.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: The DataFrame after applying the date validation checks.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.are_complete","title":"are_complete","text":"<pre><code>are_complete(df: DataFrame, rule: dict) -&gt; pl.DataFrame\n</code></pre> <p>Filters a Polars DataFrame to identify rows where specified fields contain null values and tags them with a data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'fields': A list of column names to check for null values. - 'check': A string representing the type of check (e.g., \"is_null\"). - 'value': A value associated with the check (not used in this function).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A filtered DataFrame containing only rows where at least one of the</p> <code>DataFrame</code> <p>specified fields is null, with an additional column \"dq_status\" indicating the</p> <code>DataFrame</code> <p>data quality status.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.are_unique","title":"are_unique","text":"<pre><code>are_unique(df: DataFrame, rule: dict) -&gt; pl.DataFrame\n</code></pre> <p>Checks for duplicate combinations of specified fields in a Polars DataFrame and returns a DataFrame containing the rows with duplicates along with a data quality status column.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to check for duplicates.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected          to include the following keys:          - 'fields': A list of column names to check for uniqueness.          - 'check': A string representing the type of check (e.g., \"unique\").          - 'value': A value associated with the check (e.g., \"True\").</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A DataFrame containing rows with duplicate combinations of           the specified fields. An additional column, \"dq_status\",           is added to indicate the data quality status in the format           \"{fields}:{check}:{value}\".</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.extract_schema","title":"extract_schema","text":"<pre><code>extract_schema(df) -&gt; List[Dict[str, Any]]\n</code></pre> <p>Converts the schema of a Polars DataFrame into a list of dictionaries, where each dictionary represents a field in the schema.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The Polars DataFrame whose schema is to be converted.</p> required <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List[Dict[str, Any]]: A list of dictionaries, each containing the following keys: - \"field\" (str): The name of the field. - \"data_type\" (str): The data type of the field, converted to lowercase. - \"nullable\" (bool): Always set to True, as Polars does not expose nullability in the schema. - \"max_length\" (None): Always set to None, as max length is not applicable.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.has_cardinality","title":"has_cardinality","text":"<pre><code>has_cardinality(df: DataFrame, rule: dict) -&gt; pl.DataFrame\n</code></pre> <p>Checks if the cardinality (number of unique values) of a specified field in the given DataFrame satisfies a condition defined in the rule. If the cardinality exceeds the specified value, a new column \"dq_status\" is added to the DataFrame with a string indicating the rule violation. Otherwise, an empty DataFrame is returned.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to evaluate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - \"field\" (str): The column name to check. - \"check\" (str): The type of check (e.g., \"greater_than\"). - \"value\" (int): The threshold value for the cardinality.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: The original DataFrame with an added \"dq_status\" column if the rule is violated,           or an empty DataFrame if the rule is not violated.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.has_entropy","title":"has_entropy","text":"<pre><code>has_entropy(df: DataFrame, rule: dict) -&gt; pl.DataFrame\n</code></pre> <p>Evaluates the entropy of a specified field in a Polars DataFrame based on a given rule.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to evaluate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The column name in the DataFrame to evaluate. - 'check' (str): The type of check to perform (not used directly in this function). - 'value' (float): The threshold value for entropy comparison.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: - If the entropy of the specified field exceeds the given threshold (<code>value</code>),   returns the original DataFrame with an additional column <code>dq_status</code> indicating   the rule that was applied. - If the entropy does not exceed the threshold, returns an empty DataFrame with   the same schema as the input DataFrame.</p> Notes <ul> <li>The entropy is calculated as the number of unique values in the specified field.</li> <li>The <code>dq_status</code> column contains a string in the format \"{field}:{check}:{value}\".</li> </ul>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.has_infogain","title":"has_infogain","text":"<pre><code>has_infogain(df: DataFrame, rule: dict) -&gt; pl.DataFrame\n</code></pre> <p>Evaluates whether a given DataFrame satisfies an information gain condition based on a specified rule. If the condition is met, a new column indicating the rule is added; otherwise, an empty DataFrame is returned.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to evaluate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include the following keys: - 'field': The column name to evaluate. - 'check': The type of check to perform (not used directly in this function). - 'value': The threshold value for the information gain.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: The original DataFrame with an additional column named</p> <code>DataFrame</code> <p>\"dq_status\" if the condition is met, or an empty DataFrame if the</p> <code>DataFrame</code> <p>condition is not met.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.has_max","title":"has_max","text":"<pre><code>has_max(df: DataFrame, rule: dict) -&gt; pl.DataFrame\n</code></pre> <p>Filters a Polars DataFrame to include only rows where the value in a specified column exceeds a given threshold, and adds a new column indicating the rule applied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The column name to apply the filter on. - 'check' (str): The type of check being performed (e.g., \"max\"). - 'value' (numeric): The threshold value to compare against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new DataFrame containing only the rows that satisfy the condition,</p> <code>DataFrame</code> <p>with an additional column named \"dq_status\" that describes the applied rule.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.has_mean","title":"has_mean","text":"<pre><code>has_mean(df: DataFrame, rule: dict) -&gt; pl.DataFrame\n</code></pre> <p>Checks if the mean value of a specified column in a Polars DataFrame satisfies a given condition.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The name of the column to calculate the mean for. - 'check' (str): The condition to check (e.g., 'greater than'). - 'value' (float): The threshold value to compare the mean against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: - If the mean value of the specified column is greater than the threshold value,   returns the original DataFrame with an additional column \"dq_status\" containing   a string in the format \"{field}:{check}:{value}\". - If the condition is not met, returns an empty DataFrame with the same schema as the input.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.has_min","title":"has_min","text":"<pre><code>has_min(df: DataFrame, rule: dict) -&gt; pl.DataFrame\n</code></pre> <p>Filters a Polars DataFrame to include only rows where the value of a specified column is less than a given threshold and adds a new column indicating the data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The name of the column to apply the filter on. - 'check': A string representing the type of check (e.g., 'min'). - 'value': The threshold value for the filter.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new Polars DataFrame containing only the rows that satisfy</p> <code>DataFrame</code> <p>the condition, with an additional column named \"dq_status\" indicating the</p> <code>DataFrame</code> <p>applied rule in the format \"field:check:value\".</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.has_pattern","title":"has_pattern","text":"<pre><code>has_pattern(df: DataFrame, rule: dict) -&gt; pl.DataFrame\n</code></pre> <p>Filters a Polars DataFrame based on a pattern-matching rule and adds a data quality status column.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The column name in the DataFrame to apply the pattern check. - 'check': A descriptive label for the check being performed. - 'pattern': The regex pattern to match against the column values.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new DataFrame with rows not matching the pattern removed and an additional</p> <code>DataFrame</code> <p>column named \"dq_status\" indicating the rule applied in the format \"field:check:pattern\".</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.has_std","title":"has_std","text":"<pre><code>has_std(df: DataFrame, rule: dict) -&gt; pl.DataFrame\n</code></pre> <p>Evaluates whether the standard deviation of a specified column in a Polars DataFrame exceeds a given threshold and returns a modified DataFrame accordingly.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to evaluate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The name of the column to calculate the standard deviation for. - 'check' (str): A descriptive label for the check being performed. - 'value' (float): The threshold value for the standard deviation.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A modified DataFrame. If the standard deviation of the specified column</p> <code>DataFrame</code> <p>exceeds the threshold, the DataFrame will include a new column <code>dq_status</code> with a</p> <code>DataFrame</code> <p>descriptive string. Otherwise, an empty DataFrame with the <code>dq_status</code> column is returned.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.has_sum","title":"has_sum","text":"<pre><code>has_sum(df: DataFrame, rule: dict) -&gt; pl.DataFrame\n</code></pre> <p>Checks if the sum of a specified column in a Polars DataFrame exceeds a given value.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The name of the column to sum. - 'check': A string representing the check type (not used in this function). - 'value': The threshold value to compare the sum against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: If the sum of the specified column exceeds the given value,</p> <code>DataFrame</code> <p>returns the original DataFrame with an additional column <code>dq_status</code> containing</p> <code>DataFrame</code> <p>a string in the format \"{field}:{check}:{value}\". Otherwise, returns an empty DataFrame.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.is_between","title":"is_between","text":"<pre><code>is_between(df: DataFrame, rule: dict) -&gt; pl.DataFrame\n</code></pre> <p>Filters a Polars DataFrame to exclude rows where the specified field's value falls within a given range, and adds a column indicating the data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The name of the column to check. - 'check': The type of check being performed (e.g., \"is_between\"). - 'value': A string representing the range in the format \"[lo,hi]\".</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new Polars DataFrame with rows outside the specified range</p> <code>DataFrame</code> <p>and an additional column named \"dq_status\" indicating the rule applied.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the 'value' parameter is not in the expected format \"[lo,hi]\".</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.is_complete","title":"is_complete","text":"<pre><code>is_complete(df: DataFrame, rule: dict) -&gt; pl.DataFrame\n</code></pre> <p>Filters a Polars DataFrame to include only rows where the specified field is not null and appends a new column indicating the data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to be filtered and modified.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The name of the column to check for non-null values. - 'check' (str): A descriptive string for the type of check being performed. - 'value' (str): A value associated with the rule for status annotation.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new Polars DataFrame with rows filtered based on the rule and</p> <code>DataFrame</code> <p>an additional column named \"dq_status\" containing the data quality status.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.is_composite_key","title":"is_composite_key","text":"<pre><code>is_composite_key(df: DataFrame, rule: dict) -&gt; pl.DataFrame\n</code></pre> <p>Determines if the given DataFrame satisfies the composite key condition based on the provided rule.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to evaluate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary defining the rule to check for composite key uniqueness.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A DataFrame indicating whether the composite key condition is met.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.is_contained_in","title":"is_contained_in","text":"<pre><code>is_contained_in(df: DataFrame, rule: dict) -&gt; pl.DataFrame\n</code></pre> <p>Filters a Polars DataFrame to exclude rows where the specified field's value is contained in a given list of values, and adds a new column indicating the rule applied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The column name to check. - 'check': The type of check being performed (e.g., \"is_contained_in\"). - 'value': A string representation of a list of values to check against,   e.g., \"[value1, value2, value3]\".</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new DataFrame with rows filtered based on the rule and an</p> <code>DataFrame</code> <p>additional column \"dq_status\" indicating the rule applied.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.is_date_after","title":"is_date_after","text":"<pre><code>is_date_after(df: DataFrame, rule: dict) -&gt; pl.DataFrame\n</code></pre> <p>Filters a Polars DataFrame to include only rows where the specified date field is earlier than a given date, and adds a new column indicating the data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The name of the column containing date strings. - 'check' (str): A descriptive label for the check being performed. - 'date_str' (str): The date string in the format \"%Y-%m-%d\" to compare against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new Polars DataFrame with rows filtered based on the date condition</p> <code>DataFrame</code> <p>and an additional column named \"dq_status\" indicating the applied rule.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.is_date_before","title":"is_date_before","text":"<pre><code>is_date_before(df: DataFrame, rule: dict) -&gt; pl.DataFrame\n</code></pre> <p>Filters a Polars DataFrame to include only rows where the specified date field is after a given date, and adds a new column indicating the data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The name of the column to check. - 'check' (str): A descriptive label for the check being performed. - 'date_str' (str): The date string in the format \"%Y-%m-%d\" to compare against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new Polars DataFrame with rows filtered based on the date condition</p> <code>DataFrame</code> <p>and an additional column named \"dq_status\" indicating the applied rule.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.is_date_between","title":"is_date_between","text":"<pre><code>is_date_between(df: DataFrame, rule: dict) -&gt; pl.DataFrame\n</code></pre> <p>Filters a Polars DataFrame to exclude rows where the specified date field is within a given range.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the filtering rule. It should include: - 'field': The name of the column to check. - 'check': A string representing the type of check (e.g., \"is_date_between\"). - 'value': A string representing the date range in the format \"[YYYY-MM-DD,YYYY-MM-DD]\".</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new DataFrame excluding rows where the date in the specified field           falls within the given inclusive range, with an additional column           \"dq_status\" indicating the rule applied.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.is_equal","title":"is_equal","text":"<pre><code>is_equal(df: DataFrame, rule: dict) -&gt; pl.DataFrame\n</code></pre> <p>Filters rows in a Polars DataFrame that do not match a specified equality condition and adds a column indicating the data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The column name to apply the equality check on. - 'check': The type of check (expected to be 'eq' for equality). - 'value': The value to compare against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new DataFrame with rows filtered based on the rule and an</p> <code>DataFrame</code> <p>additional column named \"dq_status\" indicating the rule applied.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.is_equal_than","title":"is_equal_than","text":"<pre><code>is_equal_than(df: DataFrame, rule: dict) -&gt; pl.DataFrame\n</code></pre> <p>Filters rows in a Polars DataFrame where the specified field is not equal to a given value and adds a new column indicating the data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The name of the column to check. - 'check': The type of check (expected to be 'equal' for this function). - 'value': The value to compare against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new Polars DataFrame with rows filtered based on the rule and an</p> <code>DataFrame</code> <p>additional column named \"dq_status\" indicating the applied rule.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.is_future_date","title":"is_future_date","text":"<pre><code>is_future_date(df: DataFrame, rule: dict) -&gt; pl.DataFrame\n</code></pre> <p>Filters a Polars DataFrame to include only rows where the specified date field contains a future date, based on the current date.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to include the field name to check, the check type, and additional parameters (ignored in this function).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new DataFrame containing only rows where the specified</p> <code>DataFrame</code> <p>date field is in the future. An additional column \"dq_status\" is added</p> <code>DataFrame</code> <p>to indicate the field, check type, and today's date in the format</p> <code>DataFrame</code> <p>\"field:check:today\".</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.is_greater_or_equal_than","title":"is_greater_or_equal_than","text":"<pre><code>is_greater_or_equal_than(df: DataFrame, rule: dict) -&gt; pl.DataFrame\n</code></pre> <p>Filters a Polars DataFrame to include only rows where the specified field is greater than or equal to a given value, and adds a new column indicating the data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the filtering rule. It should include the following keys: - 'field': The name of the column to be checked. - 'check': The type of check being performed (e.g., \"greater_or_equal\"). - 'value': The threshold value for the comparison.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new Polars DataFrame with rows filtered based on the</p> <code>DataFrame</code> <p>specified rule and an additional column named \"dq_status\" indicating</p> <code>DataFrame</code> <p>the data quality status in the format \"field:check:value\".</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.is_greater_than","title":"is_greater_than","text":"<pre><code>is_greater_than(df: DataFrame, rule: dict) -&gt; pl.DataFrame\n</code></pre> <p>Filters a Polars DataFrame to include only rows where the specified field's value is less than or equal to a given value, and adds a new column indicating the data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The Polars DataFrame to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the filtering rule. It should include: - 'field': The name of the column to apply the filter on. - 'check': A string describing the check (e.g., \"greater_than\"). - 'value': The value to compare against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new DataFrame with rows filtered based on the rule and an</p> <code>DataFrame</code> <p>additional column named \"dq_status\" indicating the applied rule.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.is_in","title":"is_in","text":"<pre><code>is_in(df: DataFrame, rule: dict) -&gt; pl.DataFrame\n</code></pre> <p>Checks if the rows in the given DataFrame satisfy the conditions specified in the rule.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to evaluate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary specifying the conditions to check against the DataFrame.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A DataFrame containing rows that satisfy the specified conditions.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.is_in_billions","title":"is_in_billions","text":"<pre><code>is_in_billions(df: DataFrame, rule: dict) -&gt; pl.DataFrame\n</code></pre> <p>Filters a Polars DataFrame to include only rows where the specified field's value is less than one billion and adds a new column indicating the data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - field (str): The name of the column to check. - check (str): The type of check being performed (e.g., \"less_than\"). - value (any): The value associated with the rule (not used in this function).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new DataFrame with rows filtered based on the rule and an</p> <code>DataFrame</code> <p>additional column named \"dq_status\" containing a string in the format</p> <code>DataFrame</code> <p>\"{field}:{check}:{value}\".</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.is_in_millions","title":"is_in_millions","text":"<pre><code>is_in_millions(df: DataFrame, rule: dict) -&gt; pl.DataFrame\n</code></pre> <p>Filters a Polars DataFrame to include only rows where the specified field's value is less than one million and adds a new column indicating the data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The name of the column to check. - 'check': A string describing the check being performed. - 'value': A value associated with the rule (used for status annotation).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new Polars DataFrame with rows filtered based on the rule and</p> <code>DataFrame</code> <p>an additional column named \"dq_status\" containing the data quality status.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.is_legit","title":"is_legit","text":"<pre><code>is_legit(df: DataFrame, rule: dict) -&gt; pl.DataFrame\n</code></pre> <p>Filters a Polars DataFrame based on a validation rule and appends a data quality status column.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to validate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the validation rule. It should include: - 'field': The name of the column to validate. - 'check': The type of validation check (e.g., regex, condition). - 'value': The value or pattern to validate against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new DataFrame containing rows that failed the validation,</p> <code>DataFrame</code> <p>with an additional column 'dq_status' indicating the validation rule applied.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.is_less_or_equal_than","title":"is_less_or_equal_than","text":"<pre><code>is_less_or_equal_than(df: DataFrame, rule: dict) -&gt; pl.DataFrame\n</code></pre> <p>Filters a Polars DataFrame to include only rows where the specified field's value is greater than the given value, and adds a new column indicating the rule applied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The name of the column to apply the filter on. - 'check': The type of check being performed (e.g., 'less_or_equal_than'). - 'value': The value to compare against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new DataFrame with rows filtered based on the rule and an</p> <code>DataFrame</code> <p>additional column named \"dq_status\" indicating the rule applied.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.is_less_than","title":"is_less_than","text":"<pre><code>is_less_than(df: DataFrame, rule: dict) -&gt; pl.DataFrame\n</code></pre> <p>Filters a Polars DataFrame to include only rows where the specified field is greater than or equal to a given value. Adds a new column indicating the data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the filtering rule. It should include the following keys: - 'field': The name of the column to apply the filter on. - 'check': A string representing the type of check (not used in logic). - 'value': The threshold value for the filter.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new Polars DataFrame with rows filtered based on the</p> <code>DataFrame</code> <p>condition and an additional column named \"dq_status\" containing the</p> <code>DataFrame</code> <p>rule description in the format \"field:check:value\".</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.is_negative","title":"is_negative","text":"<pre><code>is_negative(df: DataFrame, rule: dict) -&gt; pl.DataFrame\n</code></pre> <p>Filters a Polars DataFrame to exclude rows where the specified field is negative and adds a new column indicating the data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The name of the column to check. - 'check': The type of check being performed (e.g., \"is_negative\"). - 'value': The value associated with the rule (not used in this function).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new DataFrame with rows where the specified field is non-negative</p> <code>DataFrame</code> <p>and an additional column named \"dq_status\" containing the rule details.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.is_on_friday","title":"is_on_friday","text":"<pre><code>is_on_friday(df: DataFrame, rule: dict) -&gt; pl.DataFrame\n</code></pre> <p>Filters a Polars DataFrame to include only rows where the date corresponds to a Friday.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame containing the data to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing filtering rules or parameters.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new Polars DataFrame containing only the rows where the date is a Friday.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.is_on_monday","title":"is_on_monday","text":"<pre><code>is_on_monday(df: DataFrame, rule: dict) -&gt; pl.DataFrame\n</code></pre> <p>Filters the given DataFrame to include only rows where the date corresponds to a Monday.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing rules or parameters for filtering.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new DataFrame containing only the rows where the date is a Monday.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.is_on_saturday","title":"is_on_saturday","text":"<pre><code>is_on_saturday(df: DataFrame, rule: dict) -&gt; pl.DataFrame\n</code></pre> <p>Determines if the dates in the given DataFrame fall on a Saturday.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing date information.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing rules or parameters for the operation.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A DataFrame with the result of the operation, indicating whether each date is on a Saturday.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.is_on_sunday","title":"is_on_sunday","text":"<pre><code>is_on_sunday(df: DataFrame, rule: dict) -&gt; pl.DataFrame\n</code></pre> <p>Filters the given DataFrame to include only rows where the date corresponds to Sunday.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing date-related data.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing rules or parameters for filtering.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A filtered DataFrame containing only rows where the date is a Sunday.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.is_on_thursday","title":"is_on_thursday","text":"<pre><code>is_on_thursday(df: DataFrame, rule: dict) -&gt; pl.DataFrame\n</code></pre> <p>Filters a Polars DataFrame to include only rows where the date corresponds to a Thursday.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame containing the data to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing filtering rules or parameters.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new Polars DataFrame containing only the rows where the date is a Thursday.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.is_on_tuesday","title":"is_on_tuesday","text":"<pre><code>is_on_tuesday(df: DataFrame, rule: dict) -&gt; pl.DataFrame\n</code></pre> <p>Filters the given DataFrame to include only rows where the day of the week matches Tuesday.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing rules or parameters for filtering.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new DataFrame containing only rows where the day of the week is Tuesday.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.is_on_wednesday","title":"is_on_wednesday","text":"<pre><code>is_on_wednesday(df: DataFrame, rule: dict) -&gt; pl.DataFrame\n</code></pre> <p>Filters the given DataFrame to include only rows where the day of the week matches Wednesday.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing rules or parameters for filtering.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A filtered DataFrame containing only rows corresponding to Wednesday.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.is_on_weekday","title":"is_on_weekday","text":"<pre><code>is_on_weekday(df: DataFrame, rule: dict) -&gt; pl.DataFrame\n</code></pre> <p>Filters a Polars DataFrame to include only rows where the specified date field falls on a weekday (Monday to Friday). Adds a new column indicating the rule applied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to have          keys that can be extracted using the <code>__extract_params</code> function.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new DataFrame filtered to include only rows where the date field           falls on a weekday, with an additional column named \"dq_status\"           indicating the applied rule in the format \"field:check:value\".</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.is_on_weekend","title":"is_on_weekend","text":"<pre><code>is_on_weekend(df: DataFrame, rule: dict) -&gt; pl.DataFrame\n</code></pre> <p>Filters a Polars DataFrame to include only rows where the specified date field falls on a weekend (Saturday or Sunday). Adds a new column indicating the data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to include the following keys: - 'field': The name of the column containing date strings. - 'check': A string representing the type of check being performed. - 'value': A value associated with the rule (not used in the logic).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new Polars DataFrame filtered to include only rows where</p> <code>DataFrame</code> <p>the specified date field falls on a weekend. The resulting DataFrame also</p> <code>DataFrame</code> <p>includes an additional column named \"dq_status\" with a string indicating</p> <code>DataFrame</code> <p>the rule applied.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.is_past_date","title":"is_past_date","text":"<pre><code>is_past_date(df: DataFrame, rule: dict) -&gt; pl.DataFrame\n</code></pre> <p>Filters a Polars DataFrame to include only rows where the specified date field contains a date earlier than today. Adds a new column indicating the data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to include          the field name to check, a check identifier, and additional parameters.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new DataFrame containing only rows where the specified date field           is in the past, with an additional column named \"dq_status\" that           contains a string in the format \"{field}:{check}:{today}\".</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.is_positive","title":"is_positive","text":"<pre><code>is_positive(df: DataFrame, rule: dict) -&gt; pl.DataFrame\n</code></pre> <p>Filters a Polars DataFrame to identify rows where the specified field contains negative values and appends a new column indicating the data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to include the following keys: - 'field': The name of the column to check. - 'check': The type of check being performed (e.g., \"is_positive\"). - 'value': The reference value for the check.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new Polars DataFrame containing only the rows where</p> <code>DataFrame</code> <p>the specified field has negative values, with an additional column</p> <code>DataFrame</code> <p>named \"dq_status\" that describes the rule applied.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.is_primary_key","title":"is_primary_key","text":"<pre><code>is_primary_key(df: DataFrame, rule: dict) -&gt; pl.DataFrame\n</code></pre> <p>Checks if the specified rule identifies a primary key in the given DataFrame.</p> <p>A primary key is a set of columns in a DataFrame that uniquely identifies each row. This function delegates the check to the <code>is_unique</code> function.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The DataFrame to check for primary key uniqueness.</p> required <code>rule</code> <code>dict</code> <p>A dictionary specifying the rule or criteria to determine the primary key.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A DataFrame indicating whether the rule satisfies the primary key condition.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.is_t_minus_1","title":"is_t_minus_1","text":"<pre><code>is_t_minus_1(df: DataFrame, rule: dict) -&gt; pl.DataFrame\n</code></pre> <p>Filters a Polars DataFrame to include only rows where the specified field matches the date of \"yesterday\" (T-1) and appends a new column indicating the data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to include the following keys: - 'field': The name of the column to check. - 'check': A string representing the type of check (used for metadata). - 'value': A value associated with the check (used for metadata).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new Polars DataFrame filtered to include only rows where</p> <code>DataFrame</code> <p>the specified field matches the date of yesterday (T-1). The resulting</p> <code>DataFrame</code> <p>DataFrame also includes an additional column named \"dq_status\" that</p> <code>DataFrame</code> <p>contains metadata about the rule applied.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.is_t_minus_2","title":"is_t_minus_2","text":"<pre><code>is_t_minus_2(df: DataFrame, rule: dict) -&gt; pl.DataFrame\n</code></pre> <p>Filters a Polars DataFrame to include only rows where the specified date field matches the date two days prior to the current date. Adds a new column indicating the data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to include the following keys: - 'field': The name of the date field to check. - 'check': A string representing the type of check (not used in filtering). - 'value': A value associated with the rule (not used in filtering).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new Polars DataFrame filtered to include only rows where the</p> <code>DataFrame</code> <p>specified date field matches the date two days ago. The resulting DataFrame</p> <code>DataFrame</code> <p>includes an additional column named \"dq_status\" with a string indicating the</p> <code>DataFrame</code> <p>rule applied.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.is_t_minus_3","title":"is_t_minus_3","text":"<pre><code>is_t_minus_3(df: DataFrame, rule: dict) -&gt; pl.DataFrame\n</code></pre> <p>Filters a Polars DataFrame to include only rows where the specified date field matches the date three days prior to the current date. Additionally, adds a new column indicating the data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The name of the date column to check. - 'check': A string representing the type of check (used for status annotation). - 'value': A value associated with the rule (used for status annotation).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A filtered Polars DataFrame with an additional column named</p> <code>DataFrame</code> <p>\"dq_status\" that contains a string in the format \"{field}:{check}:{value}\".</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.is_today","title":"is_today","text":"<pre><code>is_today(df: DataFrame, rule: dict) -&gt; pl.DataFrame\n</code></pre> <p>Filters a Polars DataFrame to include only rows where the specified date field matches today's date. Additionally, adds a new column \"dq_status\" with a formatted string indicating the rule applied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to have the following keys: - field (str): The name of the column to check. - check (str): A descriptive string for the type of check (used in the \"dq_status\" column). - value (str): A value associated with the rule (used in the \"dq_status\" column).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A filtered Polars DataFrame with rows matching today's date in the specified field</p> <code>DataFrame</code> <p>and an additional \"dq_status\" column describing the rule applied.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the rule dictionary does not contain the required keys or if the date parsing fails.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.is_unique","title":"is_unique","text":"<pre><code>is_unique(df: DataFrame, rule: dict) -&gt; pl.DataFrame\n</code></pre> <p>Checks for duplicate values in a specified field of a Polars DataFrame and returns a filtered DataFrame containing only the rows with duplicate values. Additionally, it adds a new column 'dq_status' with a formatted string indicating the field, check type, and value.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to check for duplicates.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected          to have keys that allow extraction of the field to check,          the type of check, and a value.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A filtered DataFrame containing rows with duplicate values           in the specified field, along with an additional column           'dq_status' describing the rule applied.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.not_contained_in","title":"not_contained_in","text":"<pre><code>not_contained_in(df: DataFrame, rule: dict) -&gt; pl.DataFrame\n</code></pre> <p>Filters a Polars DataFrame to include only rows where the specified field's value is in a given list, and adds a new column indicating the data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the filtering rule. It should include: - 'field': The column name to apply the filter on. - 'check': A string representing the type of check (not used in logic). - 'value': A string representation of a list of values (e.g., \"[value1, value2]\").</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new Polars DataFrame with rows filtered based on the rule and</p> <code>DataFrame</code> <p>an additional column \"dq_status\" indicating the applied rule.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.not_in","title":"not_in","text":"<pre><code>not_in(df: DataFrame, rule: dict) -&gt; pl.DataFrame\n</code></pre> <p>Filters a Polars DataFrame by excluding rows where the specified rule applies.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary specifying the filtering rule. The structure and expected keys of this dictionary depend on the implementation of the <code>not_contained_in</code> function.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new DataFrame with rows excluded based on the given rule.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.satisfies","title":"satisfies","text":"<pre><code>satisfies(df: DataFrame, rule: dict) -&gt; pl.DataFrame\n</code></pre> <p>Evaluates a given rule against a Polars DataFrame and returns rows that do not satisfy the rule.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to be evaluated.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule to be applied. The rule should include          the following keys:          - 'field': The column name in the DataFrame to be checked.          - 'check': The type of check or condition to be applied.          - 'value': The value or expression to validate against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A DataFrame containing rows that do not satisfy the rule, with an additional           column <code>dq_status</code> indicating the rule that was violated in the format           \"field:check:value\".</p> Example <p>rule = {\"field\": \"age\", \"check\": \"&gt;\", \"value\": \"18\"} result = satisfies(df, rule)</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.summarize","title":"summarize","text":"<pre><code>summarize(qc_df: DataFrame, rules: list[dict], total_rows: int) -&gt; pl.DataFrame\n</code></pre> <p>Summarizes quality check results by processing a DataFrame containing data quality statuses and comparing them against defined rules.</p> <p>Parameters:</p> Name Type Description Default <code>qc_df</code> <code>DataFrame</code> <p>A Polars DataFrame containing a column <code>dq_status</code> with semicolon-separated strings representing data quality statuses in the format \"column:rule:value\".</p> required <code>rules</code> <code>list[dict]</code> <p>A list of dictionaries where each dictionary defines a rule with keys such as \"column\", \"rule\", \"value\", and \"pass_threshold\".</p> required <code>total_rows</code> <code>int</code> <p>The total number of rows in the original dataset, used to calculate the pass rate.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A summarized DataFrame containing the following columns: - id: A unique identifier for each rule. - timestamp: The timestamp when the summary was generated. - check: A label indicating the type of check (e.g., \"Quality Check\"). - level: The severity level of the check (e.g., \"WARNING\"). - column: The column name associated with the rule. - rule: The rule being evaluated. - value: The specific value associated with the rule. - rows: The total number of rows in the dataset. - violations: The number of rows that violated the rule. - pass_rate: The proportion of rows that passed the rule. - pass_threshold: The threshold for passing the rule. - status: The status of the rule evaluation (\"PASS\" or \"FAIL\").</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.validate","title":"validate","text":"<pre><code>validate(df: DataFrame, rules: list[dict]) -&gt; Tuple[pl.DataFrame, pl.DataFrame]\n</code></pre> <p>Validates a Polars DataFrame against a set of rules and returns the updated DataFrame with validation statuses and a DataFrame containing the validation violations.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to validate.</p> required <code>rules</code> <code>list[dict]</code> <p>A list of dictionaries representing validation rules. Each rule should contain the following keys: - \"check_type\" (str): The type of validation to perform (e.g., \"is_primary_key\",     \"is_composite_key\", \"has_pattern\", etc.). - \"value\" (optional): The value to validate against, depending on the rule type. - \"execute\" (bool, optional): Whether to execute the rule. Defaults to True.</p> required <p>Returns:</p> Type Description <code>Tuple[DataFrame, DataFrame]</code> <p>Tuple[pl.DataFrame, pl.DataFrame]: A tuple containing: - The original DataFrame with an additional \"dq_status\" column indicating the     validation status for each row. - A DataFrame containing rows that violated the validation rules, including     details of the violations.</p> Notes <ul> <li>The function dynamically resolves validation functions based on the \"check_type\"     specified in the rules.</li> <li>If a rule's \"check_type\" is unknown, a warning is issued, and the rule is skipped.</li> <li>The \"__id\" column is temporarily added to the DataFrame for internal processing     and is removed in the final output.</li> </ul>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.validate_date_format","title":"validate_date_format","text":"<pre><code>validate_date_format(df: DataFrame, rule: dict) -&gt; pl.DataFrame\n</code></pre> <p>Validates the date format of a specified field in a Polars DataFrame based on a given rule.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to validate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the validation rule. It should include: - field (str): The name of the column to validate. - check (str): The name of the validation check. - fmt (str): The expected date format to validate against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A new DataFrame containing only the rows where the specified field</p> <code>DataFrame</code> <p>does not match the expected date format or is null. An additional column</p> <code>DataFrame</code> <p>\"dq_status\" is added to indicate the validation status in the format</p> <code>DataFrame</code> <p>\"{field}:{check}:{fmt}\".</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.validate_schema","title":"validate_schema","text":"<pre><code>validate_schema(df, expected) -&gt; tuple[bool, list[dict[str, Any]]]\n</code></pre> <p>Validates the schema of a given DataFrame against an expected schema.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <p>The DataFrame whose schema needs to be validated.</p> required <code>expected</code> <p>The expected schema, represented as a list of tuples where each tuple       contains the column name and its data type.</p> required <p>Returns:</p> Type Description <code>tuple[bool, list[dict[str, Any]]]</code> <p>Tuple[bool, List[Tuple[str, str]]]: A tuple containing: - A boolean indicating whether the schema matches the expected schema. - A list of tuples representing the errors, where each tuple contains   the column name and a description of the mismatch.</p>"},{"location":"sumeh/engines/pyspark/","title":"pyspark","text":""},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine","title":"sumeh.engines.pyspark_engine","text":"<p>This module provides a set of functions for performing data quality checks on PySpark DataFrames. It includes various validation rules, schema validation, and summarization utilities.</p> <p>Functions:</p> Name Description <code>is_positive</code> <p>Filters rows where the specified field is negative and adds a data quality status column.</p> <code>is_negative</code> <p>Filters rows where the specified field is non-negative and adds a data quality status column.</p> <code>is_in_millions</code> <p>Retains rows where the field value is at least 1,000,000 and flags them with dq_status.</p> <code>is_positive</code> <p>Filters rows where the specified field is negative and adds a data quality status column.</p> <code>is_negative</code> <p>Filters rows where the specified field is non-negative and adds a data quality status column.</p> <code>is_in_millions</code> <p>Retains rows where the field value is at least 1,000,000 and flags them with dq_status.</p> <code>is_in_billions</code> <p>Retains rows where the field value is at least 1,000,000,000 and flags them with dq_status.</p> <code>is_t_minus_1</code> <p>Retains rows where the date field equals yesterday (T-1) and flags them with dq_status.</p> <code>is_t_minus_2</code> <p>Retains rows where the date field equals two days ago (T-2) and flags them with dq_status.</p> <code>is_t_minus_3</code> <p>Retains rows where the date field equals three days ago (T-3) and flags them with dq_status.</p> <code>is_today</code> <p>Retains rows where the date field equals today and flags them with dq_status.</p> <code>is_yesterday</code> <p>Retains rows where the date field equals yesterday and flags them with dq_status.</p> <code>is_on_weekday</code> <p>Retains rows where the date field falls on a weekday (Mon-Fri) and flags them with dq_status.</p> <code>is_on_weekend</code> <p>Retains rows where the date field is on a weekend (Sat-Sun) and flags them with dq_status.</p> <code>is_on_monday</code> <p>Retains rows where the date field is on Monday and flags them with dq_status.</p> <code>is_on_tuesday</code> <p>Retains rows where the date field is on Tuesday and flags them with dq_status.</p> <code>is_on_wednesday</code> <p>Retains rows where the date field is on Wednesday and flags them with dq_status.</p> <code>is_on_thursday</code> <p>Retains rows where the date field is on Thursday and flags them with dq_status.</p> <code>is_on_friday</code> <p>Retains rows where the date field is on Friday and flags them with dq_status.</p> <code>is_on_saturday</code> <p>Retains rows where the date field is on Saturday and flags them with dq_status.</p> <code>is_on_sunday</code> <p>Retains rows where the date field is on Sunday and flags them with dq_status.</p> <code>is_complete</code> <p>Filters rows where the specified field is null and adds a data quality status column.</p> <code>is_unique</code> <p>Identifies duplicate rows based on the specified field and adds a data quality status column.</p> <code>are_complete</code> <p>Filters rows where any of the specified fields are null and adds a data quality status column.</p> <code>are_unique</code> <p>Identifies duplicate rows based on a combination of specified fields and adds a data quality status column.</p> <code>is_greater_than</code> <p>Filters rows where the specified field is less than or equal to the given value.</p> <code>is_greater_or_equal_than</code> <p>Filters rows where the specified field is less than the given value.</p> <code>is_less_than</code> <p>Filters rows where the specified field is greater than or equal to the given value.</p> <code>is_less_or_equal_than</code> <p>Filters rows where the specified field is greater than the given value.</p> <code>is_equal</code> <p>Filters rows where the specified field is not equal to the given value.</p> <code>is_equal_than</code> <p>Alias for <code>is_equal</code>.</p> <code>is_contained_in</code> <p>Filters rows where the specified field is not in the given list of values.</p> <code>not_contained_in</code> <p>Filters rows where the specified field is in the given list of values.</p> <code>is_between</code> <p>Filters rows where the specified field is not within the given range.</p> <code>has_pattern</code> <p>Filters rows where the specified field does not match the given regex pattern.</p> <code>is_legit</code> <p>Filters rows where the specified field is null or does not match a non-whitespace pattern.</p> <code>is_primary_key</code> <p>DataFrame, rule: dict):</p> <code>is_composite_key</code> <p>DataFrame, rule: dict):</p> <code>has_max</code> <p>Filters rows where the specified field exceeds the given maximum value.</p> <code>has_min</code> <p>Filters rows where the specified field is below the given minimum value.</p> <code>has_std</code> <p>Checks if the standard deviation of the specified field exceeds the given value.</p> <code>has_mean</code> <p>Checks if the mean of the specified field exceeds the given value.</p> <code>has_sum</code> <p>Checks if the sum of the specified field exceeds the given value.</p> <code>has_cardinality</code> <p>Checks if the cardinality (distinct count) of the specified field exceeds the given value.</p> <code>has_infogain</code> <p>Checks if the information gain (distinct count) of the specified field exceeds the given value.</p> <code>has_entropy</code> <p>Checks if the entropy (distinct count) of the specified field exceeds the given value.</p> <code>all_date_checks</code> <p>Filters rows where the specified date field is earlier than the current date.</p> <code>satisfies</code> <p>Filters rows where the specified field matches the given regex pattern.</p> <code>validate</code> <p>Applies a list of validation rules to the DataFrame and returns the results.</p> <code>summarize</code> <p>Summarizes the results of data quality checks, including pass rates and violations.</p> <code>validate_schema</code> <p>Validates the schema of the DataFrame against the expected schema.</p> <code>__rules_to_df</code> <p>Converts a list of rules into a DataFrame for further processing.</p> <code>__pyspark_schema_to_list</code> <p>Converts the schema of a DataFrame into a list of dictionaries for comparison.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.__rules_to_df","title":"__rules_to_df","text":"<pre><code>__rules_to_df(rules: List[Dict]) -&gt; DataFrame\n</code></pre> <p>Converts a list of rule dictionaries into a PySpark DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>rules</code> <code>List[Dict]</code> <p>A list of dictionaries where each dictionary represents a rule. Each rule dictionary should contain the following keys:     - \"field\" (str or list): The name of the field or a list of field names.     - \"check_type\" (str): The type of rule or check to be applied.     - \"threshold\" (float, optional): The threshold value for the rule. Defaults to 1.0 if not provided.     - \"value\" (str, optional): The value associated with the rule. Defaults to \"N/A\" if not provided.     - \"execute\" (bool, optional): A flag indicating whether the rule should be executed. Defaults to True.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A PySpark DataFrame containing the following columns: - \"column\" (str): The name of the field. - \"rule\" (str): The type of rule or check. - \"pass_threshold\" (float): The threshold value for the rule. - \"value\" (str): The value associated with the rule.</p> Notes <ul> <li>Rows with \"execute\" set to False are skipped.</li> <li>Duplicate rows based on the \"column\" and \"rule\" columns are removed.</li> </ul>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.all_date_checks","title":"all_date_checks","text":"<pre><code>all_date_checks(df: DataFrame, rule: dict) -&gt; DataFrame\n</code></pre> <p>Filters the input DataFrame based on a date-related rule and adds a data quality status column.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The name of the column to apply the rule on. - 'check': The type of check to perform (e.g., comparison operator). - 'value': The value to be used in the check.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame filtered based on the rule, with an additional column</p> <code>DataFrame</code> <p>\"dq_status\" indicating the data quality status in the format \"field:check:value\".</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.are_complete","title":"are_complete","text":"<pre><code>are_complete(df: DataFrame, rule: dict) -&gt; DataFrame\n</code></pre> <p>Filters rows in a DataFrame that do not meet the completeness rule and adds a data quality status column.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - \"fields\" (list): A list of column names to check for completeness (non-null values). - \"check\" (str): A descriptive label for the type of check being performed. - \"value\" (str): A descriptive value associated with the check.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame containing only the rows that fail the completeness check,</p> <code>DataFrame</code> <p>with an additional column \"dq_status\" describing the failed rule.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.are_unique","title":"are_unique","text":"<pre><code>are_unique(df: DataFrame, rule: dict) -&gt; DataFrame\n</code></pre> <p>Checks for uniqueness of specified fields in a PySpark DataFrame based on the provided rule.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'fields': A list of column names to check for uniqueness. - 'check': A string representing the type of check (e.g., \"unique\"). - 'value': A value associated with the rule for logging or identification.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A DataFrame containing rows that violate the uniqueness rule.</p> <code>DataFrame</code> <p>The resulting DataFrame includes an additional column <code>dq_status</code> that</p> <code>DataFrame</code> <p>describes the rule violation in the format: \"[fields]:[check]:[value]\".</p> Notes <ul> <li>The function concatenates the specified fields into a single column   and checks for duplicate values within that column.</li> <li>Rows that do not meet the uniqueness criteria are returned, while   rows that satisfy the criteria are excluded from the result.</li> </ul>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.extract_schema","title":"extract_schema","text":"<pre><code>extract_schema(df) -&gt; List[Dict[str, Any]]\n</code></pre> <p>Extract schema from PySpark DataFrame.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.has_cardinality","title":"has_cardinality","text":"<pre><code>has_cardinality(df: DataFrame, rule: dict) -&gt; DataFrame\n</code></pre> <p>Checks the cardinality of a specified field in a DataFrame against a given rule.</p> <p>This function evaluates whether the distinct count of values in a specified column (field) of the DataFrame exceeds a given threshold (value) as defined in the rule. If the cardinality exceeds the threshold, a new column <code>dq_status</code> is added to the DataFrame with information about the rule violation. Otherwise, an empty DataFrame is returned.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The name of the column to check. - 'check': The type of check being performed (e.g., \"cardinality\"). - 'value': The threshold value for the cardinality.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A DataFrame with the <code>dq_status</code> column added if the cardinality</p> <code>DataFrame</code> <p>exceeds the threshold, or an empty DataFrame if the condition is not met.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.has_entropy","title":"has_entropy","text":"<pre><code>has_entropy(df: DataFrame, rule: dict) -&gt; DataFrame\n</code></pre> <p>Evaluates the entropy of a specified field in a DataFrame and applies a rule to determine whether the DataFrame should be processed further or filtered out.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to evaluate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The name of the column to evaluate. - 'check' (str): The type of check being performed (e.g., \"entropy\"). - 'value' (float): The threshold value for the entropy check.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>If the entropy of the specified field exceeds the given value, returns the</p> <code>DataFrame</code> <p>original DataFrame with an additional column \"dq_status\" indicating the rule applied.</p> <code>DataFrame</code> <p>Otherwise, returns an empty DataFrame with the same schema as the input.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.has_infogain","title":"has_infogain","text":"<pre><code>has_infogain(df: DataFrame, rule: dict) -&gt; DataFrame\n</code></pre> <p>Evaluates whether a given DataFrame satisfies an information gain condition based on the provided rule. If the condition is met, it appends a column indicating the status; otherwise, it returns an empty DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to evaluate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should          include the following keys:          - 'field': The column name to evaluate.          - 'check': The condition type (not used directly in the logic).          - 'value': The threshold value for information gain.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A DataFrame with an additional \"dq_status\" column if the        information gain condition is met, or an empty DataFrame        if the condition is not satisfied.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.has_max","title":"has_max","text":"<pre><code>has_max(df: DataFrame, rule: dict) -&gt; DataFrame\n</code></pre> <p>Filters a PySpark DataFrame to include only rows where the value of a specified field is greater than a given threshold. Adds a new column 'dq_status' to indicate the rule applied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The name of the column to apply the rule on. - 'check' (str): The type of check being performed (e.g., 'max'). - 'value' (numeric): The threshold value to compare against.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame filtered based on the rule, with an additional column 'dq_status'</p> <code>DataFrame</code> <p>describing the rule applied in the format \"field:check:value\".</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.has_mean","title":"has_mean","text":"<pre><code>has_mean(df: DataFrame, rule: dict) -&gt; DataFrame\n</code></pre> <p>Evaluates whether the mean value of a specified column in a DataFrame satisfies a given rule.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to evaluate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The name of the column to calculate the mean for. - 'check' (str): The type of check being performed (e.g., 'greater_than'). - 'value' (float): The threshold value to compare the mean against.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>If the mean value of the specified column exceeds the threshold,</p> <code>DataFrame</code> <p>returns the original DataFrame with an additional column <code>dq_status</code> indicating</p> <code>DataFrame</code> <p>the rule violation. If the mean value satisfies the rule, returns an empty DataFrame.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.has_min","title":"has_min","text":"<pre><code>has_min(df: DataFrame, rule: dict) -&gt; DataFrame\n</code></pre> <p>Filters rows in a DataFrame where the value of a specified field is less than a given threshold and adds a new column indicating the data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The name of the column to check. - 'check' (str): The type of check being performed (e.g., \"min\"). - 'value' (numeric): The threshold value for the check.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame with rows filtered based on the rule and an additional</p> <code>DataFrame</code> <p>\"dq_status\" column containing a string representation of the rule applied.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.has_pattern","title":"has_pattern","text":"<pre><code>has_pattern(df: DataFrame, rule: dict) -&gt; DataFrame\n</code></pre> <p>Filters a PySpark DataFrame based on a pattern match and adds a data quality status column.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The name of the column to apply the pattern check. - 'check': A descriptive label for the type of check being performed. - 'value': The regex pattern to match against the column values.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame with rows that do not match the pattern filtered out.        Additionally, a \"dq_status\" column is added, containing a string        representation of the rule applied in the format \"field:check:value\".</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.has_std","title":"has_std","text":"<pre><code>has_std(df: DataFrame, rule: dict) -&gt; DataFrame\n</code></pre> <p>Checks if the standard deviation of a specified field in a DataFrame exceeds a given value.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The name of the column to calculate the standard deviation for. - 'check' (str): A descriptive label for the check being performed. - 'value' (float): The threshold value for the standard deviation.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>If the standard deviation of the specified field exceeds the given value,</p> <code>DataFrame</code> <p>returns the original DataFrame with an additional column \"dq_status\" indicating the</p> <code>DataFrame</code> <p>field, check, and value. Otherwise, returns an empty DataFrame.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.has_sum","title":"has_sum","text":"<pre><code>has_sum(df: DataFrame, rule: dict) -&gt; DataFrame\n</code></pre> <p>Checks if the sum of values in a specified column of a DataFrame exceeds a given threshold.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to evaluate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The name of the column to sum. - 'check' (str): A descriptive label for the check being performed. - 'value' (float): The threshold value to compare the sum against.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>If the sum of the specified column exceeds the threshold, returns the original</p> <code>DataFrame</code> <p>DataFrame with an additional column <code>dq_status</code> indicating the rule details. If the sum</p> <code>DataFrame</code> <p>does not exceed the threshold, returns an empty DataFrame.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.is_between","title":"is_between","text":"<pre><code>is_between(df: DataFrame, rule: dict) -&gt; DataFrame\n</code></pre> <p>Filters rows in a PySpark DataFrame where the value of a specified field is not within a given range. Adds a new column 'dq_status' to indicate the rule that was applied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The name of the column to check. - 'check': A string representing the type of check (e.g., \"between\"). - 'value': A string representing the range in the format \"[min_value,max_value]\".</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame with rows filtered based on the rule and an additional</p> <code>DataFrame</code> <p>'dq_status' column indicating the applied rule.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.is_complete","title":"is_complete","text":"<pre><code>is_complete(df: DataFrame, rule: dict) -&gt; DataFrame\n</code></pre> <p>Filters a DataFrame to identify rows where a specified field is null and adds a \"dq_status\" column indicating the data quality rule applied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the data quality rule. It should include: - \"field\" (str): The name of the field to check for null values. - \"check\" (str): A description of the check being performed. - \"value\" (str): Additional information about the rule.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame filtered to include only rows where the specified</p> <code>DataFrame</code> <p>field is null, with an additional \"dq_status\" column describing the rule.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.is_composite_key","title":"is_composite_key","text":"<pre><code>is_composite_key(df: DataFrame, rule: dict)\n</code></pre> <p>Determines if the given DataFrame satisfies the composite key condition based on the provided rule.</p> <p>A composite key is a combination of two or more columns in a DataFrame that uniquely identify a row.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The PySpark DataFrame to be evaluated.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rules or criteria to determine the composite key.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <p>True if the DataFrame satisfies the composite key condition, False otherwise.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.is_contained_in","title":"is_contained_in","text":"<pre><code>is_contained_in(df: DataFrame, rule: dict) -&gt; DataFrame\n</code></pre> <p>Filters rows in a PySpark DataFrame based on whether a specified column's value is not contained in a given list of values. Adds a new column 'dq_status' to indicate the rule applied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The name of the column to check. - 'check': The type of check being performed (e.g., \"is_contained_in\"). - 'value': A string representation of a list of values (e.g., \"[value1,value2]\").</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new PySpark DataFrame with rows filtered based on the rule</p> <code>DataFrame</code> <p>and an additional column 'dq_status' describing the rule applied.</p> Example <p>rule = {\"field\": \"column_name\", \"check\": \"is_contained_in\", \"value\": \"[value1,value2]\"} result_df = is_contained_in(input_df, rule)</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.is_date_after","title":"is_date_after","text":"<pre><code>is_date_after(df: DataFrame, rule: dict) -&gt; DataFrame\n</code></pre> <p>Filters a DataFrame to identify rows where a specified field has a date lower than the date informed in the rule and adds a \"dq_status\" column indicating the data quality rule applied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the data quality rule. It should include: - \"field\" (str): The name of the field to check for null values. - \"check\" (str): A description of the check being performed. - \"value\" (str): Additional information about the rule.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame filtered to include only rows where the specified</p> <code>DataFrame</code> <p>field is null, with an additional \"dq_status\" column describing the rule.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.is_date_before","title":"is_date_before","text":"<pre><code>is_date_before(df: DataFrame, rule: dict) -&gt; DataFrame\n</code></pre> <p>Filters a DataFrame to identify rows where a specified field has a date greater than the date informed in the rule and adds a \"dq_status\" column indicating the data quality rule applied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the data quality rule. It should include: - \"field\" (str): The name of the field to check for null values. - \"check\" (str): A description of the check being performed. - \"value\" (str): Additional information about the rule.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame filtered to include only rows where the specified</p> <code>DataFrame</code> <p>field is null, with an additional \"dq_status\" column describing the rule.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.is_date_between","title":"is_date_between","text":"<pre><code>is_date_between(df: DataFrame, rule: dict) -&gt; DataFrame\n</code></pre> <p>Filters a DataFrame to identify rows where a specified field has a date between two dates passed in the rule using the format: \"[, ]\" and adds a \"dq_status\" column indicating the data quality rule applied. <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the data quality rule. It should include: - \"field\" (str): The name of the field to check for null values. - \"check\" (str): A description of the check being performed. - \"value\" (str): Additional information about the rule.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame filtered to include only rows where the specified</p> <code>DataFrame</code> <p>field is null, with an additional \"dq_status\" column describing the rule.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.is_equal","title":"is_equal","text":"<pre><code>is_equal(df: DataFrame, rule: dict) -&gt; DataFrame\n</code></pre> <p>Filters a PySpark DataFrame based on a rule that checks for equality between a specified field and a given value. Rows that do not satisfy the equality condition are retained, and a new column \"dq_status\" is added to indicate the rule applied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - \"field\" (str): The name of the column to check. - \"check\" (str): The type of check (e.g., \"equal\"). This is used for logging purposes. - \"value\" (Any): The value to compare against.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame with rows that do not satisfy the equality condition and an</p> <code>DataFrame</code> <p>additional \"dq_status\" column describing the rule applied.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.is_equal_than","title":"is_equal_than","text":"<pre><code>is_equal_than(df: DataFrame, rule: dict) -&gt; DataFrame\n</code></pre> <p>Filters rows in a PySpark DataFrame that do not satisfy an equality condition specified in the rule dictionary and adds a \"dq_status\" column with details about the rule applied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - \"field\" (str): The name of the column to check. - \"check\" (str): The type of check being performed (e.g., \"equal\"). - \"value\" (Any): The value to compare against.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame with rows filtered based on the rule and an</p> <code>DataFrame</code> <p>additional \"dq_status\" column describing the rule applied.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.is_future_date","title":"is_future_date","text":"<pre><code>is_future_date(df: DataFrame, rule: dict) -&gt; DataFrame\n</code></pre> <p>Filters a DataFrame to identify rows where a specified field has a date greater than the current date and adds a \"dq_status\" column indicating the data quality rule applied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the data quality rule. It should include: - \"field\" (str): The name of the field to check for null values. - \"check\" (str): A description of the check being performed. - \"value\" (str): Additional information about the rule.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame filtered to include only rows where the specified</p> <code>DataFrame</code> <p>field is null, with an additional \"dq_status\" column describing the rule.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.is_greater_or_equal_than","title":"is_greater_or_equal_than","text":"<pre><code>is_greater_or_equal_than(df: DataFrame, rule: dict) -&gt; DataFrame\n</code></pre> <p>Filters rows in a DataFrame where the value of a specified field is less than a given value and adds a new column \"dq_status\" with a formatted string indicating the rule applied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - \"field\" (str): The name of the column to check. - \"check\" (str): A descriptive string for the check (e.g., \"greater_or_equal\"). - \"value\" (numeric): The threshold value for the comparison.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame with rows filtered based on the rule and an additional</p> <code>DataFrame</code> <p>\"dq_status\" column describing the rule applied.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.is_greater_than","title":"is_greater_than","text":"<pre><code>is_greater_than(df: DataFrame, rule: dict) -&gt; DataFrame\n</code></pre> <p>Filters rows in a DataFrame where the value of a specified field is less than or equal to a given threshold and adds a new column indicating the rule applied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The name of the column to apply the rule on. - 'check' (str): A descriptive string for the rule (e.g., \"greater_than\"). - 'value' (int or float): The threshold value for the comparison.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame with rows filtered based on the rule and an</p> <code>DataFrame</code> <p>additional column \"dq_status\" describing the rule applied.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.is_in","title":"is_in","text":"<pre><code>is_in(df: DataFrame, rule: dict) -&gt; DataFrame\n</code></pre> <p>Checks if the values in the specified column of a DataFrame are contained within a given set of values.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to evaluate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule for the check. It should specify the column name          and the set of values to check against.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A DataFrame with the applied rule, typically filtered or modified based on the check.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.is_in_billions","title":"is_in_billions","text":"<pre><code>is_in_billions(df, rule: dict)\n</code></pre> <p>Filters a PySpark DataFrame to include only rows where the specified field's value is greater than or equal to one billion, and adds a \"dq_status\" column with a formatted string indicating the field, check, and value.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The name of the column to check. - 'check': The type of check being performed (e.g., \"greater_than\"). - 'value': The threshold value for the check.</p> required <p>Returns:</p> Type Description <p>pyspark.sql.DataFrame: A new DataFrame filtered by the rule and with an</p> <p>additional \"dq_status\" column.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.is_in_millions","title":"is_in_millions","text":"<pre><code>is_in_millions(df, rule: dict)\n</code></pre> <p>Filters a DataFrame to include only rows where the specified field's value is greater than or equal to 1,000,000 and adds a \"dq_status\" column with a formatted string indicating the rule applied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to filter and modify.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should          include the field to check, the check type, and the value.</p> required <p>Returns:</p> Type Description <p>pyspark.sql.DataFrame: A new DataFrame with rows filtered based on the</p> <p>rule and an additional \"dq_status\" column describing the rule applied.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.is_legit","title":"is_legit","text":"<pre><code>is_legit(df: DataFrame, rule: dict) -&gt; DataFrame\n</code></pre> <p>Filters a PySpark DataFrame to identify rows that do not meet a specified rule and appends a column indicating the data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be validated.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The name of the column to validate. - 'check': The type of check being performed (e.g., \"is_legit\"). - 'value': The expected value or condition for the validation.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame containing only the rows that fail the validation</p> <code>DataFrame</code> <p>rule, with an additional column \"dq_status\" describing the validation status</p> <code>DataFrame</code> <p>in the format \"field:check:value\".</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.is_less_or_equal_than","title":"is_less_or_equal_than","text":"<pre><code>is_less_or_equal_than(df: DataFrame, rule: dict) -&gt; DataFrame\n</code></pre> <p>Filters rows in a PySpark DataFrame where the value of a specified field is greater than a given value and adds a new column \"dq_status\" with a formatted string indicating the rule applied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - \"field\" (str): The name of the column to evaluate. - \"check\" (str): A descriptive string for the check being performed. - \"value\" (numeric): The threshold value to compare against.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new PySpark DataFrame with rows filtered based on the rule and an additional</p> <code>DataFrame</code> <p>\"dq_status\" column describing the rule applied.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.is_less_than","title":"is_less_than","text":"<pre><code>is_less_than(df: DataFrame, rule: dict) -&gt; DataFrame\n</code></pre> <p>Filters rows in a PySpark DataFrame where the specified field is greater than or equal to a given value and adds a new column indicating the rule applied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The name of the column to apply the filter on. - 'check' (str): A descriptive string for the rule (e.g., \"less_than\"). - 'value' (int, float, or str): The value to compare the column against.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame with rows filtered based on the rule and an</p> <code>DataFrame</code> <p>additional column \"dq_status\" describing the rule applied.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.is_negative","title":"is_negative","text":"<pre><code>is_negative(df: DataFrame, rule: dict) -&gt; DataFrame\n</code></pre> <p>Filters rows in the given DataFrame where the specified field is non-negative and adds a new column \"dq_status\" containing a formatted string with rule details.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be filtered and modified.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The name of the column to check. - 'check' (str): A descriptive string for the check being performed. - 'value' (any): The value associated with the rule.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame with rows filtered based on the rule and an</p> <code>DataFrame</code> <p>additional \"dq_status\" column describing the rule applied.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.is_on_friday","title":"is_on_friday","text":"<pre><code>is_on_friday(df, rule: dict)\n</code></pre> <p>Filters a PySpark DataFrame to include only rows where the specified date field falls on a Friday.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to have the following keys: - 'field': The name of the column in the DataFrame to check. - 'check': A string representing the type of check (not used in this function but included for consistency). - 'value': A value associated with the rule (not used in this function but included for consistency).</p> required <p>Returns:</p> Type Description <p>pyspark.sql.DataFrame: A new DataFrame filtered to include only rows where the specified date field</p> <p>corresponds to a Friday. Additionally, a new column <code>dq_status</code> is added, which contains a string</p> <p>representation of the rule applied in the format \"field:check:value\".</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.is_on_monday","title":"is_on_monday","text":"<pre><code>is_on_monday(df, rule: dict)\n</code></pre> <p>Filters a PySpark DataFrame to include only rows where the specified date field falls on a Monday.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing rule parameters. It is expected to include: - 'field': The name of the column to check. - 'check': A string representing the type of check (not used in this function). - 'value': A value associated with the rule (not used in this function).</p> required <p>Returns:</p> Type Description <p>pyspark.sql.DataFrame: A new DataFrame filtered to include only rows where the specified</p> <p>date field corresponds to a Monday. Additionally, a new column \"dq_status\" is added,</p> <p>containing a concatenated string of the field, check, and value.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.is_on_saturday","title":"is_on_saturday","text":"<pre><code>is_on_saturday(df, rule: dict)\n</code></pre> <p>Filters a PySpark DataFrame to include only rows where the specified date field falls on a Saturday.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing rule parameters. The function expects the rule to include: - 'field': The name of the column to check. - 'check': A string representing the check being performed (not used in logic, but included in the output column). - 'value': A value to include in the output column (not used in logic, but included in the output column).</p> required <p>Returns:</p> Type Description <p>pyspark.sql.DataFrame: A new DataFrame filtered to include only rows where the specified field falls on a Saturday.</p> <p>Additionally, a new column \"dq_status\" is added, containing a string in the format \"field:check:value\".</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.is_on_sunday","title":"is_on_sunday","text":"<pre><code>is_on_sunday(df, rule: dict)\n</code></pre> <p>Filters a PySpark DataFrame to include only rows where the specified date field falls on a Sunday.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to include: - field (str): The name of the column to check. - check (str): A descriptive string for the check being performed. - value (str): A value to include in the \"dq_status\" column for context.</p> required <p>Returns:</p> Type Description <p>pyspark.sql.DataFrame: A new DataFrame filtered to include only rows where the specified</p> <p>date field corresponds to a Sunday. Additionally, a \"dq_status\" column is added to the</p> <p>DataFrame, containing a string in the format \"field:check:value\".</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.is_on_thursday","title":"is_on_thursday","text":"<pre><code>is_on_thursday(df, rule: dict)\n</code></pre> <p>Filters a PySpark DataFrame to include only rows where the specified date column falls on a Thursday.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The PySpark DataFrame to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to include: - 'field': The name of the column to check. - 'check': A string representing the type of check (not used in the filtering logic). - 'value': A value associated with the rule (not used in the filtering logic).</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <p>A new PySpark DataFrame filtered to include only rows where the specified column's day of the week is Thursday.        Additionally, a new column \"dq_status\" is added, containing a concatenated string of the field, check, and value.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.is_on_tuesday","title":"is_on_tuesday","text":"<pre><code>is_on_tuesday(df, rule: dict)\n</code></pre> <p>Filters a PySpark DataFrame to include only rows where the day of the week for a specified date column is Tuesday. Adds a new column 'dq_status' to indicate the validation status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to include: - 'field': The name of the column to check. - 'check': A string describing the check being performed. - 'value': A value associated with the check.</p> required <p>Returns:</p> Type Description <p>pyspark.sql.DataFrame: A new DataFrame filtered to include only rows</p> <p>where the specified column corresponds to Tuesday, with an additional</p> <p>'dq_status' column describing the validation status.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.is_on_wednesday","title":"is_on_wednesday","text":"<pre><code>is_on_wednesday(df, rule: dict)\n</code></pre> <p>Filters a PySpark DataFrame to include only rows where the specified date field falls on a Wednesday.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to have the following keys: - 'field': The name of the column in the DataFrame to check. - 'check': A string representing the type of check (not used in the logic but included for status reporting). - 'value': A value associated with the rule (not used in the logic but included for status reporting).</p> required <p>Returns:</p> Type Description <p>pyspark.sql.DataFrame: A new DataFrame filtered to include only rows where the specified field corresponds to a Wednesday.</p> <p>Additionally, a new column 'dq_status' is added, which contains a string in the format \"field:check:value\".</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.is_on_weekday","title":"is_on_weekday","text":"<pre><code>is_on_weekday(df, rule: dict)\n</code></pre> <p>Filters a PySpark DataFrame to include only rows where the specified date field falls on a weekday (Monday to Friday). Adds a new column 'dq_status' to indicate the rule applied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to include the following keys: - 'field': The name of the column to check. - 'check': A string representing the type of check (used for logging). - 'value': A value associated with the rule (used for logging).</p> required <p>Returns:</p> Type Description <p>pyspark.sql.DataFrame: A new DataFrame filtered to include only rows where</p> <p>the specified date field is a weekday, with an additional 'dq_status' column</p> <p>describing the rule applied.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.is_on_weekend","title":"is_on_weekend","text":"<pre><code>is_on_weekend(df, rule: dict)\n</code></pre> <p>Filters a PySpark DataFrame to include only rows where the specified date field falls on a weekend (Saturday or Sunday). Additionally, adds a new column 'dq_status' to indicate the rule applied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected          to have the following keys:          - 'field': The name of the date column to check.          - 'check': A string representing the type of check (not used in logic).          - 'value': A string representing the value to include in the 'dq_status' column.</p> required <p>Returns:</p> Type Description <p>pyspark.sql.DataFrame: A new DataFrame filtered to include only rows where</p> <p>the specified date field is on a weekend, with an additional 'dq_status' column.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.is_past_date","title":"is_past_date","text":"<pre><code>is_past_date(df: DataFrame, rule: dict) -&gt; DataFrame\n</code></pre> <p>Filters a DataFrame to identify rows where a specified field has a date lower than the current date and adds a \"dq_status\" column indicating the data quality rule applied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the data quality rule. It should include: - \"field\" (str): The name of the field to check for null values. - \"check\" (str): A description of the check being performed. - \"value\" (str): Additional information about the rule.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame filtered to include only rows where the specified</p> <code>DataFrame</code> <p>field is null, with an additional \"dq_status\" column describing the rule.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.is_positive","title":"is_positive","text":"<pre><code>is_positive(df: DataFrame, rule: dict) -&gt; DataFrame\n</code></pre> <p>Filters a DataFrame to identify rows where the specified field does not satisfy a positive check and adds a \"dq_status\" column with details of the rule applied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - \"field\" (str): The name of the column to check. - \"check\" (str): The type of check being performed (e.g., \"positive\"). - \"value\" (any): The value associated with the rule (not directly used in this function).</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame filtered to include only rows where the specified field is less than 0,</p> <code>DataFrame</code> <p>with an additional \"dq_status\" column describing the rule applied.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.is_primary_key","title":"is_primary_key","text":"<pre><code>is_primary_key(df: DataFrame, rule: dict)\n</code></pre> <p>Determines if a given DataFrame column or set of columns satisfies the primary key constraint.</p> <p>A primary key constraint requires that the specified column(s) in the DataFrame have unique values.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The PySpark DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rules or specifications for identifying the primary key.          Typically, this includes the column(s) to be checked for uniqueness.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <p>True if the specified column(s) in the DataFrame satisfy the primary key constraint, False otherwise.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.is_t_minus_1","title":"is_t_minus_1","text":"<pre><code>is_t_minus_1(df, rule: dict)\n</code></pre> <p>Filters the input DataFrame to include only rows where the specified field matches the date corresponding to \"T-1\" (yesterday). Adds a new column \"dq_status\" to indicate the rule applied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The name of the column to be checked. - 'check': The type of check being performed (not used in filtering but included in \"dq_status\"). - 'value': The value associated with the check (not used in filtering but included in \"dq_status\").</p> required <p>Returns:</p> Type Description <p>pyspark.sql.DataFrame: A new DataFrame filtered by the rule and with an additional \"dq_status\" column.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.is_t_minus_2","title":"is_t_minus_2","text":"<pre><code>is_t_minus_2(df, rule: dict)\n</code></pre> <p>Filters the input DataFrame to include only rows where the specified field matches the date that is two days prior to the current date. Adds a new column 'dq_status' to indicate the data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The name of the column to be checked. - 'check': A string representing the type of check (not used in filtering). - 'value': A value associated with the check (not used in filtering).</p> required <p>Returns:</p> Type Description <p>pyspark.sql.DataFrame: A new DataFrame filtered by the rule and with an additional</p> <p>'dq_status' column indicating the field, check, and value.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.is_t_minus_3","title":"is_t_minus_3","text":"<pre><code>is_t_minus_3(df, rule: dict)\n</code></pre> <p>Filters the input DataFrame to include only rows where the specified field matches the date that is three days prior to the current date. Adds a new column 'dq_status' to indicate the data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The name of the column to be checked. - 'check': A string representing the type of check (not used in filtering). - 'value': A value associated with the rule (not used in filtering).</p> required <p>Returns:</p> Type Description <p>pyspark.sql.DataFrame: A new DataFrame filtered by the rule and with an</p> <p>additional 'dq_status' column.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.is_today","title":"is_today","text":"<pre><code>is_today(df, rule: dict)\n</code></pre> <p>Filters a DataFrame to include only rows where the specified field matches the current date.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to have          the following keys:          - 'field': The name of the column to check.          - 'check': A string representing the type of check (not used in this function).          - 'value': A value associated with the rule (not used in this function).</p> required <p>Returns:</p> Type Description <p>pyspark.sql.DataFrame: A new DataFrame filtered by the current date and with an additional                    column \"dq_status\" indicating the rule applied in the format                    \"field:check:value\".</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.is_unique","title":"is_unique","text":"<pre><code>is_unique(df: DataFrame, rule: dict) -&gt; DataFrame\n</code></pre> <p>Checks for uniqueness of a specified field in a PySpark DataFrame based on the given rule.</p> <p>This function identifies rows where the specified field is not unique within the DataFrame. It adds a new column <code>dq_status</code> to the resulting DataFrame, which contains information about the field, the check type, and the value from the rule.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to check for uniqueness.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - <code>field</code> (str): The name of the field to check for uniqueness. - <code>check</code> (str): The type of check being performed (e.g., \"unique\"). - <code>value</code> (str): Additional value or metadata related to the check.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame containing rows where the specified field is not unique.</p> <code>DataFrame</code> <p>The resulting DataFrame includes a <code>dq_status</code> column with details about the rule violation.</p> Example <p>rule = {\"field\": \"column_name\", \"check\": \"unique\", \"value\": \"some_value\"} result_df = is_unique(input_df, rule)</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.is_yesterday","title":"is_yesterday","text":"<pre><code>is_yesterday(df, rule: dict)\n</code></pre> <p>Filters a PySpark DataFrame to include only rows where the specified field matches yesterday's date. Adds a new column 'dq_status' to indicate the data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The name of the column to check. - 'check': The type of check being performed (used for status message). - 'value': Additional value information (used for status message).</p> required <p>Returns:</p> Type Description <p>pyspark.sql.DataFrame: A filtered DataFrame with an additional 'dq_status' column.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.not_contained_in","title":"not_contained_in","text":"<pre><code>not_contained_in(df: DataFrame, rule: dict) -&gt; DataFrame\n</code></pre> <p>Filters rows in a DataFrame where the specified field's value is in a given list and adds a column indicating the data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The name of the column to check. - 'check': A string representing the type of check (e.g., \"not_contained_in\"). - 'value': A string representation of a list (e.g., \"[value1,value2,...]\")   containing the values to check against.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame with rows filtered based on the rule and an</p> <code>DataFrame</code> <p>additional column \"dq_status\" indicating the data quality status in the</p> <code>DataFrame</code> <p>format \"field:check:value\".</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.not_in","title":"not_in","text":"<pre><code>not_in(df: DataFrame, rule: dict) -&gt; DataFrame\n</code></pre> <p>Filters rows in a DataFrame where the specified rule is not contained.</p> <p>This function delegates the operation to the <code>not_contained_in</code> function.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary specifying the rule to apply for filtering.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame with rows that do not match the specified rule.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.satisfies","title":"satisfies","text":"<pre><code>satisfies(df: DataFrame, rule: dict) -&gt; DataFrame\n</code></pre> <p>Filters a PySpark DataFrame based on a rule and adds a data quality status column.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the filtering rule. It should include: - 'field': The name of the column to apply the filter on. - 'check': The type of check to perform (currently unused in this implementation). - 'value': The expression in the pattern of pyspark.sql.functions.expr.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame filtered based on the rule, with an additional column</p> <code>DataFrame</code> <p>\"dq_status\" that describes the rule applied in the format \"field:check:value\".</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.summarize","title":"summarize","text":"<pre><code>summarize(df: DataFrame, rules: List[Dict], total_rows) -&gt; DataFrame\n</code></pre> <p>Summarizes data quality results based on provided rules and total rows.</p> <p>This function processes a DataFrame containing data quality statuses, applies rules to calculate violations, and generates a summary DataFrame with metrics such as pass rate, status, and other relevant information.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing a column <code>dq_status</code> with data quality statuses in the format \"column:rule:value\".</p> required <code>rules</code> <code>List[Dict]</code> <p>A list of dictionaries representing the data quality rules. Each dictionary should define the <code>column</code>, <code>rule</code>, and optional <code>value</code> and <code>pass_threshold</code>.</p> required <code>total_rows</code> <code>int</code> <p>The total number of rows in the input DataFrame.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A summary DataFrame containing the following columns: - id: A unique identifier for each row. - timestamp: The timestamp when the summary was generated. - check: The type of check performed (e.g., \"Quality Check\"). - level: The severity level of the check (e.g., \"WARNING\"). - column: The column name associated with the rule. - rule: The rule applied to the column. - value: The value associated with the rule. - rows: The total number of rows in the input DataFrame. - violations: The number of rows that violated the rule. - pass_rate: The percentage of rows that passed the rule. - pass_threshold: The threshold for passing the rule. - status: The overall status of the rule (e.g., \"PASS\" or \"FAIL\").</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.validate","title":"validate","text":"<pre><code>validate(df: DataFrame, rules: list[dict]) -&gt; Tuple[DataFrame, DataFrame]\n</code></pre> <p>Validates a DataFrame against a set of rules and returns the validation results.</p> <p>This function applies a series of validation rules to the input DataFrame. Each rule is expected to be a dictionary containing the parameters required for validation. The function generates two DataFrames as output: 1. A summarized result DataFrame with aggregated validation statuses. 2. A raw result DataFrame containing detailed validation results.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to validate.</p> required <code>rules</code> <code>list[dict]</code> <p>A list of dictionaries, where each dictionary defines a validation rule. Each rule should include the following keys: - <code>field</code> (str): The column name to validate. - <code>rule_name</code> (str): The name of the validation function to apply. - <code>value</code> (any): The value or parameter required by the validation function.</p> required <p>Returns:</p> Type Description <code>Tuple[DataFrame, DataFrame]</code> <p>Tuple[DataFrame, DataFrame]: A tuple containing: - result (DataFrame): A DataFrame with aggregated validation statuses. - raw_result (DataFrame): A DataFrame with detailed validation results.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If a rule references a validation function that does not exist in the global scope.</p> Notes <ul> <li>The <code>dq_status</code> column is used to store validation statuses.</li> <li>The function assumes that the validation functions are defined in the global scope   and are accessible by their names.</li> <li>The <code>concat_ws</code> function is used to concatenate multiple validation statuses   into a single string for each record in the summarized result.</li> </ul> Example <p>from pyspark.sql import SparkSession spark = SparkSession.builder.getOrCreate() df = spark.createDataFrame([(1, \"Alice\"), (2, \"Bob\")], [\"id\", \"name\"]) rules = [{\"field\": \"id\", \"rule_name\": \"validate_positive\", \"value\": None}] result, raw_result = validate(df, rules)</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.validate_date_format","title":"validate_date_format","text":"<pre><code>validate_date_format(df: DataFrame, rule: dict) -&gt; DataFrame\n</code></pre> <p>Filters a DataFrame to identify rows where a specified field has wrong date format based in the format from the rule and adds a \"dq_status\" column indicating the data quality rule applied.</p> <p>YYYY = full year, ex: 2012; YY = only second part of the year, ex: 12; MM = Month number (1-12); DD = Day (1-31);</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the data quality rule. It should include: - \"field\" (str): The name of the field to check for null values. - \"check\" (str): A description of the check being performed. - \"value\" (str): Additional information about the rule.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame filtered to include only rows where the specified</p> <code>DataFrame</code> <p>field is null, with an additional \"dq_status\" column describing the rule.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.validate_schema","title":"validate_schema","text":"<pre><code>validate_schema(df: DataFrame, expected) -&gt; tuple[bool, list[dict[str, Any]]]\n</code></pre> <p>Validates the schema of a PySpark DataFrame against an expected schema.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The PySpark DataFrame whose schema is to be validated.</p> required <code>expected</code> <code>list</code> <p>The expected schema represented as a list of tuples,              where each tuple contains the column name and its data type              and a boolean, if the column is nullable or not.</p> required <p>Returns:</p> Type Description <code>tuple[bool, list[dict[str, Any]]]</code> <p>Tuple[bool, List[Tuple[str, str]]]: A tuple containing: - A boolean indicating whether the schema matches the expected schema. - A list of tuples representing the mismatched columns, where each tuple   contains the column name and the reason for the mismatch.</p>"},{"location":"sumeh/generators/generator/","title":"SQL Dialects Usage Guide","text":"<p>Complete reference for generating DDL across all supported databases.</p>"},{"location":"sumeh/generators/generator/#overview","title":"Overview","text":"<p>Sumeh supports 10+ SQL dialects for generating <code>rules</code> and <code>schema_registry</code> tables.</p> <pre><code># List all available dialects\nsumeh sql --list-dialects\n\n# Output:\n# Supported SQL dialects:\n#   - athena\n#   - bigquery\n#   - databricks\n#   - duckdb\n#   - mysql\n#   - postgres\n#   - redshift\n#   - snowflake\n#   - sqlite\n</code></pre>"},{"location":"sumeh/generators/generator/#postgresql-postgresql","title":"PostgreSQL / PostgreSQL","text":"<p>Best for: Traditional relational databases, ACID compliance</p> <pre><code># Basic table\nsumeh sql --table rules --dialect postgres\n\n# With schema\nsumeh sql --table rules --dialect postgres --schema public\n\n# Both tables\nsumeh sql --table all --dialect postgres --schema public\n\n# Save to file\nsumeh sql --table all --dialect postgres \\\n  --schema public \\\n  --output postgres_ddl.sql\n</code></pre> <p>Example output:</p> <pre><code>CREATE TABLE IF NOT EXISTS public.rules (\n    id SERIAL PRIMARY KEY,\n    environment VARCHAR(50),\n    table_name VARCHAR(255),\n    field VARCHAR(255),\n    check_type VARCHAR(100),\n    value TEXT,\n    threshold DOUBLE PRECISION,\n    execute BOOLEAN DEFAULT TRUE,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n</code></pre>"},{"location":"sumeh/generators/generator/#mysql","title":"MySQL","text":"<p>Best for: Web applications, high-read workloads</p> <pre><code># Basic table\nsumeh sql --table rules --dialect mysql\n\n# With schema and engine\nsumeh sql --table rules --dialect mysql \\\n  --schema mydb \\\n  --engine InnoDB\n\n# Both tables with custom engine\nsumeh sql --table all --dialect mysql \\\n  --schema dq \\\n  --engine InnoDB\n</code></pre> <p>Example output:</p> <pre><code>CREATE TABLE IF NOT EXISTS mydb.rules (\n    id BIGINT AUTO_INCREMENT PRIMARY KEY,\n    environment VARCHAR(50),\n    table_name VARCHAR(255),\n    field VARCHAR(255),\n    check_type VARCHAR(100),\n    value TEXT,\n    threshold DOUBLE,\n    execute BOOLEAN DEFAULT TRUE,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;\n</code></pre>"},{"location":"sumeh/generators/generator/#bigquery","title":"BigQuery","text":"<p>Best for: Data warehousing, serverless analytics, petabyte-scale</p> <pre><code># Basic table (requires dataset)\nsumeh sql --table rules --dialect bigquery --schema mydataset\n\n# With partitioning\nsumeh sql --table rules --dialect bigquery \\\n  --schema dq \\\n  --partition-by \"DATE(created_at)\"\n\n# With clustering\nsumeh sql --table rules --dialect bigquery \\\n  --schema dq \\\n  --cluster-by table_name environment\n\n# Full optimization\nsumeh sql --table all --dialect bigquery \\\n  --schema prod_dq \\\n  --partition-by \"DATE(created_at)\" \\\n  --cluster-by table_name check_type\n</code></pre> <p>Example output:</p> <pre><code>CREATE TABLE `mydataset.rules` (\n  id INT64,\n  environment STRING,\n  table_name STRING,\n  field STRING,\n  check_type STRING,\n  value STRING,\n  threshold FLOAT64,\n  execute BOOL,\n  created_at TIMESTAMP,\n  updated_at TIMESTAMP\n)\nPARTITION BY DATE(created_at)\nCLUSTER BY table_name, environment;\n</code></pre>"},{"location":"sumeh/generators/generator/#databricks","title":"Databricks","text":"<p>Best for: Lakehouse, Delta Lake, Spark workloads</p> <pre><code># Basic table\nsumeh sql --table rules --dialect databricks\n\n# With schema\nsumeh sql --table rules --dialect databricks --schema default\n\n# With Unity Catalog\nsumeh sql --table rules --dialect databricks \\\n  --catalog prod \\\n  --schema dq\n\n# With partitioning\nsumeh sql --table rules --dialect databricks \\\n  --schema default \\\n  --partition-by environment\n\n# With clustering (Z-ordering)\nsumeh sql --table rules --dialect databricks \\\n  --schema default \\\n  --cluster-by table_name\n\n# External table\nsumeh sql --table rules --dialect databricks \\\n  --schema default \\\n  --location \"s3://my-bucket/rules/\"\n\n# Full features\nsumeh sql --table all --dialect databricks \\\n  --catalog prod \\\n  --schema dq \\\n  --partition-by environment \\\n  --cluster-by \"table_name, check_type\" \\\n  --location \"s3://prod-data/dq/\"\n</code></pre> <p>Example output:</p> <pre><code>CREATE TABLE IF NOT EXISTS `prod`.`dq`.`rules` (\n  `id` BIGINT NOT NULL,\n  `environment` STRING,\n  `table_name` STRING,\n  `field` STRING,\n  `check_type` STRING,\n  `value` STRING,\n  `threshold` DOUBLE,\n  `execute` BOOLEAN,\n  `created_at` TIMESTAMP,\n  `updated_at` TIMESTAMP\n)\nUSING DELTA\nPARTITIONED BY (environment)\nCLUSTER BY (table_name, check_type)\nLOCATION 's3://prod-data/dq/';\n</code></pre>"},{"location":"sumeh/generators/generator/#duckdb","title":"DuckDB","text":"<p>Best for: Embedded analytics, OLAP queries, fast CSV/Parquet processing</p> <pre><code># Basic table\nsumeh sql --table rules --dialect duckdb\n\n# With schema\nsumeh sql --table rules --dialect duckdb --schema main\n\n# Both tables\nsumeh sql --table all --dialect duckdb --schema main\n\n# Save and execute\nsumeh sql --table all --dialect duckdb \\\n  --schema main \\\n  --output duckdb_schema.sql\n\nduckdb mydb.duckdb &lt; duckdb_schema.sql\n</code></pre> <p>Example output:</p> <pre><code>CREATE TABLE IF NOT EXISTS main.rules (\n    id INTEGER PRIMARY KEY,\n    environment VARCHAR,\n    table_name VARCHAR,\n    field VARCHAR,\n    check_type VARCHAR,\n    value VARCHAR,\n    threshold DOUBLE,\n    execute BOOLEAN DEFAULT TRUE,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n</code></pre>"},{"location":"sumeh/generators/generator/#athena","title":"Athena","text":"<p>Best for: Serverless queries on S3, Presto/Trino SQL</p> <pre><code># Basic external table (requires location)\nsumeh sql --table rules --dialect athena \\\n  --schema default \\\n  --location \"s3://my-bucket/rules/\" \\\n  --format PARQUET\n\n# With partitioning\nsumeh sql --table rules --dialect athena \\\n  --schema dq \\\n  --location \"s3://data/rules/\" \\\n  --format PARQUET \\\n  --partition-by environment\n\n# Both tables with ORC format\nsumeh sql --table all --dialect athena \\\n  --schema prod_dq \\\n  --location \"s3://prod/dq/\" \\\n  --format ORC\n</code></pre> <p>Example output:</p> <pre><code>CREATE EXTERNAL TABLE IF NOT EXISTS default.rules (\n    id BIGINT,\n    environment STRING,\n    table_name STRING,\n    field STRING,\n    check_type STRING,\n    value STRING,\n    threshold DOUBLE,\n    execute BOOLEAN,\n    created_at TIMESTAMP,\n    updated_at TIMESTAMP\n)\nSTORED AS PARQUET\nLOCATION 's3://my-bucket/rules/';\n</code></pre>"},{"location":"sumeh/generators/generator/#snowflake","title":"Snowflake","text":"<p>Best for: Cloud data warehouse, separation of compute/storage</p> <pre><code># Basic table\nsumeh sql --table rules --dialect snowflake\n\n# With schema\nsumeh sql --table rules --dialect snowflake --schema PUBLIC\n\n# With database and schema\nsumeh sql --table rules --dialect snowflake \\\n  --database PROD_DB \\\n  --schema DQ\n\n# Both tables\nsumeh sql --table all --dialect snowflake \\\n  --database ANALYTICS \\\n  --schema DATA_QUALITY\n</code></pre> <p>Example output:</p> <pre><code>CREATE TABLE IF NOT EXISTS PROD_DB.DQ.RULES (\n    ID NUMBER AUTOINCREMENT PRIMARY KEY,\n    ENVIRONMENT VARCHAR(50),\n    TABLE_NAME VARCHAR(255),\n    FIELD VARCHAR(255),\n    CHECK_TYPE VARCHAR(100),\n    VALUE TEXT,\n    THRESHOLD FLOAT,\n    EXECUTE BOOLEAN DEFAULT TRUE,\n    CREATED_AT TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP(),\n    UPDATED_AT TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP()\n);\n</code></pre>"},{"location":"sumeh/generators/generator/#redshift","title":"Redshift","text":"<p>Best for: Data warehousing, columnar storage, AWS ecosystem</p> <pre><code># Basic table\nsumeh sql --table rules --dialect redshift\n\n# With schema\nsumeh sql --table rules --dialect redshift --schema public\n\n# With distribution key\nsumeh sql --table rules --dialect redshift \\\n  --schema dq \\\n  --distkey table_name\n\n# With sort key\nsumeh sql --table rules --dialect redshift \\\n  --schema dq \\\n  --sortkey created_at\n\n# With both optimizations\nsumeh sql --table all --dialect redshift \\\n  --schema prod_dq \\\n  --distkey table_name \\\n  --sortkey created_at updated_at\n</code></pre> <p>Example output:</p> <pre><code>CREATE TABLE IF NOT EXISTS prod_dq.rules (\n    id BIGINT IDENTITY(1,1) PRIMARY KEY,\n    environment VARCHAR(50),\n    table_name VARCHAR(255),\n    field VARCHAR(255),\n    check_type VARCHAR(100),\n    value VARCHAR(MAX),\n    threshold DOUBLE PRECISION,\n    execute BOOLEAN DEFAULT TRUE,\n    created_at TIMESTAMP DEFAULT GETDATE(),\n    updated_at TIMESTAMP DEFAULT GETDATE()\n)\nDISTKEY(table_name)\nSORTKEY(created_at, updated_at);\n</code></pre>"},{"location":"sumeh/generators/generator/#sqlite","title":"SQLite","text":"<p>Best for: Embedded databases, local development, testing</p> <pre><code># Basic table\nsumeh sql --table rules --dialect sqlite\n\n# Both tables\nsumeh sql --table all --dialect sqlite\n\n# Save and execute\nsumeh sql --table all --dialect sqlite --output schema.sql\nsqlite3 mydb.sqlite &lt; schema.sql\n</code></pre> <p>Example output:</p> <pre><code>CREATE TABLE IF NOT EXISTS rules (\n    id INTEGER PRIMARY KEY AUTOINCREMENT,\n    environment TEXT,\n    table_name TEXT,\n    field TEXT,\n    check_type TEXT,\n    value TEXT,\n    threshold REAL,\n    execute INTEGER DEFAULT 1,\n    created_at TEXT DEFAULT (datetime('now')),\n    updated_at TEXT DEFAULT (datetime('now'))\n);\n</code></pre>"},{"location":"sumeh/generators/generator/#comparison-table","title":"Comparison Table","text":"Dialect Auto-increment Schema Support Partitioning Clustering External Tables PostgreSQL SERIAL \u2705 \u274c \u274c \u274c MySQL AUTO_INCREMENT \u2705 \u274c \u274c \u274c BigQuery \u274c \u2705 \u2705 \u2705 \u274c Databricks \u274c \u2705 \u2705 \u2705 \u2705 DuckDB INTEGER \u2705 \u274c \u274c \u274c Athena \u274c \u2705 \u2705 \u274c \u2705 Snowflake AUTOINCREMENT \u2705 \u274c \u274c \u2705 Redshift IDENTITY \u2705 \u274c \u2705 (SORTKEY) \u2705 SQLite AUTOINCREMENT \u274c \u274c \u274c \u274c"},{"location":"sumeh/generators/generator/#common-workflows","title":"Common Workflows","text":""},{"location":"sumeh/generators/generator/#1-local-development","title":"1. Local Development","text":"<pre><code># SQLite for quick testing\nsumeh sql --table all --dialect sqlite --output local_schema.sql\nsqlite3 dev.sqlite &lt; local_schema.sql\n</code></pre>"},{"location":"sumeh/generators/generator/#2-cloud-data-warehouse","title":"2. Cloud Data Warehouse","text":"<pre><code># BigQuery\nsumeh sql --table all --dialect bigquery \\\n  --schema dq \\\n  --partition-by \"DATE(created_at)\" \\\n  --cluster-by table_name environment \\\n  --output bigquery_schema.sql\n\n# Execute in BigQuery console or bq CLI\nbq query &lt; bigquery_schema.sql\n</code></pre>"},{"location":"sumeh/generators/generator/#3-lakehouse-platform","title":"3. Lakehouse Platform","text":"<pre><code># Databricks\nsumeh sql --table all --dialect databricks \\\n  --catalog prod \\\n  --schema data_quality \\\n  --partition-by environment \\\n  --cluster-by table_name \\\n  --output databricks_schema.sql\n\n# Execute in Databricks SQL\n</code></pre>"},{"location":"sumeh/generators/generator/#4-multi-environment-setup","title":"4. Multi-Environment Setup","text":"<pre><code># Development (SQLite)\nsumeh sql --table all --dialect sqlite &gt; dev_schema.sql\n\n# Staging (PostgreSQL)\nsumeh sql --table all --dialect postgres --schema staging_dq &gt; staging_schema.sql\n\n# Production (BigQuery)\nsumeh sql --table all --dialect bigquery \\\n  --schema prod_dq \\\n  --partition-by \"DATE(created_at)\" \\\n  --cluster-by table_name \\\n  &gt; prod_schema.sql\n</code></pre>"},{"location":"sumeh/generators/generator/#sumeh.generators.generator","title":"sumeh.generators.generator","text":"<p>Core SQL DDL generator for sumeh tables.</p>"},{"location":"sumeh/generators/generator/#sumeh.generators.generator.SQLGenerator","title":"SQLGenerator","text":"<p>Generates DDL statements for sumeh tables across different SQL dialects.</p>"},{"location":"sumeh/generators/generator/#sumeh.generators.generator.SQLGenerator.generate","title":"generate  <code>classmethod</code>","text":"<pre><code>generate(table: str, dialect: str, schema: str = None, **kwargs) -&gt; str\n</code></pre> <p>Generate DDL for a specific table and dialect.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>str</code> <p>Table name ('rules', 'schema_registry', or 'all')</p> required <code>dialect</code> <code>str</code> <p>SQL dialect (postgres, mysql, bigquery, etc.)</p> required <code>schema</code> <code>str</code> <p>Optional schema/dataset name</p> <code>None</code> <code>**kwargs</code> <p>Additional dialect-specific options</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>DDL statement(s) as string</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If table or dialect is invalid</p>"},{"location":"sumeh/generators/generator/#sumeh.generators.generator.SQLGenerator.list_dialects","title":"list_dialects  <code>classmethod</code>","text":"<pre><code>list_dialects() -&gt; List[str]\n</code></pre> <p>Return list of supported SQL dialects.</p>"},{"location":"sumeh/generators/generator/#sumeh.generators.generator.SQLGenerator.list_tables","title":"list_tables  <code>classmethod</code>","text":"<pre><code>list_tables() -&gt; List[str]\n</code></pre> <p>Return list of available tables.</p>"},{"location":"sumeh/generators/dialects/athena/","title":"athena","text":""},{"location":"sumeh/generators/dialects/athena/#sumeh.generators.dialects.athena","title":"sumeh.generators.dialects.athena","text":"<p>AWS Athena dialect for DDL generation.</p>"},{"location":"sumeh/generators/dialects/athena/#sumeh.generators.dialects.athena.AthenaDialect","title":"AthenaDialect","text":"<p>               Bases: <code>BaseDialect</code></p> <p>AWS Athena SQL dialect for DDL generation.</p>"},{"location":"sumeh/generators/dialects/athena/#sumeh.generators.dialects.athena.AthenaDialect.generate_ddl","title":"generate_ddl","text":"<pre><code>generate_ddl(table_name: str, columns, schema: str = None, **kwargs) -&gt; str\n</code></pre> <p>Override to handle Athena-specific syntax.</p>"},{"location":"sumeh/generators/dialects/athena/#sumeh.generators.dialects.athena.AthenaDialect.map_type","title":"map_type","text":"<pre><code>map_type(col_def: Dict[str, Any]) -&gt; str\n</code></pre> <p>Map generic type to Athena type.</p>"},{"location":"sumeh/generators/dialects/base/","title":"base","text":""},{"location":"sumeh/generators/dialects/base/#sumeh.generators.dialects.base","title":"sumeh.generators.dialects.base","text":"<p>Base dialect class for SQL DDL generation.</p>"},{"location":"sumeh/generators/dialects/base/#sumeh.generators.dialects.base.BaseDialect","title":"BaseDialect","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for SQL dialect implementations.</p>"},{"location":"sumeh/generators/dialects/base/#sumeh.generators.dialects.base.BaseDialect.format_default","title":"format_default  <code>abstractmethod</code>","text":"<pre><code>format_default(col_def: Dict[str, Any]) -&gt; str\n</code></pre> <p>Format default value for dialect.</p>"},{"location":"sumeh/generators/dialects/base/#sumeh.generators.dialects.base.BaseDialect.generate_ddl","title":"generate_ddl","text":"<pre><code>generate_ddl(table_name: str, columns: List[Dict[str, Any]], schema: str = None, **kwargs) -&gt; str\n</code></pre> <p>Generate complete DDL statement.</p> <p>Parameters:</p> Name Type Description Default <code>table_name</code> <code>str</code> <p>Name of the table</p> required <code>columns</code> <code>List[Dict[str, Any]]</code> <p>List of column definitions</p> required <code>schema</code> <code>str</code> <p>Optional schema/dataset name</p> <code>None</code> <code>**kwargs</code> <p>Dialect-specific options</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>Complete DDL statement</p>"},{"location":"sumeh/generators/dialects/base/#sumeh.generators.dialects.base.BaseDialect.map_type","title":"map_type  <code>abstractmethod</code>","text":"<pre><code>map_type(col_def: Dict[str, Any]) -&gt; str\n</code></pre> <p>Map generic type to dialect-specific type.</p>"},{"location":"sumeh/generators/dialects/bigquery/","title":"bigquery","text":""},{"location":"sumeh/generators/dialects/bigquery/#sumeh.generators.dialects.bigquery","title":"sumeh.generators.dialects.bigquery","text":"<p>BigQuery dialect for DDL generation.</p>"},{"location":"sumeh/generators/dialects/bigquery/#sumeh.generators.dialects.bigquery.BigQueryDialect","title":"BigQueryDialect","text":"<p>               Bases: <code>BaseDialect</code></p> <p>BigQuery-specific DDL generation.</p>"},{"location":"sumeh/generators/dialects/bigquery/#sumeh.generators.dialects.bigquery.BigQueryDialect.format_default","title":"format_default","text":"<pre><code>format_default(col_def: Dict[str, Any]) -&gt; str\n</code></pre> <p>Format default value for BigQuery.</p>"},{"location":"sumeh/generators/dialects/bigquery/#sumeh.generators.dialects.bigquery.BigQueryDialect.generate_ddl","title":"generate_ddl","text":"<pre><code>generate_ddl(table_name: str, columns, schema: str = None, **kwargs) -&gt; str\n</code></pre> <p>Override to handle BigQuery syntax.</p>"},{"location":"sumeh/generators/dialects/bigquery/#sumeh.generators.dialects.bigquery.BigQueryDialect.map_type","title":"map_type","text":"<pre><code>map_type(col_def: Dict[str, Any]) -&gt; str\n</code></pre> <p>Map generic type to BigQuery type.</p>"},{"location":"sumeh/generators/dialects/databricks/","title":"databricks","text":""},{"location":"sumeh/generators/dialects/databricks/#sumeh.generators.dialects.databricks","title":"sumeh.generators.dialects.databricks","text":"<p>Databricks dialect for DDL generation.</p>"},{"location":"sumeh/generators/dialects/databricks/#sumeh.generators.dialects.databricks.DatabricksDialect","title":"DatabricksDialect","text":"<p>               Bases: <code>BaseDialect</code></p> <p>Databricks-specific DDL generation with Delta Lake support.</p> Features <ul> <li>Delta Lake format (USING DELTA)</li> <li>Unity Catalog support (catalog.schema.table)</li> <li>Partitioning (PARTITIONED BY)</li> <li>Clustering/Z-Ordering (CLUSTER BY)</li> <li>External tables (LOCATION)</li> <li>Table properties (TBLPROPERTIES)</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from sumeh.generators import SQLGenerator\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Basic table\n&gt;&gt;&gt; ddl = SQLGenerator.generate(\"rules\", \"databricks\")\n&gt;&gt;&gt;\n&gt;&gt;&gt; # With Unity Catalog\n&gt;&gt;&gt; ddl = SQLGenerator.generate(\n...     \"rules\",\n...     \"databricks\",\n...     catalog=\"prod\",\n...     schema=\"dq\"\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # With partitioning and clustering\n&gt;&gt;&gt; ddl = SQLGenerator.generate(\n...     \"rules\",\n...     \"databricks\",\n...     schema=\"default\",\n...     partition_by=\"environment\",\n...     cluster_by=[\"table_name\", \"check_type\"]\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # External table\n&gt;&gt;&gt; ddl = SQLGenerator.generate(\n...     \"rules\",\n...     \"databricks\",\n...     schema=\"default\",\n...     location=\"s3://my-bucket/rules/\"\n... )\n</code></pre>"},{"location":"sumeh/generators/dialects/databricks/#sumeh.generators.dialects.databricks.DatabricksDialect.format_default","title":"format_default","text":"<pre><code>format_default(col_def: Dict[str, Any]) -&gt; str\n</code></pre> <p>Format default value for Databricks.</p> <p>Detects SQL functions automatically (e.g., CURRENT_TIMESTAMP, NOW(), etc).</p> <p>Parameters:</p> Name Type Description Default <code>col_def</code> <code>Dict[str, Any]</code> <p>Column definition with optional 'default' key</p> required <p>Returns:</p> Type Description <code>str</code> <p>Formatted DEFAULT clause or empty string</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; dialect = DatabricksDialect()\n&gt;&gt;&gt; dialect.format_default({\"default\": \"active\"})\n\"DEFAULT 'active'\"\n&gt;&gt;&gt; dialect.format_default({\"default\": \"current_timestamp\"})\n'DEFAULT CURRENT_TIMESTAMP'\n&gt;&gt;&gt; dialect.format_default({\"default\": True})\n'DEFAULT TRUE'\n</code></pre>"},{"location":"sumeh/generators/dialects/databricks/#sumeh.generators.dialects.databricks.DatabricksDialect.generate_ddl","title":"generate_ddl","text":"<pre><code>generate_ddl(table_name: str, columns, schema: str = None, **kwargs) -&gt; str\n</code></pre> <p>Generate Databricks DDL with Delta Lake features.</p> <p>Automatically enables allowColumnDefaults feature if any column has defaults.</p> <p>Parameters:</p> Name Type Description Default <code>table_name</code> <code>str</code> <p>Name of the table</p> required <code>columns</code> <p>List of column definitions</p> required <code>schema</code> <code>str</code> <p>Schema name (optional)</p> <code>None</code> <code>**kwargs</code> <p>Additional options: - catalog (str): Unity Catalog name - partition_by (str|list): Column(s) to partition by - cluster_by (str|list): Column(s) to cluster by (Z-ordering) - location (str): S3/DBFS path for external table - table_comment (str): Table description - properties (dict): Custom table properties</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>Complete CREATE TABLE DDL statement</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; dialect = DatabricksDialect()\n&gt;&gt;&gt; columns = [\n...     {\"name\": \"id\", \"type\": \"integer\", \"nullable\": False},\n...     {\"name\": \"created_at\", \"type\": \"timestamp\", \"default\": \"current_timestamp\"}\n... ]\n&gt;&gt;&gt; ddl = dialect.generate_ddl(\"users\", columns)\n</code></pre>"},{"location":"sumeh/generators/dialects/databricks/#sumeh.generators.dialects.databricks.DatabricksDialect.map_type","title":"map_type","text":"<pre><code>map_type(col_def: Dict[str, Any]) -&gt; str\n</code></pre> <p>Map generic type to Databricks/Spark SQL type.</p> Type mappings <ul> <li>integer \u2192 BIGINT</li> <li>varchar \u2192 STRING</li> <li>text \u2192 STRING</li> <li>float \u2192 DOUBLE</li> <li>boolean \u2192 BOOLEAN</li> <li>timestamp \u2192 TIMESTAMP</li> <li>date \u2192 DATE</li> </ul> <p>Parameters:</p> Name Type Description Default <code>col_def</code> <code>Dict[str, Any]</code> <p>Column definition with 'type' key</p> required <p>Returns:</p> Type Description <code>str</code> <p>Databricks SQL type as string</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; dialect = DatabricksDialect()\n&gt;&gt;&gt; dialect.map_type({\"type\": \"integer\"})\n'BIGINT'\n&gt;&gt;&gt; dialect.map_type({\"type\": \"varchar\"})\n'STRING'\n</code></pre>"},{"location":"sumeh/generators/dialects/duckdb/","title":"duckdb","text":""},{"location":"sumeh/generators/dialects/duckdb/#sumeh.generators.dialects.duckdb","title":"sumeh.generators.dialects.duckdb","text":"<p>DuckDB dialect for DDL generation.</p>"},{"location":"sumeh/generators/dialects/duckdb/#sumeh.generators.dialects.duckdb.DuckDBDialect","title":"DuckDBDialect","text":"<p>               Bases: <code>BaseDialect</code></p> <p>DuckDB-specific DDL generation.</p>"},{"location":"sumeh/generators/dialects/duckdb/#sumeh.generators.dialects.duckdb.DuckDBDialect.format_default","title":"format_default","text":"<pre><code>format_default(col_def: Dict[str, Any]) -&gt; str\n</code></pre> <p>Format default value for DuckDB.</p>"},{"location":"sumeh/generators/dialects/duckdb/#sumeh.generators.dialects.duckdb.DuckDBDialect.map_type","title":"map_type","text":"<pre><code>map_type(col_def: Dict[str, Any]) -&gt; str\n</code></pre> <p>Override to handle auto_increment.</p>"},{"location":"sumeh/generators/dialects/mysql/","title":"mysql","text":""},{"location":"sumeh/generators/dialects/mysql/#sumeh.generators.dialects.mysql","title":"sumeh.generators.dialects.mysql","text":"<p>MySQL dialect for DDL generation.</p>"},{"location":"sumeh/generators/dialects/mysql/#sumeh.generators.dialects.mysql.MySQLDialect","title":"MySQLDialect","text":"<p>               Bases: <code>BaseDialect</code></p> <p>MySQL-specific DDL generation.</p>"},{"location":"sumeh/generators/dialects/mysql/#sumeh.generators.dialects.mysql.MySQLDialect.format_default","title":"format_default","text":"<pre><code>format_default(col_def: Dict[str, Any]) -&gt; str\n</code></pre> <p>Format default value for MySQL.</p>"},{"location":"sumeh/generators/dialects/mysql/#sumeh.generators.dialects.mysql.MySQLDialect.map_type","title":"map_type","text":"<pre><code>map_type(col_def: Dict[str, Any]) -&gt; str\n</code></pre> <p>Map generic type to MySQL type.</p>"},{"location":"sumeh/generators/dialects/postgres/","title":"postgres","text":""},{"location":"sumeh/generators/dialects/postgres/#sumeh.generators.dialects.postgres","title":"sumeh.generators.dialects.postgres","text":"<p>PostgreSQL dialect for DDL generation.</p>"},{"location":"sumeh/generators/dialects/postgres/#sumeh.generators.dialects.postgres.PostgresDialect","title":"PostgresDialect","text":"<p>               Bases: <code>BaseDialect</code></p> <p>PostgreSQL-specific DDL generation.</p>"},{"location":"sumeh/generators/dialects/postgres/#sumeh.generators.dialects.postgres.PostgresDialect.format_default","title":"format_default","text":"<pre><code>format_default(col_def: Dict[str, Any]) -&gt; str\n</code></pre> <p>Format default value for PostgreSQL.</p>"},{"location":"sumeh/generators/dialects/postgres/#sumeh.generators.dialects.postgres.PostgresDialect.map_type","title":"map_type","text":"<pre><code>map_type(col_def: Dict[str, Any]) -&gt; str\n</code></pre> <p>Override to handle SERIAL for auto_increment.</p>"},{"location":"sumeh/generators/dialects/redshift/","title":"redshift","text":""},{"location":"sumeh/generators/dialects/redshift/#sumeh.generators.dialects.redshift","title":"sumeh.generators.dialects.redshift","text":"<p>AWS Redshift dialect for DDL generation.</p>"},{"location":"sumeh/generators/dialects/redshift/#sumeh.generators.dialects.redshift.RedshiftDialect","title":"RedshiftDialect","text":"<p>               Bases: <code>BaseDialect</code></p> <p>AWS Redshift-specific DDL generation.</p>"},{"location":"sumeh/generators/dialects/redshift/#sumeh.generators.dialects.redshift.RedshiftDialect.format_default","title":"format_default","text":"<pre><code>format_default(col_def: Dict[str, Any]) -&gt; str\n</code></pre> <p>Format default value for Redshift.</p>"},{"location":"sumeh/generators/dialects/redshift/#sumeh.generators.dialects.redshift.RedshiftDialect.generate_ddl","title":"generate_ddl","text":"<pre><code>generate_ddl(table_name: str, columns, schema: str = None, **kwargs) -&gt; str\n</code></pre> <p>Override to add Redshift-specific options.</p>"},{"location":"sumeh/generators/dialects/redshift/#sumeh.generators.dialects.redshift.RedshiftDialect.map_type","title":"map_type","text":"<pre><code>map_type(col_def: Dict[str, Any]) -&gt; str\n</code></pre> <p>Map generic type to Redshift type.</p>"},{"location":"sumeh/generators/dialects/snowflake/","title":"snowflake","text":""},{"location":"sumeh/generators/dialects/snowflake/#sumeh.generators.dialects.snowflake","title":"sumeh.generators.dialects.snowflake","text":"<p>Snowflake dialect for DDL generation.</p>"},{"location":"sumeh/generators/dialects/snowflake/#sumeh.generators.dialects.snowflake.SnowflakeDialect","title":"SnowflakeDialect","text":"<p>               Bases: <code>BaseDialect</code></p> <p>Snowflake-specific DDL generation.</p>"},{"location":"sumeh/generators/dialects/snowflake/#sumeh.generators.dialects.snowflake.SnowflakeDialect.format_default","title":"format_default","text":"<pre><code>format_default(col_def: Dict[str, Any]) -&gt; str\n</code></pre> <p>Format default value for Snowflake.</p>"},{"location":"sumeh/generators/dialects/snowflake/#sumeh.generators.dialects.snowflake.SnowflakeDialect.map_type","title":"map_type","text":"<pre><code>map_type(col_def: Dict[str, Any]) -&gt; str\n</code></pre> <p>Map generic type to Snowflake type.</p>"},{"location":"sumeh/generators/dialects/sqlite/","title":"sqlite","text":""},{"location":"sumeh/generators/dialects/sqlite/#sumeh.generators.dialects.sqlite","title":"sumeh.generators.dialects.sqlite","text":"<p>SQLite dialect for DDL generation.</p>"},{"location":"sumeh/generators/dialects/sqlite/#sumeh.generators.dialects.sqlite.SQLiteDialect","title":"SQLiteDialect","text":"<p>               Bases: <code>BaseDialect</code></p> <p>SQLite-specific DDL generation.</p>"},{"location":"sumeh/generators/dialects/sqlite/#sumeh.generators.dialects.sqlite.SQLiteDialect.format_default","title":"format_default","text":"<pre><code>format_default(col_def: Dict[str, Any]) -&gt; str\n</code></pre> <p>Format default value for SQLite.</p>"},{"location":"sumeh/generators/dialects/sqlite/#sumeh.generators.dialects.sqlite.SQLiteDialect.map_type","title":"map_type","text":"<pre><code>map_type(col_def: Dict[str, Any]) -&gt; str\n</code></pre> <p>Map generic type to SQLite type.</p>"}]}