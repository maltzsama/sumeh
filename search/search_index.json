{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"home","text":""},{"location":"#sumeh-dq","title":"Sumeh DQ","text":"<p>Sumeh is a unified data quality validation framework supporting multiple backends (PySpark, Dask, Polars, DuckDB, Pandas, BigQuery) with centralized rule configuration and schema validation.</p>"},{"location":"#installation","title":"\ud83d\ude80 Installation","text":"<pre><code># Base installation\npip install sumeh\n\n# With specific engine support\npip install sumeh[pyspark]     # PySpark support\npip install sumeh[aws]         # S3 + Pandas support\npip install sumeh[mysql]       # MySQL support\npip install sumeh[postgresql]  # PostgreSQL support\npip install sumeh[bigquery]    # BigQuery support\npip install sumeh[dashboard]   # Streamlit dashboard\n\n# All extras\npip install sumeh[dev,aws,mysql,postgresql,bigquery,dashboard]\n</code></pre> <p>Prerequisites: - Python 3.10+ - One or more of: <code>pyspark</code>, <code>dask[dataframe]</code>, <code>polars</code>, <code>duckdb</code>, <code>pandas</code></p>"},{"location":"#core-api","title":"\ud83d\udd0d Core API","text":"<p>Sumeh uses a dispatcher pattern for clean engine-specific access:</p> <pre><code>from sumeh import validate, summarize, get_rules_config\n\n# Load rules from various sources\nrules = get_rules_config.csv(\"rules.csv\")\nrules = get_rules_config.s3(\"s3://bucket/rules.csv\")\nrules = get_rules_config.mysql(host=\"localhost\", table=\"rules\")\n\n# Engine-specific validation\nresult = validate.pandas(df, rules)\nresult = validate.pyspark(spark, df, rules)\nresult = validate.duckdb(conn, df_rel, rules)\n\n# Generate summary reports\nsummary = summarize.pandas(result, rules, total_rows=len(df))\n</code></pre>"},{"location":"#supported-engines","title":"\u2699\ufe0f Supported Engines","text":"<p>All engines implement the same <code>validate()</code> + <code>summarize()</code> interface:</p> Engine Module Status Streaming Support Pandas <code>sumeh.engines.pandas_engine</code> \u2705 Fully implemented \u274c Batch only PySpark <code>sumeh.engines.pyspark_engine</code> \u2705 Fully implemented \u2705 Structured Streaming Dask <code>sumeh.engines.dask_engine</code> \u2705 Fully implemented \u274c Batch only Polars <code>sumeh.engines.polars_engine</code> \u2705 Fully implemented \u274c Batch only DuckDB <code>sumeh.engines.duckdb_engine</code> \u2705 Fully implemented \u274c Batch only BigQuery <code>sumeh.engines.bigquery_engine</code> \u2705 Fully implemented \u274c Batch only"},{"location":"#configuration-sources","title":"\ud83c\udfd7 Configuration Sources","text":"<p>Sumeh supports loading rules from multiple sources using the dispatcher pattern:</p>"},{"location":"#csv-files","title":"CSV Files","text":"<pre><code>from sumeh import get_rules_config\n\n# Local CSV\nrules = get_rules_config.csv(\"rules.csv\", delimiter=\";\")\n\n# S3 CSV\nrules = get_rules_config.s3(\"s3://bucket/path/rules.csv\", delimiter=\";\")\n</code></pre>"},{"location":"#database-sources","title":"Database Sources","text":"<pre><code># MySQL\nrules = get_rules_config.mysql(\n    host=\"localhost\",\n    user=\"root\", \n    password=\"secret\",\n    database=\"mydb\",\n    table=\"rules\"\n)\n\n# PostgreSQL\nrules = get_rules_config.postgresql(\n    host=\"localhost\",\n    user=\"postgres\",\n    password=\"secret\", \n    database=\"mydb\",\n    schema=\"public\",\n    table=\"rules\"\n)\n\n# BigQuery\nrules = get_rules_config.bigquery(\n    project_id=\"my-project\",\n    dataset_id=\"my-dataset\", \n    table_id=\"rules\"\n)\n\n# DuckDB\nimport duckdb\nconn = duckdb.connect(\"my.db\")\nrules = get_rules_config.duckdb(conn=conn, table=\"rules\")\n</code></pre>"},{"location":"#cloud-data-catalogs","title":"Cloud Data Catalogs","text":"<pre><code># AWS Glue\nfrom awsglue.context import GlueContext\nrules = get_rules_config.glue(\n    glue_context=glue_context,\n    database_name=\"my_database\",\n    table_name=\"rules\"\n)\n\n# Databricks Unity Catalog\nrules = get_rules_config.databricks(\n    spark=spark,\n    catalog=\"main\",\n    schema=\"default\", \n    table=\"rules\"\n)\n</code></pre>"},{"location":"#using-existing-connections","title":"Using Existing Connections","text":"<pre><code># Reuse MySQL connection\nimport mysql.connector\nconn = mysql.connector.connect(host=\"localhost\", user=\"root\", password=\"secret\")\nrules = get_rules_config.mysql(conn=conn, query=\"SELECT * FROM rules WHERE active=1\")\n\n# Reuse PostgreSQL connection  \nimport psycopg2\nconn = psycopg2.connect(host=\"localhost\", user=\"postgres\", password=\"secret\")\nrules = get_rules_config.postgresql(conn=conn, query=\"SELECT * FROM public.rules\")\n</code></pre>"},{"location":"#typical-workflow","title":"\ud83c\udfc3 Typical Workflow","text":""},{"location":"#modern-dispatcher-api-recommended","title":"Modern Dispatcher API (Recommended)","text":"<pre><code>from sumeh import validate, summarize, get_rules_config\nimport polars as pl\n\n# 1) Load rules and data\nrules = get_rules_config.csv(\"rules.csv\")\ndf = pl.read_csv(\"data.csv\")\n\n# 2) Run validation (returns 3 DataFrames)\ndf_errors, violations, table_summary = validate.polars(df, rules)\n\n# 3) Generate consolidated summary\nsummary = summarize.polars(\n    validation_result=(df_errors, violations, table_summary),\n    rules=rules,\n    total_rows=len(df)\n)\nprint(summary)\n</code></pre>"},{"location":"#engine-specific-examples","title":"Engine-Specific Examples","text":"<pre><code># Pandas\ndf_errors, violations, table_sum = validate.pandas(df, rules)\nsummary = summarize.pandas((df_errors, violations, table_sum), rules, len(df))\n\n# PySpark\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\ndf_errors, violations, table_sum = validate.pyspark(spark, df, rules)\nsummary = summarize.pyspark((df_errors, violations, table_sum), rules, df.count())\n\n# DuckDB\nimport duckdb\nconn = duckdb.connect()\ndf_rel = conn.sql(\"SELECT * FROM my_table\")\ndf_errors, violations, table_sum = validate.duckdb(conn, df_rel, rules)\nsummary = summarize.duckdb((df_errors, violations, table_sum), rules, df_rel.count(\"*\").fetchone()[0])\n</code></pre>"},{"location":"#legacy-api-still-supported","title":"Legacy API (Still Supported)","text":"<pre><code>from sumeh import report\n\n# Simple one-liner using cuallee backend\nreport_df = report(df, rules, name=\"Quality Check\")\n</code></pre>"},{"location":"#rule-definition","title":"\ud83d\udccb Rule Definition","text":"<p>Sumeh uses the <code>RuleDef</code> class for type-safe rule definitions with automatic validation:</p>"},{"location":"#basic-rule-structure","title":"Basic Rule Structure","text":"<pre><code>from sumeh.core.rules import RuleDef\n\n# Create rule programmatically\nrule = RuleDef(\n    field=\"customer_id\",\n    check_type=\"is_complete\", \n    threshold=0.99,\n    value=None,\n    execute=True\n)\n\n# Or from dictionary\nrule = RuleDef.from_dict({\n    \"field\": \"customer_id\",\n    \"check_type\": \"is_complete\",\n    \"threshold\": 0.99,\n    \"value\": None,\n    \"execute\": True\n})\n</code></pre>"},{"location":"#csv-format-example","title":"CSV Format Example","text":"<pre><code>field,check_type,threshold,value,execute\ncustomer_id,is_complete,0.99,,true\nemail,has_pattern,1.0,\"^[\\w\\.-]+@[\\w\\.-]+\\.[a-zA-Z]{2,}$\",true\nage,is_between,1.0,\"[18, 120]\",true\nstatus,is_contained_in,1.0,\"['active', 'inactive', 'pending']\",true\n\"[first_name,last_name]\",are_complete,0.95,,true\n</code></pre>"},{"location":"#advanced-features","title":"Advanced Features","text":"<ul> <li>Auto-parsing: Values are automatically converted to correct types (int, float, list, date, regex)</li> <li>Multi-column rules: Use <code>[col1,col2]</code> syntax for composite validations</li> <li>Metadata enrichment: Category and level are auto-populated from rule registry</li> <li>Engine compatibility: Automatic validation against supported engines</li> </ul>"},{"location":"#supported-validation-rules","title":"\ud83d\udcca Supported Validation Rules","text":"<p>Sumeh supports 60+ validation rules organized by level and category. All rules are defined in the manifest.json registry.</p>"},{"location":"#row-level-validations","title":"Row-Level Validations","text":""},{"location":"#completeness","title":"Completeness","text":"Rule Description Example <code>is_complete</code> Column has no null values <code>{\"field\": \"email\", \"check_type\": \"is_complete\"}</code> <code>are_complete</code> Multiple columns have no nulls <code>{\"field\": \"[name,email]\", \"check_type\": \"are_complete\"}</code>"},{"location":"#uniqueness","title":"Uniqueness","text":"Rule Description Example <code>is_unique</code> Column values are unique <code>{\"field\": \"user_id\", \"check_type\": \"is_unique\"}</code> <code>are_unique</code> Column combination is unique <code>{\"field\": \"[email,phone]\", \"check_type\": \"are_unique\"}</code> <code>is_primary_key</code> Alias for <code>is_unique</code> <code>{\"field\": \"id\", \"check_type\": \"is_primary_key\"}</code>"},{"location":"#comparison-range","title":"Comparison &amp; Range","text":"Rule Description Example <code>is_between</code> Value within range <code>{\"field\": \"age\", \"check_type\": \"is_between\", \"value\": [18, 65]}</code> <code>is_greater_than</code> Value &gt; threshold <code>{\"field\": \"score\", \"check_type\": \"is_greater_than\", \"value\": 0}</code> <code>is_positive</code> Value &gt; 0 <code>{\"field\": \"amount\", \"check_type\": \"is_positive\"}</code> <code>is_in_millions</code> Value &gt;= 1,000,000 <code>{\"field\": \"revenue\", \"check_type\": \"is_in_millions\"}</code>"},{"location":"#membership-pattern","title":"Membership &amp; Pattern","text":"Rule Description Example <code>is_contained_in</code> Value in allowed list <code>{\"field\": \"status\", \"check_type\": \"is_contained_in\", \"value\": [\"active\", \"inactive\"]}</code> <code>has_pattern</code> Matches regex pattern <code>{\"field\": \"email\", \"check_type\": \"has_pattern\", \"value\": \"^[\\\\w.-]+@[\\\\w.-]+\\\\.[a-zA-Z]{2,}$\"}</code> <code>is_legit</code> Non-empty, non-whitespace <code>{\"field\": \"name\", \"check_type\": \"is_legit\"}</code>"},{"location":"#date-validations","title":"Date Validations","text":"Rule Description Example <code>is_past_date</code> Date before today <code>{\"field\": \"birth_date\", \"check_type\": \"is_past_date\"}</code> <code>is_future_date</code> Date after today <code>{\"field\": \"expiry_date\", \"check_type\": \"is_future_date\"}</code> <code>is_date_between</code> Date within range <code>{\"field\": \"created_at\", \"check_type\": \"is_date_between\", \"value\": [\"2023-01-01\", \"2023-12-31\"]}</code> <code>is_on_weekday</code> Date falls on weekday <code>{\"field\": \"transaction_date\", \"check_type\": \"is_on_weekday\"}</code> <code>validate_date_format</code> Matches date format <code>{\"field\": \"date_str\", \"check_type\": \"validate_date_format\", \"value\": \"%Y-%m-%d\"}</code>"},{"location":"#sql-custom-rules","title":"SQL Custom Rules","text":"Rule Description Example <code>satisfies</code> Custom SQL condition <code>{\"field\": \"*\", \"check_type\": \"satisfies\", \"value\": \"age &gt;= 18 AND status = 'active'\"}</code>"},{"location":"#table-level-validations","title":"Table-Level Validations","text":""},{"location":"#aggregation-checks","title":"Aggregation Checks","text":"Rule Description Example <code>has_min</code> Minimum value check <code>{\"field\": \"price\", \"check_type\": \"has_min\", \"value\": 0}</code> <code>has_max</code> Maximum value check <code>{\"field\": \"age\", \"check_type\": \"has_max\", \"value\": 120}</code> <code>has_cardinality</code> Distinct count check <code>{\"field\": \"category\", \"check_type\": \"has_cardinality\", \"value\": 10}</code> <code>has_mean</code> Average value check <code>{\"field\": \"rating\", \"check_type\": \"has_mean\", \"value\": 3.5}</code> <code>has_std</code> Standard deviation check <code>{\"field\": \"scores\", \"check_type\": \"has_std\", \"value\": 2.0}</code>"},{"location":"#schema-validation","title":"Schema Validation","text":"Rule Description Example <code>validate_schema</code> Schema structure check <code>{\"field\": \"*\", \"check_type\": \"validate_schema\", \"value\": expected_schema}</code>"},{"location":"#engine-compatibility","title":"Engine Compatibility","text":"<p>All rules support all engines: Pandas, PySpark, Dask, Polars, DuckDB, BigQuery</p>"},{"location":"#schema-validation_1","title":"\ud83d\udd0d Schema Validation","text":"<p>Sumeh provides comprehensive schema validation against registries stored in multiple data sources.</p>"},{"location":"#schema-registry-structure","title":"Schema Registry Structure","text":"<p>Create a <code>schema_registry</code> table with this structure:</p> <pre><code>CREATE TABLE schema_registry (\n    id INTEGER PRIMARY KEY,\n    environment VARCHAR(50),     -- 'prod', 'staging', 'dev'\n    source_type VARCHAR(50),     -- 'bigquery', 'mysql', etc.\n    database_name VARCHAR(100),\n    catalog_name VARCHAR(100),   -- For Databricks\n    schema_name VARCHAR(100),    -- For PostgreSQL\n    table_name VARCHAR(100),\n    field VARCHAR(100),\n    data_type VARCHAR(50),\n    nullable BOOLEAN,\n    max_length INTEGER,\n    comment TEXT,\n    created_at TIMESTAMP,\n    updated_at TIMESTAMP\n);\n</code></pre>"},{"location":"#get-schema-configuration","title":"Get Schema Configuration","text":"<pre><code>from sumeh import get_schema_config\n\n# From various sources\nschema = get_schema_config.bigquery(\n    project_id=\"my-project\",\n    dataset_id=\"my-dataset\", \n    table_id=\"users\",\n    environment=\"prod\"\n)\n\nschema = get_schema_config.mysql(\n    host=\"localhost\",\n    user=\"root\",\n    password=\"secret\",\n    database=\"mydb\",\n    table=\"users\"\n)\n\nschema = get_schema_config.csv(\n    \"schema_registry.csv\",\n    table=\"users\"\n)\n\nschema = get_schema_config.s3(\n    \"s3://bucket/schema_registry.csv\",\n    table=\"users\"\n)\n</code></pre>"},{"location":"#extract-validate-schema","title":"Extract &amp; Validate Schema","text":"<pre><code>from sumeh import extract_schema, validate_schema\nimport pandas as pd\n\n# Extract actual schema from DataFrame\ndf = pd.read_csv(\"users.csv\")\nactual_schema = extract_schema.pandas(df)\n\n# Get expected schema from registry\nexpected_schema = get_schema_config.csv(\"schema_registry.csv\", table=\"users\")\n\n# Validate\nis_valid, errors = validate_schema.pandas(df, expected_schema)\n\nif is_valid:\n    print(\"\u2705 Schema is valid!\")\nelse:\n    print(\"\u274c Schema validation failed:\")\n    for field, error in errors:\n        print(f\"  - {field}: {error}\")\n</code></pre>"},{"location":"#engine-support","title":"Engine Support","text":"<p>Schema validation works with all engines: - <code>extract_schema.pandas(df)</code> - <code>extract_schema.pyspark(df)</code> - <code>extract_schema.polars(df)</code> - <code>extract_schema.duckdb(conn, relation)</code> - <code>validate_schema.pandas(df, expected)</code> - <code>validate_schema.pyspark(df, expected)</code> - etc.</p>"},{"location":"#custom-filters","title":"Custom Filters","text":"<pre><code># Add custom WHERE conditions\nschema = get_schema_config.mysql(\n    host=\"localhost\",\n    table=\"users\",\n    query=\"environment = 'prod' AND source_type = 'mysql'\"\n)\n</code></pre>"},{"location":"#table-generators","title":"\ud83d\udee0\ufe0f Table Generators","text":"<p>Sumeh includes SQL DDL generators for creating <code>rules</code> and <code>schema_registry</code> tables across multiple database dialects:</p>"},{"location":"#generate-ddl-statements","title":"Generate DDL Statements","text":"<pre><code>from sumeh.generators import SQLGenerator\n\n# Generate rules table for PostgreSQL\nddl = SQLGenerator.generate(table=\"rules\", dialect=\"postgres\", schema=\"public\")\nprint(ddl)\n\n# Generate schema_registry table for BigQuery\nddl = SQLGenerator.generate(\n    table=\"schema_registry\", \n    dialect=\"bigquery\",\n    schema=\"my_dataset\",\n    partition_by=\"DATE(created_at)\",\n    cluster_by=[\"table_name\", \"environment\"]\n)\nprint(ddl)\n\n# Generate both tables for MySQL\nddl = SQLGenerator.generate(table=\"all\", dialect=\"mysql\", engine=\"InnoDB\")\nprint(ddl)\n</code></pre>"},{"location":"#supported-dialects","title":"Supported Dialects","text":"<ul> <li>PostgreSQL (<code>postgres</code>)</li> <li>MySQL (<code>mysql</code>) </li> <li>BigQuery (<code>bigquery</code>)</li> <li>Snowflake (<code>snowflake</code>)</li> <li>Redshift (<code>redshift</code>)</li> <li>Databricks (<code>databricks</code>)</li> <li>DuckDB (<code>duckdb</code>)</li> <li>SQLite (<code>sqlite</code>)</li> <li>Athena (<code>athena</code>)</li> <li>And more...</li> </ul>"},{"location":"#dialect-specific-features","title":"Dialect-Specific Features","text":"<pre><code># BigQuery with partitioning and clustering\nSQLGenerator.generate(\n    table=\"rules\",\n    dialect=\"bigquery\",\n    partition_by=\"DATE(created_at)\",\n    cluster_by=[\"environment\", \"table_name\"]\n)\n\n# Snowflake with clustering\nSQLGenerator.generate(\n    table=\"schema_registry\",\n    dialect=\"snowflake\",\n    cluster_by=[\"table_name\", \"field\"]\n)\n\n# Redshift with distribution and sort keys\nSQLGenerator.generate(\n    table=\"rules\",\n    dialect=\"redshift\",\n    distkey=\"table_name\",\n    sortkey=[\"created_at\", \"environment\"]\n)\n</code></pre>"},{"location":"#utility-functions","title":"Utility Functions","text":"<pre><code># List available dialects\nprint(SQLGenerator.list_dialects())\n# ['athena', 'bigquery', 'databricks', 'duckdb', 'mysql', ...]\n\n# List available tables\nprint(SQLGenerator.list_tables())\n# ['rules', 'schema_registry']\n\n# Transpile SQL between dialects\nsql = \"SELECT * FROM users WHERE created_at &gt;= CURRENT_DATE - 7\"\ntranspiled = SQLGenerator.transpile(sql, \"postgres\", \"bigquery\")\nprint(transpiled)\n</code></pre>"},{"location":"#table-schemas","title":"\ud83d\udccb Table Schemas","text":""},{"location":"#rules-table-structure","title":"Rules Table Structure","text":"<pre><code>CREATE TABLE rules (\n    id INTEGER PRIMARY KEY AUTO_INCREMENT,\n    environment VARCHAR(50) NOT NULL,\n    source_type VARCHAR(50) NOT NULL,\n    database_name VARCHAR(255) NOT NULL,\n    catalog_name VARCHAR(255),\n    schema_name VARCHAR(255),\n    table_name VARCHAR(255) NOT NULL,\n    field VARCHAR(255) NOT NULL,\n    level VARCHAR(100) NOT NULL,\n    category VARCHAR(100) NOT NULL,\n    check_type VARCHAR(100) NOT NULL,\n    value TEXT,\n    threshold FLOAT DEFAULT 1.0,\n    execute BOOLEAN DEFAULT TRUE,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    updated_at TIMESTAMP\n);\n</code></pre>"},{"location":"#schema-registry-table-structure","title":"Schema Registry Table Structure","text":"<pre><code>CREATE TABLE schema_registry (\n    id INTEGER PRIMARY KEY AUTO_INCREMENT,\n    environment VARCHAR(50) NOT NULL,\n    source_type VARCHAR(50) NOT NULL,\n    database_name VARCHAR(255) NOT NULL,\n    catalog_name VARCHAR(255),\n    schema_name VARCHAR(255),\n    table_name VARCHAR(255) NOT NULL,\n    field VARCHAR(255) NOT NULL,\n    data_type VARCHAR(100) NOT NULL,\n    nullable BOOLEAN DEFAULT TRUE,\n    max_length INTEGER,\n    comment TEXT,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    updated_at TIMESTAMP\n);\n</code></pre>"},{"location":"#architecture","title":"\ud83c\udfd7\ufe0f Architecture","text":"<p>Sumeh follows a modular, dispatcher-based architecture:</p> <pre><code>sumeh/\n\u251c\u2500\u2500 cli/                    # Command-line interface\n\u251c\u2500\u2500 core/                   # Core framework\n\u2502   \u251c\u2500\u2500 rules/             # Rule definitions &amp; registry\n\u2502   \u2502   \u251c\u2500\u2500 manifest.json  # 60+ validation rules\n\u2502   \u2502   \u251c\u2500\u2500 rule_model.py  # RuleDef class\n\u2502   \u2502   \u2514\u2500\u2500 regristry.py   # Rule registry\n\u2502   \u251c\u2500\u2500 __init__.py        # Dispatchers (validate, summarize, etc.)\n\u2502   \u251c\u2500\u2500 config.py          # Multi-source configuration\n\u2502   \u251c\u2500\u2500 report.py          # Legacy cuallee integration\n\u2502   \u2514\u2500\u2500 utils.py           # Utilities\n\u251c\u2500\u2500 engines/               # Engine implementations\n\u2502   \u251c\u2500\u2500 pandas_engine.py   # Pandas backend\n\u2502   \u251c\u2500\u2500 pyspark_engine.py  # PySpark backend  \n\u2502   \u251c\u2500\u2500 dask_engine.py     # Dask backend\n\u2502   \u251c\u2500\u2500 polars_engine.py   # Polars backend\n\u2502   \u251c\u2500\u2500 duckdb_engine.py   # DuckDB backend\n\u2502   \u2514\u2500\u2500 bigquery_engine.py # BigQuery backend (stub)\n\u251c\u2500\u2500 dash/                  # Streamlit dashboard\n\u2514\u2500\u2500 generators/            # SQL generation utilities\n</code></pre>"},{"location":"#key-components","title":"Key Components","text":"<ul> <li>Dispatchers: Clean API for engine-specific operations (<code>validate.pandas()</code>, <code>summarize.pyspark()</code>)</li> <li>RuleDef: Type-safe rule definitions with auto-validation</li> <li>Rule Registry: Centralized manifest of 60+ validation rules</li> <li>Multi-source Config: Load rules from CSV, S3, MySQL, PostgreSQL, BigQuery, etc.</li> <li>Schema Validation: Extract and validate DataFrame schemas</li> <li>Engine Abstraction: Consistent interface across all backends</li> </ul>"},{"location":"#row-level-vs-table-level-validations","title":"\ud83d\udd0d Row-Level vs Table-Level Validations","text":"<p>Sumeh supports two types of validation rules:</p>"},{"location":"#row-level-validations_1","title":"\ud83d\udd38 Row-Level Validations","text":"<p>Validate individual rows and return violating records:</p> <pre><code># Examples of row-level rules\nrow_rules = [\n    {\"field\": \"email\", \"check_type\": \"is_complete\", \"level\": \"ROW\"},\n    {\"field\": \"age\", \"check_type\": \"is_between\", \"value\": \"[18,120]\", \"level\": \"ROW\"},\n    {\"field\": \"status\", \"check_type\": \"is_contained_in\", \"value\": \"['active','inactive']\", \"level\": \"ROW\"}\n]\n\n# Returns: DataFrame with violating rows + dq_status column\ndf_errors, violations, _ = validate.pandas(df, row_rules)\n</code></pre>"},{"location":"#spark-structured-streaming-support","title":"\ud83c\udf0a Spark Structured Streaming Support","text":"<p>Row-level validations are fully compatible with Spark Structured Streaming for real-time data quality:</p> <pre><code>from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col\nfrom sumeh import validate\n\nspark = SparkSession.builder.getOrCreate()\n\n# Create streaming DataFrame\nstreaming_df = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n    .option(\"subscribe\", \"events\") \\\n    .load()\n\n# Apply row-level validation to streaming data\nrow_rules = [\n    {\"field\": \"user_id\", \"check_type\": \"is_complete\", \"level\": \"ROW\"},\n    {\"field\": \"event_type\", \"check_type\": \"is_contained_in\", \"value\": \"['click','view','purchase']\", \"level\": \"ROW\"}\n]\n\n# Validate streaming data\nvalidated_stream = validate.pyspark(spark, streaming_df, row_rules)\n\n# Write violations to output sink\nquery = validated_stream[0] \\\n    .writeStream \\\n    .outputMode(\"append\") \\\n    .format(\"console\") \\\n    .start()\n</code></pre> <p>Note: Table-level validations require complete datasets and are not compatible with streaming.</p>"},{"location":"#table-level-validations_1","title":"\ud83d\udd38 Table-Level Validations","text":"<p>Validate aggregate statistics and return summary metrics:</p> <pre><code># Examples of table-level rules\ntable_rules = [\n    {\"field\": \"salary\", \"check_type\": \"has_mean\", \"value\": 50000, \"level\": \"TABLE\"},\n    {\"field\": \"department\", \"check_type\": \"has_cardinality\", \"value\": 5, \"level\": \"TABLE\"},\n    {\"field\": \"score\", \"check_type\": \"has_max\", \"value\": 100, \"level\": \"TABLE\"},\n    {\"field\": \"rating\", \"check_type\": \"has_std\", \"value\": 2.0, \"level\": \"TABLE\"}\n]\n\n# Returns: Summary DataFrame with PASS/FAIL status\n_, _, table_summary = validate.pandas(df, table_rules)\n</code></pre>"},{"location":"#available-table-level-rules","title":"\ud83d\udd38 Available Table-Level Rules","text":"Rule Description Example <code>has_min</code> Minimum value check <code>{\"field\": \"age\", \"check_type\": \"has_min\", \"value\": 18}</code> <code>has_max</code> Maximum value check <code>{\"field\": \"score\", \"check_type\": \"has_max\", \"value\": 100}</code> <code>has_mean</code> Average value check <code>{\"field\": \"salary\", \"check_type\": \"has_mean\", \"value\": 50000}</code> <code>has_std</code> Standard deviation check <code>{\"field\": \"ratings\", \"check_type\": \"has_std\", \"value\": 2.0}</code> <code>has_sum</code> Total sum check <code>{\"field\": \"revenue\", \"check_type\": \"has_sum\", \"value\": 1000000}</code> <code>has_cardinality</code> Distinct count check <code>{\"field\": \"categories\", \"check_type\": \"has_cardinality\", \"value\": 10}</code> <code>has_entropy</code> Data entropy check <code>{\"field\": \"distribution\", \"check_type\": \"has_entropy\", \"value\": 2.5}</code> <code>has_infogain</code> Information gain check <code>{\"field\": \"features\", \"check_type\": \"has_infogain\", \"value\": 0.8}</code>"},{"location":"#cli-usage","title":"\ud83d\ude80 CLI Usage","text":"<p>Sumeh includes a powerful CLI built with Typer for validation workflows:</p>"},{"location":"#core-commands","title":"Core Commands","text":"<pre><code># Install with CLI support\npip install sumeh\n\n# Initialize new project\nsumeh init my-project\n\n# Validate data with rules\nsumeh validate --data data.csv --rules rules.csv --engine pandas\n\n# Get version and system info\nsumeh info\n\n# Manage rules\nsumeh rules list                    # List available rules\nsumeh rules info is_complete        # Get rule details\nsumeh rules search \"date\"           # Search rules by keyword\nsumeh rules template                # Generate rule template\n\n# Schema operations\nsumeh schema extract --data data.csv --output schema.json\nsumeh schema validate --data data.csv --registry schema_registry.csv\n\n# Generate SQL DDL for 17+ dialects\nsumeh sql generate --table rules --dialect postgres\nsumeh sql generate --table schema_registry --dialect bigquery\nsumeh sql transpile --sql \"SELECT * FROM users\" --from postgres --to bigquery\n\n# Web configuration UI\nsumeh config --port 8080\n</code></pre>"},{"location":"#available-cli-commands","title":"Available CLI Commands","text":"Command Description Example <code>init</code> Initialize new Sumeh project <code>sumeh init my-project</code> <code>validate</code> Run data validation <code>sumeh validate --data data.csv --rules rules.csv</code> <code>info</code> Show version and system info <code>sumeh info</code> <code>rules</code> Manage validation rules <code>sumeh rules list</code> <code>schema</code> Schema operations <code>sumeh schema extract --data data.csv</code> <code>sql</code> Generate/transpile SQL <code>sumeh sql generate --table rules --dialect mysql</code> <code>config</code> Launch web configuration UI <code>sumeh config --port 8080</code>"},{"location":"#dashboard","title":"\ud83d\udcca Dashboard","text":"<p>Optional Streamlit dashboard for interactive validation:</p> <pre><code># Install dashboard dependencies\npip install sumeh[dashboard]\n\n# Launch dashboard\nsumeh dashboard --port 8501\n</code></pre>"},{"location":"#advanced-usage","title":"\ud83d\udd27 Advanced Usage","text":""},{"location":"#custom-rule-development","title":"Custom Rule Development","text":"<pre><code>from sumeh.core.rules import RuleDef, RuleRegistry\n\n# Check available rules\nprint(RuleRegistry.list_rules())\n\n# Get rule details\nrule_info = RuleRegistry.get_rule(\"is_complete\")\nprint(rule_info[\"description\"])\n\n# Check engine compatibility\nprint(RuleRegistry.is_rule_supported(\"has_pattern\", \"duckdb\"))  # True\n</code></pre>"},{"location":"#performance-optimization","title":"Performance Optimization","text":"<pre><code># Filter rules by engine compatibility\nrules = get_rules_config.csv(\"rules.csv\")\ncompatible_rules = [r for r in rules if r.is_supported_by_engine(\"polars\")]\n\n# Skip disabled rules\nactive_rules = [r for r in rules if r.execute]\n\n# Level-specific validation\nrow_rules = [r for r in rules if r.is_applicable_for_level(\"ROW\")]\ntable_rules = [r for r in rules if r.is_applicable_for_level(\"TABLE\")]\n</code></pre>"},{"location":"#bigquery-engine-features","title":"BigQuery Engine Features","text":"<p>The BigQuery engine is fully implemented with advanced SQL generation using SQLGlot:</p> <pre><code>from sumeh import validate\nfrom google.cloud import bigquery\n\n# BigQuery validation with automatic SQL generation\nclient = bigquery.Client(project=\"my-project\")\ntable_ref = \"my-project.my_dataset.my_table\"\n\n# Supports all 60+ validation rules\nrules = [\n    {\"field\": \"email\", \"check_type\": \"has_pattern\", \"value\": r\"^[\\w.-]+@[\\w.-]+\\.[a-zA-Z]{2,}$\"},\n    {\"field\": \"created_at\", \"check_type\": \"is_past_date\"},\n    {\"field\": \"status\", \"check_type\": \"is_contained_in\", \"value\": \"['active','inactive']\"},\n    {\"field\": \"revenue\", \"check_type\": \"has_mean\", \"value\": 100000, \"level\": \"TABLE\"}\n]\n\n# Execute validation directly on BigQuery\ndf_errors, violations, table_summary = validate.bigquery(client, table_ref, rules)\n</code></pre>"},{"location":"#roadmap","title":"\ud83d\udcc8 Roadmap","text":"<ul> <li>\u2705 Dispatcher architecture: Clean API with engine-specific dispatchers</li> <li>\u2705 60+ validation rules across all engines</li> <li>\u2705 Multi-source configuration (CSV, S3, MySQL, PostgreSQL, BigQuery, etc.)</li> <li>\u2705 Type-safe rule definitions with auto-validation</li> <li>\u2705 Schema extraction &amp; validation</li> <li>\u2705 Complete BigQuery engine implementation with SQLGlot</li> <li>\u2705 CLI with Typer: 7 commands (validate, init, info, rules, schema, sql, config)</li> <li>\u2705 Row-level vs Table-level validation distinction</li> <li>\u2705 SQL DDL generators for 17+ dialects</li> <li>\u2705 Web configuration UI for interactive setup</li> <li>\ud83d\udd27 Performance optimizations &amp; caching</li> <li>\u2705 Real-time streaming validation (PySpark Structured Streaming)</li> <li>\ud83d\udd27 Plugin architecture for custom engines</li> </ul>"},{"location":"#contributing","title":"\ud83e\udd1d Contributing","text":"<ol> <li>Fork the repository</li> <li>Create a feature branch: <code>git checkout -b feature/amazing-feature</code></li> <li>Implement following existing patterns:</li> <li>Add rules to <code>manifest.json</code></li> <li>Implement in all engines</li> <li>Add comprehensive tests</li> <li>Test: <code>pytest tests/</code></li> <li>Submit a Pull Request</li> </ol>"},{"location":"#development-setup","title":"Development Setup","text":"<pre><code>git clone https://github.com/maltzsama/sumeh.git\ncd sumeh\npoetry install --with dev\npoetry run pytest\n</code></pre>"},{"location":"#license","title":"\ud83d\udcdc License","text":"<p>Licensed under the Apache License 2.0.</p> <p>Built with \u2764\ufe0f by the Sumeh team</p>"},{"location":"quickstart/","title":"Quickstart \ud83d\ude80","text":"<p>A concise guide to get started with Sumeh\u2019s unified data quality framework.</p>"},{"location":"quickstart/#1-installation","title":"1. Installation \ud83d\udcbb","text":"<p>Install Sumeh via pip (recommended) or conda:</p> <pre><code>pip install sumeh\n# or\nconda install -c conda-forge sumeh\n</code></pre>"},{"location":"quickstart/#2-loading-rules-and-schema-configuration","title":"2. Loading Rules and Schema Configuration \u2699\ufe0f","text":"<p>Use <code>get_rules_config</code> and <code>get_schema_config</code> to fetch your validation rules and expected schema from various sources.</p> <pre><code>from sumeh import get_rules_config, get_schema_config\n\n# Load rules from CSV\nrules = get_rules_config(\"path/to/rules.csv\", delimiter=';')\n\n# Load expected schema from Glue Data Catalog\nschema = get_schema_config(\n    \"glue\",\n    catalog_name=\"my_catalog\",\n    database_name=\"my_db\",\n    table_name=\"my_table\"\n)\n</code></pre> <p>Supported rule/schema sources include:</p> <ul> <li><code>bigquery://project.dataset.table</code> \ud83c\udf10</li> <li><code>s3://bucket/path</code> \u2601\ufe0f</li> <li>Local CSV (<code>*.csv</code>) \ud83d\udcc4</li> <li>Relational (\"mysql\", \"postgresql\") via kwargs \ud83d\uddc4\ufe0f</li> <li>AWS Glue (<code>\"glue\"</code>) \ud83d\udd25</li> <li>DuckDB (<code>duckdb://db_path.table</code>) \ud83e\udd86</li> <li>Databricks (<code>databricks://catalog.schema.table</code>) \ud83d\udc8e</li> </ul>"},{"location":"quickstart/#3-schema-validation","title":"3. Schema Validation \ud83d\udcd0","text":"<p>Before validating data, ensure your DataFrame or connection matches the expected schema:</p> <pre><code>from sumeh import validate_schema\n\n# For a Spark DataFrame:\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\ndf = spark.read.parquet(\"data.parquet\")\n\nis_valid, errors = validate_schema(\n    df,\n    expected=schema,\n    engine=\"pyspark_engine\"\n)\nif not is_valid:\n    print(\"Schema mismatches:\", errors)\n</code></pre>"},{"location":"quickstart/#4-data-validation","title":"4. Data Validation \ud83d\udd0d","text":"<p>Apply your loaded rules to any supported DataFrame using <code>validate</code>:</p> <pre><code>from sumeh import validate\n\n# Example with Pandas:\nimport pandas as pd\ndf = pd.read_csv(\"data.csv\")\n\n# Validate (detects engine automatically)\nresult = validate(df, rules)\n# `result` structure depends on engine (e.g., CheckResult for cuallee engines)\n</code></pre>"},{"location":"quickstart/#5-summarization","title":"5. Summarization \ud83d\udcca","text":"<p>Generate a tabular summary of violations and pass rates with <code>summarize</code>:</p> <pre><code>from sumeh import summarize\n\n# For DataFrames requiring manual total_rows (e.g., Pandas):\ntotal = len(df)\nsummary_df = summarize(\n    df=result,       # could be validation output or raw DataFrame\n    rules=rules,\n    total_rows=total\n)\nprint(summary_df)\n</code></pre>"},{"location":"quickstart/#6-one-step-reporting","title":"6. One-Step Reporting \ud83d\udcdd","text":"<p>Use <code>report</code> for an end-to-end quality check and summary in one call:</p> <pre><code>from sumeh import report\n\nreport_df = report(\n    df,              # your DataFrame or connection\n    rules,\n    name=\"My Quality Check\"\n)\nprint(report_df)\n</code></pre> <p>For deeper customization and engine-specific options, explore the full API and examples in the Sumeh repository.</p>"},{"location":"api/services/services-config/","title":"Module <code>sumeh.core</code>","text":""},{"location":"api/services/services-config/#sumeh.core","title":"sumeh.core","text":"<p>Sumeh Core Module - Data Quality Validation Framework</p> <p>This module provides dispatchers for data validation, summarization, and configuration retrieval across multiple data sources and engines (Pandas, Dask, PySpark, Polars, DuckDB, BigQuery).</p> <p>Classes:</p> Name Description <code>_ValidateDispatcher</code> <p>Dispatcher for validate functions across engines</p> <code>_SummarizeDispatcher</code> <p>Dispatcher for summarize functions across engines</p> <code>_RulesConfigDispatcher</code> <p>Dispatcher for retrieving validation rules</p> <code>_SchemaConfigDispatcher</code> <p>Dispatcher for retrieving schema configurations</p> <code>_ExtractSchemaDispatcher</code> <p>Dispatcher for extracting schemas</p> <code>_ValidateSchemaDispatcher</code> <p>Dispatcher for validating schemas</p> Exports <p>validate: Dispatcher instance for validation summarize: Dispatcher instance for summarization get_rules_config: Dispatcher instance for rules retrieval get_schema_config: Dispatcher instance for schema retrieval extract_schema: Dispatcher instance for schema extraction validate_schema: Dispatcher instance for schema validation report: Legacy cuallee-based validation (deprecated)</p> Usage <p>from sumeh import validate, summarize, get_rules_config, extract_schema</p>"},{"location":"api/services/services-config/#sumeh.core--get-rules","title":"Get rules","text":"<p>rules = get_rules_config.csv(\"rules.csv\")</p>"},{"location":"api/services/services-config/#sumeh.core--validate","title":"Validate","text":"<p>result = validate.pandas(df, rules)</p>"},{"location":"api/services/services-config/#sumeh.core--summarize","title":"Summarize","text":"<p>summary = summarize.pandas(result, rules, total_rows=len(df))</p>"},{"location":"api/services/services-config/#sumeh.core--extract-schema","title":"Extract schema","text":"<p>schema = extract_schema.pandas(df)</p>"},{"location":"sumeh/cli/","title":"CLI","text":""},{"location":"sumeh/cli/#sumeh.cli","title":"sumeh.cli","text":"<p>Sumeh CLI - Command-line interface for quick file-based operations.</p> <p>For programmatic use (including database sources), use the Python API directly.</p>"},{"location":"sumeh/cli/#sumeh.cli.config","title":"config","text":"<pre><code>config()\n</code></pre> <p>Launch configuration web interface.</p> <p>Opens a simple web UI for exploring rules and generating configurations.</p>"},{"location":"sumeh/cli/#sumeh.cli.info","title":"info","text":"<pre><code>info()\n</code></pre> <p>Show Sumeh version and available engines.</p>"},{"location":"sumeh/cli/#sumeh.cli.init","title":"init","text":"<pre><code>init(path: Path = typer.Argument(Path('.'), help='Project directory'))\n</code></pre> <p>Initialize a new Sumeh project with example files.</p> Creates <ul> <li>rules.csv (example rules)</li> <li>data/ (directory for data files)</li> <li>README.md (usage guide)</li> </ul> <p>[bold]Example:[/bold]     sumeh init     sumeh init my-project</p>"},{"location":"sumeh/cli/#sumeh.cli.rules_info","title":"rules_info","text":"<pre><code>rules_info(rule_name: str = typer.Argument(..., help='Rule name'))\n</code></pre> <p>Show detailed information about a specific rule.</p>"},{"location":"sumeh/cli/#sumeh.cli.rules_list","title":"rules_list","text":"<pre><code>rules_list(category: Optional[str] = typer.Option(None, '--category', '-c', help='Filter by category'), level: Optional[str] = typer.Option(None, '--level', '-l', help='Filter by level (ROW/TABLE)'), engine: Optional[str] = typer.Option(None, '--engine', '-e', help='Filter by engine support'))\n</code></pre> <p>List all available quality rules.</p>"},{"location":"sumeh/cli/#sumeh.cli.rules_search","title":"rules_search","text":"<pre><code>rules_search(keyword: str = typer.Argument(..., help='Search keyword'))\n</code></pre> <p>Search for rules by keyword.</p>"},{"location":"sumeh/cli/#sumeh.cli.rules_template","title":"rules_template","text":"<pre><code>rules_template(output: Path = typer.Option('rules.csv', '--output', '-o', help='Output file path'), example: bool = typer.Option(False, '--example', '-e', help='Include example rules'))\n</code></pre> <p>Generate a rules configuration template CSV.</p>"},{"location":"sumeh/cli/#sumeh.cli.schema_extract","title":"schema_extract","text":"<pre><code>schema_extract(data_file: Path = typer.Argument(..., exists=True, help='Data file'), output: Optional[Path] = typer.Option(None, '--output', '-o', help='Output file'))\n</code></pre> <p>Extract schema from data file.</p>"},{"location":"sumeh/cli/#sumeh.cli.schema_validate","title":"schema_validate","text":"<pre><code>schema_validate(data_file: Path = typer.Argument(..., exists=True, help='Data file'), schema_file: Path = typer.Argument(..., exists=True, help='Schema definition (JSON)'))\n</code></pre> <p>Validate data file against schema definition.</p>"},{"location":"sumeh/cli/#sumeh.cli.sql","title":"sql","text":"<pre><code>sql(table: Optional[str] = typer.Option(None, '--table', '-t', help='Table name'), dialect: Optional[str] = typer.Option(None, '--dialect', '-d', help='SQL dialect'), schema: Optional[str] = typer.Option(None, '--schema', '-s', help='Schema/dataset name'), output: Optional[Path] = typer.Option(None, '--output', '-o', help='Output file'), list_dialects: bool = typer.Option(False, '--list-dialects', help='List supported dialects'), list_tables: bool = typer.Option(False, '--list-tables', help='List available tables'))\n</code></pre> <p>Generate SQL DDL for Sumeh system tables.</p> <p>[bold]Examples:[/bold]     sumeh sql --table rules --dialect postgres     sumeh sql --table all --dialect bigquery --schema mydataset     sumeh sql --list-dialects</p>"},{"location":"sumeh/cli/#sumeh.cli.validate","title":"validate","text":"<pre><code>validate(data_file: Path = typer.Argument(..., exists=True, help='Data file (CSV, Parquet, JSON, Excel)'), rules_file: Path = typer.Argument(..., exists=True, help='Rules file (CSV, JSON)'), output: Optional[Path] = typer.Option(None, '--output', '-o', help='Output file path'), format: OutputFormat = typer.Option(OutputFormat.json, '--format', '-f', help='Output format'), engine: Engine = typer.Option(Engine.pandas, '--engine', '-e', help='DataFrame engine to use'), dashboard: bool = typer.Option(False, '--dashboard', '-d', help='Launch interactive dashboard'), fail_on_error: bool = typer.Option(False, '--fail-on-error', help='Exit with code 1 if checks fail'), verbose: int = typer.Option(0, '--verbose', '-v', count=True, help='Increase verbosity'))\n</code></pre> <p>Validate data file against quality rules.</p> <p>[bold]Examples:[/bold]     sumeh validate data.csv rules.csv     sumeh validate data.parquet rules.csv --engine polars     sumeh validate data.csv rules.csv --dashboard     sumeh validate data.csv rules.csv -o results.json --fail-on-error</p>"},{"location":"sumeh/generators/","title":"SQL Generator","text":"<p>Sumeh's SQL Generator provides cross-dialect DDL generation for creating <code>rules</code> and <code>schema_registry</code> tables across 17+ database systems using SQLGlot transpilation.</p>"},{"location":"sumeh/generators/#overview","title":"Overview","text":"<p>The SQLGenerator class automatically generates CREATE TABLE statements optimized for each database dialect, handling differences in:</p> <ul> <li>Data types (VARCHAR vs STRING vs TEXT)</li> <li>Auto-increment (SERIAL vs AUTO_INCREMENT vs IDENTITY)</li> <li>Constraints and indexing</li> <li>Dialect-specific optimizations (partitioning, clustering, distribution keys)</li> </ul>"},{"location":"sumeh/generators/#supported-dialects","title":"Supported Dialects","text":"Dialect Status Auto-increment Schema Support Advanced Features PostgreSQL \u2705 Full SERIAL \u2705 Indexes, constraints MySQL \u2705 Full AUTO_INCREMENT \u2705 Engine selection, charset BigQuery \u2705 Full \u274c \u2705 Partitioning, clustering Snowflake \u2705 Full AUTOINCREMENT \u2705 Clustering keys Databricks \u2705 Full \u274c \u2705 Delta Lake, Z-ordering Redshift \u2705 Full IDENTITY \u2705 DISTKEY, SORTKEY DuckDB \u2705 Full INTEGER \u2705 Fast analytics SQLite \u2705 Full AUTOINCREMENT \u274c Embedded use Athena \u2705 Full \u274c \u2705 External tables, S3 Oracle \u2705 Full SEQUENCE \u2705 Enterprise features Teradata \u2705 Full IDENTITY \u2705 MPP architecture Trino/Presto \u2705 Full \u274c \u2705 Federated queries ClickHouse \u2705 Full \u274c \u2705 Columnar OLAP Hive \u2705 Full \u274c \u2705 Hadoop ecosystem Spark SQL \u2705 Full \u274c \u2705 Distributed processing TSQL \u2705 Full IDENTITY \u2705 SQL Server/Azure SQL Apache Drill \u2705 Basic \u274c \u2705 Schema-free queries"},{"location":"sumeh/generators/#table-schemas","title":"Table Schemas","text":""},{"location":"sumeh/generators/#rules-table","title":"Rules Table","text":"<p>Stores data quality validation rules with metadata:</p> <pre><code>CREATE TABLE rules (\n    id INTEGER PRIMARY KEY AUTO_INCREMENT,\n    environment VARCHAR(50) NOT NULL,\n    source_type VARCHAR(50) NOT NULL,\n    database_name VARCHAR(255) NOT NULL,\n    catalog_name VARCHAR(255),\n    schema_name VARCHAR(255),\n    table_name VARCHAR(255) NOT NULL,\n    field VARCHAR(255) NOT NULL,\n    level VARCHAR(100) NOT NULL,\n    category VARCHAR(100) NOT NULL,\n    check_type VARCHAR(100) NOT NULL,\n    value TEXT,\n    threshold FLOAT DEFAULT 1.0,\n    execute BOOLEAN DEFAULT TRUE,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    updated_at TIMESTAMP\n);\n</code></pre>"},{"location":"sumeh/generators/#schema-registry-table","title":"Schema Registry Table","text":"<p>Stores expected schema definitions for validation:</p> <pre><code>CREATE TABLE schema_registry (\n    id INTEGER PRIMARY KEY AUTO_INCREMENT,\n    environment VARCHAR(50) NOT NULL,\n    source_type VARCHAR(50) NOT NULL,\n    database_name VARCHAR(255) NOT NULL,\n    catalog_name VARCHAR(255),\n    schema_name VARCHAR(255),\n    table_name VARCHAR(255) NOT NULL,\n    field VARCHAR(255) NOT NULL,\n    data_type VARCHAR(100) NOT NULL,\n    nullable BOOLEAN DEFAULT TRUE,\n    max_length INTEGER,\n    comment TEXT,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    updated_at TIMESTAMP\n);\n</code></pre>"},{"location":"sumeh/generators/#usage-examples","title":"Usage Examples","text":""},{"location":"sumeh/generators/#basic-generation","title":"Basic Generation","text":"<pre><code>from sumeh.generators import SQLGenerator\n\n# Generate rules table for PostgreSQL\nddl = SQLGenerator.generate(table=\"rules\", dialect=\"postgres\")\nprint(ddl)\n\n# Generate both tables for MySQL\nddl = SQLGenerator.generate(table=\"all\", dialect=\"mysql\", schema=\"dq\")\nprint(ddl)\n</code></pre>"},{"location":"sumeh/generators/#advanced-features","title":"Advanced Features","text":""},{"location":"sumeh/generators/#bigquery-with-optimization","title":"BigQuery with Optimization","text":"<pre><code># Partitioned and clustered table\nddl = SQLGenerator.generate(\n    table=\"rules\",\n    dialect=\"bigquery\",\n    schema=\"prod_dq\",\n    partition_by=\"DATE(created_at)\",\n    cluster_by=[\"table_name\", \"environment\"]\n)\n</code></pre>"},{"location":"sumeh/generators/#redshift-with-distribution","title":"Redshift with Distribution","text":"<pre><code># Optimized for analytics workload\nddl = SQLGenerator.generate(\n    table=\"rules\",\n    dialect=\"redshift\",\n    schema=\"analytics\",\n    distkey=\"table_name\",\n    sortkey=[\"created_at\", \"environment\"]\n)\n</code></pre>"},{"location":"sumeh/generators/#databricks-delta-lake","title":"Databricks Delta Lake","text":"<pre><code># Unity Catalog with Delta optimizations\nddl = SQLGenerator.generate(\n    table=\"schema_registry\",\n    dialect=\"databricks\",\n    schema=\"main.dq\",\n    partition_by=\"environment\",\n    cluster_by=[\"table_name\", \"field\"],\n    location=\"s3://data-lake/schema-registry/\"\n)\n</code></pre>"},{"location":"sumeh/generators/#dialect-specific-features","title":"Dialect-Specific Features","text":""},{"location":"sumeh/generators/#cloud-data-warehouses","title":"Cloud Data Warehouses","text":"<ul> <li>BigQuery: Partitioning, clustering, nested/repeated fields</li> <li>Snowflake: Clustering keys, time travel, zero-copy cloning</li> <li>Redshift: Distribution keys, sort keys, columnar compression</li> <li>Databricks: Delta Lake, Z-ordering, Unity Catalog integration</li> </ul>"},{"location":"sumeh/generators/#traditional-databases","title":"Traditional Databases","text":"<ul> <li>PostgreSQL: Advanced indexing, constraints, extensions</li> <li>MySQL: Storage engines (InnoDB, MyISAM), charset selection</li> <li>Oracle: Sequences, tablespaces, advanced partitioning</li> <li>SQL Server: Identity columns, filegroups, compression</li> </ul>"},{"location":"sumeh/generators/#analytics-engines","title":"Analytics Engines","text":"<ul> <li>DuckDB: Columnar storage, vectorized execution</li> <li>ClickHouse: MergeTree engines, materialized views</li> <li>Athena: External tables, Parquet/ORC optimization</li> <li>Presto/Trino: Federated queries, connector-specific optimizations</li> </ul>"},{"location":"sumeh/generators/#cli-integration","title":"CLI Integration","text":"<p>Generate DDL directly from command line:</p> <pre><code># List supported dialects\nsumeh sql --list-dialects\n\n# Generate PostgreSQL DDL\nsumeh sql --table rules --dialect postgres --schema public\n\n# Generate all tables for BigQuery\nsumeh sql --table all --dialect bigquery --schema prod_dq\n\n# Save to file\nsumeh sql --table rules --dialect mysql --output schema.sql\n</code></pre>"},{"location":"sumeh/generators/#api-reference","title":"API Reference","text":""},{"location":"sumeh/generators/#sumeh.generators.SQLGenerator","title":"sumeh.generators.SQLGenerator","text":"<p>Generates DDL statements for sumeh tables using SQLGlot.</p>"},{"location":"sumeh/generators/#sumeh.generators.SQLGenerator.generate","title":"generate  <code>classmethod</code>","text":"<pre><code>generate(table: str, dialect: str, schema: str = None, **kwargs) -&gt; str\n</code></pre> <p>Generate DDL for a specific table and dialect using SQLGlot.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>str</code> <p>Table name ('rules', 'schema_registry', or 'all')</p> required <code>dialect</code> <code>str</code> <p>SQL dialect (postgres, bigquery, snowflake, etc.)</p> required <code>schema</code> <code>str</code> <p>Optional schema/dataset name</p> <code>None</code> <code>**kwargs</code> <p>Additional dialect-specific options</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>DDL statement(s) as string</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If table or dialect is invalid</p>"},{"location":"sumeh/generators/#sumeh.generators.SQLGenerator.list_dialects","title":"list_dialects  <code>classmethod</code>","text":"<pre><code>list_dialects() -&gt; List[str]\n</code></pre> <p>Return list of supported SQL dialects.</p>"},{"location":"sumeh/generators/#sumeh.generators.SQLGenerator.list_tables","title":"list_tables  <code>classmethod</code>","text":"<pre><code>list_tables() -&gt; List[str]\n</code></pre> <p>Return list of available tables.</p>"},{"location":"sumeh/generators/#sumeh.generators.SQLGenerator.transpile","title":"transpile  <code>classmethod</code>","text":"<pre><code>transpile(sql: str, from_dialect: str, to_dialect: str) -&gt; str\n</code></pre> <p>Transpile SQL from one dialect to another.</p> <p>Parameters:</p> Name Type Description Default <code>sql</code> <code>str</code> <p>SQL statement to transpile</p> required <code>from_dialect</code> <code>str</code> <p>Source dialect</p> required <code>to_dialect</code> <code>str</code> <p>Target dialect</p> required <p>Returns:</p> Type Description <code>str</code> <p>Transpiled SQL statement</p>"},{"location":"sumeh/core/config/","title":"Configuration","text":""},{"location":"sumeh/core/config/#sumeh.core.config","title":"sumeh.core.config","text":"<p>This module provides a set of utility functions to retrieve and parse configuration data from various data sources, including S3, MySQL, PostgreSQL, BigQuery, CSV files, AWS Glue Data Catalog, DuckDB, and Databricks. Additionally, it includes functions to infer schema information from these sources.</p> <p>Functions:</p> Name Description <code>get_config_from_s3</code> <p>str, delimiter: Optional[str] = \",\") -&gt; List[Dict[str, Any]]:</p> <code>get_config_from_mysql</code> <code>get_config_from_postgresql</code> <code>get_config_from_bigquery</code> <code>get_config_from_csv</code> <p>str, delimiter: Optional[str] = \",\") -&gt; List[Dict[str, str]]: Retrieves configuration data from a local CSV file.</p> <code>get_config_from_glue_data_catalog</code> <code>get_config_from_duckdb</code> <p>Retrieves configuration data from a DuckDB database.</p> <code>get_config_from_databricks</code> <p>Retrieves configuration data from a Databricks table.</p> <code>get_schema_from_csv</code> <p>str, delimiter: str = \",\", sample_size: int = 1_000) -&gt; List[Dict[str, Any]]: Infers the schema of a CSV file based on its content.</p> <code>get_schema_from_s3</code> <p>str, **kwargs) -&gt; List[Dict[str, Any]]: Infers the schema of a CSV file stored in S3.</p> <code>get_schema_from_mysql</code> <p>Retrieves schema information from a MySQL database table.</p> <code>get_schema_from_postgresql</code> <p>Retrieves schema information from a PostgreSQL database table.</p> <code>get_schema_from_bigquery</code> <p>Retrieves schema information from a Google BigQuery table.</p> <code>get_schema_from_glue</code> <p>Retrieves schema information from AWS Glue Data Catalog.</p> <code>get_schema_from_duckdb</code> <p>Retrieves schema information from a DuckDB database table.</p> <code>get_schema_from_databricks</code> <p>Retrieves schema information from a Databricks table.</p> <code>__read_s3_file</code> <p>str) -&gt; Optional[str]:</p> <code>__parse_s3_path</code> <p>str) -&gt; Tuple[str, str]:</p> <code>__read_local_file</code> <p>str) -&gt; str:</p> <code>__read_csv_file</code> <p>str, delimiter: Optional[str] = \",\") -&gt; List[Dict[str, str]]:</p> <code>__parse_data</code> <p>list[dict]) -&gt; list[dict]: Parses the configuration data into a structured format.</p> <code>__create_connection</code> <code>infer_basic_type</code> <p>str) -&gt; str: Infers the basic data type of given value.</p>"},{"location":"sumeh/core/config/#sumeh.core.config.__create_connection","title":"__create_connection","text":"<pre><code>__create_connection(connect_func, host, user, password, database, port) -&gt; Any\n</code></pre> <p>Helper function to create a database connection.</p> <p>Parameters:</p> Name Type Description Default <code>connect_func</code> <p>A connection function (e.g., <code>mysql.connector.connect</code> or <code>psycopg2.connect</code>).</p> required <code>host</code> <code>str</code> <p>The host of the database server.</p> required <code>user</code> <code>str</code> <p>The username for the database.</p> required <code>password</code> <code>str</code> <p>The password for the database.</p> required <code>database</code> <code>str</code> <p>The name of the database.</p> required <code>port</code> <code>int</code> <p>The port to connect to.</p> required <p>Returns:</p> Name Type Description <code>Connection</code> <code>Any</code> <p>A connection object for the database.</p> <p>Raises:</p> Type Description <code>ConnectionError</code> <p>If there is an error establishing the connection.</p>"},{"location":"sumeh/core/config/#sumeh.core.config.__parse_data","title":"__parse_data","text":"<pre><code>__parse_data(data: list[dict]) -&gt; List[RuleDef]\n</code></pre> <p>Parse configuration data into validated Rule objects.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>list[dict]</code> <p>Raw configuration data as list of dictionaries</p> required <p>Returns:</p> Type Description <code>List[RuleDef]</code> <p>List[Rule]: Validated Rule objects with enriched metadata</p> Note <p>Engine compatibility is not validated here - only rule existence. Engine validation happens during execution in validate().</p>"},{"location":"sumeh/core/config/#sumeh.core.config.__parse_s3_path","title":"__parse_s3_path","text":"<pre><code>__parse_s3_path(s3_path: str) -&gt; Tuple[str, str]\n</code></pre> <p>Parses an S3 path into its bucket and key components.</p> <p>Parameters:</p> Name Type Description Default <code>s3_path</code> <code>str</code> <p>The S3 path to parse. Must start with \"s3://\".</p> required <p>Returns:</p> Type Description <code>Tuple[str, str]</code> <p>Tuple[str, str]: A tuple containing the bucket name and the key.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the S3 path does not start with \"s3://\", or if the path         format is invalid and cannot be split into bucket and key.</p>"},{"location":"sumeh/core/config/#sumeh.core.config.__read_csv_file","title":"__read_csv_file","text":"<pre><code>__read_csv_file(file_content: str, delimiter: Optional[str] = ',') -&gt; List[Dict[str, str]]\n</code></pre> <p>Parses the content of a CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>file_content</code> <code>str</code> <p>The content of the CSV file as a string.</p> required <code>delimiter</code> <code>str</code> <p>The delimiter used in the CSV file.</p> <code>','</code> <p>Returns:</p> Type Description <code>List[Dict[str, str]]</code> <p>List[Dict[str, str]]: A list of dictionaries representing the parsed CSV data.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If there is an error parsing the CSV content.</p>"},{"location":"sumeh/core/config/#sumeh.core.config.__read_local_file","title":"__read_local_file","text":"<pre><code>__read_local_file(file_path: str) -&gt; str\n</code></pre> <p>Reads the content of a local file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The local file path to be read.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The content of the file.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the file is not found.</p>"},{"location":"sumeh/core/config/#sumeh.core.config.__read_s3_file","title":"__read_s3_file","text":"<pre><code>__read_s3_file(s3_path: str) -&gt; Optional[str]\n</code></pre> <p>Reads the content of a file stored in S3.</p> <p>Parameters:</p> Name Type Description Default <code>s3_path</code> <code>str</code> <p>The S3 path of the file.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Optional[str]</code> <p>The content of the S3 file.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If there is an error retrieving the file from S3.</p>"},{"location":"sumeh/core/config/#sumeh.core.config.get_config_from_bigquery","title":"get_config_from_bigquery","text":"<pre><code>get_config_from_bigquery(project_id: str, dataset_id: str, table_id: str, credentials_path: Optional[str] = None, client: Optional[Any] = None, query: Optional[str] = None) -&gt; List[RuleDef]\n</code></pre> <p>Retrieves configuration data from a Google BigQuery table.</p> <p>Parameters:</p> Name Type Description Default <code>project_id</code> <code>str</code> <p>Google Cloud project ID.</p> required <code>dataset_id</code> <code>str</code> <p>BigQuery dataset ID.</p> required <code>table_id</code> <code>str</code> <p>BigQuery table ID.</p> required <code>credentials_path</code> <code>Optional[str]</code> <p>Path to service account credentials file (if not provided, uses default credentials).</p> <code>None</code> <code>client</code> <code>Optional[Any]</code> <p>Optional instance of google.cloud.bigquery.Client. If provided, it will be used and credentials_path ignored.</p> <code>None</code> <code>query</code> <code>Optional[str]</code> <p>Optional custom SQL query. If not provided, defaults to SELECT * FROM <code>project.dataset.table</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[RuleDef]</code> <p>List[Dict[str, Any]]: A list of records (dicts) returned by BigQuery (optionally parsed by __parse_data).</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If there is an error while querying BigQuery or with credentials.</p>"},{"location":"sumeh/core/config/#sumeh.core.config.get_config_from_csv","title":"get_config_from_csv","text":"<pre><code>get_config_from_csv(file_path: str, delimiter: Optional[str] = ',') -&gt; List[RuleDef]\n</code></pre> <p>Retrieves configuration data from a CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The local file path to the CSV file.</p> required <code>delimiter</code> <code>Optional[str]</code> <p>The delimiter used in the CSV file (default is \",\").</p> <code>','</code> <p>Returns:</p> Type Description <code>List[RuleDef]</code> <p>List[Dict[str, str]]: A list of dictionaries representing the parsed configuration data.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If there is an error reading or processing the file.</p>"},{"location":"sumeh/core/config/#sumeh.core.config.get_config_from_databricks","title":"get_config_from_databricks","text":"<pre><code>get_config_from_databricks(spark, catalog: Optional[str], schema: Optional[str], table: str, **kwargs) -&gt; List[RuleDef]\n</code></pre> <p>Retrieves configuration data from a Databricks table and returns it as a list of dictionaries.</p> <p>Parameters:</p> Name Type Description Default <code>spark SparkSession</code> <p>Spark Session to get information from Databricks</p> required <code>catalog</code> <code>Optional[str]</code> <p>The catalog name in Databricks. If provided, it will be included in the table's full path.</p> required <code>schema</code> <code>Optional[str]</code> <p>The schema name in Databricks. If provided, it will be included in the table's full path.</p> required <code>table</code> <code>str</code> <p>The name of the table to retrieve data from.</p> required <code>query</code> <p>Additional keyword arguments (currently unused).</p> required <p>Returns:</p> Type Description <code>List[RuleDef]</code> <p>List[Dict[str, Any]]: A list of dictionaries, where each dictionary represents a row of data from the table.</p>"},{"location":"sumeh/core/config/#sumeh.core.config.get_config_from_duckdb","title":"get_config_from_duckdb","text":"<pre><code>get_config_from_duckdb(table: str = None, query: str = None, conn=None) -&gt; List[RuleDef]\n</code></pre> <p>Retrieve configuration data from a DuckDB database.</p> <p>This function fetches data from a DuckDB database either by executing a custom SQL query or by selecting all rows from a specified table. The data is then parsed into a list of dictionaries.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>str</code> <p>The name of the table to fetch data from. Defaults to None.</p> <code>None</code> <code>query</code> <code>str</code> <p>A custom SQL query to execute. Defaults to None.</p> <code>None</code> <code>conn</code> <p>A valid DuckDB connection object.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[RuleDef]</code> <p>List[Dict[str, Any]]: A list of dictionaries representing the fetched data.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If neither <code>table</code> nor <code>query</code> is provided, or if a valid <code>conn</code> is not supplied.</p> Example <p>import duckdb conn = duckdb.connect('my_db.duckdb') config = get_config_from_duckdb('my_db.duckdb', table='rules', conn=conn)</p>"},{"location":"sumeh/core/config/#sumeh.core.config.get_config_from_glue_data_catalog","title":"get_config_from_glue_data_catalog","text":"<pre><code>get_config_from_glue_data_catalog(glue_context, database_name: str, table_name: str, query: Optional[str] = None) -&gt; List[RuleDef]\n</code></pre> <p>Retrieves configuration data from AWS Glue Data Catalog.</p> <p>Using Spark directly - works with all table formats (Parquet, ORC, CSV, Iceberg, Delta, Hudi).</p> <p>Parameters:</p> Name Type Description Default <code>glue_context</code> <p>An instance of <code>GlueContext</code>.</p> required <code>database_name</code> <code>str</code> <p>Glue database name.</p> required <code>table_name</code> <code>str</code> <p>Glue table name.</p> required <code>query</code> <code>Optional[str]</code> <p>Custom SQL query to fetch data (if provided).</p> <code>None</code> <p>Returns:</p> Type Description <code>List[RuleDef]</code> <p>List[Dict[str, str]]: A list of dictionaries representing the parsed configuration data.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If there is an error querying Glue Data Catalog.</p>"},{"location":"sumeh/core/config/#sumeh.core.config.get_config_from_mysql","title":"get_config_from_mysql","text":"<pre><code>get_config_from_mysql(host: str = None, user: str = None, password: str = None, database: str = None, schema: str = None, table: str = None, port: int = 3306, query: str = None, conn=None) -&gt; List[RuleDef]\n</code></pre> <p>Get configuration from MySQL table</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <code>str</code> <p>MySQL host (not needed if conn is provided)</p> <code>None</code> <code>user</code> <code>str</code> <p>MySQL user (not needed if conn is provided)</p> <code>None</code> <code>password</code> <code>str</code> <p>MySQL password (not needed if conn is provided)</p> <code>None</code> <code>database</code> <code>str</code> <p>Database name (not needed if conn is provided)</p> <code>None</code> <code>schema</code> <code>str</code> <p>Schema name (optional)</p> <code>None</code> <code>table</code> <code>str</code> <p>Table name to query</p> <code>None</code> <code>port</code> <code>int</code> <p>MySQL port (default: 3306)</p> <code>3306</code> <code>query</code> <code>str</code> <p>Optional custom query (if not provided, uses schema and table)</p> <code>None</code> <code>conn</code> <p>Existing MySQL connection (optional)</p> <code>None</code> <p>Returns:</p> Type Description <code>List[RuleDef]</code> <p>List of dicts with configuration data</p>"},{"location":"sumeh/core/config/#sumeh.core.config.get_config_from_postgresql","title":"get_config_from_postgresql","text":"<pre><code>get_config_from_postgresql(connection: Optional = None, host: Optional[str] = None, user: Optional[str] = None, password: Optional[str] = None, database: Optional[str] = None, port: Optional[int] = 5432, schema: Optional[str] = None, table: Optional[str] = None, query: Optional[str] = None) -&gt; List[RuleDef]\n</code></pre> <p>Retrieves configuration data from a PostgreSQL database.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Optional</code> <p>An existing PostgreSQL connection object.</p> <code>None</code> <code>host</code> <code>Optional[str]</code> <p>Host of the PostgreSQL server.</p> <code>None</code> <code>user</code> <code>Optional[str]</code> <p>Username to connect to PostgreSQL.</p> <code>None</code> <code>password</code> <code>Optional[str]</code> <p>Password for the PostgreSQL user.</p> <code>None</code> <code>database</code> <code>Optional[str]</code> <p>Database name to query.</p> <code>None</code> <code>port</code> <code>Optional[int]</code> <p>The port for the PostgreSQL connection (default is 5432).</p> <code>5432</code> <code>schema</code> <code>Optional[str]</code> <p>Schema name if query is not provided.</p> <code>None</code> <code>table</code> <code>Optional[str]</code> <p>Table name if query is not provided.</p> <code>None</code> <code>query</code> <code>Optional[str]</code> <p>Custom SQL query to fetch data (if not provided, <code>schema</code> and <code>table</code> must be given).</p> <code>None</code> <p>Returns:</p> Type Description <code>List[RuleDef]</code> <p>List[Dict[str, Any]]: A list of dictionaries representing the parsed configuration data.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If neither <code>query</code> nor both <code>schema</code> and <code>table</code> are provided.</p> <code>ConnectionError</code> <p>If there is an error connecting to PostgreSQL.</p> <code>RuntimeError</code> <p>If there is an error executing the query or processing the data.</p>"},{"location":"sumeh/core/config/#sumeh.core.config.get_config_from_s3","title":"get_config_from_s3","text":"<pre><code>get_config_from_s3(s3_path: str, delimiter: Optional[str] = ',')\n</code></pre> <p>Retrieves configuration data from a CSV file stored in an S3 bucket.</p> <p>Parameters:</p> Name Type Description Default <code>s3_path</code> <code>str</code> <p>The S3 path to the CSV file.</p> required <code>delimiter</code> <code>Optional[str]</code> <p>The delimiter used in the CSV file (default is \",\").</p> <code>','</code> <p>Returns:</p> Type Description <p>List[Dict[str, Any]]: A list of dictionaries representing the parsed configuration data.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If there is an error reading or processing the S3 file.</p>"},{"location":"sumeh/core/config/#sumeh.core.config.get_schema_from_bigquery","title":"get_schema_from_bigquery","text":"<pre><code>get_schema_from_bigquery(project_id: str, dataset_id: str, table_id: str, credentials_path: str = None, registry_table: str = 'schema_registry', query: str = None) -&gt; List[Dict[str, Any]]\n</code></pre> <p>Get schema from BigQuery schema_registry table</p> <p>Parameters:</p> Name Type Description Default <code>project_id</code> <code>str</code> <p>BigQuery project ID</p> required <code>dataset_id</code> <code>str</code> <p>BigQuery dataset ID</p> required <code>table_id</code> <code>str</code> <p>Table name to look up in the registry</p> required <code>credentials_path</code> <code>str</code> <p>Path to service account credentials file</p> <code>None</code> <code>registry_table</code> <code>str</code> <p>Name of the schema registry table</p> <code>'schema_registry'</code> <code>query</code> <code>str</code> <p>Optional custom WHERE clause for additional filters</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List of dicts with schema information</p>"},{"location":"sumeh/core/config/#sumeh.core.config.get_schema_from_csv","title":"get_schema_from_csv","text":"<pre><code>get_schema_from_csv(file_path: str, table: str, delimiter: str = ',', query: str = None) -&gt; List[Dict[str, Any]]\n</code></pre> <p>Get schema from CSV schema_registry file</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to the schema_registry CSV file</p> required <code>table</code> <code>str</code> <p>Table name to look up in the registry</p> required <code>delimiter</code> <code>str</code> <p>CSV delimiter (default: ',')</p> <code>','</code> <code>query</code> <code>str</code> <p>Optional custom WHERE clause for additional filters (NOT SUPPORTED for CSV)</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List of dicts with schema information</p>"},{"location":"sumeh/core/config/#sumeh.core.config.get_schema_from_databricks","title":"get_schema_from_databricks","text":"<pre><code>get_schema_from_databricks(spark, catalog: str, schema: str, table: str, registry_table: str = 'schema_registry', query: str = None) -&gt; List[Dict[str, Any]]\n</code></pre> <p>Get schema from Databricks Unity Catalog schema_registry table</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <p>SparkSession instance</p> required <code>catalog</code> <code>str</code> <p>Unity Catalog name containing the registry</p> required <code>schema</code> <code>str</code> <p>Schema name containing the registry table</p> required <code>table</code> <code>str</code> <p>Table name to look up in the registry</p> required <code>registry_table</code> <code>str</code> <p>Name of the schema registry table (default: 'schema_registry')</p> <code>'schema_registry'</code> <code>query</code> <code>str</code> <p>Optional custom WHERE clause for additional filters</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List of dicts with schema information</p>"},{"location":"sumeh/core/config/#sumeh.core.config.get_schema_from_duckdb","title":"get_schema_from_duckdb","text":"<pre><code>get_schema_from_duckdb(conn, table: str, registry_table: str = 'schema_registry', query: str = None) -&gt; List[Dict[str, Any]]\n</code></pre> <p>Get schema from DuckDB schema_registry table</p> <p>Parameters:</p> Name Type Description Default <code>conn</code> <p>DuckDB connection object</p> required <code>table</code> <code>str</code> <p>Table name to look up in the registry</p> required <code>registry_table</code> <code>str</code> <p>Name of the schema registry table (default: 'schema_registry')</p> <code>'schema_registry'</code> <code>query</code> <code>str</code> <p>Optional custom WHERE clause for additional filters</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List of dicts with schema information</p>"},{"location":"sumeh/core/config/#sumeh.core.config.get_schema_from_glue","title":"get_schema_from_glue","text":"<pre><code>get_schema_from_glue(glue_context, database_name: str, table_name: str, registry_table: str = 'schema_registry', query: str = None) -&gt; List[Dict[str, Any]]\n</code></pre> <p>Get schema from Glue Data Catalog schema_registry table</p> <p>Parameters:</p> Name Type Description Default <code>glue_context</code> <p>GlueContext instance</p> required <code>database_name</code> <code>str</code> <p>Glue database containing the registry table</p> required <code>table_name</code> <code>str</code> <p>Table name to look up in the registry</p> required <code>registry_table</code> <code>str</code> <p>Name of the schema registry table (default: 'schema_registry')</p> <code>'schema_registry'</code> <code>query</code> <code>str</code> <p>Optional custom WHERE clause for additional filters</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List of dicts with schema information</p>"},{"location":"sumeh/core/config/#sumeh.core.config.get_schema_from_mysql","title":"get_schema_from_mysql","text":"<pre><code>get_schema_from_mysql(host: str = None, user: str = None, password: str = None, database: str = None, table: str = None, port: int = 3306, registry_table: str = 'schema_registry', query: str = None, conn=None) -&gt; List[Dict[str, Any]]\n</code></pre> <p>Get schema from MySQL schema_registry table</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <code>str</code> <p>MySQL host (not needed if conn is provided)</p> <code>None</code> <code>user</code> <code>str</code> <p>MySQL user (not needed if conn is provided)</p> <code>None</code> <code>password</code> <code>str</code> <p>MySQL password (not needed if conn is provided)</p> <code>None</code> <code>database</code> <code>str</code> <p>Database containing the registry table (not needed if conn is provided)</p> <code>None</code> <code>table</code> <code>str</code> <p>Table name to look up in the registry</p> <code>None</code> <code>port</code> <code>int</code> <p>MySQL port (default: 3306)</p> <code>3306</code> <code>registry_table</code> <code>str</code> <p>Name of the schema registry table (default: 'schema_registry')</p> <code>'schema_registry'</code> <code>query</code> <code>str</code> <p>Optional custom WHERE clause for additional filters</p> <code>None</code> <code>conn</code> <p>Existing MySQL connection (optional, will create new if not provided)</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List of dicts with schema information</p>"},{"location":"sumeh/core/config/#sumeh.core.config.get_schema_from_postgresql","title":"get_schema_from_postgresql","text":"<pre><code>get_schema_from_postgresql(host: str = None, user: str = None, password: str = None, database: str = None, schema: str = None, table: str = None, port: int = 5432, registry_table: str = 'schema_registry', query: str = None, conn=None) -&gt; List[Dict[str, Any]]\n</code></pre> <p>Get schema from PostgreSQL schema_registry table</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <code>str</code> <p>PostgreSQL host (not needed if conn is provided)</p> <code>None</code> <code>user</code> <code>str</code> <p>PostgreSQL user (not needed if conn is provided)</p> <code>None</code> <code>password</code> <code>str</code> <p>PostgreSQL password (not needed if conn is provided)</p> <code>None</code> <code>database</code> <code>str</code> <p>Database containing the registry table (not needed if conn is provided)</p> <code>None</code> <code>schema</code> <code>str</code> <p>Schema containing the registry table</p> <code>None</code> <code>table</code> <code>str</code> <p>Table name to look up in the registry</p> <code>None</code> <code>port</code> <code>int</code> <p>PostgreSQL port (default: 5432)</p> <code>5432</code> <code>registry_table</code> <code>str</code> <p>Name of the schema registry table (default: 'schema_registry')</p> <code>'schema_registry'</code> <code>query</code> <code>str</code> <p>Optional custom WHERE clause for additional filters</p> <code>None</code> <code>conn</code> <p>Existing PostgreSQL connection (optional, will create new if not provided)</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List of dicts with schema information</p>"},{"location":"sumeh/core/config/#sumeh.core.config.get_schema_from_s3","title":"get_schema_from_s3","text":"<pre><code>get_schema_from_s3(s3_path: str, table: str, delimiter: str = ',', query: str = None) -&gt; List[Dict[str, Any]]\n</code></pre> <p>Get schema from S3 schema_registry CSV file</p> <p>Parameters:</p> Name Type Description Default <code>s3_path</code> <code>str</code> <p>S3 URI to the schema_registry CSV file (e.g., 's3://bucket/path/schema_registry.csv')</p> required <code>table</code> <code>str</code> <p>Table name to look up in the registry</p> required <code>delimiter</code> <code>str</code> <p>CSV delimiter (default: ',')</p> <code>','</code> <code>query</code> <code>str</code> <p>Optional custom WHERE clause for additional filters (NOT SUPPORTED for S3/CSV)</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List of dicts with schema information</p>"},{"location":"sumeh/core/utils/","title":"utils","text":""},{"location":"sumeh/core/utils/#sumeh.core.utils","title":"sumeh.core.utils","text":""},{"location":"sumeh/core/utils/#sumeh.core.utils.__compare_schemas","title":"__compare_schemas","text":"<pre><code>__compare_schemas(actual: List[SchemaDef], expected: List[SchemaDef]) -&gt; Tuple[bool, List[Dict[str, Any]]]\n</code></pre> <p>Compare two lists of schema definitions and identify discrepancies.</p> <p>Parameters:</p> Name Type Description Default <code>actual</code> <code>List[SchemaDef]</code> <p>The list of actual schema definitions.</p> required <code>expected</code> <code>List[SchemaDef]</code> <p>The list of expected schema definitions.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>Tuple[bool, List[Tuple[str, str]]]: A tuple where the first element is a boolean indicating</p> <code>List[Dict[str, Any]]</code> <p>whether the schemas match (True if they match, False otherwise), and the second element</p> <code>Tuple[bool, List[Dict[str, Any]]]</code> <p>is a list of tuples describing the discrepancies. Each tuple contains: - The field name (str). - A description of the discrepancy (str), such as \"missing\", \"type mismatch\",   \"nullable but expected non-nullable\", or \"extra column\".</p> Notes <ul> <li>A field is considered \"missing\" if it exists in the expected schema but not in the actual schema.</li> <li>A \"type mismatch\" occurs if the data type of field in the actual schema does not match   the expected data type.</li> <li>A field is considered \"nullable but expected non-nullable\" if it is nullable in the actual   schema but not nullable in the expected schema.</li> <li>An \"extra column\" is a field that exists in the actual schema but not in the expected schema.</li> </ul>"},{"location":"sumeh/core/utils/#sumeh.core.utils.__convert_value","title":"__convert_value","text":"<pre><code>__convert_value(value)\n</code></pre> <p>Converts the provided value to the appropriate type (date, float, or int).</p> <p>Depending on the format of the input value, it will be converted to a datetime object, a floating-point number (float), or an integer (int).</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>The value to be converted, represented as a string.</p> required <p>Returns:</p> Type Description <p>Union[datetime, float, int]: The converted value, which can be a datetime object, float, or int.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the value does not match an expected format.</p>"},{"location":"sumeh/core/utils/#sumeh.core.utils.__detect_engine","title":"__detect_engine","text":"<pre><code>__detect_engine(df, **context)\n</code></pre> <p>Detects the engine type of the given DataFrame based on its module.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <p>The DataFrame object whose engine type is to be detected.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>A string representing the detected engine type. Possible values are: - \"pyspark_engine\" for PySpark DataFrames - \"dask_engine\" for Dask DataFrames - \"polars_engine\" for Polars DataFrames - \"pandas_engine\" for Pandas DataFrames - \"duckdb_engine\" for DuckDB or BigQuery DataFrames</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the DataFrame type is unsupported.</p>"},{"location":"sumeh/core/utils/#sumeh.core.utils.__parse_databricks_uri","title":"__parse_databricks_uri","text":"<pre><code>__parse_databricks_uri(uri: str) -&gt; Dict[str, Optional[str]]\n</code></pre> <p>Parses a Databricks URI into its catalog, schema, and table components.</p> <p>The URI is expected to follow the format <code>protocol://catalog.schema.table</code> or <code>protocol://schema.table</code>. If the catalog is not provided, it will be set to <code>None</code>. If the schema is not provided, the current database from the active Spark session will be used.</p> <p>Parameters:</p> Name Type Description Default <code>uri</code> <code>str</code> <p>The Databricks URI to parse.</p> required <p>Returns:</p> Type Description <code>Dict[str, Optional[str]]</code> <p>Dict[str, Optional[str]]: A dictionary containing the parsed components: - \"catalog\" (Optional[str]): The catalog name, or <code>None</code> if not provided. - \"schema\" (Optional[str]): The schema name, or the current database if not provided. - \"table\" (Optional[str]): The table name.</p>"},{"location":"sumeh/dash/","title":"Dashboard","text":""},{"location":"sumeh/dash/#sumeh.dash.app","title":"sumeh.dash.app","text":"<p>Sumeh Data Quality Dashboard - The Ultimate Validation Experience</p>"},{"location":"sumeh/dash/#sumeh.dash.app.launch_dashboard","title":"launch_dashboard","text":"<pre><code>launch_dashboard(validation_results=None, summary=None, metadata=None) -&gt; None\n</code></pre> <p>Launch the ultimate Sumeh dashboard!</p> <p>Parameters:</p> Name Type Description Default <code>validation_results</code> <p>Results from validate() function</p> <code>None</code> <code>summary</code> <p>Validation summary</p> <code>None</code> <code>metadata</code> <p>Additional metadata</p> <code>None</code>"},{"location":"sumeh/engines/bigquery/","title":"bigquery","text":""},{"location":"sumeh/engines/bigquery/#sumeh.engines.bigquery_engine","title":"sumeh.engines.bigquery_engine","text":"<p>BigQuery data quality validation engine for Sumeh.</p> <p>This module provides validation functions for data quality rules in BigQuery using SQLGlot for SQL generation. It supports various validation types including completeness, uniqueness, pattern matching, date validations, and numeric comparisons.</p>"},{"location":"sumeh/engines/bigquery/#sumeh.engines.bigquery_engine.__RuleCtx","title":"__RuleCtx  <code>dataclass</code>","text":"<pre><code>__RuleCtx(column: Any, value: Any, name: str)\n</code></pre> <p>Context for validation rule execution.</p> <p>Attributes:</p> Name Type Description <code>column</code> <code>Any</code> <p>Column name(s) to validate (str or list of str)</p> <code>value</code> <code>Any</code> <p>Threshold or comparison value for the rule</p> <code>name</code> <code>str</code> <p>Check type identifier</p>"},{"location":"sumeh/engines/bigquery/#sumeh.engines.bigquery_engine.__rules_to_bq_sql","title":"__rules_to_bq_sql","text":"<pre><code>__rules_to_bq_sql(rules: List[Dict]) -&gt; str\n</code></pre> <p>Converts rule definitions into SQL representation using SQLGlot.</p> <p>Generates SQL query that represents each rule as a row with columns: col, rule, pass_threshold, value</p> <p>Parameters:</p> Name Type Description Default <code>rules</code> <code>List[Dict]</code> <p>List of validation rule dictionaries</p> required <p>Returns:</p> Type Description <code>str</code> <p>SQL query string with DISTINCT rule definitions</p>"},{"location":"sumeh/engines/bigquery/#sumeh.engines.bigquery_engine.count_rows","title":"count_rows","text":"<pre><code>count_rows(client: Client, table_ref: str) -&gt; int\n</code></pre> <p>Counts total number of rows in a BigQuery table using SQLGlot.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Client</code> <p>Authenticated BigQuery client instance</p> required <code>table_ref</code> <code>str</code> <p>Fully qualified table reference (project.dataset.table)</p> required <p>Returns:</p> Type Description <code>int</code> <p>Total row count as integer</p>"},{"location":"sumeh/engines/bigquery/#sumeh.engines.bigquery_engine.extract_schema","title":"extract_schema","text":"<pre><code>extract_schema(table: Table) -&gt; List[Dict[str, Any]]\n</code></pre> <p>Extracts schema definition from BigQuery table object.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>Table</code> <p>BigQuery Table object with schema information</p> required <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List of schema field dictionaries, each containing: - field: Field name - data_type: BigQuery data type - nullable: Whether field allows NULL values - max_length: Always None (reserved for future use)</p>"},{"location":"sumeh/engines/bigquery/#sumeh.engines.bigquery_engine.has_cardinality","title":"has_cardinality","text":"<pre><code>has_cardinality(client: Client, table_ref: str, rule: RuleDef) -&gt; dict\n</code></pre> <p>Checks if cardinality meets expectations.</p>"},{"location":"sumeh/engines/bigquery/#sumeh.engines.bigquery_engine.has_max","title":"has_max","text":"<pre><code>has_max(client: Client, table_ref: str, rule: RuleDef) -&gt; dict\n</code></pre> <p>Checks if maximum value meets expectations.</p>"},{"location":"sumeh/engines/bigquery/#sumeh.engines.bigquery_engine.has_mean","title":"has_mean","text":"<pre><code>has_mean(client: Client, table_ref: str, rule: RuleDef) -&gt; dict\n</code></pre> <p>Checks if mean meets expectations.</p>"},{"location":"sumeh/engines/bigquery/#sumeh.engines.bigquery_engine.has_min","title":"has_min","text":"<pre><code>has_min(client: Client, table_ref: str, rule: RuleDef) -&gt; dict\n</code></pre> <p>Checks if minimum value meets expectations.</p>"},{"location":"sumeh/engines/bigquery/#sumeh.engines.bigquery_engine.has_std","title":"has_std","text":"<pre><code>has_std(client: Client, table_ref: str, rule: RuleDef) -&gt; dict\n</code></pre> <p>Checks if standard deviation meets expectations.</p>"},{"location":"sumeh/engines/bigquery/#sumeh.engines.bigquery_engine.has_sum","title":"has_sum","text":"<pre><code>has_sum(client: Client, table_ref: str, rule: RuleDef) -&gt; dict\n</code></pre> <p>Checks if sum meets expectations.</p>"},{"location":"sumeh/engines/bigquery/#sumeh.engines.bigquery_engine.summarize","title":"summarize","text":"<pre><code>summarize(rules: List[RuleDef], total_rows: int, df_with_errors: Optional[RowIterator] = None, table_error: Optional[DataFrame] = None) -&gt; pd.DataFrame\n</code></pre> <p>Summarizes validation results from both row-level and table-level checks.</p> <p>Parameters:</p> Name Type Description Default <code>rules</code> <code>List[RuleDef]</code> <p>List of all validation rules</p> required <code>total_rows</code> <code>int</code> <p>Total number of rows in table</p> required <code>df_with_errors</code> <code>Optional[RowIterator]</code> <p>Row iterator with row-level violations</p> <code>None</code> <code>table_error</code> <code>Optional[DataFrame]</code> <p>DataFrame with table-level results</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Summary DataFrame with aggregated metrics</p>"},{"location":"sumeh/engines/bigquery/#sumeh.engines.bigquery_engine.validate","title":"validate","text":"<pre><code>validate(client: Client, table_ref: str, rules: List[Dict]) -&gt; Tuple[bigquery.table.RowIterator, bigquery.table.RowIterator]\n</code></pre> <p>Validates BigQuery table data against specified quality rules.</p> <p>Executes two queries: 1. Raw violations - all violating rows with individual dq_status 2. Aggregated violations - rows grouped with concatenated dq_status</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Client</code> <p>Authenticated BigQuery client instance</p> required <code>table_ref</code> <code>str</code> <p>Fully qualified table reference (project.dataset.table)</p> required <code>rules</code> <code>List[Dict]</code> <p>List of validation rule dictionaries</p> required <p>Returns:</p> Type Description <code>Tuple[RowIterator, RowIterator]</code> <p>Tuple containing: - Aggregated results with grouped violations - Raw results with individual violations</p>"},{"location":"sumeh/engines/bigquery/#sumeh.engines.bigquery_engine.validate_row_level","title":"validate_row_level","text":"<pre><code>validate_row_level(client: Client, table_ref: str, rules: List[RuleDef]) -&gt; Tuple[bigquery.table.RowIterator, bigquery.table.RowIterator]\n</code></pre> <p>Validates BigQuery table at row level using specified rules.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Client</code> <p>BigQuery client instance</p> required <code>table_ref</code> <code>str</code> <p>Fully qualified table reference</p> required <code>rules</code> <code>List[RuleDef]</code> <p>List of row-level validation rules</p> required <p>Returns:</p> Type Description <code>Tuple[RowIterator, RowIterator]</code> <p>Tuple of (aggregated results, raw violations)</p>"},{"location":"sumeh/engines/bigquery/#sumeh.engines.bigquery_engine.validate_schema","title":"validate_schema","text":"<pre><code>validate_schema(client: Client, expected: List[Dict[str, Any]], table_ref: str) -&gt; tuple[bool, list[dict[str, Any]]]\n</code></pre> <p>Validates BigQuery table schema against expected schema definition.</p> <p>Compares actual table schema with expected schema and identifies mismatches in field names, data types, and nullability constraints.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Client</code> <p>Authenticated BigQuery client instance</p> required <code>expected</code> <code>List[Dict[str, Any]]</code> <p>List of expected schema field dictionaries</p> required <code>table_ref</code> <code>str</code> <p>Fully qualified table reference (project.dataset.table)</p> required <p>Returns:</p> Type Description <code>tuple[bool, list[dict[str, Any]]]</code> <p>Tuple containing: - Boolean indicating if schemas match exactly - List of error dictionaries describing any mismatches</p>"},{"location":"sumeh/engines/bigquery/#sumeh.engines.bigquery_engine.validate_table_level","title":"validate_table_level","text":"<pre><code>validate_table_level(client: Client, table_ref: str, rules: List[RuleDef]) -&gt; pd.DataFrame\n</code></pre> <p>Validates BigQuery table at table level using specified rules.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Client</code> <p>BigQuery client instance</p> required <code>table_ref</code> <code>str</code> <p>Fully qualified table reference</p> required <code>rules</code> <code>List[RuleDef]</code> <p>List of table-level validation rules</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with validation results</p>"},{"location":"sumeh/engines/dask/","title":"dask","text":""},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine","title":"sumeh.engines.dask_engine","text":"<p>Dask data quality validation engine for Sumeh.</p> <p>This module provides validation functions for data quality rules in Dask DataFrames. It supports row-level and table-level validations including completeness, uniqueness, pattern matching, date validations, and numeric comparisons.</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.all_date_checks","title":"all_date_checks","text":"<pre><code>all_date_checks(df: DataFrame, rule: RuleDef) -&gt; dd.DataFrame\n</code></pre> <p>Applies date validation checks on a Dask DataFrame based on the provided rule.</p> <p>This function serves as an alias for the <code>is_past_date</code> function, which performs checks to determine if dates in the DataFrame meet the specified criteria.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The Dask DataFrame containing the data to be validated.</p> required <code>rule</code> <code>RuleDef</code> <p>A dictionary specifying the validation rules to be applied.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A Dask DataFrame with the results of the date validation checks.</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.are_complete","title":"are_complete","text":"<pre><code>are_complete(df: DataFrame, rule: RuleDef) -&gt; dd.DataFrame\n</code></pre> <p>Filters rows where any of the specified fields are null (violation).</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input Dask DataFrame to validate</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing fields list, check_type, and value</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with violations and dq_status column</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.are_unique","title":"are_unique","text":"<pre><code>are_unique(df: DataFrame, rule: RuleDef) -&gt; dd.DataFrame\n</code></pre> <p>Identifies duplicate rows based on combination of specified fields.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input Dask DataFrame to validate</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing fields list, check_type, and value</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with violations and dq_status column</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.extract_schema","title":"extract_schema","text":"<pre><code>extract_schema(df: DataFrame) -&gt; List[Dict[str, Any]]\n</code></pre> <p>Extracts schema from Dask DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input Dask DataFrame</p> required <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List of dictionaries containing field information</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.has_cardinality","title":"has_cardinality","text":"<pre><code>has_cardinality(df: DataFrame, rule: RuleDef) -&gt; dd.DataFrame\n</code></pre> <p>Checks if the cardinality (number of unique values) of a specified field in a Dask DataFrame exceeds a given threshold and returns a modified DataFrame based on the result.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to evaluate.</p> required <code>rule</code> <code>RuleDef</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The column name to check cardinality for. - 'check' (str): A descriptive label for the check (used in the output). - 'value' (int): The maximum allowed cardinality.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: If the cardinality of the specified field exceeds the given value,</p> <code>DataFrame</code> <p>returns the original DataFrame with an additional column <code>dq_status</code> indicating</p> <code>DataFrame</code> <p>the rule violation. Otherwise, returns an empty DataFrame with the same structure</p> <code>DataFrame</code> <p>as the input DataFrame.</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.has_entropy","title":"has_entropy","text":"<pre><code>has_entropy(df: DataFrame, rule: RuleDef) -&gt; dd.DataFrame\n</code></pre> <p>Evaluates the entropy of a specified field in a Dask DataFrame and applies a rule to determine if the entropy exceeds a given threshold. If the threshold is exceeded, a new column <code>dq_status</code> is added to the DataFrame with information about the rule violation. Otherwise, an empty DataFrame is returned.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to evaluate.</p> required <code>rule</code> <code>RuleDef</code> <p>A dictionary containing the rule parameters. It should include: - <code>field</code> (str): The column name to evaluate. - <code>check</code> (str): The type of check being performed (used for status message). - <code>value</code> (float): The threshold value for the entropy.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A DataFrame with the <code>dq_status</code> column added if the entropy exceeds the threshold,</p> <code>DataFrame</code> <p>or an empty DataFrame if the threshold is not exceeded.</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.has_infogain","title":"has_infogain","text":"<pre><code>has_infogain(df: DataFrame, rule: RuleDef) -&gt; dd.DataFrame\n</code></pre> <p>Evaluates whether a given field in a Dask DataFrame satisfies an information gain condition based on the specified rule. If the condition is met, the DataFrame is updated with a <code>dq_status</code> column indicating the rule applied. Otherwise, an empty DataFrame is returned.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to evaluate.</p> required <code>rule</code> <code>RuleDef</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The column name to evaluate. - 'check' (str): The type of check being performed (used for status annotation). - 'value' (float): The threshold value for the information gain.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: The original DataFrame with an added <code>dq_status</code> column if the condition</p> <code>DataFrame</code> <p>is met, or an empty DataFrame if the condition is not satisfied.</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.has_max","title":"has_max","text":"<pre><code>has_max(df: DataFrame, rule: RuleDef) -&gt; dd.DataFrame\n</code></pre> <p>Identifies rows in a Dask DataFrame where the value of a specified field exceeds a given maximum value.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to be checked.</p> required <code>rule</code> <code>RuleDef</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The name of the column to check. - 'check': A string describing the check (e.g., 'max'). - 'value': The maximum allowable value for the specified field.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame containing only the rows that violate the rule.           An additional column <code>dq_status</code> is added to indicate the rule violation           in the format \"{field}:{check}:{value}\".</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.has_mean","title":"has_mean","text":"<pre><code>has_mean(df: DataFrame, rule: RuleDef) -&gt; dd.DataFrame\n</code></pre> <p>Checks if the mean of a specified field in a Dask DataFrame satisfies a given condition.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to evaluate.</p> required <code>rule</code> <code>RuleDef</code> <p>A dictionary containing the rule to apply. It should include: - 'field' (str): The column name to calculate the mean for. - 'check' (str): The type of check to perform (e.g., 'greater_than'). - 'value' (float): The threshold value to compare the mean against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame with an additional column <code>dq_status</code> if the mean</p> <code>DataFrame</code> <p>satisfies the condition. If the condition is not met, an empty Dask DataFrame is returned.</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.has_min","title":"has_min","text":"<pre><code>has_min(df: DataFrame, rule: RuleDef) -&gt; dd.DataFrame\n</code></pre> <p>Checks if the values in a specified field of a Dask DataFrame are greater than or equal to a given minimum value. Returns a DataFrame containing rows that violate this rule, with an additional column indicating the data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to validate.</p> required <code>rule</code> <code>RuleDef</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The column name to check. - 'check': The type of check being performed (e.g., 'min'). - 'value': The minimum value to compare against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A DataFrame containing rows that do not meet the minimum value</p> <code>DataFrame</code> <p>requirement, with an additional column <code>dq_status</code> indicating the rule</p> <code>DataFrame</code> <p>violation in the format \"field:check:value\".</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.has_pattern","title":"has_pattern","text":"<pre><code>has_pattern(df: DataFrame, rule: RuleDef) -&gt; dd.DataFrame\n</code></pre> <p>Identifies rows in a Dask DataFrame that do not match a specified pattern.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to be checked.</p> required <code>rule</code> <code>RuleDef</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The column name in the DataFrame to apply the pattern check. - 'check': A descriptive label for the type of check being performed. - 'value': The regex pattern to match against the specified column.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A DataFrame containing rows that do not match the specified pattern.           An additional column <code>dq_status</code> is added, indicating the rule details           in the format \"{field}:{check}:{value}\".</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.has_std","title":"has_std","text":"<pre><code>has_std(df: DataFrame, rule: RuleDef) -&gt; dict[str, str | float | None | Any] | dict[str, str | None | Any]\n</code></pre> <p>Checks if the standard deviation of a specified field in a Dask DataFrame exceeds a given value.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to evaluate.</p> required <code>rule</code> <code>RuleDef</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The name of the column to calculate the standard deviation for. - 'check' (str): A descriptive label for the check being performed. - 'value' (float): The threshold value for the standard deviation.</p> required <p>Returns:</p> Type Description <code>dict[str, str | float | None | Any] | dict[str, str | None | Any]</code> <p>dd.DataFrame: - If the standard deviation of the specified field exceeds the given value,   returns the original DataFrame with an additional column <code>dq_status</code> indicating the rule details. - If the standard deviation does not exceed the value, returns an empty DataFrame with the same structure.</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.has_sum","title":"has_sum","text":"<pre><code>has_sum(df: DataFrame, rule: RuleDef) -&gt; dd.DataFrame\n</code></pre> <p>Checks if the sum of a specified field in a Dask DataFrame exceeds a given value and returns a modified DataFrame with a status column if the condition is met.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to evaluate.</p> required <code>rule</code> <code>RuleDef</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The column name to sum. - 'check' (str): A descriptive label for the check (used in the status message). - 'value' (float): The threshold value to compare the sum against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame. If the sum exceeds the threshold, the DataFrame</p> <code>DataFrame</code> <p>will include a <code>dq_status</code> column with a status message. Otherwise, an empty</p> <code>DataFrame</code> <p>DataFrame with the same structure as the input is returned.</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.is_between","title":"is_between","text":"<pre><code>is_between(df: DataFrame, rule: RuleDef) -&gt; dd.DataFrame\n</code></pre> <p>Filters a Dask DataFrame to identify rows where a specified field's value does not fall within a given range.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to be checked.</p> required <code>rule</code> <code>RuleDef</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The column name in the DataFrame to check. - 'check': The type of check being performed (e.g., \"between\"). - 'value': A string representing the range in the format \"[lo,hi]\".</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new DataFrame containing only the rows that violate</p> <code>DataFrame</code> <p>the specified range condition. An additional column <code>dq_status</code> is</p> <code>DataFrame</code> <p>added to indicate the field, check, and value that caused the violation.</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.is_complete","title":"is_complete","text":"<pre><code>is_complete(df: DataFrame, rule: RuleDef) -&gt; dd.DataFrame\n</code></pre> <p>Filters rows where the specified field is null (violation).</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input Dask DataFrame to validate</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, check_type, and value</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with violations and dq_status column</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.is_composite_key","title":"is_composite_key","text":"<pre><code>is_composite_key(df: DataFrame, rule: RuleDef) -&gt; dd.DataFrame\n</code></pre> <p>Determines if the given DataFrame satisfies the composite key condition based on the provided rule.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A Dask DataFrame to be checked.</p> required <code>rule</code> <code>RuleDef</code> <p>A dictionary defining the composite key rule.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A Dask DataFrame indicating whether the composite key condition is met.</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.is_contained_in","title":"is_contained_in","text":"<pre><code>is_contained_in(df: DataFrame, rule: RuleDef) -&gt; dd.DataFrame\n</code></pre> <p>Filters a Dask DataFrame to identify rows where the values in a specified field are not contained within a given list of allowed values.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to be checked.</p> required <code>rule</code> <code>RuleDef</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The column name in the DataFrame to check. - 'check': The type of check being performed (e.g., \"is_contained_in\"). - 'value': A string representation of a list of allowed values (e.g., \"[value1, value2]\").</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame containing only the rows that violate the rule.</p> <code>DataFrame</code> <p>An additional column <code>dq_status</code> is added to indicate the rule violation in the format:</p> <code>DataFrame</code> <p>\"{field}:{check}:{value}\".</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.is_date_after","title":"is_date_after","text":"<pre><code>is_date_after(df: DataFrame, rule: RuleDef) -&gt; dd.DataFrame\n</code></pre> <p>Filters a Dask DataFrame to identify rows where a specified date field is earlier than a given reference date.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to be checked.</p> required <code>rule</code> <code>RuleDef</code> <p>A dictionary containing the rule parameters. It should include: - field (str): The name of the column to check. - check (str): A descriptive label for the check (used in the   output status). - date_str (str): The reference date as a string in a format   compatible with <code>pd.Timestamp</code>.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame containing only the rows where the</p> <code>DataFrame</code> <p>specified date field is earlier than the reference date. An additional</p> <code>DataFrame</code> <p>column <code>dq_status</code> is added, which contains a string describing the</p> <code>DataFrame</code> <p>rule violation in the format <code>field:check:date_str</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the <code>rule</code> dictionary does not contain the required keys.</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.is_date_before","title":"is_date_before","text":"<pre><code>is_date_before(df: DataFrame, rule: RuleDef) -&gt; dd.DataFrame\n</code></pre> <p>Checks if the values in a specified date column of a Dask DataFrame are before a given reference date.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame containing the data to be validated.</p> required <code>rule</code> <code>RuleDef</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The name of the column to check. - 'check': A descriptive string for the check (e.g., \"is_date_before\"). - 'value': The reference date as a string in a format parsable by pandas.Timestamp.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame containing only the rows where the date in the specified column</p> <code>DataFrame</code> <p>is after the reference date. An additional column 'dq_status' is added to indicate the validation</p> <code>DataFrame</code> <p>status in the format \"{field}:{check}:{value}\".</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.is_date_between","title":"is_date_between","text":"<pre><code>is_date_between(df: DataFrame, rule: RuleDef) -&gt; dd.DataFrame\n</code></pre> <p>Filters a Dask DataFrame to identify rows where a date field does not fall within a specified range.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame containing the data to be checked.</p> required <code>rule</code> <code>RuleDef</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The name of the column in the DataFrame to check. - 'check': A string representing the type of check (used for status annotation). - 'val': A string representing the date range in the format \"[start_date, end_date]\".</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A DataFrame containing rows where the date field does not fall within the specified range.           An additional column 'dq_status' is added to indicate the rule violation in the format           \"{field}:{check}:{val}\".</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.is_equal","title":"is_equal","text":"<pre><code>is_equal(df: DataFrame, rule: RuleDef) -&gt; dd.DataFrame\n</code></pre> <p>Filters a Dask DataFrame to identify rows where a specified field does not equal a given value.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to be checked.</p> required <code>rule</code> <code>RuleDef</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The column name in the DataFrame to be checked. - 'check': The type of check to perform (expected to be 'equal' for this function). - 'value': The value to compare against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new DataFrame containing rows that violate the equality rule.           An additional column <code>dq_status</code> is added, indicating the rule details           in the format \"{field}:{check}:{value}\".</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.is_equal_than","title":"is_equal_than","text":"<pre><code>is_equal_than(df: DataFrame, rule: RuleDef) -&gt; dd.DataFrame\n</code></pre> <p>Filters a Dask DataFrame to identify rows where the specified field does not equal the given value.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to be checked.</p> required <code>rule</code> <code>RuleDef</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The column name in the DataFrame to check. - 'check': The type of check being performed (expected to be 'equal' for this function). - 'value': The value to compare against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new DataFrame containing rows that violate the equality rule.           An additional column <code>dq_status</code> is added, indicating the rule details           in the format \"{field}:{check}:{value}\".</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.is_future_date","title":"is_future_date","text":"<pre><code>is_future_date(df: DataFrame, rule: RuleDef) -&gt; dd.DataFrame\n</code></pre> <p>Checks for rows in a Dask DataFrame where the specified date field contains a future date.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to validate.</p> required <code>rule</code> <code>RuleDef</code> <p>A dictionary containing the rule parameters. It is expected to include: - field: The name of the column to check. - check: A descriptive label for the check (used in the output). - _: Additional parameters (ignored in this function).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame containing only the rows where the date in the specified</p> <code>DataFrame</code> <p>field is in the future. An additional column <code>dq_status</code> is added to indicate the status</p> <code>DataFrame</code> <p>of the validation in the format: \"::\". Notes <ul> <li>The function coerces the specified column to datetime format, and invalid parsing results   in NaT (Not a Time).</li> <li>Rows with NaT in the specified column are excluded from the output.</li> <li>The current date is determined using the system's local date.</li> </ul>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.is_greater_or_equal_than","title":"is_greater_or_equal_than","text":"<pre><code>is_greater_or_equal_than(df: DataFrame, rule: RuleDef) -&gt; dd.DataFrame\n</code></pre> <p>Filters a Dask DataFrame to identify rows where a specified field's value is less than a given threshold, and annotates the resulting rows with a data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to be checked.</p> required <code>rule</code> <code>RuleDef</code> <p>A dictionary containing the rule parameters. It should          include the following keys:          - 'field': The column name in the DataFrame to check.          - 'check': The type of check being performed (e.g., 'greater_or_equal').          - 'value': The threshold value to compare against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame containing only the rows that           violate the rule, with an additional column <code>dq_status</code>           indicating the field, check type, and threshold value.</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.is_greater_than","title":"is_greater_than","text":"<pre><code>is_greater_than(df: DataFrame, rule: RuleDef) -&gt; dd.DataFrame\n</code></pre> <p>Filters a Dask DataFrame to identify rows where the value in a specified field is greater than a given threshold and annotates the result with a data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to be checked.</p> required <code>rule</code> <code>RuleDef</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The column name to check. - 'check' (str): The type of check being performed (e.g., 'greater_than'). - 'value' (numeric): The threshold value to compare against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A filtered DataFrame containing rows that violate the rule,</p> <code>DataFrame</code> <p>with an additional column <code>dq_status</code> indicating the rule details in the format</p> <code>DataFrame</code> <p>\"{field}:{check}:{value}\".</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.is_in","title":"is_in","text":"<pre><code>is_in(df: DataFrame, rule: RuleDef) -&gt; dd.DataFrame\n</code></pre> <p>Checks if the specified rule is contained within the given Dask DataFrame.</p> <p>This function acts as a wrapper for the <code>is_contained_in</code> function, passing the provided DataFrame and rule to it.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The Dask DataFrame to evaluate.</p> required <code>rule</code> <code>RuleDef</code> <p>A dictionary representing the rule to check against the DataFrame.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A Dask DataFrame resulting from the evaluation of the rule.</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.is_in_billions","title":"is_in_billions","text":"<pre><code>is_in_billions(df: DataFrame, rule: RuleDef) -&gt; dd.DataFrame\n</code></pre> <p>Identifies rows in a Dask DataFrame where the value in a specified field is greater than or equal to one billion and marks them with a data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to be checked.</p> required <code>rule</code> <code>RuleDef</code> <p>A dictionary containing the rule parameters. It is expected          to include the field name, check type, and value.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A Dask DataFrame containing only the rows where the specified           field's value is greater than or equal to one billion. An           additional column <code>dq_status</code> is added, indicating the field,           check type, and value that triggered the rule.</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.is_in_millions","title":"is_in_millions","text":"<pre><code>is_in_millions(df: DataFrame, rule: RuleDef) -&gt; dd.DataFrame\n</code></pre> <p>Checks if the values in a specified field of a Dask DataFrame are in the millions (greater than or equal to 1,000,000) and returns a DataFrame of violations.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to check.</p> required <code>rule</code> <code>RuleDef</code> <p>A dictionary containing the rule parameters. It is expected to          include the field name, check type, and value.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A DataFrame containing rows where the specified field's value           is greater than or equal to 1,000,000. An additional column           <code>dq_status</code> is added to indicate the field, check, and value           that triggered the violation.</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.is_legit","title":"is_legit","text":"<pre><code>is_legit(df: DataFrame, rule: RuleDef) -&gt; dd.DataFrame\n</code></pre> <p>Validates a Dask DataFrame against a specified rule and returns rows that violate the rule.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The Dask DataFrame to validate.</p> required <code>rule</code> <code>RuleDef</code> <p>A dictionary containing the validation rule. It should include: - 'field': The column name in the DataFrame to validate. - 'check': The type of validation check (e.g., regex, condition). - 'value': The value or pattern to validate against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new DataFrame containing rows that violate the rule, with an additional</p> <code>DataFrame</code> <p>column <code>dq_status</code> indicating the field, check, and value of the failed validation.</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.is_less_or_equal_than","title":"is_less_or_equal_than","text":"<pre><code>is_less_or_equal_than(df: DataFrame, rule: RuleDef) -&gt; dd.DataFrame\n</code></pre> <p>Filters a Dask DataFrame to identify rows where the value in a specified field is greater than a given threshold, violating a \"less or equal than\" rule.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to be checked.</p> required <code>rule</code> <code>RuleDef</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The column name in the DataFrame to be checked. - 'check': The type of check being performed (e.g., \"less_or_equal_than\"). - 'value': The threshold value to compare against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new DataFrame containing only the rows that violate the rule.</p> <code>DataFrame</code> <p>An additional column <code>dq_status</code> is added to indicate the rule violation</p> <code>DataFrame</code> <p>in the format \"{field}:{check}:{value}\".</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.is_less_than","title":"is_less_than","text":"<pre><code>is_less_than(df: DataFrame, rule: RuleDef) -&gt; dd.DataFrame\n</code></pre> <p>Filters a Dask DataFrame to identify rows where the value in a specified field is greater than or equal to a given threshold, and marks them with a data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to be checked.</p> required <code>rule</code> <code>RuleDef</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The column name to check. - 'check' (str): The type of check being performed (e.g., \"less_than\"). - 'value' (numeric): The threshold value for the check.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame containing only the rows that violate the rule,</p> <code>DataFrame</code> <p>with an additional column <code>dq_status</code> indicating the rule that was violated in the</p> <code>DataFrame</code> <p>format \"{field}:{check}:{value}\".</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.is_negative","title":"is_negative","text":"<pre><code>is_negative(df: DataFrame, rule: RuleDef) -&gt; dd.DataFrame\n</code></pre> <p>Filters rows where the specified field is non-negative (violation).</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input Dask DataFrame to validate</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, check_type, and value</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with violations and dq_status column</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.is_on_friday","title":"is_on_friday","text":"<pre><code>is_on_friday(df: DataFrame, rule: RuleDef) -&gt; dd.DataFrame\n</code></pre> <p>Filters a Dask DataFrame to identify rows where a specified date field falls on a Friday.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame containing the data to be checked.</p> required <code>rule</code> <code>RuleDef</code> <p>A dictionary containing the rule parameters. It is expected to have the following keys: - field (str): The name of the column in the DataFrame to check. - check (str): A descriptive string for the check being performed. - value (str): A value associated with the rule, used for status annotation.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame containing only the rows where the specified</p> <code>DataFrame</code> <p>date field falls on a Friday. An additional column <code>dq_status</code> is added to the</p> <code>DataFrame</code> <p>DataFrame, containing a string in the format \"{field}:{check}:{value}\" to indicate</p> <code>DataFrame</code> <p>the rule applied.</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.is_on_monday","title":"is_on_monday","text":"<pre><code>is_on_monday(df: DataFrame, rule: RuleDef) -&gt; dd.DataFrame\n</code></pre> <p>Filters a Dask DataFrame to identify rows where the date in a specified column falls on a Monday.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame containing the data to be checked.</p> required <code>rule</code> <code>RuleDef</code> <p>A dictionary containing the rule parameters. It is expected to include: - 'field': The name of the column in the DataFrame to check. - 'check': A string representing the type of check (used for status assignment). - 'value': A value associated with the rule (used for status assignment).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame containing only the rows where the date in the specified</p> <code>DataFrame</code> <p>column falls on a Monday. An additional column <code>dq_status</code> is added to indicate the rule</p> <code>DataFrame</code> <p>applied in the format \"{field}:{check}:{value}\".</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.is_on_saturday","title":"is_on_saturday","text":"<pre><code>is_on_saturday(df: DataFrame, rule: RuleDef) -&gt; dd.DataFrame\n</code></pre> <p>Filters a Dask DataFrame to identify rows where the date in a specified column falls on a Saturday.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame containing the data to be checked.</p> required <code>rule</code> <code>RuleDef</code> <p>A dictionary containing the rule parameters. It is expected to include: - 'field': The name of the column in the DataFrame to check. - 'check': A string representing the type of check (used for status assignment). - 'value': A value associated with the rule (used for status assignment).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame containing only the rows where the date in the specified</p> <code>DataFrame</code> <p>column falls on a Saturday. An additional column <code>dq_status</code> is added to indicate the rule</p> <code>DataFrame</code> <p>applied in the format \"{field}:{check}:{value}\".</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.is_on_sunday","title":"is_on_sunday","text":"<pre><code>is_on_sunday(df: DataFrame, rule: RuleDef) -&gt; dd.DataFrame\n</code></pre> <p>Filters a Dask DataFrame to identify rows where a specified date field falls on a Sunday.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame containing the data to be checked.</p> required <code>rule</code> <code>RuleDef</code> <p>A dictionary containing the rule parameters. It is expected to include: - field (str): The name of the column in the DataFrame to check. - check (str): A descriptive string for the check being performed. - value (str): A value associated with the rule, used for status annotation.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame containing only the rows where the specified</p> <code>DataFrame</code> <p>date field falls on a Sunday. An additional column <code>dq_status</code> is added to indicate</p> <code>DataFrame</code> <p>the rule applied in the format \"{field}:{check}:{value}\".</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.is_on_thursday","title":"is_on_thursday","text":"<pre><code>is_on_thursday(df: DataFrame, rule: RuleDef) -&gt; dd.DataFrame\n</code></pre> <p>Filters a Dask DataFrame to identify rows where the specified date field falls on a Thursday.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame containing the data to be checked.</p> required <code>rule</code> <code>RuleDef</code> <p>A dictionary containing the rule parameters. It is expected to include: - field (str): The name of the column in the DataFrame to check. - check (str): A descriptive string for the type of check being performed. - value (str): A value associated with the rule (not used in the logic but included in the output).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame containing only the rows where the specified date field</p> <code>DataFrame</code> <p>falls on a Thursday. An additional column <code>dq_status</code> is added to indicate the rule applied</p> <code>DataFrame</code> <p>in the format \"{field}:{check}:{value}\".</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.is_on_tuesday","title":"is_on_tuesday","text":"<pre><code>is_on_tuesday(df: DataFrame, rule: RuleDef) -&gt; dd.DataFrame\n</code></pre> <p>Filters a Dask DataFrame to identify rows where the specified date field falls on a Tuesday.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame containing the data to be checked.</p> required <code>rule</code> <code>RuleDef</code> <p>A dictionary containing the rule parameters. It is expected to include: - 'field': The name of the column in the DataFrame to check. - 'check': A string representing the type of check (used for status annotation). - 'value': A value associated with the rule (used for status annotation).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame containing only the rows where the specified date field</p> <code>DataFrame</code> <p>falls on a Tuesday. An additional column <code>dq_status</code> is added to indicate the rule applied</p> <code>DataFrame</code> <p>in the format \"{field}:{check}:{value}\".</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.is_on_wednesday","title":"is_on_wednesday","text":"<pre><code>is_on_wednesday(df: DataFrame, rule: RuleDef) -&gt; dd.DataFrame\n</code></pre> <p>Filters a Dask DataFrame to identify rows where the date in a specified column falls on a Wednesday.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame containing the data to be checked.</p> required <code>rule</code> <code>RuleDef</code> <p>A dictionary containing the rule parameters. It is expected to include: - <code>field</code> (str): The name of the column in the DataFrame to check. - <code>check</code> (str): A descriptive string for the check being performed. - <code>value</code> (str): A value associated with the rule (not directly used in the function).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame containing only the rows where the date in the specified column</p> <code>DataFrame</code> <p>falls on a Wednesday. An additional column <code>dq_status</code> is added to indicate the rule applied in the</p> <code>DataFrame</code> <p>format <code>{field}:{check}:{value}</code>.</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.is_on_weekday","title":"is_on_weekday","text":"<pre><code>is_on_weekday(df: DataFrame, rule: RuleDef) -&gt; dd.DataFrame\n</code></pre> <p>Filters a Dask DataFrame to include only rows where the date in the specified field falls on a weekday (Monday to Friday) and assigns a data quality status column.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame containing the data to be filtered.</p> required <code>rule</code> <code>RuleDef</code> <p>A dictionary containing the rule parameters. It is expected to have the following keys: - field (str): The name of the column in the DataFrame containing date values. - check (str): A descriptive string for the check being performed. - value (str): A value associated with the rule, used for constructing the <code>dq_status</code> column.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame containing only the rows where the date in the specified field</p> <code>DataFrame</code> <p>falls on a weekday. An additional column <code>dq_status</code> is added to indicate the rule applied.</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.is_on_weekend","title":"is_on_weekend","text":"<pre><code>is_on_weekend(df: DataFrame, rule: RuleDef) -&gt; dd.DataFrame\n</code></pre> <p>Identifies rows in a Dask DataFrame where the date in a specified column falls on a weekend.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame containing the data to be checked.</p> required <code>rule</code> <code>RuleDef</code> <p>A dictionary containing the rule parameters. It is expected to have          the following keys:          - 'field': The name of the column in the DataFrame to check.          - 'check': A string representing the type of check (used for status annotation).          - 'value': A value associated with the rule (used for status annotation).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame containing only the rows where the date in the specified           column falls on a weekend (Saturday or Sunday). An additional column <code>dq_status</code>           is added to indicate the rule applied in the format \"{field}:{check}:{value}\".</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.is_past_date","title":"is_past_date","text":"<pre><code>is_past_date(df: DataFrame, rule: RuleDef) -&gt; dd.DataFrame\n</code></pre> <p>Checks if the values in a specified date column of a Dask DataFrame are in the past.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame containing the data to be checked.</p> required <code>rule</code> <code>RuleDef</code> <p>A dictionary containing the rule parameters. It is expected to include          the field name to check, the check type, and additional parameters.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A Dask DataFrame containing rows where the date in the specified column           is in the past. An additional column <code>dq_status</code> is added to indicate           the field, check type, and the date of the check in the format           \"field:check:YYYY-MM-DD\".</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.is_positive","title":"is_positive","text":"<pre><code>is_positive(df: DataFrame, rule: RuleDef) -&gt; dd.DataFrame\n</code></pre> <p>Filters rows where the specified field is negative (violation).</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input Dask DataFrame to validate</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, check_type, and value</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with violations and dq_status column</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.is_primary_key","title":"is_primary_key","text":"<pre><code>is_primary_key(df: DataFrame, rule: RuleDef) -&gt; dd.DataFrame\n</code></pre> <p>Determines if the specified rule identifies a primary key in the given Dask DataFrame.</p> <p>This function checks whether the combination of columns specified in the rule results in unique values across the DataFrame, effectively identifying a primary key.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The Dask DataFrame to evaluate.</p> required <code>rule</code> <code>RuleDef</code> <p>A dictionary defining the rule to check for primary key uniqueness.          Typically, this includes the column(s) to be evaluated.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A Dask DataFrame indicating whether the rule satisfies the primary key condition.</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.is_t_minus_1","title":"is_t_minus_1","text":"<pre><code>is_t_minus_1(df: DataFrame, rule: RuleDef) -&gt; dd.DataFrame\n</code></pre> <p>Filters a Dask DataFrame to identify rows where a specified datetime column matches the date of \"T-1\" (yesterday) and assigns a data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame containing the data to be checked.</p> required <code>rule</code> <code>RuleDef</code> <p>A dictionary containing the rule parameters. It is expected to include the following keys: - 'field': The name of the column to check. - 'check': A string describing the check being performed. - 'value': Additional value or metadata related to the check.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame containing only the rows where the</p> <code>DataFrame</code> <p>specified column matches \"T-1\". An additional column <code>dq_status</code> is added</p> <code>DataFrame</code> <p>to indicate the data quality status in the format \"{field}:{check}:{value}\".</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.is_t_minus_2","title":"is_t_minus_2","text":"<pre><code>is_t_minus_2(df: DataFrame, rule: RuleDef) -&gt; dd.DataFrame\n</code></pre> <p>Filters a Dask DataFrame to identify rows where a specified datetime column matches the date two days prior to the current date.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame containing the data to be filtered.</p> required <code>rule</code> <code>RuleDef</code> <p>A dictionary containing the rule parameters. It is expected to include the following keys: - 'field': The name of the column in the DataFrame to check. - 'check': A string representing the type of check (used for metadata). - 'value': A value associated with the rule (used for metadata).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame containing only the rows where the specified</p> <code>DataFrame</code> <p>column matches the target date (two days prior to the current date). An additional</p> <code>DataFrame</code> <p>column <code>dq_status</code> is added to indicate the rule applied in the format</p> <code>DataFrame</code> <p>\"{field}:{check}:{value}\".</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.is_t_minus_3","title":"is_t_minus_3","text":"<pre><code>is_t_minus_3(df: DataFrame, rule: RuleDef) -&gt; dd.DataFrame\n</code></pre> <p>Filters a Dask DataFrame to identify rows where the specified date field matches exactly three days prior to the current date.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame containing the data to be checked.</p> required <code>rule</code> <code>RuleDef</code> <p>A dictionary containing rule parameters. It is expected to include          the field name to check, the type of check, and the value (unused in this function).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A filtered Dask DataFrame containing only the rows where the specified           date field matches three days prior to the current date. An additional           column <code>dq_status</code> is added to indicate the rule applied in the format           \"{field}:{check}:{value}\".</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.is_today","title":"is_today","text":"<pre><code>is_today(df: DataFrame, rule: RuleDef) -&gt; dd.DataFrame\n</code></pre> <p>Filters a Dask DataFrame to identify rows where the specified field matches today's date.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to be filtered.</p> required <code>rule</code> <code>RuleDef</code> <p>A dictionary containing the rule parameters. It is expected to have the following keys: - field (str): The name of the column in the DataFrame to check. - check (str): A descriptive label for the type of check being performed. - value (str): A descriptive label for the expected value.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame containing only the rows where the specified</p> <code>DataFrame</code> <p>field matches today's date. An additional column <code>dq_status</code> is added to indicate</p> <code>DataFrame</code> <p>the rule applied in the format \"{field}:{check}:{value}\".</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.is_unique","title":"is_unique","text":"<pre><code>is_unique(df: DataFrame, rule: RuleDef) -&gt; dd.DataFrame\n</code></pre> <p>Identifies duplicate rows based on the specified field.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input Dask DataFrame to validate</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, check_type, and value</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with violations and dq_status column</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.is_yesterday","title":"is_yesterday","text":"<pre><code>is_yesterday(df: DataFrame, rule: RuleDef) -&gt; dd.DataFrame\n</code></pre> <p>Determines if the rows in a Dask DataFrame correspond to \"yesterday\" based on a given rule.</p> <p>This function acts as a wrapper for the <code>is_t_minus_1</code> function, applying the same logic to check if the data corresponds to the previous day.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to evaluate.</p> required <code>rule</code> <code>RuleDef</code> <p>A dictionary containing the rule or criteria          to determine \"yesterday\".</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A Dask DataFrame with the evaluation results.</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.not_contained_in","title":"not_contained_in","text":"<pre><code>not_contained_in(df: DataFrame, rule: RuleDef) -&gt; dd.DataFrame\n</code></pre> <p>Filters a Dask DataFrame to identify rows where the specified field's value is contained in a given list, and assigns a data quality status to the resulting rows.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to be filtered.</p> required <code>rule</code> <code>RuleDef</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The column name in the DataFrame to check. - 'check': The type of check being performed (e.g., \"not_contained_in\"). - 'value': A string representation of a list of values to check against,   formatted as \"[value1, value2, ...]\".</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new DataFrame containing only the rows where the specified</p> <code>DataFrame</code> <p>field's value is in the provided list, with an additional column <code>dq_status</code></p> <code>DataFrame</code> <p>indicating the rule applied in the format \"field:check:value\".</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.not_in","title":"not_in","text":"<pre><code>not_in(df: DataFrame, rule: RuleDef) -&gt; dd.DataFrame\n</code></pre> <p>Filters a Dask DataFrame by excluding rows where the specified rule is satisfied.</p> <p>This function delegates the filtering logic to the <code>not_contained_in</code> function.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to be filtered.</p> required <code>rule</code> <code>RuleDef</code> <p>A dictionary defining the filtering rule. The structure and          interpretation of this rule depend on the implementation of          <code>not_contained_in</code>.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame with rows excluded based on the rule.</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.satisfies","title":"satisfies","text":"<pre><code>satisfies(df: DataFrame, rule: RuleDef) -&gt; dd.DataFrame\n</code></pre> <p>Filters a Dask DataFrame based on a rule and returns rows that do not satisfy the rule.</p> <p>The function evaluates a rule on the given Dask DataFrame and identifies rows that violate the rule. The rule is specified as a dictionary containing a field, a check, and a value. The rule's logical expression is converted to Python syntax for evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Dask DataFrame to be filtered.</p> required <code>rule</code> <code>RuleDef</code> <p>A dictionary specifying the rule to evaluate. It should contain: - 'field': The column name in the DataFrame to evaluate. - 'check': The type of check or condition to apply. - 'value': The value or expression to evaluate against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A new Dask DataFrame containing rows that do not satisfy the rule.</p> <code>DataFrame</code> <p>An additional column <code>dq_status</code> is added, which contains a string in the format</p> <code>DataFrame</code> <p>\"{field}:{check}:{value}\" to indicate the rule that was violated.</p> Example <p>import dask.dataframe as dd data = {'col1': [1, 2, 3], 'col2': [4, 5, 6]} df = dd.from_pandas(pd.DataFrame(data), npartitions=1) rule = {'field': 'col1', 'check': '&gt;', 'value': '2'} result = satisfies(df, rule) result.compute()</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.summarize","title":"summarize","text":"<pre><code>summarize(rules: List[RuleDef], total_rows: int, df_with_errors: Optional[DataFrame] = None, table_error: Optional[DataFrame] = None) -&gt; pd.DataFrame\n</code></pre> <p>Summarizes validation results from both row-level and table-level checks.</p> <p>Parameters:</p> Name Type Description Default <code>rules</code> <code>List[RuleDef]</code> <p>List of all validation rules</p> required <code>total_rows</code> <code>int</code> <p>Total number of rows in the input DataFrame</p> required <code>df_with_errors</code> <code>Optional[DataFrame]</code> <p>DataFrame with row-level violations</p> <code>None</code> <code>table_error</code> <code>Optional[DataFrame]</code> <p>DataFrame with table-level results</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Summary DataFrame with aggregated validation metrics</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.validate","title":"validate","text":"<pre><code>validate(df: DataFrame, rules: list[RuleDef]) -&gt; tuple[dd.DataFrame, dd.DataFrame]\n</code></pre> <p>Main validation function that orchestrates row-level and table-level validations.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input Dask DataFrame to validate</p> required <code>rules</code> <code>list[RuleDef]</code> <p>List of all validation rules</p> required <p>Returns:</p> Type Description <code>tuple[DataFrame, DataFrame]</code> <p>Tuple of (aggregated_violations, raw_violations, table_summary)</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.validate_date_format","title":"validate_date_format","text":"<pre><code>validate_date_format(df: DataFrame, rule: RuleDef) -&gt; dd.DataFrame\n</code></pre> <p>Validates the date format of a specified column in a Dask DataFrame.</p> <p>This function checks whether the values in a specified column of the DataFrame conform to a given date format. Rows with invalid date formats are returned with an additional column indicating the validation status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The Dask DataFrame to validate.</p> required <code>rule</code> <code>RuleDef</code> <p>A dictionary containing the validation rule. It should          include the following keys:          - 'field': The name of the column to validate.          - 'check': A string describing the validation check.          - 'fmt': The expected date format (e.g., '%Y-%m-%d').</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: A DataFrame containing rows where the date format           validation failed. An additional column <code>dq_status</code>           is added, which contains a string describing the           validation status in the format \"{field}:{check}:{fmt}\".</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.validate_row_level","title":"validate_row_level","text":"<pre><code>validate_row_level(df: DataFrame, rules: List[RuleDef]) -&gt; Tuple[dd.DataFrame, dd.DataFrame]\n</code></pre> <p>Validates DataFrame at row level using specified rules.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input Dask DataFrame to validate</p> required <code>rules</code> <code>List[RuleDef]</code> <p>List of row-level validation rules</p> required <p>Returns:</p> Type Description <code>Tuple[DataFrame, DataFrame]</code> <p>Tuple of (aggregated violations, raw violations)</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.validate_schema","title":"validate_schema","text":"<pre><code>validate_schema(df: DataFrame, expected: List[Dict[str, Any]]) -&gt; tuple[bool, list[dict[str, Any]]]\n</code></pre> <p>Validates the schema of a Dask DataFrame against an expected schema.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Dask DataFrame to validate</p> required <code>expected</code> <code>List[Dict[str, Any]]</code> <p>Expected schema definition</p> required <p>Returns:</p> Type Description <code>tuple[bool, list[dict[str, Any]]]</code> <p>Tuple of (is_valid, list_of_errors)</p>"},{"location":"sumeh/engines/dask/#sumeh.engines.dask_engine.validate_table_level","title":"validate_table_level","text":"<pre><code>validate_table_level(df: DataFrame, rules: List[RuleDef]) -&gt; pd.DataFrame\n</code></pre> <p>Validates DataFrame at table level using specified rules.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input Dask DataFrame to validate</p> required <code>rules</code> <code>List[RuleDef]</code> <p>List of table-level validation rules</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Summary DataFrame with validation results</p>"},{"location":"sumeh/engines/duckdb/","title":"duckdb","text":""},{"location":"sumeh/engines/duckdb/#sumeh.engines.duckdb_engine","title":"sumeh.engines.duckdb_engine","text":"<p>This module provides utilities for generating and validating SQL expressions and data quality rules using DuckDB. It includes functions for building SQL expressions, validating dataframes against rules, summarizing rule violations, and schema validation.</p> <p>Classes:</p> Name Description <code>__RuleCtx</code> <p>A dataclass representing the context required to generate SQL       expressions for data quality rules.</p> <p>Functions:</p> Name Description <code>__escape_single_quotes</code> <p>Escapes single quotes in a string for SQL compatibility.</p> <code>__format_sequence</code> <p>Formats a sequence (list, tuple, or string) into a SQL-compatible representation for IN/NOT IN clauses.</p> <code>_is_complete</code> <p>Generates a SQL expression to check if a column is not NULL.</p> <code>_are_complete</code> <p>Generates a SQL expression to check if all columns in a list are not NULL.</p> <code>_is_unique</code> <p>Generates a SQL expression to check if a column has unique values.</p> <code>_are_unique</code> <p>Generates a SQL expression to check if a combination of columns has unique values.</p> <code>_is_greater_than</code> <p>Generates a SQL expression to check if a column's value is greater than a given value.</p> <code>_is_less_than</code> <p>Generates a SQL expression to check if a column's value is less than a given value.</p> <code>_is_greater_or_equal_than</code> <p>Generates a SQL expression to check if a column's value is greater than or equal to a given value.</p> <code>_is_less_or_equal_than</code> <p>Generates a SQL expression to check if a column's value is less than or equal to a given value.</p> <code>_is_equal_than</code> <p>Generates a SQL expression to check if a column's value is equal to a given value.</p> <code>_is_between</code> <p>Generates a SQL expression to check if a column's value is between two values.</p> <code>_has_pattern</code> <p>Generates a SQL expression to check if a column's value matches a regular expression pattern.</p> <code>_is_contained_in</code> <p>Generates a SQL expression to check if a column's value is in a given sequence.</p> <code>_not_contained_in</code> <p>Generates a SQL expression to check if a column's value is not in a given sequence.</p> <code>_satisfies</code> <p>Generates a SQL expression based on a custom condition provided as a string.</p> <code>_build_union_sql</code> <p>Builds a SQL query that combines multiple rule-based conditions into a UNION ALL query.</p> <code>validate</code> <p>Validates a DuckDB dataframe against a set of rules and returns the results.</p> <code>summarize</code> <p>Summarizes rule violations and calculates pass rates for each rule.</p> <code>validate_schema</code> <p>Validates the schema of a DuckDB table against an expected schema.</p>"},{"location":"sumeh/engines/duckdb/#sumeh.engines.duckdb_engine.__RuleCtx","title":"__RuleCtx  <code>dataclass</code>","text":"<pre><code>__RuleCtx(column: Any, value: Any, name: str)\n</code></pre> <p>Context class used to generate SQL expressions for data quality rules.</p> <p>Attributes:</p> Name Type Description <code>column</code> <code>Any</code> <p>Column(s) to apply the rule.</p> <code>value</code> <code>Any</code> <p>Value associated with the rule.</p> <code>name</code> <code>str</code> <p>Name of the rule (check_type).</p>"},{"location":"sumeh/engines/duckdb/#sumeh.engines.duckdb_engine.__escape_single_quotes","title":"__escape_single_quotes","text":"<pre><code>__escape_single_quotes(txt: str) -&gt; str\n</code></pre> <p>Escapes single quotes for SQL compatibility.</p>"},{"location":"sumeh/engines/duckdb/#sumeh.engines.duckdb_engine.__format_sequence","title":"__format_sequence","text":"<pre><code>__format_sequence(value: Any) -&gt; List[str]\n</code></pre> <p>Formats a sequence into a list of values for IN/NOT IN clauses.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>Input value (string, list, or tuple).</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of formatted values.</p>"},{"location":"sumeh/engines/duckdb/#sumeh.engines.duckdb_engine.extract_schema","title":"extract_schema","text":"<pre><code>extract_schema(conn: DuckDBPyConnection, table: str) -&gt; List[Dict[str, Any]]\n</code></pre> <p>Extracts schema from DuckDB table.</p> <p>Parameters:</p> Name Type Description Default <code>conn</code> <code>DuckDBPyConnection</code> <p>DuckDB connection.</p> required <code>table</code> <code>str</code> <p>Table name.</p> required <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List[Dict[str, Any]]: List of dictionaries containing field information.</p>"},{"location":"sumeh/engines/duckdb/#sumeh.engines.duckdb_engine.has_cardinality","title":"has_cardinality","text":"<pre><code>has_cardinality(conn: DuckDBPyConnection, rule: RuleDef) -&gt; dict\n</code></pre> <p>Checks if the cardinality meets expectations.</p>"},{"location":"sumeh/engines/duckdb/#sumeh.engines.duckdb_engine.has_entropy","title":"has_entropy","text":"<pre><code>has_entropy(conn: DuckDBPyConnection, rule: RuleDef) -&gt; dict\n</code></pre> <p>Checks if the entropy meets expectations.</p>"},{"location":"sumeh/engines/duckdb/#sumeh.engines.duckdb_engine.has_infogain","title":"has_infogain","text":"<pre><code>has_infogain(conn: DuckDBPyConnection, rule: RuleDef) -&gt; dict\n</code></pre> <p>Checks if the information gain meets expectations.</p>"},{"location":"sumeh/engines/duckdb/#sumeh.engines.duckdb_engine.has_max","title":"has_max","text":"<pre><code>has_max(conn: DuckDBPyConnection, rule: RuleDef) -&gt; dict\n</code></pre> <p>Checks if the maximum value meets expectations.</p>"},{"location":"sumeh/engines/duckdb/#sumeh.engines.duckdb_engine.has_mean","title":"has_mean","text":"<pre><code>has_mean(conn: DuckDBPyConnection, rule: RuleDef) -&gt; dict\n</code></pre> <p>Checks if the mean meets expectations.</p>"},{"location":"sumeh/engines/duckdb/#sumeh.engines.duckdb_engine.has_min","title":"has_min","text":"<pre><code>has_min(conn: DuckDBPyConnection, rule: RuleDef) -&gt; dict\n</code></pre> <p>Checks if the minimum value meets expectations.</p>"},{"location":"sumeh/engines/duckdb/#sumeh.engines.duckdb_engine.has_std","title":"has_std","text":"<pre><code>has_std(conn: DuckDBPyConnection, rule: RuleDef) -&gt; dict\n</code></pre> <p>Checks if the standard deviation meets expectations.</p>"},{"location":"sumeh/engines/duckdb/#sumeh.engines.duckdb_engine.has_sum","title":"has_sum","text":"<pre><code>has_sum(conn: DuckDBPyConnection, rule: RuleDef) -&gt; dict\n</code></pre> <p>Checks if the sum meets expectations.</p>"},{"location":"sumeh/engines/duckdb/#sumeh.engines.duckdb_engine.summarize","title":"summarize","text":"<pre><code>summarize(conn: DuckDBPyConnection, rules: List[RuleDef], total_rows: int, df_with_errors: Optional[DuckDBPyRelation] = None, table_error: Optional[DuckDBPyRelation] = None) -&gt; dk.DuckDBPyRelation\n</code></pre> <p>Summarizes validation results from both row-level and table-level checks.</p> <p>Parameters:</p> Name Type Description Default <code>conn</code> <code>DuckDBPyConnection</code> <p>DuckDB connection.</p> required <code>rules</code> <code>List[RuleDef]</code> <p>List of all validation rules.</p> required <code>total_rows</code> <code>int</code> <p>Total number of rows in the input relation.</p> required <code>df_with_errors</code> <code>Optional[DuckDBPyRelation]</code> <p>Relation with row-level violations.</p> <code>None</code> <code>table_error</code> <code>Optional[DuckDBPyRelation]</code> <p>Relation with table-level results.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DuckDBPyRelation</code> <code>DuckDBPyRelation</code> <p>Summary relation with aggregated validation metrics.</p>"},{"location":"sumeh/engines/duckdb/#sumeh.engines.duckdb_engine.validate","title":"validate","text":"<pre><code>validate(conn: DuckDBPyConnection, df_rel: DuckDBPyRelation, rules: List[RuleDef]) -&gt; Tuple[DuckDBPyRelation, DuckDBPyRelation, DuckDBPyRelation]\n</code></pre> <p>Main validation function that orchestrates row-level and table-level validations.</p> <p>Parameters:</p> Name Type Description Default <code>conn</code> <code>DuckDBPyConnection</code> <p>DuckDB connection.</p> required <code>df_rel</code> <code>DuckDBPyRelation</code> <p>Input DuckDB relation to validate.</p> required <code>rules</code> <code>List[RuleDef]</code> <p>List of all validation rules.</p> required <p>Returns:</p> Type Description <code>Tuple[DuckDBPyRelation, DuckDBPyRelation, DuckDBPyRelation]</code> <p>Tuple[DuckDBPyRelation, DuckDBPyRelation, DuckDBPyRelation]: - Relation with row-level violations and dq_status - Raw row-level violations relation - Table-level summary relation</p>"},{"location":"sumeh/engines/duckdb/#sumeh.engines.duckdb_engine.validate_row_level","title":"validate_row_level","text":"<pre><code>validate_row_level(conn: DuckDBPyConnection, df_rel: DuckDBPyRelation, rules: List[RuleDef]) -&gt; Tuple[DuckDBPyRelation, DuckDBPyRelation]\n</code></pre> <p>Validates DataFrame at row level using specified rules.</p> <p>Parameters:</p> Name Type Description Default <code>conn</code> <code>DuckDBPyConnection</code> <p>DuckDB connection.</p> required <code>df_rel</code> <code>DuckDBPyRelation</code> <p>Input DuckDB relation to validate.</p> required <code>rules</code> <code>List[RuleDef]</code> <p>List of row-level validation rules.</p> required <p>Returns:</p> Type Description <code>Tuple[DuckDBPyRelation, DuckDBPyRelation]</code> <p>Tuple[DuckDBPyRelation, DuckDBPyRelation]: - Relation with violations and dq_status column - Raw violations relation</p>"},{"location":"sumeh/engines/duckdb/#sumeh.engines.duckdb_engine.validate_schema","title":"validate_schema","text":"<pre><code>validate_schema(conn: DuckDBPyConnection, expected: List[Dict[str, Any]], table: str) -&gt; Tuple[bool, List[Dict[str, Any]]]\n</code></pre> <p>Validates the schema of a DuckDB table against an expected schema.</p> <p>Parameters:</p> Name Type Description Default <code>conn</code> <code>DuckDBPyConnection</code> <p>DuckDB connection.</p> required <code>expected</code> <code>List[Dict[str, Any]]</code> <p>Expected schema.</p> required <code>table</code> <code>str</code> <p>Table name.</p> required <p>Returns:</p> Type Description <code>Tuple[bool, List[Dict[str, Any]]]</code> <p>Tuple[bool, List[Dict[str, Any]]]: - Boolean indicating whether the schema matches - List of schema errors/mismatches</p>"},{"location":"sumeh/engines/duckdb/#sumeh.engines.duckdb_engine.validate_table_level","title":"validate_table_level","text":"<pre><code>validate_table_level(conn: DuckDBPyConnection, df_rel: DuckDBPyRelation, rules: List[RuleDef]) -&gt; dk.DuckDBPyRelation\n</code></pre> <p>Validates DataFrame at table level using specified rules.</p> <p>Parameters:</p> Name Type Description Default <code>conn</code> <code>DuckDBPyConnection</code> <p>DuckDB connection.</p> required <code>df_rel</code> <code>DuckDBPyRelation</code> <p>Input DuckDB relation to validate.</p> required <code>rules</code> <code>List[RuleDef]</code> <p>List of table-level validation rules.</p> required <p>Returns:</p> Name Type Description <code>DuckDBPyRelation</code> <code>DuckDBPyRelation</code> <p>Summary relation with validation results.</p>"},{"location":"sumeh/engines/engines/","title":"Engines Overview","text":"<p>Sumeh supports multiple DataFrame processing engines to validate data at any scale, from local CSVs to distributed big data workloads.</p>"},{"location":"sumeh/engines/engines/#what-are-engines","title":"What are Engines?","text":"<p>Engines are adapters that allow Sumeh to work with different DataFrame implementations. Each engine provides the same validation API but optimized for different use cases and scales.</p> <pre><code>from sumeh import validate, summarize\nfrom sumeh.core.config import get_rules_config\n\n# Works with ANY engine!\nrules = get_rules_config(source=\"rules.csv\")\ninvalid_raw, invalid_agg = validate(df, rules)\nsummary = summarize(invalid_raw, rules, total_rows=len(df))\n</code></pre> <p>Sumeh automatically detects which engine your DataFrame is using and applies the appropriate validation logic.</p>"},{"location":"sumeh/engines/engines/#supported-engines","title":"Supported Engines","text":"Engine Best For Scale Speed Memory Pandas General purpose, prototyping Small-Medium Moderate High Polars Fast analytics, large datasets Medium-Large Very Fast Low Dask Distributed computing, clusters Large-Huge Parallel Scalable PySpark Big data, Spark clusters Huge Parallel Distributed DuckDB Analytical queries, OLAP Medium-Large Very Fast Low BigQuery Cloud data warehouse Huge Serverless Cloud"},{"location":"sumeh/engines/engines/#engine-selection-guide","title":"Engine Selection Guide","text":""},{"location":"sumeh/engines/engines/#pandas","title":"\ud83d\udc3c Pandas","text":"<p>Use when: - Dataset &lt; 10GB - Local development - Quick prototyping - Standard data science workflow</p> <pre><code>import pandas as pd\nfrom sumeh import validate\n\ndf = pd.read_csv(\"data.csv\")\ninvalid_raw, invalid_agg = validate(df, rules)\n</code></pre> <p>Pros: Simple, widely-used, great ecosystem Cons: Single-threaded, memory-intensive</p>"},{"location":"sumeh/engines/engines/#polars","title":"\ud83d\udc3b\u200d\u2744\ufe0f Polars","text":"<p>Use when: - Dataset 1GB - 100GB - Need better performance than Pandas - Memory efficiency is important - Modern Python syntax</p> <pre><code>import polars as pl\nfrom sumeh import validate\n\ndf = pl.read_csv(\"data.csv\")\ninvalid_raw, invalid_agg = validate(df, rules)\n</code></pre> <p>Pros: 10-100x faster than Pandas, lazy evaluation, low memory Cons: Newer ecosystem, fewer integrations</p>"},{"location":"sumeh/engines/engines/#dask","title":"\u26a1 Dask","text":"<p>Use when: - Dataset &gt; 100GB - Multi-core processing needed - Out-of-memory computation - Scaling to multiple machines</p> <pre><code>import dask.dataframe as dd\nfrom sumeh import validate\n\ndf = dd.read_csv(\"data/*.csv\")\ninvalid_raw, invalid_agg = validate(df, rules)\n</code></pre> <p>Pros: Scales to clusters, familiar Pandas API, parallel Cons: Overhead for small data, complex debugging</p>"},{"location":"sumeh/engines/engines/#pyspark","title":"\ud83d\udd25 PySpark","text":"<p>Use when: - Dataset &gt; 1TB - Already using Spark infrastructure - Complex distributed processing - Databricks, EMR, or Dataproc</p> <pre><code>from pyspark.sql import SparkSession\nfrom sumeh import validate\n\nspark = SparkSession.builder.getOrCreate()\ndf = spark.read.csv(\"data.csv\", header=True)\ninvalid_raw, invalid_agg = validate(df, rules)\n</code></pre> <p>Pros: Industry standard for big data, fault-tolerant, mature Cons: JVM overhead, complex setup, verbose API</p>"},{"location":"sumeh/engines/engines/#duckdb","title":"\ud83e\udd86 DuckDB","text":"<p>Use when: - Analytical queries on large CSVs/Parquet - Need SQL-like performance - In-process OLAP - Embedded analytics</p> <pre><code>import duckdb\nfrom sumeh import validate\n\nconn = duckdb.connect()\ndf = conn.execute(\"SELECT * FROM 'data.parquet'\").df()\ninvalid_raw, invalid_agg = validate(df, rules, conn=conn)\n</code></pre> <p>Pros: Extremely fast for analytics, SQL support, columnar Cons: Less mature than Pandas, limited ML ecosystem</p>"},{"location":"sumeh/engines/engines/#bigquery","title":"\u2601\ufe0f BigQuery","text":"<p>Use when: - Data already in BigQuery - Serverless compute needed - Petabyte-scale datasets - Google Cloud Platform</p> <pre><code>from google.cloud import bigquery\nfrom sumeh import validate\n\nclient = bigquery.Client()\nquery = \"SELECT * FROM `project.dataset.table` LIMIT 1000000\"\ndf = client.query(query).to_dataframe()\ninvalid_raw, invalid_agg = validate(df, rules)\n</code></pre> <p>Pros: Serverless, auto-scaling, SQL interface Cons: Network latency, costs per query</p>"},{"location":"sumeh/engines/engines/#engine-auto-detection","title":"Engine Auto-Detection","text":"<p>Sumeh automatically detects your DataFrame engine:</p> <pre><code>from sumeh.core import __detect_engine\n\n# Returns engine name based on DataFrame type\nengine = __detect_engine(df)\n# \u2192 \"pandas_engine\" | \"polars_engine\" | \"pyspark_engine\" | ...\n</code></pre> <p>Detection logic:</p> <pre><code>def __detect_engine(df):\n    mod = df.__class__.__module__\n\n    if mod.startswith(\"pyspark\"):\n        return \"pyspark_engine\"\n    elif mod.startswith(\"dask\"):\n        return \"dask_engine\"\n    elif mod.startswith(\"polars\"):\n        return \"polars_engine\"\n    elif mod.startswith(\"pandas\"):\n        return \"pandas_engine\"\n    elif mod.startswith(\"duckdb\"):\n        return \"duckdb_engine\"\n    else:\n        raise TypeError(f\"Unsupported DataFrame type: {type(df)}\")\n</code></pre>"},{"location":"sumeh/engines/engines/#cli-engine-selection","title":"CLI Engine Selection","text":"<p>Use the <code>--engine</code> flag to specify which engine to use for loading data:</p> <pre><code># Use Pandas (default)\nsumeh validate data.csv rules.csv\n\n# Use Polars for better performance\nsumeh validate data.csv rules.csv --engine polars\n\n# Use Dask for large files\nsumeh validate data.csv rules.csv --engine dask\n</code></pre> <p>Available engines: - <code>pandas</code> (default) - <code>polars</code> - <code>dask</code></p>"},{"location":"sumeh/engines/engines/#engine-specific-features","title":"Engine-Specific Features","text":""},{"location":"sumeh/engines/engines/#context-parameters","title":"Context Parameters","text":"<p>Some engines require additional context:</p> <pre><code># DuckDB requires connection\nimport duckdb\nconn = duckdb.connect()\ninvalid_raw, invalid_agg = validate(df, rules, conn=conn)\n\n# PySpark can use custom executors\ninvalid_raw, invalid_agg = validate(\n    df, \n    rules, \n    num_partitions=100\n)\n</code></pre>"},{"location":"sumeh/engines/engines/#lazy-evaluation","title":"Lazy Evaluation","text":"<p>Polars and Dask support lazy evaluation:</p> <pre><code>import polars as pl\n\n# Build computation graph (lazy)\ndf = pl.scan_csv(\"data.csv\")\ndf_filtered = df.filter(pl.col(\"age\") &gt; 18)\n\n# Execute validation (triggers computation)\ninvalid_raw, invalid_agg = validate(df_filtered, rules)\n</code></pre>"},{"location":"sumeh/engines/engines/#custom-engine-implementation","title":"Custom Engine Implementation","text":"<p>Want to add support for a new DataFrame library? Implement these functions:</p> <pre><code># sumeh/engines/myengine_engine.py\n\ndef validate(df, rules):\n    \"\"\"Run validation checks.\"\"\"\n    # Implementation here\n    return invalid_raw, invalid_agg\n\ndef validate_schema(df, expected, **kwargs):\n    \"\"\"Validate schema matches expected.\"\"\"\n    # Implementation here\n    return is_valid, errors\n\ndef summarize(df, rules, total_rows=None):\n    \"\"\"Summarize validation results.\"\"\"\n    # Implementation here\n    return summary_df\n</code></pre> <p>See Custom Engine Guide for details.</p>"},{"location":"sumeh/engines/engines/#best-practices","title":"Best Practices","text":""},{"location":"sumeh/engines/engines/#1-match-engine-to-data-size","title":"1. Match Engine to Data Size","text":"<pre><code># &lt; 1GB \u2192 Pandas\nif data_size &lt; 1_000_000_000:\n    engine = \"pandas\"\n\n# 1-100GB \u2192 Polars or DuckDB\nelif data_size &lt; 100_000_000_000:\n    engine = \"polars\"  # or \"duckdb\"\n\n# &gt; 100GB \u2192 Dask or Spark\nelse:\n    engine = \"dask\"  # or \"pyspark\"\n</code></pre>"},{"location":"sumeh/engines/engines/#2-use-lazy-evaluation","title":"2. Use Lazy Evaluation","text":"<pre><code># Polars\ndf = pl.scan_csv(\"*.csv\")  # Lazy\ndf = df.filter(...)  # Still lazy\nresult = validate(df, rules)  # Triggers execution\n\n# Dask\ndf = dd.read_csv(\"*.csv\")  # Lazy\ndf = df[df.age &gt; 18]  # Still lazy\nresult = validate(df, rules)  # Triggers execution\n</code></pre>"},{"location":"sumeh/engines/engines/#3-optimize-for-your-environment","title":"3. Optimize for Your Environment","text":"<pre><code># Local laptop \u2192 Pandas/Polars\n# Corporate server \u2192 DuckDB/Dask\n# Cloud \u2192 BigQuery/Spark\n# Edge/IoT \u2192 Polars (smallest footprint)\n</code></pre>"},{"location":"sumeh/engines/engines/#4-consider-data-format","title":"4. Consider Data Format","text":"Format Best Engine CSV DuckDB, Polars Parquet DuckDB, Polars, Spark JSON Pandas, Polars Database Native connectors Cloud Storage BigQuery, Spark"},{"location":"sumeh/engines/engines/#troubleshooting","title":"Troubleshooting","text":""},{"location":"sumeh/engines/engines/#unsupported-dataframe-type","title":"\"Unsupported DataFrame type\"","text":"<p>Problem: Engine not detected</p> <p>Solution: Check if the DataFrame is from a supported library</p> <pre><code>print(type(df))\nprint(df.__class__.__module__)\n\n# Should be one of:\n# - pandas.core.frame.DataFrame\n# - polars.dataframe.frame.DataFrame\n# - dask.dataframe.core.DataFrame\n# - pyspark.sql.dataframe.DataFrame\n</code></pre>"},{"location":"sumeh/engines/engines/#memory-errors","title":"Memory Errors","text":"<p>Problem: Pandas runs out of memory</p> <p>Solutions: 1. Switch to Polars (lower memory footprint) 2. Use Dask (out-of-core processing) 3. Sample the data first 4. Use columnar format (Parquet)</p> <pre><code># Option 1: Polars\nimport polars as pl\ndf = pl.read_csv(\"large.csv\")\n\n# Option 2: Dask\nimport dask.dataframe as dd\ndf = dd.read_csv(\"large.csv\")\n\n# Option 3: Sample\ndf = pd.read_csv(\"large.csv\", nrows=100000)\n</code></pre>"},{"location":"sumeh/engines/engines/#slow-performance","title":"Slow Performance","text":"<p>Problem: Validation takes too long</p> <p>Solutions: 1. Use faster engine (Polars, DuckDB) 2. Parallelize with Dask/Spark 3. Reduce data size (filter first) 4. Optimize rules (remove duplicates)</p> <pre><code># Use faster engine\nimport polars as pl\ndf = pl.read_csv(\"data.csv\")\n\n# Or parallelize\nimport dask.dataframe as dd\ndf = dd.read_csv(\"data.csv\")\n</code></pre>"},{"location":"sumeh/engines/engines/#migration-guide","title":"Migration Guide","text":""},{"location":"sumeh/engines/engines/#pandas-polars","title":"Pandas \u2192 Polars","text":"<pre><code># Before (Pandas)\nimport pandas as pd\ndf = pd.read_csv(\"data.csv\")\ndf_clean = df[df.age &gt; 18]\n\n# After (Polars)\nimport polars as pl\ndf = pl.read_csv(\"data.csv\")\ndf_clean = df.filter(pl.col(\"age\") &gt; 18)\n\n# Validation works the same!\ninvalid_raw, invalid_agg = validate(df_clean, rules)\n</code></pre>"},{"location":"sumeh/engines/engines/#pandas-dask","title":"Pandas \u2192 Dask","text":"<pre><code># Before (Pandas)\nimport pandas as pd\ndf = pd.read_csv(\"data.csv\")\n\n# After (Dask)\nimport dask.dataframe as dd\ndf = dd.read_csv(\"data.csv\")\n\n# Validation works the same!\ninvalid_raw, invalid_agg = validate(df, rules)\n</code></pre>"},{"location":"sumeh/engines/pandas/","title":"pandas","text":""},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine","title":"sumeh.engines.pandas_engine","text":"<p>Pandas engine for data quality validation.</p> <p>Provides row-level and table-level validation functions for pandas DataFrames, including completeness, uniqueness, range checks, pattern matching, date validations, SQL-style custom expressions, and schema validation.</p> <p>Functions:</p> Name Description <code>is_positive</code> <p>Filters rows where the specified field is less than zero.</p> <code>is_negative</code> <p>Filters rows where the specified field is greater than or equal to zero.</p> <code>is_in_millions</code> <p>Retains rows where the field value is at least 1,000,000 and flags them with dq_status.</p> <code>is_in_billions</code> <p>Retains rows where the field value is at least 1,000,000,000 and flags them with dq_status.</p> <code>is_t_minus_1</code> <p>Retains rows where the date field equals yesterday (T-1) and flags them with dq_status.</p> <code>is_t_minus_2</code> <p>Retains rows where the date field equals two days ago (T-2) and flags them with dq_status.</p> <code>is_t_minus_3</code> <p>Retains rows where the date field equals three days ago (T-3) and flags them with dq_status.</p> <code>is_today</code> <p>Retains rows where the date field equals today and flags them with dq_status.</p> <code>is_yesterday</code> <p>Retains rows where the date field equals yesterday and flags them with dq_status.</p> <code>is_on_weekday</code> <p>Retains rows where the date field falls on a weekday (Mon-Fri) and flags them with dq_status.</p> <code>is_on_weekend</code> <p>Retains rows where the date field is on a weekend (Sat-Sun) and flags them with dq_status.</p> <code>is_on_monday</code> <p>Retains rows where the date field is on Monday and flags them with dq_status.</p> <code>is_on_tuesday</code> <p>Retains rows where the date field is on Tuesday and flags them with dq_status.</p> <code>is_on_wednesday</code> <p>Retains rows where the date field is on Wednesday and flags them with dq_status.</p> <code>is_on_thursday</code> <p>Retains rows where the date field is on Thursday and flags them with dq_status.</p> <code>is_on_friday</code> <p>Retains rows where the date field is on Friday and flags them with dq_status.</p> <code>is_on_saturday</code> <p>Retains rows where the date field is on Saturday and flags them with dq_status.</p> <code>is_on_sunday</code> <p>Retains rows where the date field is on Sunday and flags them with dq_status.</p> <code>is_complete</code> <p>Filters rows where the specified field is null.</p> <code>is_unique</code> <p>Filters rows with duplicate values in the specified field.</p> <code>are_complete</code> <p>Filters rows where any of the specified fields are null.</p> <code>are_unique</code> <p>Filters rows with duplicate combinations of the specified fields.</p> <code>is_greater_than</code> <p>Filters rows where the specified field is less than or equal to the given value.</p> <code>is_greater_or_equal_than</code> <p>Filters rows where the specified field is less than the given value.</p> <code>is_less_than</code> <p>Filters rows where the specified field is greater than or equal to the given value.</p> <code>is_less_or_equal_than</code> <p>Filters rows where the specified field is greater than the given value.</p> <code>is_equal</code> <p>Filters rows where the specified field is not equal to the given value.</p> <code>is_equal_than</code> <p>Alias for <code>is_equal</code>.</p> <code>is_contained_in</code> <p>Filters rows where the specified field is not in the given list of values.</p> <code>not_contained_in</code> <p>Filters rows where the specified field is in the given list of values.</p> <code>is_between</code> <p>Filters rows where the specified field is not within the given range.</p> <code>has_pattern</code> <p>Filters rows where the specified field does not match the given regex pattern.</p> <code>is_legit</code> <p>Filters rows where the specified field is null or contains whitespace.</p> <code>has_max</code> <p>Filters rows where the specified field exceeds the given maximum value.</p> <code>has_min</code> <p>Filters rows where the specified field is below the given minimum value.</p> <code>has_std</code> <p>Checks if the standard deviation of the specified field exceeds the given value.</p> <code>has_mean</code> <p>Checks if the mean of the specified field exceeds the given value.</p> <code>has_sum</code> <p>Checks if the sum of the specified field exceeds the given value.</p> <code>has_cardinality</code> <p>Checks if the cardinality (number of unique values) of the specified field exceeds the given value.</p> <code>has_infogain</code> <p>Placeholder for information gain validation (currently uses cardinality).</p> <code>has_entropy</code> <p>Placeholder for entropy validation (currently uses cardinality).</p> <code>satisfies</code> <p>Filters rows that do not satisfy the given custom expression.</p> <code>validate_date_format</code> <p>Filters rows where the specified field does not match the expected date format or is null.</p> <code>is_future_date</code> <p>Filters rows where the specified date field is after today\u2019s date.</p> <code>is_past_date</code> <p>Filters rows where the specified date field is before today\u2019s date.</p> <code>is_date_between</code> <p>Filters rows where the specified date field is not within the given [start,end] range.</p> <code>is_date_after</code> <p>Filters rows where the specified date field is before the given date.</p> <code>is_date_before</code> <p>Filters rows where the specified date field is after the given date.</p> <code>all_date_checks</code> <p>Alias for <code>is_past_date</code> (checks date against today).</p> <code>validate</code> <p>Validates a DataFrame against a list of rules and returns the original DataFrame with data quality status and a DataFrame of violations.</p> <code>__build_rules_df</code> <p>Converts a list of rules into a Pandas DataFrame for summarization.</p> <code>summarize</code> <p>Summarizes the results of data quality checks, including pass rates and statuses.</p> <code>validate_schema</code> <p>Validates the schema of a DataFrame against an expected schema and returns a boolean result and a list of errors.</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.all_date_checks","title":"all_date_checks","text":"<pre><code>all_date_checks(df: DataFrame, rule: RuleDef) -&gt; pd.DataFrame\n</code></pre> <p>Applies all date-related validation checks on the given DataFrame based on the specified rule.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing the data to be validated.</p> required <code>rule</code> <code>dict</code> <p>A dictionary specifying the validation rules to be applied.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame with the results of the date validation checks.</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.are_complete","title":"are_complete","text":"<pre><code>are_complete(df: DataFrame, rule: RuleDef) -&gt; pd.DataFrame\n</code></pre> <p>Checks for completeness of specified fields in a DataFrame based on a given rule.</p> <p>This function identifies rows in the DataFrame where any of the specified fields contain missing values (NaN). It returns a DataFrame containing only the rows that violate the completeness rule, along with an additional column <code>dq_status</code> that describes the rule violation.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to check for completeness.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to include the following keys: - fields: A list of column names to check for completeness. - check: A string describing the type of check (e.g., \"completeness\"). - value: A value associated with the rule (e.g., a threshold or description).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing rows that violate the completeness rule.</p> <code>DataFrame</code> <p>The returned DataFrame includes all original columns and an additional column</p> <code>DataFrame</code> <p><code>dq_status</code> that describes the rule violation in the format \"fields:check:value\".</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.are_unique","title":"are_unique","text":"<pre><code>are_unique(df: DataFrame, rule: RuleDef) -&gt; pd.DataFrame\n</code></pre> <p>Checks for duplicate rows in the specified fields of a DataFrame based on a given rule.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to check for uniqueness.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - fields: A list of column names to check for uniqueness. - check: A string representing the type of check (e.g., \"unique\"). - value: A value associated with the rule (e.g., a description or identifier).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing the rows that violate the uniqueness rule.           An additional column 'dq_status' is added to indicate the rule           that was violated in the format \"{fields}:{check}:{value}\".</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.has_cardinality","title":"has_cardinality","text":"<pre><code>has_cardinality(df: DataFrame, rule: RuleDef) -&gt; dict\n</code></pre> <p>Validates that the number of distinct values (cardinality) in the specified column meets the expected threshold. Applies tolerance logic for partial matches.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing the data to be validated.</p> required <code>rule</code> <code>RuleDef</code> <p>A dictionary containing the rule parameters. It should include: - field (str): The name of the column to calculate cardinality for. - value (int): The expected number of distinct values to validate against. - threshold (float, optional): Tolerance threshold (default: 1.0).   If &lt; 1.0, represents minimum acceptable percentage of expected value.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing validation results with keys: - status (str): \"PASS\", \"FAIL\", or \"ERROR\" - expected (int): The expected threshold value - actual (int): The actual computed cardinality - message (str): Description of failure or error, None if passed</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.has_entropy","title":"has_entropy","text":"<pre><code>has_entropy(df: DataFrame, rule: RuleDef) -&gt; dict\n</code></pre> <p>Validates the Shannon entropy of the specified column. Entropy measures the randomness/disorder in data distribution: - High entropy = data is widely distributed across values - Low entropy = data is concentrated in few values</p> <p>Formula: H(X) = -\u03a3(p(x) * log2(p(x)))</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing the data to be validated.</p> required <code>rule</code> <code>RuleDef</code> <p>A dictionary containing the rule parameters. It should include: - field (str): The name of the column to calculate entropy for. - value (float): The expected entropy value to validate against. - threshold (float, optional): Tolerance threshold (default: 1.0).   If &lt; 1.0, represents minimum acceptable percentage of expected value.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing validation results with keys: - status (str): \"PASS\", \"FAIL\", or \"ERROR\" - expected (float): The expected threshold value - actual (float): The actual computed entropy - message (str): Description of failure or error, None if passed</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.has_infogain","title":"has_infogain","text":"<pre><code>has_infogain(df: DataFrame, rule: RuleDef) -&gt; dict\n</code></pre> <p>Validates the information gain of the specified column. Information gain measures how much useful variability the column possesses. Calculated as normalized entropy by the maximum possible entropy.</p> <p>Value ranges from 0 to 1: - 1.0 = perfectly uniform distribution (maximum information) - 0.0 = all values are identical (no information)</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing the data to be validated.</p> required <code>rule</code> <code>RuleDef</code> <p>A dictionary containing the rule parameters. It should include: - field (str): The name of the column to calculate information gain for. - value (float): The expected information gain value to validate against. - threshold (float, optional): Tolerance threshold (default: 1.0).   If &lt; 1.0, represents minimum acceptable percentage of expected value.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing validation results with keys: - status (str): \"PASS\", \"FAIL\", or \"ERROR\" - expected (float): The expected threshold value - actual (float): The actual computed information gain - message (str): Description of failure or error, None if passed</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.has_max","title":"has_max","text":"<pre><code>has_max(df: DataFrame, rule: RuleDef) -&gt; pd.DataFrame\n</code></pre> <p>Identifies rows in a DataFrame where the value in a specified field exceeds a given maximum value.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The column name to check. - 'check' (str): The type of check being performed (e.g., 'max'). - 'value' (numeric): The maximum allowable value for the specified field.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing rows that violate the rule, with an additional column</p> <code>DataFrame</code> <p>'dq_status' indicating the rule violation in the format \"field:check:value\".</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.has_mean","title":"has_mean","text":"<pre><code>has_mean(df: DataFrame, rule: RuleDef) -&gt; dict\n</code></pre> <p>Validates that the mean (average) value of the specified column meets the expected threshold. Applies tolerance logic where threshold &lt; 1.0 represents a percentage of the expected value.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing the data to be validated.</p> required <code>rule</code> <code>RuleDef</code> <p>A dictionary containing the rule parameters. It should include: - field (str): The name of the column to calculate the mean for. - value (float): The expected mean value to validate against. - threshold (float, optional): Tolerance threshold (default: 1.0).   If &lt; 1.0, represents minimum acceptable percentage of expected value.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing validation results with keys: - status (str): \"PASS\", \"FAIL\", or \"ERROR\" - expected (float): The expected threshold value - actual (float): The actual computed mean - message (str): Description of failure or error, None if passed</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.has_min","title":"has_min","text":"<pre><code>has_min(df: DataFrame, rule: RuleDef) -&gt; pd.DataFrame\n</code></pre> <p>Filters a DataFrame to identify rows where a specified field's value is less than a given threshold.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The column name in the DataFrame to be checked. - 'check': The type of check being performed (e.g., 'min'). - 'value': The threshold value for the check.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A new DataFrame containing rows that violate the rule, with an additional</p> <code>DataFrame</code> <p>column 'dq_status' indicating the field, check type, and threshold value.</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.has_pattern","title":"has_pattern","text":"<pre><code>has_pattern(df: DataFrame, rule: RuleDef) -&gt; pd.DataFrame\n</code></pre> <p>Checks if the values in a specified column of a DataFrame match a given pattern.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to check.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The column name in the DataFrame to check. - 'check': A descriptive label for the check being performed. - 'pattern': The regex pattern to match against the column values.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing rows that do not match the pattern.           An additional column 'dq_status' is added to indicate the           field, check, and pattern that caused the violation.</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.has_std","title":"has_std","text":"<pre><code>has_std(df: DataFrame, rule: RuleDef) -&gt; dict\n</code></pre> <p>Validates that the standard deviation of the specified column meets the expected threshold. Supports both range-based validation (for thresholds &lt; 1.0) and simple comparison.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing the data to be validated.</p> required <code>rule</code> <code>RuleDef</code> <p>A dictionary containing the rule parameters. It should include: - field (str): The name of the column to calculate the standard deviation for. - value (float): The expected standard deviation value to validate against. - threshold (float, optional): Tolerance threshold (default: 1.0).   If &lt; 1.0, creates an acceptable range around the expected value.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing validation results with keys: - status (str): \"PASS\", \"FAIL\", or \"ERROR\" - expected (float): The expected threshold value - actual (float): The actual computed standard deviation - message (str): Description of failure or error, None if passed</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.has_sum","title":"has_sum","text":"<pre><code>has_sum(df: DataFrame, rule: RuleDef) -&gt; dict\n</code></pre> <p>Validates that the sum of values in the specified column meets the expected threshold. Applies tolerance logic where threshold &lt; 1.0 represents a percentage of the expected value.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing the data to be validated.</p> required <code>rule</code> <code>RuleDef</code> <p>A dictionary containing the rule parameters. It should include: - field (str): The name of the column to calculate the sum for. - value (float): The expected sum value to validate against. - threshold (float, optional): Tolerance threshold (default: 1.0).   If &lt; 1.0, represents minimum acceptable percentage of expected value.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing validation results with keys: - status (str): \"PASS\", \"FAIL\", or \"ERROR\" - expected (float): The expected threshold value - actual (float): The actual computed sum - message (str): Description of failure or error, None if passed</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.is_between","title":"is_between","text":"<pre><code>is_between(df: DataFrame, rule: RuleDef) -&gt; pd.DataFrame\n</code></pre> <p>Filters a DataFrame to identify rows where a specified field's values are not within a given range.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The column name in the DataFrame to check. - 'check': A descriptive label for the check being performed. - 'value': A string representation of the range in the format '[lo, hi]'.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing rows that violate the range condition.           An additional column 'dq_status' is added to indicate the rule violation in the format 'field:check:value'.</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.is_complete","title":"is_complete","text":"<pre><code>is_complete(df: DataFrame, rule: RuleDef) -&gt; pd.DataFrame\n</code></pre> <p>Checks for missing values in a specified field of a DataFrame based on a given rule.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to check for completeness.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The name of the field/column to check for missing values. - 'check': The type of check being performed (not used in this function). - 'value': Additional value associated with the rule (not used in this function).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing rows where the specified field has missing values.           An additional column 'dq_status' is added to indicate the rule that was violated.</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.is_contained_in","title":"is_contained_in","text":"<pre><code>is_contained_in(df: DataFrame, rule: RuleDef) -&gt; pd.DataFrame\n</code></pre> <p>Filters a DataFrame to identify rows where the values in a specified field are not contained within a given set of values.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected          to include the following keys:          - 'field': The column name in the DataFrame to check.          - 'check': A descriptive string for the check being performed.          - 'value': A list or string representation of the allowed values.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing rows from the input DataFrame that           do not meet the rule criteria. An additional column           'dq_status' is added to indicate the rule violation in           the format \"field:check:value\".</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.is_date_after","title":"is_date_after","text":"<pre><code>is_date_after(df: DataFrame, rule: RuleDef) -&gt; pd.DataFrame\n</code></pre> <p>Filters a DataFrame to return rows where a specified date field is earlier than a given target date.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing the data to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - field (str): The name of the column in the DataFrame to check. - check (str): A descriptive label for the check being performed. - date_str (str): The target date as a string in a format parsable by <code>pd.to_datetime</code>.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing rows where the date in the specified field is earlier</p> <code>DataFrame</code> <p>than the target date. An additional column <code>dq_status</code> is added to indicate the rule that</p> <code>DataFrame</code> <p>was violated in the format \"{field}:{check}:{date_str}\".</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.is_date_before","title":"is_date_before","text":"<pre><code>is_date_before(df: DataFrame, rule: RuleDef) -&gt; pd.DataFrame\n</code></pre> <p>Filters a DataFrame to identify rows where a date field is after a specified target date.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing the data to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - field (str): The name of the column in the DataFrame containing date values. - check (str): A descriptive label for the check being performed. - date_str (str): The target date as a string in a format parsable by <code>pd.to_datetime</code>.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing rows where the date in the specified field is after</p> <code>DataFrame</code> <p>the target date. An additional column <code>dq_status</code> is added to indicate the rule that was</p> <code>DataFrame</code> <p>violated in the format \"{field}:{check}:{date_str}\".</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.is_date_between","title":"is_date_between","text":"<pre><code>is_date_between(df: DataFrame, rule: RuleDef) -&gt; pd.DataFrame\n</code></pre> <p>Filters rows in a DataFrame where the values in a specified date column are not within a given date range.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing the data to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected          to include the following:          - field: The name of the column to check.          - check: A string representing the type of check (used for                   status annotation).          - raw: A string representing the date range in the format                 '[start_date, end_date]'.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing the rows where the date values in           the specified column are outside the given range. An           additional column 'dq_status' is added to indicate the           rule that was violated.</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.is_equal","title":"is_equal","text":"<pre><code>is_equal(df: DataFrame, rule: RuleDef) -&gt; pd.DataFrame\n</code></pre> <p>Filters a DataFrame to identify rows where the value in a specified field does not match a given value, and annotates these rows with a data quality status.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The column name in the DataFrame to check. - 'check': A string describing the check being performed (e.g., \"is_equal\"). - 'value': The value to compare against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing rows that do not satisfy the equality check.</p> <code>DataFrame</code> <p>An additional column 'dq_status' is added to indicate the data quality status</p> <code>DataFrame</code> <p>in the format \"{rule.field}:{rule.check_type}:{rule.value}\".</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.is_equal_than","title":"is_equal_than","text":"<pre><code>is_equal_than(df: DataFrame, rule: RuleDef) -&gt; pd.DataFrame\n</code></pre> <p>Compares the values in a DataFrame against a specified rule and returns the result.</p> <p>This function acts as a wrapper for the <code>is_equal</code> function, passing the given DataFrame and rule to it.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The DataFrame to be evaluated.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the comparison rule.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame indicating the result of the comparison.</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.is_future_date","title":"is_future_date","text":"<pre><code>is_future_date(df: DataFrame, rule: RuleDef) -&gt; pd.DataFrame\n</code></pre> <p>Identifies rows in a DataFrame where the date in a specified field is in the future.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing the data to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to include          the field name to check and the check type.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing only the rows where the date in the specified           field is in the future. An additional column 'dq_status' is added to           indicate the field, check type, and the current date in ISO format.</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.is_greater_or_equal_than","title":"is_greater_or_equal_than","text":"<pre><code>is_greater_or_equal_than(df: DataFrame, rule: RuleDef) -&gt; pd.DataFrame\n</code></pre> <p>Filters a DataFrame to include only rows where the value in a specified field is greater than or equal to a given threshold. Adds a 'dq_status' column to indicate the rule applied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The column name to apply the rule on. - 'check' (str): The type of check being performed (e.g., 'greater_or_equal'). - 'value' (numeric): The threshold value for the comparison.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A new DataFrame containing only the rows that satisfy the rule,</p> <code>DataFrame</code> <p>with an additional 'dq_status' column describing the rule applied.</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.is_greater_than","title":"is_greater_than","text":"<pre><code>is_greater_than(df: DataFrame, rule: RuleDef) -&gt; pd.DataFrame\n</code></pre> <p>Filters a DataFrame to return rows where a specified field's value is greater than a given threshold.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The column name in the DataFrame to be checked. - 'check' (str): The type of check being performed (e.g., 'greater_than'). - 'value' (numeric): The threshold value to compare against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A new DataFrame containing rows where the specified field's value is greater than the given threshold.           An additional column 'dq_status' is added to indicate the rule applied in the format \"field:check:value\".</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.is_in","title":"is_in","text":"<pre><code>is_in(df: DataFrame, rule: RuleDef) -&gt; pd.DataFrame\n</code></pre> <p>Checks if the values in a DataFrame satisfy a given rule by delegating the operation to the <code>is_contained_in</code> function.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to be evaluated.</p> required <code>rule</code> <code>dict</code> <p>A dictionary defining the rule to check against the DataFrame.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame indicating whether each element satisfies the rule.</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.is_in_billions","title":"is_in_billions","text":"<pre><code>is_in_billions(df: DataFrame, rule: RuleDef) -&gt; pd.DataFrame\n</code></pre> <p>Filters a DataFrame to include only rows where the specified field's value is greater than or equal to one billion, and adds a data quality status column.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - field (str): The column name to check. - check (str): The type of check being performed (used for status annotation). - value (any): The value associated with the rule (used for status annotation).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A new DataFrame containing rows where the specified field's</p> <code>DataFrame</code> <p>value is greater than or equal to one billion. Includes an additional</p> <code>DataFrame</code> <p>column <code>dq_status</code> with the format \"{rule.field}:{rule.check_type}:{rule.value}\".</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.is_in_millions","title":"is_in_millions","text":"<pre><code>is_in_millions(df: DataFrame, rule: RuleDef) -&gt; pd.DataFrame\n</code></pre> <p>Filters rows in the DataFrame where the specified field's value is greater than or equal to one million and adds a \"dq_status\" column with a formatted string indicating the rule applied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to include: - field (str): The column name to check. - check (str): The type of check being performed (e.g., \"greater_than\"). - value (any): The value associated with the rule (not used in this function).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A new DataFrame containing rows where the specified field's value is &gt;= 1,000,000.           Includes an additional \"dq_status\" column with the rule details.</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.is_legit","title":"is_legit","text":"<pre><code>is_legit(df: DataFrame, rule: RuleDef) -&gt; pd.DataFrame\n</code></pre> <p>Validates a DataFrame against a specified rule and identifies rows that violate the rule.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to validate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the validation rule. It is expected to have          keys that define the field to check, the type of check, and the value          to validate against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing rows that violate the rule. An additional           column 'dq_status' is added to indicate the field, check, and value           that caused the violation in the format \"{rule.field}:{rule.check_type}:{rule.value}\".</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.is_less_or_equal_than","title":"is_less_or_equal_than","text":"<pre><code>is_less_or_equal_than(df: DataFrame, rule: RuleDef) -&gt; pd.DataFrame\n</code></pre> <p>Filters rows in a DataFrame where the value in a specified field is less than or equal to a given value.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The column name in the DataFrame to apply the rule on. - 'check' (str): A descriptive label for the check being performed. - 'value' (numeric): The threshold value to compare against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A new DataFrame containing only the rows that satisfy the condition.           An additional column 'dq_status' is added to indicate the rule applied           in the format \"{rule.field}:{rule.check_type}:{rule.value}\".</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.is_less_than","title":"is_less_than","text":"<pre><code>is_less_than(df: DataFrame, rule: RuleDef) -&gt; pd.DataFrame\n</code></pre> <p>Filters a DataFrame to return rows where a specified field's value is less than a given threshold.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field' (str): The column name in the DataFrame to be checked. - 'check' (str): A descriptive string for the check (e.g., \"less_than\"). - 'value' (numeric): The threshold value to compare against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A new DataFrame containing only the rows where the specified field's value</p> <code>DataFrame</code> <p>is less than the given threshold. An additional column 'dq_status' is added to indicate</p> <code>DataFrame</code> <p>the rule applied in the format \"field:check:value\".</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.is_negative","title":"is_negative","text":"<pre><code>is_negative(df: DataFrame, rule: RuleDef) -&gt; pd.DataFrame\n</code></pre> <p>Filters a DataFrame to identify rows where a specified field does not satisfy a \"negative\" condition.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to be checked.</p> required <code>rule</code> <code>RuleDef</code> <p>A dictionary containing the rule parameters. It is expected to include: - 'field': The column name in the DataFrame to check. - 'check': The type of check being performed (e.g., \"negative\"). - 'value': Additional value associated with the rule (not used in this function).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A new DataFrame containing rows where the specified field is non-negative (&gt;= 0).           An additional column 'dq_status' is added to indicate the rule violation in the format           \"{rule.field}:{rule.check_type}:{rule.value}\".</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.is_on_friday","title":"is_on_friday","text":"<pre><code>is_on_friday(df: DataFrame, rule: RuleDef) -&gt; pd.DataFrame\n</code></pre> <p>Filters the rows of a DataFrame based on whether a specific date column corresponds to a Friday.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing the data to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rules or parameters for filtering.          It should specify the column to check for the day of the week.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A filtered DataFrame containing only the rows where the specified date column corresponds to a Friday.</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.is_on_monday","title":"is_on_monday","text":"<pre><code>is_on_monday(df: DataFrame, rule: RuleDef) -&gt; pd.DataFrame\n</code></pre> <p>Filters the rows of a DataFrame based on whether a specific date column corresponds to a Monday.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing the data to be filtered.</p> required <code>rule</code> <code>RuleDef</code> <p>A dictionary containing the filtering rules, including the column to check.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A filtered DataFrame containing only the rows where the specified date column corresponds to a Monday.</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.is_on_saturday","title":"is_on_saturday","text":"<pre><code>is_on_saturday(df: DataFrame, rule: RuleDef) -&gt; pd.DataFrame\n</code></pre> <p>Filters a DataFrame to include only rows where the date corresponds to a Saturday.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing date information.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing rules or parameters for filtering.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A filtered DataFrame containing only rows where the date is a Saturday.</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.is_on_sunday","title":"is_on_sunday","text":"<pre><code>is_on_sunday(df: DataFrame, rule: RuleDef) -&gt; pd.DataFrame\n</code></pre> <p>Determines whether the dates in a given DataFrame fall on a Sunday.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing date-related data.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing rules or parameters for the operation.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame indicating whether each date falls on a Sunday.</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.is_on_thursday","title":"is_on_thursday","text":"<pre><code>is_on_thursday(df: DataFrame, rule: RuleDef) -&gt; pd.DataFrame\n</code></pre> <p>Filters the rows of a DataFrame based on whether a date column corresponds to a Thursday.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing the data to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the filtering rules, including the column to check.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A filtered DataFrame containing only the rows where the specified date column           corresponds to a Thursday.</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.is_on_tuesday","title":"is_on_tuesday","text":"<pre><code>is_on_tuesday(df: DataFrame, rule: RuleDef) -&gt; pd.DataFrame\n</code></pre> <p>Filters the rows of a DataFrame based on whether a specific date column corresponds to a Tuesday.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing the data to be filtered.</p> required <code>rule</code> <code>RuleDef</code> <p>A dictionary containing the filtering rules, including the column to check.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A filtered DataFrame containing only the rows where the specified date column corresponds to a Tuesday.</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.is_on_wednesday","title":"is_on_wednesday","text":"<pre><code>is_on_wednesday(df: DataFrame, rule: RuleDef) -&gt; pd.DataFrame\n</code></pre> <p>Filters the rows of a DataFrame based on whether a date column corresponds to Wednesday.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing the data to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule configuration.          It is expected to specify the column to evaluate.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A filtered DataFrame containing only the rows where the specified date column           corresponds to Wednesday.</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.is_on_weekday","title":"is_on_weekday","text":"<pre><code>is_on_weekday(df: DataFrame, rule: RuleDef) -&gt; pd.DataFrame\n</code></pre> <p>Filters a DataFrame to include only rows where the specified date field falls on a weekday (Monday to Friday) and adds a \"dq_status\" column indicating the rule applied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing the data to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - field (str): The name of the date column to check. - check (str): A descriptive string for the check being performed. - value (str): A value associated with the rule for documentation purposes.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A filtered DataFrame containing only rows where the specified date field</p> <code>DataFrame</code> <p>falls on a weekday, with an additional \"dq_status\" column describing the rule applied.</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.is_on_weekend","title":"is_on_weekend","text":"<pre><code>is_on_weekend(df: DataFrame, rule: RuleDef) -&gt; pd.DataFrame\n</code></pre> <p>Filters a DataFrame to include only rows where the specified date field falls on a weekend (Saturday or Sunday) and adds a \"dq_status\" column indicating the rule applied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing the data to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to include: - field (str): The name of the date column to check. - check (str): A descriptive string for the type of check being performed. - value (str): A value associated with the rule for documentation purposes.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A new DataFrame containing only the rows where the specified date field</p> <code>DataFrame</code> <p>falls on a weekend. Includes an additional \"dq_status\" column with the rule details.</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.is_past_date","title":"is_past_date","text":"<pre><code>is_past_date(df: DataFrame, rule: RuleDef) -&gt; pd.DataFrame\n</code></pre> <p>Identifies rows in a DataFrame where the date in a specified column is in the past.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing the data to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to include          the field name to check and the check type.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing the rows where the date in the specified column           is earlier than the current date. An additional column 'dq_status' is           added to indicate the field, check type, and the current date.</p> Notes <ul> <li>The function uses <code>pd.to_datetime</code> to convert the specified column to datetime format.   Any invalid date entries will be coerced to NaT (Not a Time).</li> <li>Rows with invalid or missing dates are excluded from the result.</li> </ul>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.is_positive","title":"is_positive","text":"<pre><code>is_positive(df: DataFrame, rule: RuleDef) -&gt; pd.DataFrame\n</code></pre> <p>Identifies rows in a DataFrame where the specified field contains negative values.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to be checked.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to include: - 'field': The column name in the DataFrame to check. - 'check': A descriptive label for the type of check being performed. - 'value': A value associated with the rule (not directly used in this function).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing only the rows where the specified field has negative values.           An additional column 'dq_status' is added to indicate the rule violation in the format           \"{rule.field}:{rule.check_type}:{rule.value}\".</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.is_t_minus_2","title":"is_t_minus_2","text":"<pre><code>is_t_minus_2(df: DataFrame, rule: RuleDef) -&gt; pd.DataFrame\n</code></pre> <p>Filters a DataFrame to include only rows where the specified date field matches the date two days prior to the current date.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing the data to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to include the field name, check type, and value.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A filtered DataFrame containing only the rows where the</p> <code>DataFrame</code> <p>specified date field matches the target date (two days prior). An</p> <code>DataFrame</code> <p>additional column \"dq_status\" is added to indicate the rule applied.</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.is_t_minus_3","title":"is_t_minus_3","text":"<pre><code>is_t_minus_3(df: DataFrame, rule: RuleDef) -&gt; pd.DataFrame\n</code></pre> <p>Filters a DataFrame to include only rows where the specified date field matches the date three days prior to the current date.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing the data to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. The rule should include the field to check, the type of check, and the value.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A filtered DataFrame containing only the rows where the</p> <code>DataFrame</code> <p>specified date field matches the target date (three days prior). An</p> <code>DataFrame</code> <p>additional column \"dq_status\" is added to indicate the rule applied.</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.is_today","title":"is_today","text":"<pre><code>is_today(df: DataFrame, rule: RuleDef) -&gt; pd.DataFrame\n</code></pre> <p>Filters a DataFrame to include only rows where the specified date field matches today's date.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing the data to filter.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to include          the field name, a check operation, and a value.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A new DataFrame containing only the rows where the specified date field           matches today's date. An additional column \"dq_status\" is added to indicate           the rule applied in the format \"{rule.field}:{rule.check_type}:{rule.value}\".</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.is_unique","title":"is_unique","text":"<pre><code>is_unique(df: DataFrame, rule: RuleDef) -&gt; pd.DataFrame\n</code></pre> <p>Checks for duplicate values in a specified field of a DataFrame based on a rule.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to check for duplicates.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It is expected to          include the field to check, the type of check, and a value.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing the rows with duplicate values in the           specified field. An additional column 'dq_status' is added           to indicate the field, check type, and value associated with           the rule.</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.is_yesterday","title":"is_yesterday","text":"<pre><code>is_yesterday(df: DataFrame, rule: RuleDef) -&gt; pd.DataFrame\n</code></pre> <p>Filters a DataFrame to include only rows where the specified date field matches yesterday's date.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing the data to filter.</p> required <code>rule</code> <code>RuleDef</code> <p>A rule parameters. It is expected to have           to return the field name,          check type, and value.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A filtered DataFrame containing only rows where the specified date field           matches yesterday's date. An additional column <code>dq_status</code> is added to           indicate the data quality status in the format \"{rule.field}:{rule.check_type}:{rule.value}\".</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.not_contained_in","title":"not_contained_in","text":"<pre><code>not_contained_in(df: DataFrame, rule: RuleDef) -&gt; pd.DataFrame\n</code></pre> <p>Filters a DataFrame to return rows where the specified field contains values that are not allowed according to the provided rule.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the rule parameters. It should include: - 'field': The column name in the DataFrame to check. - 'check': The type of check being performed (used for status annotation). - 'value': A list or string representation of values that are not allowed.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing rows that violate the rule. An additional</p> <code>DataFrame</code> <p>column 'dq_status' is added to indicate the rule violation in the format</p> <code>DataFrame</code> <p>\"{rule.field}:{rule.check_type}:{rule.value}\".</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.not_in","title":"not_in","text":"<pre><code>not_in(df: DataFrame, rule: RuleDef) -&gt; pd.DataFrame\n</code></pre> <p>Filters a DataFrame by excluding rows that match the specified rule.</p> <p>This function is a wrapper around the <code>not_contained_in</code> function, which performs the actual filtering logic.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to be filtered.</p> required <code>rule</code> <code>dict</code> <p>A dictionary specifying the filtering criteria.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A new DataFrame with rows that do not match the rule.</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.satisfies","title":"satisfies","text":"<pre><code>satisfies(df: DataFrame, rule: RuleDef) -&gt; pd.DataFrame\n</code></pre> <p>Filters a DataFrame based on a rule and returns rows that do not satisfy the rule.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to be evaluated.</p> required <code>rule</code> <code>RuleDef</code> <p>A dictionary containing the rule to be applied.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing rows that do not satisfy the rule. An additional</p> <code>DataFrame</code> <p>column <code>dq_status</code> is added to indicate the field, check, and expression that failed.</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.summarize","title":"summarize","text":"<pre><code>summarize(rules: List[RuleDef], total_rows: int, df_with_errors: Optional[DataFrame] = None, table_error: Optional[DataFrame] = None) -&gt; pd.DataFrame\n</code></pre> <p>Summarizes quality check results for a given DataFrame based on specified rules.</p> <p>Parameters:</p> Name Type Description Default <code>rules</code> <code>List[RuleDef]</code> <p>List of RuleDef objects representing quality check rules</p> required <code>total_rows</code> <code>int</code> <p>Total number of rows in the original dataset</p> required <code>df_with_errors</code> <code>Optional[DataFrame]</code> <p>DataFrame with 'dq_status' column from row-level validations</p> <code>None</code> <code>table_error</code> <code>Optional[DataFrame]</code> <p>DataFrame with table-level validation results</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with columns: - id: Unique identifier for each rule - timestamp: Summary generation timestamp - check: Type of check performed - level: Validation level (ROW or TABLE from RuleDef) - category: Rule category (from RuleDef) - column: Column name associated with the rule - rule: Rule being checked - value: Value associated with the rule - rows: Total number of rows in dataset - violations: Number of rows that violated the rule - pass_rate: Proportion of rows that passed - pass_threshold: Threshold for passing - status: PASS or FAIL based on pass rate</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.validate","title":"validate","text":"<pre><code>validate(df: DataFrame, rules: List[RuleDef]) -&gt; Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]\n</code></pre> <p>Main validation function that orchestrates row-level and table-level validations.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame to validate.</p> required <code>rules</code> <code>List[RuleDef]</code> <p>List of all validation rules.</p> required <p>Returns:</p> Type Description <code>Tuple[DataFrame, DataFrame, DataFrame]</code> <p>Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]: - DataFrame with row-level violations and dq_status - Raw row-level violations DataFrame - Table-level summary DataFrame</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.validate_date_format","title":"validate_date_format","text":"<pre><code>validate_date_format(df: DataFrame, rule: RuleDef) -&gt; pd.DataFrame\n</code></pre> <p>Validates the date format of a specified field in a DataFrame against a given format.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing the data to validate.</p> required <code>rule</code> <code>dict</code> <p>A dictionary containing the validation rule. It should include: - 'field': The name of the column to validate. - 'check': A description or identifier for the validation check. - 'fmt': The expected date format to validate against.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing rows that violate the date format rule.           An additional column 'dq_status' is added to indicate the           validation status in the format \"{field}:{check}:{fmt}\".</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.validate_row_level","title":"validate_row_level","text":"<pre><code>validate_row_level(df: DataFrame, rules: List[RuleDef]) -&gt; Tuple[pd.DataFrame, pd.DataFrame]\n</code></pre> <p>Validates a pandas DataFrame against a set of rules.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame to validate</p> required <code>rules</code> <code>List[RuleDef]</code> <p>List of Rule objects defining validation checks</p> required <p>Returns:</p> Type Description <code>Tuple[DataFrame, DataFrame]</code> <p>Tuple containing: - DataFrame with ONLY rows that failed validation + 'dq_status' column - DataFrame with violations exploded (one row per violation per rule)</p> Notes <ul> <li>Only returns rows that violated at least one rule</li> <li>'dq_status' column summarizes all validation issues per row</li> </ul>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.validate_schema","title":"validate_schema","text":"<pre><code>validate_schema(df, expected) -&gt; tuple[bool, list[dict[str, Any]]]\n</code></pre> <p>Validates the schema of a given DataFrame against an expected schema.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <p>The DataFrame whose schema needs to be validated.</p> required <code>expected</code> <p>The expected schema, represented as a list of tuples where each tuple       contains the column name and its data type.</p> required <p>Returns:</p> Type Description <code>tuple[bool, list[dict[str, Any]]]</code> <p>Tuple[bool, List[Tuple[str, str]]]: A tuple containing: - A boolean indicating whether the schema matches the expected schema. - A list of tuples representing the errors, where each tuple contains   the column name and a description of the mismatch.</p>"},{"location":"sumeh/engines/pandas/#sumeh.engines.pandas_engine.validate_table_level","title":"validate_table_level","text":"<pre><code>validate_table_level(df: DataFrame, rules: List[RuleDef]) -&gt; pd.DataFrame\n</code></pre> <p>Validates table-level rules (aggregations and schema).</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame to validate.</p> required <code>rules</code> <code>List[RuleDef]</code> <p>List of table-level validation rules.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Summary DataFrame with validation results.</p>"},{"location":"sumeh/engines/polars/","title":"polars","text":""},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine","title":"sumeh.engines.polars_engine","text":""},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.all_date_checks","title":"all_date_checks","text":"<pre><code>all_date_checks(df: DataFrame, rule: RuleDef) -&gt; pl.DataFrame\n</code></pre> <p>Default date check - filters past dates.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to be filtered.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, check_type, and value.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: Filtered DataFrame with violations and dq_status column.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.are_complete","title":"are_complete","text":"<pre><code>are_complete(df: DataFrame, rule: RuleDef) -&gt; pl.DataFrame\n</code></pre> <p>Filters rows where any of the specified fields are null and adds a data quality status column.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to be checked.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing fields (list), check_type, and value.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: Filtered DataFrame with violations and dq_status column.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.are_unique","title":"are_unique","text":"<pre><code>are_unique(df: DataFrame, rule: RuleDef) -&gt; pl.DataFrame\n</code></pre> <p>Identifies duplicate rows based on a combination of specified fields and adds a data quality status column.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to check.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing fields (list), check_type, and value.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: DataFrame containing rows where the field combination is not unique with dq_status column.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.extract_schema","title":"extract_schema","text":"<pre><code>extract_schema(df: DataFrame) -&gt; List[Dict[str, Any]]\n</code></pre> <p>Extracts schema from Polars DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input Polars DataFrame.</p> required <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List[Dict[str, Any]]: List of dictionaries containing field information.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.has_cardinality","title":"has_cardinality","text":"<pre><code>has_cardinality(df: DataFrame, rule: RuleDef) -&gt; dict\n</code></pre> <p>Checks if the cardinality (distinct count) of the specified field meets expectations.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to be checked.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, value (expected cardinality), and optional threshold.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Validation result with status, expected, actual, and message.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.has_entropy","title":"has_entropy","text":"<pre><code>has_entropy(df: DataFrame, rule: RuleDef) -&gt; dict\n</code></pre> <p>Checks if the entropy of the specified field meets expectations.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to evaluate.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, value (expected entropy), and optional threshold.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Validation result with status, expected, actual, and message.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.has_infogain","title":"has_infogain","text":"<pre><code>has_infogain(df: DataFrame, rule: RuleDef) -&gt; dict\n</code></pre> <p>Checks if the information gain of the specified field meets expectations.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to evaluate.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, value (expected info gain), and optional threshold.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Validation result with status, expected, actual, and message.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.has_max","title":"has_max","text":"<pre><code>has_max(df: DataFrame, rule: RuleDef) -&gt; dict\n</code></pre> <p>Checks if the maximum value of the specified field meets expectations.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to be checked.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, value (expected max), and optional threshold.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Validation result with status, expected, actual, and message.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.has_mean","title":"has_mean","text":"<pre><code>has_mean(df: DataFrame, rule: RuleDef) -&gt; dict\n</code></pre> <p>Checks if the mean of the specified field meets expectations.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, value (expected mean), and optional threshold.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Validation result with status, expected, actual, and message.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.has_min","title":"has_min","text":"<pre><code>has_min(df: DataFrame, rule: RuleDef) -&gt; dict\n</code></pre> <p>Checks if the minimum value of the specified field meets expectations.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to be checked.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, value (expected min), and optional threshold.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Validation result with status, expected, actual, and message.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.has_pattern","title":"has_pattern","text":"<pre><code>has_pattern(df: DataFrame, rule: RuleDef) -&gt; pl.DataFrame\n</code></pre> <p>Filters rows where the specified field does not match the given regex pattern.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to be filtered.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, check_type, and value (regex pattern).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: Filtered DataFrame with violations and dq_status column.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.has_std","title":"has_std","text":"<pre><code>has_std(df: DataFrame, rule: RuleDef) -&gt; dict\n</code></pre> <p>Checks if the standard deviation of the specified field meets expectations.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, value (expected std), and optional threshold.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Validation result with status, expected, actual, and message.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.has_sum","title":"has_sum","text":"<pre><code>has_sum(df: DataFrame, rule: RuleDef) -&gt; dict\n</code></pre> <p>Checks if the sum of the specified field meets expectations.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, value (expected sum), and optional threshold.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Validation result with status, expected, actual, and message.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.is_between","title":"is_between","text":"<pre><code>is_between(df: DataFrame, rule: RuleDef) -&gt; pl.DataFrame\n</code></pre> <p>Filters rows where the specified field is not within the given range.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, check_type, and value (range format).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: Filtered DataFrame with violations and dq_status column.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.is_complete","title":"is_complete","text":"<pre><code>is_complete(df: DataFrame, rule: RuleDef) -&gt; pl.DataFrame\n</code></pre> <p>Filters rows where the specified field is null and adds a data quality status column.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to be checked.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, check_type, and value.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: Filtered DataFrame with violations and dq_status column.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.is_contained_in","title":"is_contained_in","text":"<pre><code>is_contained_in(df: DataFrame, rule: RuleDef) -&gt; pl.DataFrame\n</code></pre> <p>Filters rows where the specified field is not in the given list of values.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to filter.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, check_type, and value (list format).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: Filtered DataFrame with violations and dq_status column.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.is_date_after","title":"is_date_after","text":"<pre><code>is_date_after(df: DataFrame, rule: RuleDef) -&gt; pl.DataFrame\n</code></pre> <p>Filters rows where the specified field has a date lower than the date informed in the rule.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to be checked.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, check_type, and value (target date).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: Filtered DataFrame with violations and dq_status column.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.is_date_before","title":"is_date_before","text":"<pre><code>is_date_before(df: DataFrame, rule: RuleDef) -&gt; pl.DataFrame\n</code></pre> <p>Filters rows where the specified field has a date greater than the date informed in the rule.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to be checked.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, check_type, and value (target date).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: Filtered DataFrame with violations and dq_status column.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.is_date_between","title":"is_date_between","text":"<pre><code>is_date_between(df: DataFrame, rule: RuleDef) -&gt; pl.DataFrame\n</code></pre> <p>Filters rows where date field is not between two dates in format: \"[, ]\". <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to be checked.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, check_type, and value (date range).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: Filtered DataFrame with violations and dq_status column.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.is_equal","title":"is_equal","text":"<pre><code>is_equal(df: DataFrame, rule: RuleDef) -&gt; pl.DataFrame\n</code></pre> <p>Filters rows where the specified field is not equal to the given value.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to be filtered.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, check_type, and value.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: Filtered DataFrame with violations and dq_status column.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.is_equal_than","title":"is_equal_than","text":"<pre><code>is_equal_than(df: DataFrame, rule: RuleDef) -&gt; pl.DataFrame\n</code></pre> <p>Alias for is_equal.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to be filtered.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, check_type, and value.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: Filtered DataFrame with violations and dq_status column.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.is_future_date","title":"is_future_date","text":"<pre><code>is_future_date(df: DataFrame, rule: RuleDef) -&gt; pl.DataFrame\n</code></pre> <p>Filters rows where the specified field has a date greater than the current date.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to be checked.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, check_type, and value.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: Filtered DataFrame with violations and dq_status column.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.is_greater_or_equal_than","title":"is_greater_or_equal_than","text":"<pre><code>is_greater_or_equal_than(df: DataFrame, rule: RuleDef) -&gt; pl.DataFrame\n</code></pre> <p>Filters rows where the specified field is less than the given value.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to filter.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, check_type, and value.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: Filtered DataFrame with violations and dq_status column.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.is_greater_than","title":"is_greater_than","text":"<pre><code>is_greater_than(df: DataFrame, rule: RuleDef) -&gt; pl.DataFrame\n</code></pre> <p>Filters rows where the specified field is less than or equal to the given value.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The Polars DataFrame to filter.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, check_type, and value.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: Filtered DataFrame with violations and dq_status column.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.is_in","title":"is_in","text":"<pre><code>is_in(df: DataFrame, rule: RuleDef) -&gt; pl.DataFrame\n</code></pre> <p>Alias for is_contained_in.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to evaluate.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: Filtered DataFrame with violations.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.is_in_billions","title":"is_in_billions","text":"<pre><code>is_in_billions(df: DataFrame, rule: RuleDef) -&gt; pl.DataFrame\n</code></pre> <p>Filters rows where the field value is less than 1,000,000,000.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to filter.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, check_type, and value.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: Filtered DataFrame with violations and dq_status column.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.is_in_millions","title":"is_in_millions","text":"<pre><code>is_in_millions(df: DataFrame, rule: RuleDef) -&gt; pl.DataFrame\n</code></pre> <p>Filters rows where the field value is less than 1,000,000.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to filter and modify.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, check_type, and value.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: Filtered DataFrame with violations and dq_status column.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.is_legit","title":"is_legit","text":"<pre><code>is_legit(df: DataFrame, rule: RuleDef) -&gt; pl.DataFrame\n</code></pre> <p>Filters rows where the specified field is null or does not match a non-whitespace pattern.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to be validated.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, check_type, and value.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: Filtered DataFrame with violations and dq_status column.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.is_less_or_equal_than","title":"is_less_or_equal_than","text":"<pre><code>is_less_or_equal_than(df: DataFrame, rule: RuleDef) -&gt; pl.DataFrame\n</code></pre> <p>Filters rows where the specified field is greater than the given value.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to be filtered.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, check_type, and value.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: Filtered DataFrame with violations and dq_status column.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.is_less_than","title":"is_less_than","text":"<pre><code>is_less_than(df: DataFrame, rule: RuleDef) -&gt; pl.DataFrame\n</code></pre> <p>Filters rows where the specified field is greater than or equal to the given value.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to filter.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, check_type, and value.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: Filtered DataFrame with violations and dq_status column.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.is_negative","title":"is_negative","text":"<pre><code>is_negative(df: DataFrame, rule: RuleDef) -&gt; pl.DataFrame\n</code></pre> <p>Filters rows where the specified field is non-negative and adds a data quality status column.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to be filtered.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, check_type, and value.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: Filtered DataFrame with violations and dq_status column.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.is_on_friday","title":"is_on_friday","text":"<pre><code>is_on_friday(df: DataFrame, rule: RuleDef) -&gt; pl.DataFrame\n</code></pre> <p>Filters rows where date is not Friday.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.is_on_monday","title":"is_on_monday","text":"<pre><code>is_on_monday(df: DataFrame, rule: RuleDef) -&gt; pl.DataFrame\n</code></pre> <p>Filters rows where date is not Monday.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.is_on_saturday","title":"is_on_saturday","text":"<pre><code>is_on_saturday(df: DataFrame, rule: RuleDef) -&gt; pl.DataFrame\n</code></pre> <p>Filters rows where date is not Saturday.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.is_on_sunday","title":"is_on_sunday","text":"<pre><code>is_on_sunday(df: DataFrame, rule: RuleDef) -&gt; pl.DataFrame\n</code></pre> <p>Filters rows where date is not Sunday.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.is_on_thursday","title":"is_on_thursday","text":"<pre><code>is_on_thursday(df: DataFrame, rule: RuleDef) -&gt; pl.DataFrame\n</code></pre> <p>Filters rows where date is not Thursday.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.is_on_tuesday","title":"is_on_tuesday","text":"<pre><code>is_on_tuesday(df: DataFrame, rule: RuleDef) -&gt; pl.DataFrame\n</code></pre> <p>Filters rows where date is not Tuesday.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.is_on_wednesday","title":"is_on_wednesday","text":"<pre><code>is_on_wednesday(df: DataFrame, rule: RuleDef) -&gt; pl.DataFrame\n</code></pre> <p>Filters rows where date is not Wednesday.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.is_on_weekday","title":"is_on_weekday","text":"<pre><code>is_on_weekday(df: DataFrame, rule: RuleDef) -&gt; pl.DataFrame\n</code></pre> <p>Filters rows where the date field does not fall on a weekday (Mon-Fri).</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, check_type, and value.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: Filtered DataFrame with violations and dq_status column.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.is_on_weekend","title":"is_on_weekend","text":"<pre><code>is_on_weekend(df: DataFrame, rule: RuleDef) -&gt; pl.DataFrame\n</code></pre> <p>Filters rows where the date field does not fall on a weekend (Sat-Sun).</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, check_type, and value.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: Filtered DataFrame with violations and dq_status column.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.is_past_date","title":"is_past_date","text":"<pre><code>is_past_date(df: DataFrame, rule: RuleDef) -&gt; pl.DataFrame\n</code></pre> <p>Filters rows where the specified field has a date lower than the current date.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to be checked.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, check_type, and value.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: Filtered DataFrame with violations and dq_status column.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.is_positive","title":"is_positive","text":"<pre><code>is_positive(df: DataFrame, rule: RuleDef) -&gt; pl.DataFrame\n</code></pre> <p>Filters rows where the specified field is negative and adds a data quality status column.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to be filtered.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, check_type, and value.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: Filtered DataFrame with violations and dq_status column.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.is_t_minus_1","title":"is_t_minus_1","text":"<pre><code>is_t_minus_1(df: DataFrame, rule: RuleDef) -&gt; pl.DataFrame\n</code></pre> <p>Filters rows where the date field does not equal one day ago (T-1).</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to be filtered.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, check_type, and value.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: Filtered DataFrame with violations and dq_status column.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.is_t_minus_2","title":"is_t_minus_2","text":"<pre><code>is_t_minus_2(df: DataFrame, rule: RuleDef) -&gt; pl.DataFrame\n</code></pre> <p>Filters rows where the date field does not equal two days ago (T-2).</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to be filtered.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, check_type, and value.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: Filtered DataFrame with violations and dq_status column.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.is_t_minus_3","title":"is_t_minus_3","text":"<pre><code>is_t_minus_3(df: DataFrame, rule: RuleDef) -&gt; pl.DataFrame\n</code></pre> <p>Filters rows where the date field does not equal three days ago (T-3).</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to be filtered.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, check_type, and value.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: Filtered DataFrame with violations and dq_status column.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.is_today","title":"is_today","text":"<pre><code>is_today(df: DataFrame, rule: RuleDef) -&gt; pl.DataFrame\n</code></pre> <p>Filters rows where the date field does not equal today.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to filter.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, check_type, and value.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: Filtered DataFrame with violations and dq_status column.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.is_unique","title":"is_unique","text":"<pre><code>is_unique(df: DataFrame, rule: RuleDef) -&gt; pl.DataFrame\n</code></pre> <p>Identifies duplicate rows based on the specified field and adds a data quality status column.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to check for uniqueness.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, check_type, and value.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: DataFrame containing rows where the field is not unique with dq_status column.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.is_yesterday","title":"is_yesterday","text":"<pre><code>is_yesterday(df: DataFrame, rule: RuleDef) -&gt; pl.DataFrame\n</code></pre> <p>Alias for is_t_minus_1. Filters rows where date field does not equal yesterday.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, check_type, and value.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: Filtered DataFrame with violations and dq_status column.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.not_contained_in","title":"not_contained_in","text":"<pre><code>not_contained_in(df: DataFrame, rule: RuleDef) -&gt; pl.DataFrame\n</code></pre> <p>Filters rows where the specified field is in the given list of values.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to filter.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, check_type, and value (list format).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: Filtered DataFrame with violations and dq_status column.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.not_in","title":"not_in","text":"<pre><code>not_in(df: DataFrame, rule: RuleDef) -&gt; pl.DataFrame\n</code></pre> <p>Alias for not_contained_in.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to be filtered.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: Filtered DataFrame with violations.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.satisfies","title":"satisfies","text":"<pre><code>satisfies(df: DataFrame, rule: RuleDef) -&gt; pl.DataFrame\n</code></pre> <p>Filters rows where the specified expression is not satisfied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to be filtered.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition with value containing SQL expression.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: Filtered DataFrame with violations and dq_status column.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.summarize","title":"summarize","text":"<pre><code>summarize(rules: List[RuleDef], total_rows: int, df_with_errors: Optional[DataFrame] = None, table_error: Optional[DataFrame] = None) -&gt; pl.DataFrame\n</code></pre> <p>Summarizes validation results from both row-level and table-level checks.</p> <p>Parameters:</p> Name Type Description Default <code>rules</code> <code>List[RuleDef]</code> <p>List of all validation rules.</p> required <code>total_rows</code> <code>int</code> <p>Total number of rows in the input DataFrame.</p> required <code>df_with_errors</code> <code>Optional[DataFrame]</code> <p>DataFrame with row-level violations.</p> <code>None</code> <code>table_error</code> <code>Optional[DataFrame]</code> <p>DataFrame with table-level results.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: Summary DataFrame with aggregated validation metrics.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.validate","title":"validate","text":"<pre><code>validate(df: DataFrame, rules: List[RuleDef]) -&gt; Tuple[pl.DataFrame, pl.DataFrame, pl.DataFrame]\n</code></pre> <p>Main validation function that orchestrates row-level and table-level validations.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input Polars DataFrame to validate.</p> required <code>rules</code> <code>List[RuleDef]</code> <p>List of all validation rules.</p> required <p>Returns:</p> Type Description <code>Tuple[DataFrame, DataFrame, DataFrame]</code> <p>Tuple[pl.DataFrame, pl.DataFrame, pl.DataFrame]: - DataFrame with row-level violations and dq_status - Raw row-level violations DataFrame - Table-level summary DataFrame</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.validate_date_format","title":"validate_date_format","text":"<pre><code>validate_date_format(df: DataFrame, rule: RuleDef) -&gt; pl.DataFrame\n</code></pre> <p>Filters rows where the specified field has wrong date format based on the format from the rule.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Polars DataFrame to be checked.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, check_type, and value (date format).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: Filtered DataFrame with violations and dq_status column.</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.validate_row_level","title":"validate_row_level","text":"<pre><code>validate_row_level(df: DataFrame, rules: List[RuleDef]) -&gt; Tuple[pl.DataFrame, pl.DataFrame]\n</code></pre> <p>Validates DataFrame at row level using specified rules.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input Polars DataFrame to validate.</p> required <code>rules</code> <code>List[RuleDef]</code> <p>List of row-level validation rules.</p> required <p>Returns:</p> Type Description <code>Tuple[DataFrame, DataFrame]</code> <p>Tuple[pl.DataFrame, pl.DataFrame]: - DataFrame with violations and dq_status column - Raw violations DataFrame</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.validate_schema","title":"validate_schema","text":"<pre><code>validate_schema(df: DataFrame, expected) -&gt; Tuple[bool, List[Dict[str, Any]]]\n</code></pre> <p>Validates the schema of a Polars DataFrame against an expected schema.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The Polars DataFrame whose schema is to be validated.</p> required <code>expected</code> <code>list</code> <p>The expected schema.</p> required <p>Returns:</p> Type Description <code>Tuple[bool, List[Dict[str, Any]]]</code> <p>Tuple[bool, List[Dict[str, Any]]]: - Boolean indicating whether the schema matches - List of schema errors/mismatches</p>"},{"location":"sumeh/engines/polars/#sumeh.engines.polars_engine.validate_table_level","title":"validate_table_level","text":"<pre><code>validate_table_level(df: DataFrame, rules: List[RuleDef]) -&gt; pl.DataFrame\n</code></pre> <p>Validates DataFrame at table level using specified rules.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input Polars DataFrame to validate.</p> required <code>rules</code> <code>List[RuleDef]</code> <p>List of table-level validation rules.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: Summary DataFrame with validation results.</p>"},{"location":"sumeh/engines/pyspark/","title":"pyspark","text":""},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine","title":"sumeh.engines.pyspark_engine","text":""},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.all_date_checks","title":"all_date_checks","text":"<pre><code>all_date_checks(df: DataFrame, rule: RuleDef) -&gt; DataFrame\n</code></pre> <p>Default date check - filters past dates.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be filtered.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, check_type, and value.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Filtered DataFrame with violations and dq_status column.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.are_complete","title":"are_complete","text":"<pre><code>are_complete(df: DataFrame, rule: RuleDef) -&gt; DataFrame\n</code></pre> <p>Filters rows where any of the specified fields are null and adds a data quality status column.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be checked.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing fields (list), check_type, and value.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Filtered DataFrame with violations and dq_status column.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.are_unique","title":"are_unique","text":"<pre><code>are_unique(df: DataFrame, rule: RuleDef) -&gt; DataFrame\n</code></pre> <p>Identifies duplicate rows based on a combination of specified fields and adds a data quality status column.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to check.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing fields (list), check_type, and value.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame containing rows where the field combination is not unique with dq_status column.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.extract_schema","title":"extract_schema","text":"<pre><code>extract_schema(df: DataFrame) -&gt; List[Dict[str, Any]]\n</code></pre> <p>Extracts schema from PySpark DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input PySpark DataFrame.</p> required <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List[Dict[str, Any]]: List of dictionaries containing field information.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.has_cardinality","title":"has_cardinality","text":"<pre><code>has_cardinality(df: DataFrame, rule: RuleDef) -&gt; dict\n</code></pre> <p>Checks if the cardinality (distinct count) of the specified field meets expectations.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be checked.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, value (expected cardinality), and optional threshold.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Validation result with status, expected, actual, and message.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.has_entropy","title":"has_entropy","text":"<pre><code>has_entropy(df: DataFrame, rule: RuleDef) -&gt; dict\n</code></pre> <p>Checks if the entropy of the specified field meets expectations.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to evaluate.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, value (expected entropy), and optional threshold.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Validation result with status, expected, actual, and message.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.has_infogain","title":"has_infogain","text":"<pre><code>has_infogain(df: DataFrame, rule: RuleDef) -&gt; dict\n</code></pre> <p>Checks if the information gain of the specified field meets expectations.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to evaluate.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, value (expected info gain), and optional threshold.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Validation result with status, expected, actual, and message.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.has_max","title":"has_max","text":"<pre><code>has_max(df: DataFrame, rule: RuleDef) -&gt; dict\n</code></pre> <p>Checks if the maximum value of the specified field meets expectations.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be checked.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, value (expected max), and optional threshold.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Validation result with status, expected, actual, and message.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.has_mean","title":"has_mean","text":"<pre><code>has_mean(df: DataFrame, rule: RuleDef) -&gt; dict\n</code></pre> <p>Checks if the mean of the specified field meets expectations.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, value (expected mean), and optional threshold.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Validation result with status, expected, actual, and message.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.has_min","title":"has_min","text":"<pre><code>has_min(df: DataFrame, rule: RuleDef) -&gt; dict\n</code></pre> <p>Checks if the minimum value of the specified field meets expectations.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be checked.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, value (expected min), and optional threshold.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Validation result with status, expected, actual, and message.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.has_pattern","title":"has_pattern","text":"<pre><code>has_pattern(df: DataFrame, rule: RuleDef) -&gt; DataFrame\n</code></pre> <p>Filters rows where the specified field does not match the given regex pattern.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be filtered.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, check_type, and value (regex pattern).</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Filtered DataFrame with violations and dq_status column.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.has_std","title":"has_std","text":"<pre><code>has_std(df: DataFrame, rule: RuleDef) -&gt; dict\n</code></pre> <p>Checks if the standard deviation of the specified field meets expectations.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, value (expected std), and optional threshold.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Validation result with status, expected, actual, and message.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.has_sum","title":"has_sum","text":"<pre><code>has_sum(df: DataFrame, rule: RuleDef) -&gt; dict\n</code></pre> <p>Checks if the sum of the specified field meets expectations.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, value (expected sum), and optional threshold.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Validation result with status, expected, actual, and message.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.is_between","title":"is_between","text":"<pre><code>is_between(df: DataFrame, rule: RuleDef) -&gt; DataFrame\n</code></pre> <p>Filters rows where the specified field is not within the given range.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, check_type, and value (range format).</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Filtered DataFrame with violations and dq_status column.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.is_complete","title":"is_complete","text":"<pre><code>is_complete(df: DataFrame, rule: RuleDef) -&gt; DataFrame\n</code></pre> <p>Filters rows where the specified field is null and adds a data quality status column.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be checked.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, check_type, and value.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Filtered DataFrame with violations and dq_status column.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.is_contained_in","title":"is_contained_in","text":"<pre><code>is_contained_in(df: DataFrame, rule: RuleDef) -&gt; DataFrame\n</code></pre> <p>Filters rows where the specified field is not in the given list of values.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to filter.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, check_type, and value (list format).</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Filtered DataFrame with violations and dq_status column.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.is_date_after","title":"is_date_after","text":"<pre><code>is_date_after(df: DataFrame, rule: RuleDef) -&gt; DataFrame\n</code></pre> <p>Filters rows where the specified field has a date lower than the date informed in the rule.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be checked.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, check_type, and value (target date).</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Filtered DataFrame with violations and dq_status column.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.is_date_before","title":"is_date_before","text":"<pre><code>is_date_before(df: DataFrame, rule: RuleDef) -&gt; DataFrame\n</code></pre> <p>Filters rows where the specified field has a date greater than the date informed in the rule.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be checked.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, check_type, and value (target date).</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Filtered DataFrame with violations and dq_status column.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.is_date_between","title":"is_date_between","text":"<pre><code>is_date_between(df: DataFrame, rule: RuleDef) -&gt; DataFrame\n</code></pre> <p>Filters rows where date field is not between two dates in format: \"[, ]\". <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be checked.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, check_type, and value (date range).</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Filtered DataFrame with violations and dq_status column.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.is_equal","title":"is_equal","text":"<pre><code>is_equal(df: DataFrame, rule: RuleDef) -&gt; DataFrame\n</code></pre> <p>Filters rows where the specified field is not equal to the given value.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be filtered.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, check_type, and value.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Filtered DataFrame with violations and dq_status column.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.is_equal_than","title":"is_equal_than","text":"<pre><code>is_equal_than(df: DataFrame, rule: RuleDef) -&gt; DataFrame\n</code></pre> <p>Alias for is_equal.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be filtered.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, check_type, and value.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Filtered DataFrame with violations and dq_status column.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.is_future_date","title":"is_future_date","text":"<pre><code>is_future_date(df: DataFrame, rule: RuleDef) -&gt; DataFrame\n</code></pre> <p>Filters rows where the specified field has a date greater than the current date.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be checked.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, check_type, and value.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Filtered DataFrame with violations and dq_status column.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.is_greater_or_equal_than","title":"is_greater_or_equal_than","text":"<pre><code>is_greater_or_equal_than(df: DataFrame, rule: RuleDef) -&gt; DataFrame\n</code></pre> <p>Filters rows where the specified field is less than the given value.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to filter.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, check_type, and value.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Filtered DataFrame with violations and dq_status column.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.is_greater_than","title":"is_greater_than","text":"<pre><code>is_greater_than(df: DataFrame, rule: RuleDef) -&gt; DataFrame\n</code></pre> <p>Filters rows where the specified field is less than or equal to the given value.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to filter.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, check_type, and value.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Filtered DataFrame with violations and dq_status column.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.is_in","title":"is_in","text":"<pre><code>is_in(df: DataFrame, rule: RuleDef) -&gt; DataFrame\n</code></pre> <p>Alias for is_contained_in.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to evaluate.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Filtered DataFrame with violations.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.is_in_billions","title":"is_in_billions","text":"<pre><code>is_in_billions(df: DataFrame, rule: RuleDef) -&gt; DataFrame\n</code></pre> <p>Filters rows where the field value is less than 1,000,000,000.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to filter.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, check_type, and value.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Filtered DataFrame with violations and dq_status column.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.is_in_millions","title":"is_in_millions","text":"<pre><code>is_in_millions(df: DataFrame, rule: RuleDef) -&gt; DataFrame\n</code></pre> <p>Filters rows where the field value is less than 1,000,000.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to filter and modify.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, check_type, and value.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Filtered DataFrame with violations and dq_status column.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.is_legit","title":"is_legit","text":"<pre><code>is_legit(df: DataFrame, rule: RuleDef) -&gt; DataFrame\n</code></pre> <p>Filters rows where the specified field is null or does not match a non-whitespace pattern.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be validated.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, check_type, and value.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Filtered DataFrame with violations and dq_status column.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.is_less_or_equal_than","title":"is_less_or_equal_than","text":"<pre><code>is_less_or_equal_than(df: DataFrame, rule: RuleDef) -&gt; DataFrame\n</code></pre> <p>Filters rows where the specified field is greater than the given value.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to filter.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, check_type, and value.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Filtered DataFrame with violations and dq_status column.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.is_less_than","title":"is_less_than","text":"<pre><code>is_less_than(df: DataFrame, rule: RuleDef) -&gt; DataFrame\n</code></pre> <p>Filters rows where the specified field is greater than or equal to the given value.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to filter.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, check_type, and value.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Filtered DataFrame with violations and dq_status column.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.is_negative","title":"is_negative","text":"<pre><code>is_negative(df: DataFrame, rule: RuleDef) -&gt; DataFrame\n</code></pre> <p>Filters rows where the specified field is non-negative and adds a data quality status column.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be filtered.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, check_type, and value.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Filtered DataFrame with violations and dq_status column.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.is_on_friday","title":"is_on_friday","text":"<pre><code>is_on_friday(df: DataFrame, rule: RuleDef) -&gt; DataFrame\n</code></pre> <p>Filters rows where date is not Friday.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.is_on_monday","title":"is_on_monday","text":"<pre><code>is_on_monday(df: DataFrame, rule: RuleDef) -&gt; DataFrame\n</code></pre> <p>Filters rows where date is not Monday.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.is_on_saturday","title":"is_on_saturday","text":"<pre><code>is_on_saturday(df: DataFrame, rule: RuleDef) -&gt; DataFrame\n</code></pre> <p>Filters rows where date is not Saturday.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.is_on_sunday","title":"is_on_sunday","text":"<pre><code>is_on_sunday(df: DataFrame, rule: RuleDef) -&gt; DataFrame\n</code></pre> <p>Filters rows where date is not Sunday.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.is_on_thursday","title":"is_on_thursday","text":"<pre><code>is_on_thursday(df: DataFrame, rule: RuleDef) -&gt; DataFrame\n</code></pre> <p>Filters rows where date is not Thursday.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.is_on_tuesday","title":"is_on_tuesday","text":"<pre><code>is_on_tuesday(df: DataFrame, rule: RuleDef) -&gt; DataFrame\n</code></pre> <p>Filters rows where date is not Tuesday.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.is_on_wednesday","title":"is_on_wednesday","text":"<pre><code>is_on_wednesday(df: DataFrame, rule: RuleDef) -&gt; DataFrame\n</code></pre> <p>Filters rows where date is not Wednesday.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.is_on_weekday","title":"is_on_weekday","text":"<pre><code>is_on_weekday(df: DataFrame, rule: RuleDef) -&gt; DataFrame\n</code></pre> <p>Filters rows where the date field does not fall on a weekday (Mon-Fri).</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, check_type, and value.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Filtered DataFrame with violations and dq_status column.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.is_on_weekend","title":"is_on_weekend","text":"<pre><code>is_on_weekend(df: DataFrame, rule: RuleDef) -&gt; DataFrame\n</code></pre> <p>Filters rows where the date field does not fall on a weekend (Sat-Sun).</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, check_type, and value.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Filtered DataFrame with violations and dq_status column.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.is_past_date","title":"is_past_date","text":"<pre><code>is_past_date(df: DataFrame, rule: RuleDef) -&gt; DataFrame\n</code></pre> <p>Filters rows where the specified field has a date lower than the current date.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be checked.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, check_type, and value.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Filtered DataFrame with violations and dq_status column.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.is_positive","title":"is_positive","text":"<pre><code>is_positive(df: DataFrame, rule: RuleDef) -&gt; DataFrame\n</code></pre> <p>Filters rows where the specified field is negative and adds a data quality status column.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be filtered.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, check_type, and value.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Filtered DataFrame with violations and dq_status column.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.is_t_minus_1","title":"is_t_minus_1","text":"<pre><code>is_t_minus_1(df: DataFrame, rule: RuleDef) -&gt; DataFrame\n</code></pre> <p>Alias for is_t_minus_1. Filters rows where date field does not equal yesterday.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, check_type, and value.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Filtered DataFrame with violations and dq_status column.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.is_t_minus_2","title":"is_t_minus_2","text":"<pre><code>is_t_minus_2(df: DataFrame, rule: RuleDef) -&gt; DataFrame\n</code></pre> <p>Filters rows where the date field does not equal two days ago (T-2).</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to be filtered.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, check_type, and value.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Filtered DataFrame with violations and dq_status column.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.is_t_minus_3","title":"is_t_minus_3","text":"<pre><code>is_t_minus_3(df: DataFrame, rule: RuleDef) -&gt; DataFrame\n</code></pre> <p>Filters rows where the date field does not equal three days ago (T-3).</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to be filtered.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, check_type, and value.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Filtered DataFrame with violations and dq_status column.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.is_today","title":"is_today","text":"<pre><code>is_today(df: DataFrame, rule: RuleDef) -&gt; DataFrame\n</code></pre> <p>Filters rows where the date field does not equal today.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to filter.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, check_type, and value.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Filtered DataFrame with violations and dq_status column.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.is_unique","title":"is_unique","text":"<pre><code>is_unique(df: DataFrame, rule: RuleDef) -&gt; DataFrame\n</code></pre> <p>Identifies duplicate rows based on the specified field and adds a data quality status column.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to check for uniqueness.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, check_type, and value.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame containing rows where the field is not unique with dq_status column.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.is_yesterday","title":"is_yesterday","text":"<pre><code>is_yesterday(df: DataFrame, rule: RuleDef) -&gt; DataFrame\n</code></pre> <p>Alias for is_t_minus_1. Filters rows where date field does not equal yesterday.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, check_type, and value.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Filtered DataFrame with violations and dq_status column.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.not_contained_in","title":"not_contained_in","text":"<pre><code>not_contained_in(df: DataFrame, rule: RuleDef) -&gt; DataFrame\n</code></pre> <p>Filters rows where the specified field is in the given list of values.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to filter.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, check_type, and value (list format).</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Filtered DataFrame with violations and dq_status column.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.not_in","title":"not_in","text":"<pre><code>not_in(df: DataFrame, rule: RuleDef) -&gt; DataFrame\n</code></pre> <p>Alias for not_contained_in.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to be filtered.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Filtered DataFrame with violations.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.satisfies","title":"satisfies","text":"<pre><code>satisfies(df: DataFrame, rule: RuleDef) -&gt; DataFrame\n</code></pre> <p>Filters rows where the specified expression is not satisfied.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be filtered.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition with value containing PySpark SQL expression.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Filtered DataFrame with violations and dq_status column.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.summarize","title":"summarize","text":"<pre><code>summarize(spark: SparkSession, rules: List[RuleDef], total_rows: int, df_with_errors: Optional[DataFrame] = None, table_error: Optional[DataFrame] = None) -&gt; DataFrame\n</code></pre> <p>Summarizes validation results from both row-level and table-level checks.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Active SparkSession instance.</p> required <code>rules</code> <code>List[RuleDef]</code> <p>List of all validation rules.</p> required <code>total_rows</code> <code>int</code> <p>Total number of rows in the input DataFrame.</p> required <code>df_with_errors</code> <code>Optional[DataFrame]</code> <p>DataFrame with row-level violations.</p> <code>None</code> <code>table_error</code> <code>Optional[DataFrame]</code> <p>DataFrame with table-level results.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Summary DataFrame with aggregated validation metrics.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.validate","title":"validate","text":"<pre><code>validate(spark: SparkSession, df: DataFrame, rules: List[RuleDef]) -&gt; Tuple[DataFrame, DataFrame, DataFrame]\n</code></pre> <p>Main validation function that orchestrates row-level and table-level validations.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Active SparkSession instance.</p> required <code>df</code> <code>DataFrame</code> <p>Input PySpark DataFrame to validate.</p> required <code>rules</code> <code>List[RuleDef]</code> <p>List of all validation rules.</p> required <p>Returns:</p> Type Description <code>Tuple[DataFrame, DataFrame, DataFrame]</code> <p>Tuple[DataFrame, DataFrame, DataFrame]: - DataFrame with row-level violations and dq_status - Raw row-level violations DataFrame - Table-level summary DataFrame</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.validate_date_format","title":"validate_date_format","text":"<pre><code>validate_date_format(df: DataFrame, rule: RuleDef) -&gt; DataFrame\n</code></pre> <p>Filters rows where the specified field has wrong date format based on the format from the rule.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame to be checked.</p> required <code>rule</code> <code>RuleDef</code> <p>Rule definition containing field, check_type, and value (date format).</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Filtered DataFrame with violations and dq_status column.</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.validate_row_level","title":"validate_row_level","text":"<pre><code>validate_row_level(spark: SparkSession, df: DataFrame, rules: List[RuleDef]) -&gt; Tuple[DataFrame, DataFrame]\n</code></pre> <p>Validates DataFrame at row level using specified rules.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Active SparkSession instance.</p> required <code>df</code> <code>DataFrame</code> <p>Input PySpark DataFrame to validate.</p> required <code>rules</code> <code>List[RuleDef]</code> <p>List of row-level validation rules.</p> required <p>Returns:</p> Type Description <code>Tuple[DataFrame, DataFrame]</code> <p>Tuple[DataFrame, DataFrame]: - DataFrame with violations and dq_status column - Raw violations DataFrame</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.validate_schema","title":"validate_schema","text":"<pre><code>validate_schema(df: DataFrame, expected) -&gt; Tuple[bool, List[Dict[str, Any]]]\n</code></pre> <p>Validates the schema of a PySpark DataFrame against an expected schema.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The PySpark DataFrame whose schema is to be validated.</p> required <code>expected</code> <code>list</code> <p>The expected schema.</p> required <p>Returns:</p> Type Description <code>Tuple[bool, List[Dict[str, Any]]]</code> <p>Tuple[bool, List[Dict[str, Any]]]: - Boolean indicating whether the schema matches - List of schema errors/mismatches</p>"},{"location":"sumeh/engines/pyspark/#sumeh.engines.pyspark_engine.validate_table_level","title":"validate_table_level","text":"<pre><code>validate_table_level(spark: SparkSession, df: DataFrame, rules: List[RuleDef]) -&gt; DataFrame\n</code></pre> <p>Validates DataFrame at table level using specified rules.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Active SparkSession instance.</p> required <code>df</code> <code>DataFrame</code> <p>Input PySpark DataFrame to validate.</p> required <code>rules</code> <code>List[RuleDef]</code> <p>List of table-level validation rules.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Summary DataFrame with validation results.</p>"}]}